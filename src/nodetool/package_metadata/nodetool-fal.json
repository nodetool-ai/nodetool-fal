{
  "name": "nodetool-fal",
  "description": "Nodetool FAL nodes",
  "version": "0.6.3-rc.17",
  "authors": [
    "Matthias Georgi <matti.georgi@gmail.com>"
  ],
  "repo_id": "",
  "nodes": [
    {
      "title": "Bytedance Seed 3D Image To 3D",
      "description": "Bytedance\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.BytedanceSeed3DImageTo3D",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image for the 3D asset generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image_url"
      ]
    },
    {
      "title": "Hunyuan 3DV 3 Image To 3D",
      "description": "Hunyuan3d V3\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Hunyuan3DV3ImageTo3D",
      "properties": [
        {
          "name": "input_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image Url",
          "description": "URL of image to use while generating the 3D model."
        },
        {
          "name": "polygon_type",
          "type": {
            "type": "enum",
            "values": [
              "triangle",
              "quadrilateral"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hunyuan3DV3ImageTo3D.PolygonType"
          },
          "default": "triangle",
          "title": "Polygon Type",
          "description": "Polygon type. Only takes effect when GenerateType is LowPoly."
        },
        {
          "name": "face_count",
          "type": {
            "type": "int"
          },
          "default": 500000,
          "title": "Face Count",
          "description": "Target face count. Range: 40000-1500000"
        },
        {
          "name": "right_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Right Image Url",
          "description": "Optional right view image URL for better 3D reconstruction."
        },
        {
          "name": "back_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Back Image Url",
          "description": "Optional back view image URL for better 3D reconstruction."
        },
        {
          "name": "enable_pbr",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Pbr",
          "description": "Whether to enable PBR material generation. Does not take effect when generate_type is Geometry."
        },
        {
          "name": "generate_type",
          "type": {
            "type": "enum",
            "values": [
              "Normal",
              "LowPoly",
              "Geometry"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hunyuan3DV3ImageTo3D.GenerateType"
          },
          "default": "Normal",
          "title": "Generate Type",
          "description": "Generation type. Normal: textured model. LowPoly: polygon reduction. Geometry: white model without texture."
        },
        {
          "name": "left_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Left Image Url",
          "description": "Optional left view image URL for better 3D reconstruction."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "input_image_url",
        "polygon_type",
        "face_count",
        "right_image_url",
        "back_image_url"
      ]
    },
    {
      "title": "Hunyuan 3DV 3 Sketch To 3D",
      "description": "Hunyuan3d V3\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Hunyuan3DV3SketchTo3D",
      "properties": [
        {
          "name": "input_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image Url",
          "description": "URL of sketch or line art image to transform into a 3D model. Image resolution must be between 128x128 and 5000x5000 pixels."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the 3D content attributes such as color, category, and material."
        },
        {
          "name": "face_count",
          "type": {
            "type": "int"
          },
          "default": 500000,
          "title": "Face Count",
          "description": "Target face count. Range: 40000-1500000"
        },
        {
          "name": "enable_pbr",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Pbr",
          "description": "Whether to enable PBR material generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "input_image_url",
        "prompt",
        "face_count",
        "enable_pbr"
      ]
    },
    {
      "title": "Hunyuan World Image To World",
      "description": "Hunyuan World\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Hunyuan_WorldImageToWorld",
      "properties": [
        {
          "name": "classes",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Classes",
          "description": "Classes to use for the world generation."
        },
        {
          "name": "export_drc",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Export Drc",
          "description": "Whether to export DRC (Dynamic Resource Configuration)."
        },
        {
          "name": "labels_fg1",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Labels Fg1",
          "description": "Labels for the first foreground object."
        },
        {
          "name": "labels_fg2",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Labels Fg2",
          "description": "Labels for the second foreground object."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to convert to a world."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "classes",
        "export_drc",
        "labels_fg1",
        "labels_fg2",
        "image_url"
      ]
    },
    {
      "title": "Hyper 3D Rodin V2",
      "description": "Hyper3d\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Hyper3DRodinV2",
      "properties": [
        {
          "name": "quality_mesh_option",
          "type": {
            "type": "enum",
            "values": [
              "4K Quad",
              "8K Quad",
              "18K Quad",
              "50K Quad",
              "2K Triangle",
              "20K Triangle",
              "150K Triangle",
              "500K Triangle"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.QualityMeshOption"
          },
          "default": "500K Triangle",
          "title": "Quality Mesh Option",
          "description": "Combined quality and mesh type selection. Quad = smooth surfaces, Triangle = detailed geometry. These corresponds to `mesh_mode` (if the option contains 'Triangle', mesh_mode is 'Raw', otherwise 'Quad') and `quality_override` (the numeric part of the option) parameters in Hyper3D API."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "A textual prompt to guide model generation. Optional for Image-to-3D mode - if empty, AI will generate a prompt based on your images."
        },
        {
          "name": "preview_render",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preview Render",
          "description": "Generate a preview render image of the 3D model along with the model files."
        },
        {
          "name": "bbox_condition",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "int"
              }
            ]
          },
          "default": [],
          "title": "Bbox Condition",
          "description": "An array that specifies the bounding box dimensions [width, height, length]."
        },
        {
          "name": "TAPose",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Tapose",
          "description": "Generate characters in T-pose or A-pose format, making them easier to rig and animate in 3D software."
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "URL of images to use while generating the 3D model. Required for Image-to-3D mode. Up to 5 images allowed."
        },
        {
          "name": "use_original_alpha",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Original Alpha",
          "description": "When enabled, preserves the transparency channel from input images during 3D generation."
        },
        {
          "name": "geometry_file_format",
          "type": {
            "type": "enum",
            "values": [
              "glb",
              "usdz",
              "fbx",
              "obj",
              "stl"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.GeometryFileFormat"
          },
          "default": "glb",
          "title": "Geometry File Format",
          "description": "Format of the geometry file. Possible values: glb, usdz, fbx, obj, stl. Default is glb."
        },
        {
          "name": "addons",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "HighPack"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.Addons"
          },
          "default": null,
          "title": "Addons",
          "description": "The HighPack option will provide 4K resolution textures instead of the default 1K, as well as models with high-poly. It will cost **triple the billable units**."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed value for randomization, ranging from 0 to 65535. Optional."
        },
        {
          "name": "material",
          "type": {
            "type": "enum",
            "values": [
              "PBR",
              "Shaded",
              "All"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.Material"
          },
          "default": "All",
          "title": "Material",
          "description": "Material type. PBR: Physically-based materials with realistic lighting. Shaded: Simple materials with baked lighting. All: Both types included."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "quality_mesh_option",
        "prompt",
        "preview_render",
        "bbox_condition",
        "TAPose"
      ]
    },
    {
      "title": "Meshy V5 Multi Image To 3D",
      "description": "Meshy 5 Multi\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.MeshyV5MultiImageTo3D",
      "properties": [
        {
          "name": "enable_pbr",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Pbr",
          "description": "Generate PBR Maps (metallic, roughness, normal) in addition to base color. Requires should_texture to be true."
        },
        {
          "name": "should_texture",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Should Texture",
          "description": "Whether to generate textures. False provides mesh without textures for 5 credits, True adds texture generation for additional 10 credits."
        },
        {
          "name": "target_polycount",
          "type": {
            "type": "int"
          },
          "default": 30000,
          "title": "Target Polycount",
          "description": "Target number of polygons in the generated model"
        },
        {
          "name": "is_a_t_pose",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Is A T Pose",
          "description": "Whether to generate the model in an A/T pose"
        },
        {
          "name": "texture_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Texture Image Url",
          "description": "2D image to guide the texturing process. Requires should_texture to be true."
        },
        {
          "name": "topology",
          "type": {
            "type": "enum",
            "values": [
              "quad",
              "triangle"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.MeshyV5MultiImageTo3D.Topology"
          },
          "default": "triangle",
          "title": "Topology",
          "description": "Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "symmetry_mode",
          "type": {
            "type": "enum",
            "values": [
              "off",
              "auto",
              "on"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.MeshyV5MultiImageTo3D.SymmetryMode"
          },
          "default": "auto",
          "title": "Symmetry Mode",
          "description": "Controls symmetry behavior during model generation."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "1 to 4 images for 3D model creation. All images should depict the same object from different angles. Supports .jpg, .jpeg, .png formats, and AVIF/HEIF which will be automatically converted. If more than 4 images are provided, only the first 4 will be used."
        },
        {
          "name": "texture_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Texture Prompt",
          "description": "Text prompt to guide the texturing process. Requires should_texture to be true."
        },
        {
          "name": "should_remesh",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Should Remesh",
          "description": "Whether to enable the remesh phase. When false, returns triangular mesh ignoring topology and target_polycount."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "enable_pbr",
        "should_texture",
        "target_polycount",
        "is_a_t_pose",
        "texture_image_url"
      ]
    },
    {
      "title": "Meshy V6 Preview Image To 3D",
      "description": "Meshy 6 Preview\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.MeshyV6PreviewImageTo3D",
      "properties": [
        {
          "name": "enable_pbr",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Pbr",
          "description": "Generate PBR Maps (metallic, roughness, normal) in addition to base color"
        },
        {
          "name": "is_a_t_pose",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Is A T Pose",
          "description": "Whether to generate the model in an A/T pose"
        },
        {
          "name": "target_polycount",
          "type": {
            "type": "int"
          },
          "default": 30000,
          "title": "Target Polycount",
          "description": "Target number of polygons in the generated model"
        },
        {
          "name": "should_texture",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Should Texture",
          "description": "Whether to generate textures"
        },
        {
          "name": "texture_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Texture Image Url",
          "description": "2D image to guide the texturing process"
        },
        {
          "name": "topology",
          "type": {
            "type": "enum",
            "values": [
              "quad",
              "triangle"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.MeshyV6PreviewImageTo3D.Topology"
          },
          "default": "triangle",
          "title": "Topology",
          "description": "Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL or base64 data URI for 3D model creation. Supports .jpg, .jpeg, and .png formats. Also supports AVIF and HEIF formats which will be automatically converted."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "symmetry_mode",
          "type": {
            "type": "enum",
            "values": [
              "off",
              "auto",
              "on"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.MeshyV6PreviewImageTo3D.SymmetryMode"
          },
          "default": "auto",
          "title": "Symmetry Mode",
          "description": "Controls symmetry behavior during model generation. Off disables symmetry, Auto determines it automatically, On enforces symmetry."
        },
        {
          "name": "texture_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Texture Prompt",
          "description": "Text prompt to guide the texturing process"
        },
        {
          "name": "should_remesh",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Should Remesh",
          "description": "Whether to enable the remesh phase"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "enable_pbr",
        "is_a_t_pose",
        "target_polycount",
        "should_texture",
        "texture_image_url"
      ]
    },
    {
      "title": "Omnipart",
      "description": "Omnipart\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Omnipart",
      "properties": [
        {
          "name": "input_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image Url",
          "description": "URL of image to use while generating the 3D model."
        },
        {
          "name": "parts",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Parts",
          "description": "Specify which segments to merge (e.g., '0,1;3,4' merges segments 0&1 together and 3&4 together)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 765464,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "minimum_segment_size",
          "type": {
            "type": "int"
          },
          "default": 2000,
          "title": "Minimum Segment Size",
          "description": "Minimum segment size (pixels) for the model."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for the model."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "input_image_url",
        "parts",
        "seed",
        "minimum_segment_size",
        "guidance_scale"
      ]
    },
    {
      "title": "Pshuman",
      "description": "Pshuman\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Pshuman",
      "properties": [
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "Guidance scale for the diffusion process. Controls how much the output adheres to the generated views."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducibility. If None, a random seed will be used."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "A direct URL to the input image of a person."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "guidance_scale",
        "seed",
        "image_url"
      ]
    },
    {
      "title": "Sam 33D Body",
      "description": "Sam 3\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Sam33DBody",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image containing humans"
        },
        {
          "name": "include_3d_keypoints",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Include 3D Keypoints",
          "description": "Include 3D keypoint markers (spheres) in the GLB mesh for visualization"
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "Optional URL of a binary mask image (white=person, black=background). When provided, skips auto human detection and uses this mask instead. Bbox is auto-computed from the mask."
        },
        {
          "name": "export_meshes",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Export Meshes",
          "description": "Export individual mesh files (.ply) per person"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image_url",
        "include_3d_keypoints",
        "mask_url",
        "export_meshes"
      ]
    },
    {
      "title": "Sam 33D Objects",
      "description": "Sam 3\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Sam33DObjects",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "car",
          "title": "Prompt",
          "description": "Text prompt for auto-segmentation when no masks provided (e.g., 'chair', 'lamp')"
        },
        {
          "name": "export_textured_glb",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Export Textured Glb",
          "description": "If True, exports GLB with baked texture and UVs instead of vertex colors."
        },
        {
          "name": "detection_threshold",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Detection Threshold",
          "description": "Detection confidence threshold (0.1-1.0). Lower = more detections but less precise. If not set, uses the model's default."
        },
        {
          "name": "pointmap_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Pointmap Url",
          "description": "Optional URL to external pointmap/depth data (NPY or NPZ format) for improved 3D reconstruction depth estimation"
        },
        {
          "name": "box_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Box Prompts",
          "description": "Box prompts for auto-segmentation when no masks provided. Multiple boxes supported - each produces a separate object mask for 3D reconstruction."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to reconstruct in 3D"
        },
        {
          "name": "mask_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Mask Urls",
          "description": "Optional list of mask URLs (one per object). If not provided, use prompt/point_prompts/box_prompts to auto-segment, or entire image will be used."
        },
        {
          "name": "point_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Point Prompts",
          "description": "Point prompts for auto-segmentation when no masks provided"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "export_textured_glb",
        "detection_threshold",
        "pointmap_url",
        "box_prompts"
      ]
    },
    {
      "title": "Sam 33D Align",
      "description": "Sam 3\n    3d_to_3d\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.3d_to_3d",
      "node_type": "fal.3d_to_3d.Sam33DAlign",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the original image used for MoGe depth estimation"
        },
        {
          "name": "body_mesh_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Body Mesh Url",
          "description": "URL of the SAM-3D Body mesh file (.ply or .glb) to align"
        },
        {
          "name": "object_mesh_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Object Mesh Url",
          "description": "Optional URL of SAM-3D Object mesh (.glb) to create combined scene"
        },
        {
          "name": "focal_length",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Focal Length",
          "description": "Focal length from SAM-3D Body metadata. If not provided, estimated from MoGe."
        },
        {
          "name": "body_mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Body Mask Url",
          "description": "URL of the human mask image. If not provided, uses full image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image_url",
        "body_mesh_url",
        "object_mesh_url",
        "focal_length",
        "body_mask_url"
      ]
    },
    {
      "title": "Ultrashape",
      "description": "Ultrashape\n    3d_to_3d\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.3d_to_3d",
      "node_type": "fal.3d_to_3d.Ultrashape",
      "properties": [
        {
          "name": "octree_resolution",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Octree Resolution",
          "description": "Marching cubes resolution."
        },
        {
          "name": "remove_background",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Remove Background",
          "description": "Remove image background."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Diffusion steps."
        },
        {
          "name": "model_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model Url",
          "description": "URL of the coarse mesh (.glb or .obj) to refine."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Random seed."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the reference image for mesh refinement."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "octree_resolution",
        "remove_background",
        "num_inference_steps",
        "model_url",
        "seed"
      ]
    },
    {
      "title": "Ai Baby And Aging Generator Multi",
      "description": "AI Baby and Aging Generator Multi shows age progression or regression for multiple people in one image.\n    image, aging, age-progression, multi-face\n\n    Use cases:\n    - Show age progression for multiple people\n    - Generate family aging visualizations\n    - Create multi-person aging results\n    - Produce group age transformations\n    - Visualize multiple people at different ages",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.AiBabyAndAgingGeneratorMulti",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "a newborn baby, well dressed",
          "title": "Prompt",
          "description": "Text prompt to guide the image generation"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image"
        },
        {
          "name": "father_weight",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Father Weight",
          "description": "Weight of the father's influence in multi mode generation"
        },
        {
          "name": "mother_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Mother Image Urls",
          "description": "List of mother images for multi mode"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiBabyAndAgingGeneratorMulti.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image. Choose from: 'jpeg' or 'png'."
        },
        {
          "name": "age_group",
          "type": {
            "type": "enum",
            "values": [
              "baby",
              "toddler",
              "preschool",
              "gradeschooler",
              "teen",
              "adult",
              "mid",
              "senior"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiBabyAndAgingGeneratorMulti.AgeGroup"
          },
          "default": "",
          "title": "Age Group",
          "description": "Age group for the generated image. Choose from: 'baby' (0-12 months), 'toddler' (1-3 years), 'preschool' (3-5 years), 'gradeschooler' (6-12 years), 'teen' (13-19 years), 'adult' (20-40 years), 'mid' (40-60 years), 'senior' (60+ years)."
        },
        {
          "name": "gender",
          "type": {
            "type": "enum",
            "values": [
              "male",
              "female"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiBabyAndAgingGeneratorMulti.Gender"
          },
          "default": "",
          "title": "Gender",
          "description": "Gender for the generated image. Choose from: 'male' or 'female'."
        },
        {
          "name": "father_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Father Image Urls",
          "description": "List of father images for multi mode"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed will be used"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Ai Baby And Aging Generator Single",
      "description": "AI Baby and Aging Generator Single shows age progression or regression for a single person.\n    image, aging, age-progression, face-manipulation\n\n    Use cases:\n    - Show age progression of person\n    - Generate younger or older versions\n    - Create aging visualizations\n    - Produce age transformation results\n    - Visualize person at different ages",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.AiBabyAndAgingGeneratorSingle",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "a newborn baby, well dressed",
          "title": "Prompt",
          "description": "Text prompt to guide the image generation"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image"
        },
        {
          "name": "id_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Id Image Urls",
          "description": "List of ID images for single mode (or general reference images)"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiBabyAndAgingGeneratorSingle.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image. Choose from: 'jpeg' or 'png'."
        },
        {
          "name": "age_group",
          "type": {
            "type": "enum",
            "values": [
              "baby",
              "toddler",
              "preschool",
              "gradeschooler",
              "teen",
              "adult",
              "mid",
              "senior"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiBabyAndAgingGeneratorSingle.AgeGroup"
          },
          "default": "",
          "title": "Age Group",
          "description": "Age group for the generated image. Choose from: 'baby' (0-12 months), 'toddler' (1-3 years), 'preschool' (3-5 years), 'gradeschooler' (6-12 years), 'teen' (13-19 years), 'adult' (20-40 years), 'mid' (40-60 years), 'senior' (60+ years)."
        },
        {
          "name": "gender",
          "type": {
            "type": "enum",
            "values": [
              "male",
              "female"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiBabyAndAgingGeneratorSingle.Gender"
          },
          "default": "",
          "title": "Gender",
          "description": "Gender for the generated image. Choose from: 'male' or 'female'."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed will be used"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Ai Face Swap Image",
      "description": "AI Face Swap replaces faces in images with source faces while maintaining natural appearance.\n    image, face-swap, ai, face-manipulation\n\n    Use cases:\n    - Swap faces between images\n    - Replace faces in photos\n    - Create face-swapped variations\n    - Generate face replacement results\n    - Produce face-substituted images",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.AiFaceSwapImage",
      "properties": [
        {
          "name": "enable_occlusion_prevention",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Occlusion Prevention",
          "description": "Enable occlusion prevention for handling faces covered by hands/objects. Warning: Enabling this runs an occlusion-aware model which costs 2x more."
        },
        {
          "name": "source_face_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Source Face Url",
          "description": "Source face image. Allowed items: bmp, jpeg, png, tiff, webp"
        },
        {
          "name": "target_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Target Image Url",
          "description": "Target image URL. Allowed items: bmp, jpeg, png, tiff, webp"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Ai Home Edit",
      "description": "AI Home Edit modifies interior spaces with renovations, furniture changes, and design adjustments.\n    image, interior-design, editing, home, renovation\n\n    Use cases:\n    - Edit interior spaces\n    - Modify room furniture and decor\n    - Create renovation visualizations\n    - Generate design modification options\n    - Produce home editing results",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.AiHomeEdit",
      "properties": [
        {
          "name": "input_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image Url",
          "description": "URL of the image to do architectural editing"
        },
        {
          "name": "editing_type",
          "type": {
            "type": "enum",
            "values": [
              "structural editing",
              "virtual staging",
              "both"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeEdit.EditingType"
          },
          "default": "",
          "title": "Editing Type",
          "description": "Type of editing. Structural editing only edits structural elements such as windows, walls etc. Virtual staging edits your furniture. Both do full editing including structural and furniture"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "minimalistic-interior",
              "farmhouse-interior",
              "luxury-interior",
              "modern-interior",
              "zen-interior",
              "mid century-interior",
              "airbnb-interior",
              "cozy-interior",
              "rustic-interior",
              "christmas-interior",
              "bohemian-interior",
              "tropical-interior",
              "industrial-interior",
              "japanese-interior",
              "vintage-interior",
              "loft-interior",
              "halloween-interior",
              "soho-interior",
              "baroque-interior",
              "kids room-interior",
              "girls room-interior",
              "boys room-interior",
              "scandinavian-interior",
              "french country-interior",
              "mediterranean-interior",
              "cyberpunk-interior",
              "hot pink-interior",
              "biophilic-interior",
              "ancient egypt-interior",
              "pixel-interior",
              "art deco-interior",
              "modern-exterior",
              "minimalistic-exterior",
              "farmhouse-exterior",
              "cozy-exterior",
              "luxury-exterior",
              "colonial-exterior",
              "zen-exterior",
              "asian-exterior",
              "creepy-exterior",
              "airstone-exterior",
              "ancient greek-exterior",
              "art deco-exterior",
              "brutalist-exterior",
              "christmas lights-exterior",
              "contemporary-exterior",
              "cottage-exterior",
              "dutch colonial-exterior",
              "federal colonial-exterior",
              "fire-exterior",
              "french provincial-exterior",
              "full glass-exterior",
              "georgian colonial-exterior",
              "gothic-exterior",
              "greek revival-exterior",
              "ice-exterior",
              "italianate-exterior",
              "mediterranean-exterior",
              "midcentury-exterior",
              "middle eastern-exterior",
              "minecraft-exterior",
              "morocco-exterior",
              "neoclassical-exterior",
              "spanish-exterior",
              "tudor-exterior",
              "underwater-exterior",
              "winter-exterior",
              "yard lighting-exterior"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeEdit.Style"
          },
          "default": "",
          "title": "Style",
          "description": "Style for furniture and decor"
        },
        {
          "name": "additional_elements",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Additional Elements",
          "description": "Additional elements to include in the options above (e.g., plants, lighting)"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeEdit.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image. Choose from: 'jpeg' or 'png'."
        },
        {
          "name": "architecture_type",
          "type": {
            "type": "enum",
            "values": [
              "living room-interior",
              "bedroom-interior",
              "kitchen-interior",
              "dining room-interior",
              "bathroom-interior",
              "laundry room-interior",
              "home office-interior",
              "study room-interior",
              "dorm room-interior",
              "coffee shop-interior",
              "gaming room-interior",
              "restaurant-interior",
              "office-interior",
              "attic-interior",
              "toilet-interior",
              "other-interior",
              "house-exterior",
              "villa-exterior",
              "backyard-exterior",
              "courtyard-exterior",
              "ranch-exterior",
              "office-exterior",
              "retail-exterior",
              "tower-exterior",
              "apartment-exterior",
              "school-exterior",
              "museum-exterior",
              "commercial-exterior",
              "residential-exterior",
              "other-exterior"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeEdit.ArchitectureType"
          },
          "default": "",
          "title": "Architecture Type",
          "description": "Type of architecture for appropriate furniture selection"
        },
        {
          "name": "color_palette",
          "type": {
            "type": "enum",
            "values": [
              "surprise me",
              "golden beige",
              "refined blues",
              "dusky elegance",
              "emerald charm",
              "crimson luxury",
              "golden sapphire",
              "soft pastures",
              "candy sky",
              "peach meadow",
              "muted sands",
              "ocean breeze",
              "frosted pastels",
              "spring bloom",
              "gentle horizon",
              "seaside breeze",
              "azure coast",
              "golden shore",
              "mediterranean gem",
              "ocean serenity",
              "serene blush",
              "muted horizon",
              "pastel shores",
              "dusky calm",
              "woodland retreat",
              "meadow glow",
              "forest canopy",
              "riverbank calm",
              "earthy tones",
              "earthy neutrals",
              "arctic mist",
              "aqua drift",
              "blush bloom",
              "coral haze",
              "retro rust",
              "autumn glow",
              "rustic charm",
              "vintage sage",
              "faded plum",
              "electric lime",
              "violet pulse",
              "neon sorbet",
              "aqua glow",
              "fluorescent sunset",
              "lavender bloom",
              "petal fresh",
              "meadow light",
              "sunny pastures",
              "frosted mauve",
              "snowy hearth",
              "icy blues",
              "winter twilight",
              "earthy hues",
              "stone balance",
              "neutral sands",
              "slate shades"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeEdit.ColorPalette"
          },
          "default": "",
          "title": "Color Palette",
          "description": "Color palette for furniture and decor"
        },
        {
          "name": "custom_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Custom Prompt",
          "description": "Custom prompt for architectural editing, it overrides above options when used"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ai Home Style",
      "description": "AI Home Style transforms interior spaces with different design styles and aesthetics.\n    image, interior-design, style-transfer, home, decoration\n\n    Use cases:\n    - Transform interior design styles\n    - Apply different home aesthetics\n    - Create styled room variations\n    - Generate interior design options\n    - Produce home styling transformations",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.AiHomeStyle",
      "properties": [
        {
          "name": "input_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image Url",
          "description": "URL of the image to do architectural styling"
        },
        {
          "name": "input_image_strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Input Image Strength",
          "description": "Strength of the input image"
        },
        {
          "name": "additional_elements",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Additional Elements",
          "description": "Additional elements to include in the options above (e.g., plants, lighting)"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeStyle.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image. Choose from: 'jpeg' or 'png'."
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "minimalistic-interior",
              "farmhouse-interior",
              "luxury-interior",
              "modern-interior",
              "zen-interior",
              "mid century-interior",
              "airbnb-interior",
              "cozy-interior",
              "rustic-interior",
              "christmas-interior",
              "bohemian-interior",
              "tropical-interior",
              "industrial-interior",
              "japanese-interior",
              "vintage-interior",
              "loft-interior",
              "halloween-interior",
              "soho-interior",
              "baroque-interior",
              "kids room-interior",
              "girls room-interior",
              "boys room-interior",
              "scandinavian-interior",
              "french country-interior",
              "mediterranean-interior",
              "cyberpunk-interior",
              "hot pink-interior",
              "biophilic-interior",
              "ancient egypt-interior",
              "pixel-interior",
              "art deco-interior",
              "modern-exterior",
              "minimalistic-exterior",
              "farmhouse-exterior",
              "cozy-exterior",
              "luxury-exterior",
              "colonial-exterior",
              "zen-exterior",
              "asian-exterior",
              "creepy-exterior",
              "airstone-exterior",
              "ancient greek-exterior",
              "art deco-exterior",
              "brutalist-exterior",
              "christmas lights-exterior",
              "contemporary-exterior",
              "cottage-exterior",
              "dutch colonial-exterior",
              "federal colonial-exterior",
              "fire-exterior",
              "french provincial-exterior",
              "full glass-exterior",
              "georgian colonial-exterior",
              "gothic-exterior",
              "greek revival-exterior",
              "ice-exterior",
              "italianate-exterior",
              "mediterranean-exterior",
              "midcentury-exterior",
              "middle eastern-exterior",
              "minecraft-exterior",
              "morocco-exterior",
              "neoclassical-exterior",
              "spanish-exterior",
              "tudor-exterior",
              "underwater-exterior",
              "winter-exterior",
              "yard lighting-exterior"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeStyle.Style"
          },
          "default": "",
          "title": "Style",
          "description": "Style for furniture and decor"
        },
        {
          "name": "architecture_type",
          "type": {
            "type": "enum",
            "values": [
              "living room-interior",
              "bedroom-interior",
              "kitchen-interior",
              "dining room-interior",
              "bathroom-interior",
              "laundry room-interior",
              "home office-interior",
              "study room-interior",
              "dorm room-interior",
              "coffee shop-interior",
              "gaming room-interior",
              "restaurant-interior",
              "office-interior",
              "attic-interior",
              "toilet-interior",
              "other-interior",
              "house-exterior",
              "villa-exterior",
              "backyard-exterior",
              "courtyard-exterior",
              "ranch-exterior",
              "office-exterior",
              "retail-exterior",
              "tower-exterior",
              "apartment-exterior",
              "school-exterior",
              "museum-exterior",
              "commercial-exterior",
              "residential-exterior",
              "other-exterior"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeStyle.ArchitectureType"
          },
          "default": "",
          "title": "Architecture Type",
          "description": "Type of architecture for appropriate furniture selection"
        },
        {
          "name": "color_palette",
          "type": {
            "type": "enum",
            "values": [
              "surprise me",
              "golden beige",
              "refined blues",
              "dusky elegance",
              "emerald charm",
              "crimson luxury",
              "golden sapphire",
              "soft pastures",
              "candy sky",
              "peach meadow",
              "muted sands",
              "ocean breeze",
              "frosted pastels",
              "spring bloom",
              "gentle horizon",
              "seaside breeze",
              "azure coast",
              "golden shore",
              "mediterranean gem",
              "ocean serenity",
              "serene blush",
              "muted horizon",
              "pastel shores",
              "dusky calm",
              "woodland retreat",
              "meadow glow",
              "forest canopy",
              "riverbank calm",
              "earthy tones",
              "earthy neutrals",
              "arctic mist",
              "aqua drift",
              "blush bloom",
              "coral haze",
              "retro rust",
              "autumn glow",
              "rustic charm",
              "vintage sage",
              "faded plum",
              "electric lime",
              "violet pulse",
              "neon sorbet",
              "aqua glow",
              "fluorescent sunset",
              "lavender bloom",
              "petal fresh",
              "meadow light",
              "sunny pastures",
              "frosted mauve",
              "snowy hearth",
              "icy blues",
              "winter twilight",
              "earthy hues",
              "stone balance",
              "neutral sands",
              "slate shades"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeStyle.ColorPalette"
          },
          "default": "",
          "title": "Color Palette",
          "description": "Color palette for furniture and decor"
        },
        {
          "name": "style_image_url",
          "type": {
            "type": "image"
          },
          "default": "",
          "title": "Style Image Url",
          "description": "URL of the style image, optional. If given, other parameters are ignored"
        },
        {
          "name": "custom_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Custom Prompt",
          "description": "Custom prompt for architectural editing, it overrides above options when used"
        },
        {
          "name": "enhanced_rendering",
          "type": {
            "type": "str"
          },
          "default": false,
          "title": "Enhanced Rendering",
          "description": "It gives better rendering quality with more processing time, additional cost is 0.01 USD per image"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bi Ref Net",
      "description": "BiRefNet (Bilateral Reference Network) performs high-quality background removal with precise edge detection and detail preservation.\n    image, background-removal, segmentation, birefnet, mask\n\n    Use cases:\n    - Remove backgrounds from product photos\n    - Create transparent PNGs from images\n    - Extract subjects for compositing\n    - Generate clean cutouts for design work\n    - Prepare images for background replacement",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BiRefNet",
      "properties": [
        {
          "name": "operating_resolution",
          "type": {
            "type": "enum",
            "values": [
              "1024x1024",
              "2048x2048"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BiRefNet.OperatingResolution"
          },
          "default": "1024x1024",
          "title": "Operating Resolution",
          "description": "The resolution to operate on. The higher the resolution, the more accurate the output will be for high res input images."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "webp",
              "png",
              "gif"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BiRefNet.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to remove background from"
        },
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "General Use (Light)",
              "General Use (Heavy)",
              "Portrait"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BiRefNet.Model"
          },
          "default": "General Use (Light)",
          "title": "Model",
          "description": "Model to use for background removal. The 'General Use (Light)' model is the original model used in the BiRefNet repository. The 'General Use (Heavy)' model is a slower but more accurate model. The 'Portrait' model is a model trained specifically for portrait images. The 'General Use (Light)' model is recommended for most use cases. The corresponding models are as follows: - 'General Use (Light)': BiRefNet-DIS_ep580.pth - 'General Use (Heavy)': BiRefNet-massive-epoch_240.pth - 'Portrait': BiRefNet-portrait-TR_P3M_10k-epoch_120.pth"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_mask",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Output Mask",
          "description": "Whether to output the mask used to remove the background"
        },
        {
          "name": "refine_foreground",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Refine Foreground",
          "description": "Whether to refine the foreground using the estimated mask"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Bria Background Replace",
      "description": "Bria Background Replace swaps image backgrounds with new content. Intelligently separates subjects and generates contextually appropriate backgrounds.\n    image, background, replacement, segmentation, bria\n\n    Use cases:\n    - Replace photo backgrounds with custom scenes\n    - Create product shots with various backgrounds\n    - Change image context while preserving subject\n    - Generate professional portraits with studio backgrounds\n    - Create marketing materials with branded backgrounds",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaBackgroundReplace",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Description of the new background to generate"
        },
        {
          "name": "steps_num",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Steps Num",
          "description": "Number of inference steps."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If true, returns the image directly in the response (increases latency)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 4925634,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for background replacement."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": "https://v3b.fal.media/files/b/0a8bea8c/Mztgx0NG3HPdby-4iPqwH_a_coffee_machine_standing_in_the_kitchen.png",
          "title": "Image Url",
          "description": "Reference image (file or URL)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Eraser",
      "description": "Bria Eraser removes unwanted objects from images using intelligent inpainting. Seamlessly fill removed areas with contextually appropriate content.\n    image, eraser, removal, inpainting, bria, cleanup\n\n    Use cases:\n    - Remove unwanted objects from photos\n    - Clean up image backgrounds\n    - Erase text or watermarks\n    - Delete distracting elements\n    - Create clean product shots",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaEraser",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "preserve_alpha",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preserve Alpha",
          "description": "If set to true, attempts to preserve the alpha channel of the input image."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The URL of the binary mask image that represents the area that will be cleaned."
        },
        {
          "name": "mask_type",
          "type": {
            "type": "enum",
            "values": [
              "manual",
              "automatic"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BriaEraser.MaskType"
          },
          "default": "manual",
          "title": "Mask Type",
          "description": "You can use this parameter to specify the type of the input mask from the list. 'manual' opttion should be used in cases in which the mask had been generated by a user (e.g. with a brush tool), and 'automatic' mask type should be used when mask had been generated by an algorithm like 'SAM'."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Input Image to erase from"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "mask"
      ]
    },
    {
      "title": "Bria Fibo Edit",
      "description": "Bria FIBO Edit provides general-purpose image editing with AI-powered modifications and enhancements.\n    image, editing, bria, fibo, general\n\n    Use cases:\n    - Edit images with general-purpose AI\n    - Apply various modifications to photos\n    - Create edited versions of images\n    - Transform images with flexible edits\n    - Produce AI-powered modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEdit",
      "properties": [
        {
          "name": "steps_num",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Steps Num",
          "description": "Number of inference steps."
        },
        {
          "name": "instruction",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Instruction",
          "description": "Instruction for image editing."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Reference image (file or URL)."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If true, returns the image directly in the response (increases latency)."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "str"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for text."
        },
        {
          "name": "structured_instruction",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Structured Instruction",
          "description": "The structured prompt to generate an image from."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "Mask image (file or URL). Optional"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 5555,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Add Object By Text",
      "description": "Bria FIBO Edit Add Object by Text inserts new objects into images using text descriptions.\n    image, editing, bria, fibo, object-insertion\n\n    Use cases:\n    - Add objects to images with text\n    - Insert elements using descriptions\n    - Place new items in scenes\n    - Augment images with additional objects\n    - Generate object additions",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditAddObjectByText",
      "properties": [
        {
          "name": "instruction",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Instruction",
          "description": "The full natural language command describing what to add and where."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Blend",
      "description": "Bria FIBO Edit Blend seamlessly combines multiple images or elements with natural transitions.\n    image, editing, bria, fibo, blending\n\n    Use cases:\n    - Blend multiple images together\n    - Create seamless compositions\n    - Merge elements naturally\n    - Combine images with smooth transitions\n    - Generate blended composites",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditBlend",
      "properties": [
        {
          "name": "instruction",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Instruction",
          "description": "Instruct what elements you would like to blend in your image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Colorize",
      "description": "Bria FIBO Edit Colorize adds realistic colors to grayscale or black-and-white images.\n    image, editing, bria, fibo, colorization\n\n    Use cases:\n    - Colorize black and white photos\n    - Add colors to grayscale images\n    - Restore color in old photographs\n    - Transform monochrome to color\n    - Generate colored versions of grayscale images",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditColorize",
      "properties": [
        {
          "name": "color",
          "type": {
            "type": "enum",
            "values": [
              "contemporary color",
              "vivid color",
              "black and white colors",
              "sepia vintage"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BriaFiboEditColorize.Color"
          },
          "default": "",
          "title": "Color",
          "description": "Select the color palette or aesthetic for the output image"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Bria Fibo Edit Erase By Text",
      "description": "Bria FIBO Edit Erase by Text removes objects from images using natural language descriptions.\n    image, editing, bria, fibo, object-removal\n\n    Use cases:\n    - Remove objects using text descriptions\n    - Erase unwanted elements from photos\n    - Clean up images by describing what to remove\n    - Delete specific items from scenes\n    - Remove objects with natural language",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditEraseByText",
      "properties": [
        {
          "name": "object_name",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Object Name",
          "description": "The name of the object to remove."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Relight",
      "description": "Bria FIBO Edit Relight adjusts lighting conditions in images for dramatic or natural effects.\n    image, editing, bria, fibo, relighting\n\n    Use cases:\n    - Adjust lighting in photos\n    - Change illumination conditions\n    - Create dramatic lighting effects\n    - Relight scenes for better ambiance\n    - Transform lighting in images",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditRelight",
      "properties": [
        {
          "name": "light_type",
          "type": {
            "type": "enum",
            "values": [
              "midday",
              "blue hour light",
              "low-angle sunlight",
              "sunrise light",
              "spotlight on subject",
              "overcast light",
              "soft overcast daylight lighting",
              "cloud-filtered lighting",
              "fog-diffused lighting",
              "moonlight lighting",
              "starlight nighttime",
              "soft bokeh lighting",
              "harsh studio lighting"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BriaFiboEditRelight.LightType"
          },
          "default": "",
          "title": "Light Type",
          "description": "The quality/style/time of day."
        },
        {
          "name": "light_direction",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Light Direction",
          "description": "Where the light comes from."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Replace Object By Text",
      "description": "Bria FIBO Edit Replace Object by Text replaces objects in images with new ones specified by text.\n    image, editing, bria, fibo, object-replacement\n\n    Use cases:\n    - Replace objects using text descriptions\n    - Swap elements in photos\n    - Change specific items in scenes\n    - Transform objects with text guidance\n    - Substitute objects with new ones",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditReplaceObjectByText",
      "properties": [
        {
          "name": "instruction",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Instruction",
          "description": "The full natural language command describing what to replace."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Reseason",
      "description": "Bria FIBO Edit Reseason changes the seasonal appearance of outdoor scenes in images.\n    image, editing, bria, fibo, seasonal\n\n    Use cases:\n    - Change seasons in outdoor photos\n    - Transform summer to winter scenes\n    - Modify seasonal appearance\n    - Create seasonal variations\n    - Generate different season versions",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditReseason",
      "properties": [
        {
          "name": "season",
          "type": {
            "type": "enum",
            "values": [
              "spring",
              "summer",
              "autumn",
              "winter"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BriaFiboEditReseason.Season"
          },
          "default": "",
          "title": "Season",
          "description": "The desired season."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Restore",
      "description": "Bria FIBO Edit Restore repairs and enhances damaged or degraded images with AI reconstruction.\n    image, editing, bria, fibo, restoration\n\n    Use cases:\n    - Restore damaged photographs\n    - Repair degraded images\n    - Enhance old photo quality\n    - Fix scratches and artifacts\n    - Reconstruct missing image parts",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditRestore",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Bria Fibo Edit Restyle",
      "description": "Bria FIBO Edit Restyle transforms images with artistic style transfers and visual aesthetics.\n    image, editing, bria, fibo, style-transfer\n\n    Use cases:\n    - Apply artistic styles to images\n    - Transform photos with new aesthetics\n    - Create stylized versions of images\n    - Generate artistic variations\n    - Produce style-transferred images",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditRestyle",
      "properties": [
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "3D Render",
              "Cubism",
              "Oil Painting",
              "Anime",
              "Cartoon",
              "Coloring Book",
              "Retro Ad",
              "Pop Art Halftone",
              "Vector Art",
              "Story Board",
              "Art Nouveau",
              "Cross Etching",
              "Wood Cut"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BriaFiboEditRestyle.Style"
          },
          "default": "",
          "title": "Style",
          "description": "Select the desired artistic style for the output image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Rewrite Text",
      "description": "Bria FIBO Edit Rewrite Text modifies or replaces text content within images naturally.\n    image, editing, bria, fibo, text-editing\n\n    Use cases:\n    - Change text in images\n    - Replace written content in photos\n    - Modify signs and labels\n    - Update text naturally in scenes\n    - Edit textual elements in images",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditRewriteText",
      "properties": [
        {
          "name": "new_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "New Text",
          "description": "The new text string to appear in the image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Sketch To Colored Image",
      "description": "Bria FIBO Edit Sketch to Colored Image transforms sketches and line art into full-color images.\n    image, editing, bria, fibo, sketch-to-image\n\n    Use cases:\n    - Convert sketches to colored images\n    - Transform line art to full color\n    - Generate colored versions of drawings\n    - Create realistic images from sketches\n    - Produce colored artwork from outlines",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditSketchToColoredImage",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Reimagine",
      "description": "Bria\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaReimagine",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt you would like to use to generate images."
        },
        {
          "name": "num_results",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Results",
          "description": "How many images you would like to generate. When using any Guidance Method, Value is set to 1."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "structure_ref_influence",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Structure Ref Influence",
          "description": "The influence of the structure reference on the generated image."
        },
        {
          "name": "fast",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Fast",
          "description": "Whether to use the fast model"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of iterations the model goes through to refine the generated image. This parameter is optional."
        },
        {
          "name": "structure_image_url",
          "type": {
            "type": "image"
          },
          "default": "",
          "title": "Structure Image Url",
          "description": "The URL of the structure reference image. Use \"\" to leave empty. Accepted formats are jpeg, jpg, png, webp."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Reimagine 3 2",
      "description": "Reimagine\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaReimagine3_2",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt for image generation."
        },
        {
          "name": "depth_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Depth Preprocess",
          "description": "Depth image preprocess."
        },
        {
          "name": "canny_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Canny Preprocess",
          "description": "Canny image preprocess."
        },
        {
          "name": "depth_image_url",
          "type": {
            "type": "image"
          },
          "default": "",
          "title": "Depth Image Url",
          "description": "Depth control image (file or URL)."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for text."
        },
        {
          "name": "canny_image_url",
          "type": {
            "type": "image"
          },
          "default": "",
          "title": "Canny Image Url",
          "description": "Canny edge control image (file or URL)."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Logo,Watermark,Ugly,Morbid,Extra fingers,Poorly drawn hands,Mutation,Blurry,Extra limbs,Gross proportions,Missing arms,Mutated hands,Long neck,Duplicate,Mutilated,Mutilated hands,Poorly drawn face,Deformed,Bad anatomy,Cloned face,Malformed limbs,Missing legs,Too many fingers",
          "title": "Negative Prompt",
          "description": "Negative prompt for image generation."
        },
        {
          "name": "depth_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Depth Scale",
          "description": "Depth control strength (0.0 to 1.0)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "2:3",
              "3:2",
              "3:4",
              "4:3",
              "4:5",
              "5:4",
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BriaReimagine3_2.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "Aspect ratio. Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If true, returns the image directly in the response (increases latency)."
        },
        {
          "name": "prompt_enhancer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Enhancer",
          "description": "Whether to improve the prompt."
        },
        {
          "name": "truncate_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Truncate Prompt",
          "description": "Whether to truncate the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 5555,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "canny_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Canny Scale",
          "description": "Canny edge control strength (0.0 to 1.0)."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bytedance Seedream V45 Edit",
      "description": "ByteDance SeeDream v4.5 Edit provides advanced image editing with cutting-edge AI technology.\n    image, editing, bytedance, seedream, v4.5\n\n    Use cases:\n    - Edit images with SeeDream v4.5\n    - Apply advanced modifications\n    - Create high-quality edits\n    - Transform images with latest tech\n    - Produce cutting-edge modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BytedanceSeedreamV45Edit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to edit the image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of separate model generations to be run with the prompt."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Width and height must be between 1920 and 4096, or total number of pixels must be between 2560*1440 and 4096*4096."
        },
        {
          "name": "max_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Max Images",
          "description": "If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`. The total number of images (image inputs + image outputs) must not exceed 15"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control the stochasticity of image generation."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of URLs of input images for editing. Presently, up to 10 image inputs are allowed. If over 10 images are sent, only the last 10 will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bytedance Seedream V4 Edit",
      "description": "Bytedance Seedream v4 Edit\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BytedanceSeedreamV4Edit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to edit the image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of separate model generations to be run with the prompt."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. The minimum total image area is 921600 pixels. Failing this, the image size will be adjusted to by scaling it up, while maintaining the aspect ratio."
        },
        {
          "name": "max_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Max Images",
          "description": "If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`. The total number of images (image inputs + image outputs) must not exceed 15"
        },
        {
          "name": "enhance_prompt_mode",
          "type": {
            "type": "enum",
            "values": [
              "standard",
              "fast"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BytedanceSeedreamV4Edit.EnhancePromptMode"
          },
          "default": "standard",
          "title": "Enhance Prompt Mode",
          "description": "The mode to use for enhancing prompt enhancement. Standard mode provides higher quality results but takes longer to generate. Fast mode provides average quality results but takes less time to generate."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control the stochasticity of image generation."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of URLs of input images for editing. Presently, up to 10 image inputs are allowed. If over 10 images are sent, only the last 10 will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Calligrapher",
      "description": "Calligrapher\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Calligrapher",
      "properties": [
        {
          "name": "use_context",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Context",
          "description": "Whether to prepend context reference to the input"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "How many images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "Target image size for generation"
        },
        {
          "name": "auto_mask_generation",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Mask Generation",
          "description": "Whether to automatically generate mask from detected text"
        },
        {
          "name": "reference_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Reference Image Url",
          "description": "Optional base64 reference image for style"
        },
        {
          "name": "source_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Source Image Url",
          "description": "Base64-encoded source image with drawn mask layers"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to inpaint or customize"
        },
        {
          "name": "mask_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image Url",
          "description": "Base64-encoded mask image (optional if using auto_mask_generation)"
        },
        {
          "name": "source_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Source Text",
          "description": "Source text to replace (if empty, masks all detected text)"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps (1-100)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Cfg Scale",
          "description": "Guidance or strength scale for the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Chrono Edit",
      "description": "Chrono Edit\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ChronoEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ChronoEdit.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the output image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ChronoEdit.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image to edit."
        },
        {
          "name": "turbo_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Turbo Mode",
          "description": "Enable turbo mode to use for faster inference."
        },
        {
          "name": "num_temporal_reasoning_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Temporal Reasoning Steps",
          "description": "The number of temporal reasoning steps to perform."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "Whether to return the image in sync mode."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The guidance scale for the inference."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_temporal_reasoning",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Temporal Reasoning",
          "description": "Whether to enable temporal reasoning."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the inference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Chrono Edit Lora",
      "description": "Chrono Edit Lora\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ChronoEditLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "Optional additional LoRAs to merge for this request (max 3)."
        },
        {
          "name": "turbo_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Turbo Mode",
          "description": "Enable turbo mode to use for faster inference."
        },
        {
          "name": "enable_temporal_reasoning",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Temporal Reasoning",
          "description": "Whether to enable temporal reasoning."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The guidance scale for the inference."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ChronoEditLora.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the output image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ChronoEditLora.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "num_temporal_reasoning_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Temporal Reasoning Steps",
          "description": "The number of temporal reasoning steps to perform."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "Whether to return the image in sync mode."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image to edit."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the inference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Chrono Edit Lora Gallery Paintbrush",
      "description": "Chrono Edit Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ChronoEditLoraGalleryPaintbrush",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Describe how to transform the sketched regions."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ChronoEditLoraGalleryPaintbrush.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the output image."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA adapter."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ChronoEditLoraGalleryPaintbrush.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "Whether to return the image in sync mode."
        },
        {
          "name": "turbo_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Turbo Mode",
          "description": "Enable turbo mode to use faster inference."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "Optional additional LoRAs to merge (max 3)."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps to run."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "Optional mask image where black areas indicate regions to sketch/paint."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the inference."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Chrono Edit Lora Gallery Upscaler",
      "description": "Chrono Edit Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ChronoEditLoraGalleryUpscaler",
      "properties": [
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA adapter."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ChronoEditLoraGalleryUpscaler.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image to upscale."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "Whether to return the image in sync mode."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "Optional additional LoRAs to merge (max 3)."
        },
        {
          "name": "upscale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Upscale Factor",
          "description": "Target scale factor for the output resolution."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The guidance scale for the inference."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for the upscaling pass."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the inference."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Clarity Upscaler",
      "description": "Clarity Upscaler increases image resolution using AI-powered super-resolution. Enhance image quality, sharpness, and detail up to 4x scale.\n    image, upscaling, enhancement, super-resolution, clarity\n\n    Use cases:\n    - Increase image resolution for printing\n    - Improve clarity of low-quality images\n    - Enhance textures and fine details\n    - Prepare images for large displays\n    - Restore detail in compressed images",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ClarityUpscaler",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "masterpiece, best quality, highres",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "resemblance",
          "type": {
            "type": "float"
          },
          "default": 0.6,
          "title": "Resemblance",
          "description": "The resemblance of the upscaled image to the original image. The higher the resemblance, the more the model will try to keep the original image. Refers to the strength of the ControlNet."
        },
        {
          "name": "creativity",
          "type": {
            "type": "float"
          },
          "default": 0.35,
          "title": "Creativity",
          "description": "The creativity of the model. The higher the creativity, the more the model will deviate from the prompt. Refers to the denoise strength of the sampling."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to upscale."
        },
        {
          "name": "upscale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Upscale Factor",
          "description": "The upscale factor"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 18,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(worst quality, low quality, normal quality:2)",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to false, the safety checker will be disabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "scale"
      ]
    },
    {
      "title": "Clarityai Crystal Upscaler",
      "description": "Crystal Upscaler\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ClarityaiCrystalUpscaler",
      "properties": [
        {
          "name": "creativity",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Creativity",
          "description": "Creativity level for upscaling"
        },
        {
          "name": "scale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Scale Factor",
          "description": "Scale factor"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL to the input image"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Code Former",
      "description": "CodeFormer restores and enhances face quality in images. Advanced face restoration with fidelity control for natural-looking results.\n    image, face-restoration, enhancement, codeformer, quality\n\n    Use cases:\n    - Restore quality in degraded face photos\n    - Enhance facial details in low-quality images\n    - Improve portrait quality for professional use\n    - Fix compressed or damaged face images\n    - Enhance facial features while maintaining identity",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.CodeFormer",
      "properties": [
        {
          "name": "aligned",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Aligned",
          "description": "Should faces etc should be aligned."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to be used for relighting"
        },
        {
          "name": "upscale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Upscale Factor",
          "description": "Upscaling factor"
        },
        {
          "name": "fidelity",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Fidelity",
          "description": "Fidelity level (0-1, higher = more faithful to input)"
        },
        {
          "name": "face_upscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Face Upscale",
          "description": "Should faces be upscaled"
        },
        {
          "name": "only_center_face",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Only Center Face",
          "description": "Should only center face be restored"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "fidelity"
      ]
    },
    {
      "title": "Dreamomni 2 Edit",
      "description": "DreamOmni2\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Dreamomni2Edit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of URLs of input images for editing."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Emu 3 5 Image Edit Image",
      "description": "Emu 3.5 Image\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Emu3_5ImageEditImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Emu3_5ImageEditImage.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the output image."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Emu3_5ImageEditImage.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the output image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Emu3_5ImageEditImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "Whether to return the image in sync mode."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the inference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Film",
      "description": "FILM\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Film",
      "properties": [
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Film.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the output video. Only applicable if output_type is 'video'."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Frames",
          "description": "The number of frames to generate between the input images."
        },
        {
          "name": "include_start",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Include Start",
          "description": "Whether to include the start image in the output."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Film.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the output video. Only applicable if output_type is 'video'."
        },
        {
          "name": "include_end",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Include End",
          "description": "Whether to include the end image in the output."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Fps",
          "description": "Frames per second for the output video. Only applicable if output_type is 'video'."
        },
        {
          "name": "start_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Start Image Url",
          "description": "The URL of the first image to use as the starting point for interpolation."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the second image to use as the ending point for interpolation."
        },
        {
          "name": "image_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Film.ImageFormat"
          },
          "default": "jpeg",
          "title": "Image Format",
          "description": "The format of the output images. Only applicable if output_type is 'images'."
        },
        {
          "name": "output_type",
          "type": {
            "type": "enum",
            "values": [
              "images",
              "video"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Film.OutputType"
          },
          "default": "images",
          "title": "Output Type",
          "description": "The type of output to generate; either individual images or a video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "enum",
            "values": [
              "images",
              "video"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Film.OutputType"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 1 Krea Image To Image",
      "description": "FLUX.1 Krea [dev]\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux1KreaImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1KreaImageToImage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1KreaImageToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "The strength of the initial image. Higher strength values are better for this model."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 1 Krea Redux",
      "description": "FLUX.1 Krea [dev] Redux\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux1KreaRedux",
      "properties": [
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1KreaRedux.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1KreaRedux.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 1 Srpo Image To Image",
      "description": "FLUX.1 SRPO [dev]\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux1SrpoImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1SrpoImageToImage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1SrpoImageToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "The strength of the initial image. Higher strength values are better for this model."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Flash Edit",
      "description": "FLUX-2 Flash Edit provides ultra-fast image editing for rapid iteration and quick modifications.\n    image, editing, flux-2, flash, ultra-fast\n\n    Use cases:\n    - Edit images with ultra-fast processing\n    - Apply instant modifications to photos\n    - Create rapid edits for quick turnaround\n    - Transform images at maximum speed\n    - Produce instant image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2FlashEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the image to generate. The width and height must be between 512 and 2048 pixels."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2FlashEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for editing. A maximum of 4 images are allowed, if more are provided, only the first 4 will be used."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "If set to true, the prompt will be expanded for better results."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Flex Edit",
      "description": "FLUX-2 Flex Edit provides flexible image editing with customizable parameters and versatile control.\n    image, editing, flux-2, flex, versatile\n\n    Use cases:\n    - Edit images with flexible controls\n    - Apply customizable modifications\n    - Create versatile edits\n    - Transform images with adaptable settings\n    - Produce flexible image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2FlexEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the generation."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The size of the generated image. If `auto`, the size will be determined by the model."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2FlexEdit.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2FlexEdit.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to expand the prompt using the model's own knowledge."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of URLs of input images for editing"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 4B Base Edit",
      "description": "FLUX-2 Klein 4B Base Edit provides fast image editing with the 4-billion parameter model.\n    image, editing, flux-2, klein, 4b\n\n    Use cases:\n    - Edit images with FLUX-2 Klein 4B\n    - Apply fast modifications to photos\n    - Create quick edits with AI assistance\n    - Transform images efficiently\n    - Produce rapid image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2Klein4BBaseEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, uses the input image size."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein4BBaseEdit.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for image generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein4BBaseEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for editing. A maximum of 4 images are allowed."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for classifier-free guidance. Describes what to avoid in the image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 4B Base Edit Lora",
      "description": "FLUX-2 Klein 4B Base Edit with LoRA enables custom-trained models for specialized editing.\n    image, editing, flux-2, klein, 4b, lora\n\n    Use cases:\n    - Edit images with custom FLUX-2 models\n    - Apply specialized modifications using LoRA\n    - Create domain-specific edits\n    - Transform images with fine-tuned 4B model\n    - Produce customized modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2Klein4BBaseEditLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, uses the input image size."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein4BBaseEditLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for image generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein4BBaseEditLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for editing. A maximum of 4 images are allowed."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for classifier-free guidance. Describes what to avoid in the image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 4B Edit",
      "description": "FLUX-2 Klein 4B Edit provides efficient image editing with the streamlined 4-billion parameter model.\n    image, editing, flux-2, klein, 4b, efficient\n\n    Use cases:\n    - Edit images efficiently with FLUX-2\n    - Apply quick modifications to photos\n    - Create fast edits for rapid workflows\n    - Transform images with streamlined model\n    - Produce quick image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2Klein4BEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, uses the input image size."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein4BEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for editing. A maximum of 4 images are allowed."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 9B Base Edit",
      "description": "FLUX-2 Klein 9B Base Edit provides high-quality image editing with the 9-billion parameter model.\n    image, editing, flux-2, klein, 9b\n\n    Use cases:\n    - Edit images with FLUX-2 Klein 9B\n    - Apply high-quality modifications to photos\n    - Create advanced edits with powerful AI\n    - Transform images with superior quality\n    - Produce professional image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2Klein9BBaseEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, uses the input image size."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein9BBaseEdit.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for image generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein9BBaseEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for editing. A maximum of 4 images are allowed."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for classifier-free guidance. Describes what to avoid in the image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 9B Base Edit Lora",
      "description": "FLUX-2 Klein 9B Base Edit with LoRA combines powerful editing with custom-trained models.\n    image, editing, flux-2, klein, 9b, lora\n\n    Use cases:\n    - Edit images with custom 9B models\n    - Apply specialized high-quality modifications\n    - Create professional custom edits\n    - Transform images with fine-tuned powerful model\n    - Produce advanced customized results",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2Klein9BBaseEditLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, uses the input image size."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein9BBaseEditLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for image generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein9BBaseEditLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for editing. A maximum of 4 images are allowed."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for classifier-free guidance. Describes what to avoid in the image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 9B Edit",
      "description": "FLUX-2 Klein 9B Edit provides advanced image editing with the full 9-billion parameter model.\n    image, editing, flux-2, klein, 9b, advanced\n\n    Use cases:\n    - Edit images with advanced FLUX-2 model\n    - Apply sophisticated modifications\n    - Create high-quality edits\n    - Transform images with powerful AI\n    - Produce superior image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2Klein9BEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, uses the input image size."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein9BEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for editing. A maximum of 4 images are allowed."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Add Background",
      "description": "FLUX-2 LoRA Gallery Add Background places subjects in new environments with realistic integration.\n    image, editing, flux-2, background, compositing\n\n    Use cases:\n    - Add backgrounds to cutout images\n    - Place subjects in new environments\n    - Create realistic background compositions\n    - Generate contextual settings\n    - Produce integrated background images",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2LoraGalleryAddBackground",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Add Background forest",
          "title": "Prompt",
          "description": "The prompt describing the background to add. Must start with 'Add Background' followed by your description."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryAddBackground.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the add background effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryAddBackground.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images. Provide an image with a white or clean background."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Apartment Staging",
      "description": "Flux 2 Lora Gallery\n    flux, editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2LoraGalleryApartmentStaging",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a furnished room. Use 'furnish this room' for best results."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryApartmentStaging.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the apartment staging effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryApartmentStaging.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the empty room image to furnish."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Face To Full Portrait",
      "description": "FLUX-2 LoRA Gallery Face to Full Portrait expands face crops into complete portrait images.\n    image, editing, flux-2, portrait, expansion\n\n    Use cases:\n    - Expand face crops to full portraits\n    - Generate complete portrait from face\n    - Create full-body images from headshots\n    - Extend facial images to portraits\n    - Produce complete portrait compositions",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2LoraGalleryFaceToFullPortrait",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Face to full portrait",
          "title": "Prompt",
          "description": "The prompt describing the full portrait to generate from the face."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryFaceToFullPortrait.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the face to full portrait effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryFaceToFullPortrait.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the cropped face image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Multiple Angles",
      "description": "FLUX-2 LoRA Gallery Multiple Angles generates images from different viewpoints for comprehensive visualization.\n    image, editing, flux-2, multi-angle, viewpoint\n\n    Use cases:\n    - Generate multiple product angles\n    - Create viewpoint variations\n    - Visualize objects from different sides\n    - Produce multi-angle image sets\n    - Generate comprehensive views",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2LoraGalleryMultipleAngles",
      "properties": [
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryMultipleAngles.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation."
        },
        {
          "name": "horizontal_angle",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Horizontal Angle",
          "description": "Horizontal rotation angle around the object in degrees. 0\u00b0=front view, 90\u00b0=right side, 180\u00b0=back view, 270\u00b0=left side, 360\u00b0=front view again."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image to adjust camera angle for."
        },
        {
          "name": "zoom",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Zoom",
          "description": "Camera zoom/distance. 0=wide shot (far away), 5=medium shot (normal), 10=close-up (very close)."
        },
        {
          "name": "vertical_angle",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Vertical Angle",
          "description": "Vertical camera angle in degrees. 0\u00b0=eye-level shot, 30\u00b0=elevated shot, 60\u00b0=high-angle shot (looking down from above)."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the multiple angles effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryMultipleAngles.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If True, the media will be returned as a data URI."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Virtual Tryon",
      "description": "FLUX-2 LoRA Gallery Virtual Try-on enables realistic clothing and accessory visualization on people.\n    image, editing, flux-2, virtual-tryon, fashion\n\n    Use cases:\n    - Visualize clothing on models\n    - Try on accessories virtually\n    - Create fashion previews\n    - Test product appearances\n    - Generate try-on images",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2LoraGalleryVirtualTryon",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a virtual try-on image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryVirtualTryon.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the virtual try-on effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryVirtualTryon.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for virtual try-on. Provide person image and clothing image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Max Edit",
      "description": "FLUX-2 Max Edit provides maximum quality image editing with the most advanced FLUX-2 model.\n    image, editing, flux-2, max, premium\n\n    Use cases:\n    - Edit images with maximum quality\n    - Apply premium modifications to photos\n    - Create professional-grade edits\n    - Transform images with best quality\n    - Produce highest quality modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2MaxEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The size of the generated image. If `auto`, the size will be determined by the model."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2MaxEdit.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2MaxEdit.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of URLs of input images for editing"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Turbo Edit",
      "description": "FLUX-2 Turbo Edit provides accelerated image editing with balanced quality and speed.\n    image, editing, flux-2, turbo, fast\n\n    Use cases:\n    - Edit images with turbo speed\n    - Apply fast modifications with good quality\n    - Create quick edits efficiently\n    - Transform images rapidly\n    - Produce fast quality modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2TurboEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the image to generate. The width and height must be between 512 and 2048 pixels."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2TurboEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for editing. A maximum of 4 images are allowed, if more are provided, only the first 4 will be used."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "If set to true, the prompt will be expanded for better results."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Dev Redux",
      "description": "FLUX.1 [dev] Redux provides advanced image transformation capabilities with superior quality and more control over the style transfer process.\n    image, transformation, style-transfer, development, flux, redux\n\n    Use cases:\n    - Transform images with advanced quality controls\n    - Create customized image variations with guidance\n    - Apply precise style modifications\n    - Generate high-quality artistic transformations\n    - Produce refined image edits with better prompt adherence",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxDevRedux",
      "properties": [
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate (1-4)"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxDevRedux.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxDevRedux.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "Output format (jpeg or png)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable safety checker to filter unsafe content"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "How strictly to follow the image structure (1-20)"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform (1-50)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "image_size",
        "guidance_scale"
      ]
    },
    {
      "title": "Flux Kontext Lora Inpaint",
      "description": "Flux Kontext Lora\n    flux, editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxKontextLoraInpaint",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for the image to image task."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKontextLoraInpaint.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "reference_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Reference Image Url",
          "description": "The URL of the reference image for inpainting."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKontextLoraInpaint.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be inpainted."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.88,
          "title": "Strength",
          "description": "The strength of the initial image. Higher strength values are better for this model."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The URL of the mask for inpainting."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Krea Image To Image",
      "description": "FLUX.1 Krea [dev]\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxKreaImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKreaImageToImage.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKreaImageToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "The strength of the initial image. Higher strength values are better for this model."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Krea Lora Image To Image",
      "description": "FLUX.1 Krea [dev] with LoRAs\n    flux, editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxKreaLoraImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKreaLoraImageToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for inpainting. or img2img"
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Krea Lora Inpainting",
      "description": "FLUX.1 Krea [dev] Inpainting with LoRAs\n    flux, editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxKreaLoraInpainting",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKreaLoraInpainting.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for inpainting. or img2img"
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "The mask to area to Inpaint in."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Krea Redux",
      "description": "FLUX.1 Krea [dev] Redux\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxKreaRedux",
      "properties": [
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKreaRedux.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKreaRedux.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Pro Canny",
      "description": "FLUX.1 Pro with Canny edge detection control. Generate images guided by edge maps for precise structural control while maintaining FLUX's quality.\n    image, controlnet, canny, edges, flux, professional\n\n    Use cases:\n    - Generate images following edge structures\n    - Transform images while preserving edges\n    - Create controlled variations with edge guidance\n    - Apply style transfers with structural constraints\n    - Generate content from edge maps",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxProCanny",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the desired output"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProCanny.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProCanny.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "How strictly to follow the prompt"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform"
        },
        {
          "name": "control_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Image Url",
          "description": "The control image URL to generate the Canny edge map from."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image",
        "control_strength"
      ]
    },
    {
      "title": "Flux Pro Depth",
      "description": "FLUX.1 Pro with depth map control. Generate images guided by depth information for precise 3D structure control while maintaining FLUX's quality.\n    image, controlnet, depth, 3d, flux, professional\n\n    Use cases:\n    - Generate images following depth structures\n    - Transform images while preserving 3D composition\n    - Create controlled variations with depth guidance\n    - Apply style transfers with spatial constraints\n    - Generate content from depth maps",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxProDepth",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the desired output"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProDepth.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProDepth.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "How strictly to follow the prompt"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform"
        },
        {
          "name": "control_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Image Url",
          "description": "The control image URL to generate the depth map from."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image",
        "control_strength"
      ]
    },
    {
      "title": "Flux Pro Fill",
      "description": "FLUX.1 Pro Fill provides professional inpainting and outpainting capabilities. Generate or modify image content within masked regions with precise prompt control.\n    image, inpainting, outpainting, fill, flux, professional\n\n    Use cases:\n    - Fill masked regions with new content\n    - Extend images beyond their boundaries (outpainting)\n    - Remove unwanted objects and fill gaps\n    - Generate content-aware image expansions\n    - Create seamless image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxProFill",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt describing what to generate in the masked area"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProFill.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. Needs to match the dimensions of the mask."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProFill.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "Safety tolerance level (1-6, higher is stricter)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The mask URL to inpaint the image. Needs to match the dimensions of the input image."
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image",
        "mask"
      ]
    },
    {
      "title": "Flux Pro Redux",
      "description": "FLUX.1 Pro Redux delivers professional-grade image transformations with the highest quality and safety controls for commercial use.\n    image, transformation, style-transfer, professional, flux, redux\n\n    Use cases:\n    - Create professional-quality image transformations\n    - Apply commercial-grade style transfers\n    - Generate high-fidelity image variations\n    - Produce brand-safe image modifications\n    - Transform images for production use",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxProRedux",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate (1-4)"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProRedux.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "Output format (jpeg or png)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. Needs to match the dimensions of the mask."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProRedux.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "Safety tolerance level (1-6, higher is stricter)"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "How strictly to follow the image structure (1-20)"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform (1-50)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "image_size",
        "guidance_scale"
      ]
    },
    {
      "title": "Flux Schnell Redux",
      "description": "FLUX.1 [schnell] Redux enables rapid transformation of existing images with high-quality style transfers and modifications using the fast FLUX.1 schnell model.\n    image, transformation, style-transfer, fast, flux, redux\n\n    Use cases:\n    - Transform images with artistic style transfers\n    - Apply quick modifications to photos\n    - Create image variations for rapid iteration\n    - Generate stylized versions of existing images\n    - Produce fast image transformations",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxSchnellRedux",
      "properties": [
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate (1-4)"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxSchnellRedux.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "Acceleration speed: 'none', 'regular', or 'high'"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxSchnellRedux.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "Output format (jpeg or png)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable safety checker to filter unsafe content"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform (1-50)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "image_size",
        "num_inference_steps"
      ]
    },
    {
      "title": "Flux Srpo Image To Image",
      "description": "FLUX.1 SRPO [dev]\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxSrpoImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxSrpoImageToImage.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxSrpoImageToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "The strength of the initial image. Higher strength values are better for this model."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Vision Upscaler",
      "description": "Flux Vision Upscaler\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxVisionUpscaler",
      "properties": [
        {
          "name": "guidance",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance",
          "description": "CFG/guidance scale (1-4). Controls how closely the model follows the prompt."
        },
        {
          "name": "creativity",
          "type": {
            "type": "float"
          },
          "default": 0.3,
          "title": "Creativity",
          "description": "The creativity of the model. The higher the creativity, the more the model will deviate from the original. Refers to the denoise strength of the sampling."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to upscale."
        },
        {
          "name": "upscale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Upscale Factor",
          "description": "The upscale factor (1-4x)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed to use for the upscale. If not provided, a random seed will be used."
        },
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 20,
          "title": "Steps",
          "description": "Number of inference steps (4-50)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Gemini 25 Flash Image Edit",
      "description": "Gemini 2.5 Flash Image\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Gemini25FlashImageEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image editing."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "21:9",
              "16:9",
              "3:2",
              "4:3",
              "5:4",
              "1:1",
              "4:5",
              "3:4",
              "2:3",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Gemini25FlashImageEdit.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Gemini25FlashImageEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to use for image-to-image generation or image editing."
        },
        {
          "name": "limit_generations",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Limit Generations",
          "description": "Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Gemini 3 Pro Image Preview Edit",
      "description": "Gemini 3 Pro Image Preview\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Gemini3ProImagePreviewEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image editing."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "enable_web_search",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Web Search",
          "description": "Enable web search for the image generation task. This will allow the model to use the latest information from the web to generate the image."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "1K",
              "2K",
              "4K"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Gemini3ProImagePreviewEdit.Resolution"
          },
          "default": "1K",
          "title": "Resolution",
          "description": "The resolution of the image to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Gemini3ProImagePreviewEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Gemini3ProImagePreviewEdit.SafetyTolerance"
          },
          "default": "4",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for content moderation. 1 is the most strict (blocks most content), 6 is the least strict."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to use for image-to-image generation or image editing."
        },
        {
          "name": "limit_generations",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Limit Generations",
          "description": "Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Glm Image Image To Image",
      "description": "GLM Image image-to-image transforms and modifies images using advanced AI understanding.\n    image, transformation, glm, ai-editing\n\n    Use cases:\n    - Transform images with GLM AI\n    - Apply modifications using advanced understanding\n    - Create variations with GLM model\n    - Generate modified versions\n    - Produce AI-powered transformations",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.GlmImageImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for image generation."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "Output image size."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable NSFW safety checking on the generated images."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GlmImageImageToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "Output image format."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If True, the image will be returned as a base64 data URI instead of a URL."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values make the model follow the prompt more closely."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. The same seed with the same prompt will produce the same image."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "URL(s) of the condition image(s) for image-to-image generation. Supports up to 4 URLs for multi-image references."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "If True, the prompt will be enhanced using an LLM for more detailed and higher quality results."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of diffusion denoising steps. More steps generally produce higher quality images."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Gpt Image 15 Edit",
      "description": "GPT Image 1.5 Edit provides intelligent image editing with GPT-powered understanding and control.\n    image, editing, gpt, intelligent, ai-editing\n\n    Use cases:\n    - Edit images with GPT intelligence\n    - Apply smart modifications to photos\n    - Create intelligent edits\n    - Transform images with language understanding\n    - Produce GPT-powered modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.GptImage15Edit",
      "properties": [
        {
          "name": "background",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "transparent",
              "opaque"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage15Edit.Background"
          },
          "default": "auto",
          "title": "Background",
          "description": "Background for the generated image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "1024x1024",
              "1536x1024",
              "1024x1536"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage15Edit.ImageSize"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "Aspect ratio for the generated image"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation"
        },
        {
          "name": "quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage15Edit.Quality"
          },
          "default": "high",
          "title": "Quality",
          "description": "Quality for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage15Edit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the images"
        },
        {
          "name": "input_fidelity",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage15Edit.InputFidelity"
          },
          "default": "high",
          "title": "Input Fidelity",
          "description": "Input fidelity for the generated image"
        },
        {
          "name": "mask_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image Url",
          "description": "The URL of the mask image to use for the generation. This indicates what part of the image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to use as a reference for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Gpt Image 1 Mini Edit",
      "description": "GPT Image 1 Mini\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.GptImage1MiniEdit",
      "properties": [
        {
          "name": "background",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "transparent",
              "opaque"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage1MiniEdit.Background"
          },
          "default": "auto",
          "title": "Background",
          "description": "Background for the generated image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "1024x1024",
              "1536x1024",
              "1024x1536"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage1MiniEdit.ImageSize"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "Aspect ratio for the generated image"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation"
        },
        {
          "name": "quality",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "low",
              "medium",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage1MiniEdit.Quality"
          },
          "default": "auto",
          "title": "Quality",
          "description": "Quality for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage1MiniEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the images"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to use as a reference for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Hidream E11",
      "description": "Hidream E1 1\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.HidreamE11",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Image Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your initial image when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.HidreamE11.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of an input image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "target_image_description",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Target Image Description",
          "description": "The description of the target image after your edits have been made. Leave this blank to allow the model to use its own imagination."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "low resolution, blur",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Image V3 Instruct Edit",
      "description": "Hunyuan Image v3 Instruct Edit allows precise image editing through natural language instructions with advanced understanding.\n    image, editing, hunyuan, instruct, ai-editing\n\n    Use cases:\n    - Edit images using natural language instructions\n    - Modify specific elements in photos with text commands\n    - Apply precise adjustments through conversational editing\n    - Transform images with instruction-based control\n    - Create variations with detailed text guidance",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.HunyuanImageV3InstructEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The desired size of the generated image. If auto, image size will be determined by the model."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.HunyuanImageV3InstructEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible results. If None, a random seed is used."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to use as a reference for the generation. A maximum of 2 images are supported."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Controls how much the model adheres to the prompt. Higher values mean stricter adherence."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Hunyuan World",
      "description": "Hunyuan World\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Hunyuan_World",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for the panorama generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to convert to a panorama."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ideogram Character",
      "description": "Ideogram V3 Character\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramCharacter",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to fill the masked part of the image."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The resolution of the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "AUTO",
              "REALISTIC",
              "FICTION"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramCharacter.Style"
          },
          "default": "AUTO",
          "title": "Style",
          "description": "The style type to generate with. Cannot be used with style_codes."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Determine if MagicPrompt should be used in generating the request or not."
        },
        {
          "name": "rendering_speed",
          "type": {
            "type": "enum",
            "values": [
              "TURBO",
              "BALANCED",
              "QUALITY"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramCharacter.RenderingSpeed"
          },
          "default": "BALANCED",
          "title": "Rendering Speed",
          "description": "The rendering speed to use."
        },
        {
          "name": "reference_mask_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Mask Urls",
          "description": "A set of masks to apply to the character references. Currently only 1 mask is supported, rest will be ignored. (maximum total size 10MB across all character references). The masks should be in JPEG, PNG or WebP format"
        },
        {
          "name": "reference_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Image Urls",
          "description": "A set of images to use as character references. Currently only 1 image is supported, rest will be ignored. (maximum total size 10MB across all character references). The images should be in JPEG, PNG or WebP format"
        },
        {
          "name": "image_urls",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Urls",
          "description": "A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "style_codes",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Codes",
          "description": "A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style"
        },
        {
          "name": "color_palette",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Color Palette",
          "description": "A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ideogram Character Edit",
      "description": "Ideogram V3 Character Edit\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramCharacterEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to fill the masked part of the image."
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "AUTO",
              "REALISTIC",
              "FICTION"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramCharacterEdit.Style"
          },
          "default": "AUTO",
          "title": "Style",
          "description": "The style type to generate with. Cannot be used with style_codes."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Determine if MagicPrompt should be used in generating the request or not."
        },
        {
          "name": "rendering_speed",
          "type": {
            "type": "enum",
            "values": [
              "TURBO",
              "BALANCED",
              "QUALITY"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramCharacterEdit.RenderingSpeed"
          },
          "default": "BALANCED",
          "title": "Rendering Speed",
          "description": "The rendering speed to use."
        },
        {
          "name": "reference_mask_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Mask Urls",
          "description": "A set of masks to apply to the character references. Currently only 1 mask is supported, rest will be ignored. (maximum total size 10MB across all character references). The masks should be in JPEG, PNG or WebP format"
        },
        {
          "name": "reference_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Image Urls",
          "description": "A set of images to use as character references. Currently only 1 image is supported, rest will be ignored. (maximum total size 10MB across all character references). The images should be in JPEG, PNG or WebP format"
        },
        {
          "name": "image_urls",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Urls",
          "description": "A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. MUST have the exact same dimensions (width and height) as the mask image."
        },
        {
          "name": "style_codes",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Codes",
          "description": "A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style"
        },
        {
          "name": "color_palette",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Color Palette",
          "description": "A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The mask URL to inpaint the image. MUST have the exact same dimensions (width and height) as the input image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ideogram Character Remix",
      "description": "Ideogram V3 Character Remix\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramCharacterRemix",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to remix the image with"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The resolution of the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "AUTO",
              "REALISTIC",
              "FICTION"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramCharacterRemix.Style"
          },
          "default": "AUTO",
          "title": "Style",
          "description": "The style type to generate with. Cannot be used with style_codes."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Determine if MagicPrompt should be used in generating the request or not."
        },
        {
          "name": "rendering_speed",
          "type": {
            "type": "enum",
            "values": [
              "TURBO",
              "BALANCED",
              "QUALITY"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramCharacterRemix.RenderingSpeed"
          },
          "default": "BALANCED",
          "title": "Rendering Speed",
          "description": "The rendering speed to use."
        },
        {
          "name": "reference_mask_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Mask Urls",
          "description": "A set of masks to apply to the character references. Currently only 1 mask is supported, rest will be ignored. (maximum total size 10MB across all character references). The masks should be in JPEG, PNG or WebP format"
        },
        {
          "name": "reference_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Image Urls",
          "description": "A set of images to use as character references. Currently only 1 image is supported, rest will be ignored. (maximum total size 10MB across all character references). The images should be in JPEG, PNG or WebP format"
        },
        {
          "name": "image_urls",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Urls",
          "description": "A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to remix"
        },
        {
          "name": "style_codes",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Codes",
          "description": "A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style"
        },
        {
          "name": "color_palette",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Color Palette",
          "description": "A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength of the input image in the remix"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ideogram V2 Edit",
      "description": "Transform existing images with Ideogram V2's editing capabilities. Modify, adjust, and refine images while maintaining high fidelity with precise prompt and mask control.\n    image, editing, inpainting, mask, ideogram, transformation\n\n    Use cases:\n    - Edit specific parts of images with precision\n    - Create targeted image modifications using masks\n    - Refine and enhance image details\n    - Generate contextual image edits\n    - Replace or modify masked regions",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramV2Edit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to fill the masked part of the image"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "general",
              "realistic",
              "design",
              "render_3D",
              "anime"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV2Edit.Style"
          },
          "default": "auto",
          "title": "Style",
          "description": "Style of generated image (auto, general, realistic, design, render_3D, anime)"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt with MagicPrompt functionality"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. Needs to match the dimensions of the mask."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The mask URL to inpaint the image. Needs to match the dimensions of the input image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image",
        "mask"
      ]
    },
    {
      "title": "Ideogram V2 Remix",
      "description": "Reimagine existing images with Ideogram V2's remix feature. Create variations and adaptations while preserving core elements through prompt guidance and strength control.\n    image, remix, variation, creativity, ideogram, adaptation\n\n    Use cases:\n    - Create artistic variations of images\n    - Generate style-transferred versions\n    - Produce creative image adaptations\n    - Transform images while preserving key elements\n    - Generate alternative interpretations",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramV2Remix",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to remix the image with"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "10:16",
              "16:10",
              "9:16",
              "16:9",
              "4:3",
              "3:4",
              "1:1",
              "1:3",
              "3:1",
              "3:2",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV2Remix.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "general",
              "realistic",
              "design",
              "render_3D",
              "anime"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV2Remix.Style"
          },
          "default": "auto",
          "title": "Style",
          "description": "Style of generated image (auto, general, realistic, design, render_3D, anime)"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt with MagicPrompt functionality"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to remix"
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength of the input image in the remix (0-1, higher = more variation)"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image",
        "strength"
      ]
    },
    {
      "title": "Ideogram V3 Edit",
      "description": "Transform images with Ideogram V3's enhanced editing capabilities. Latest generation editing with improved quality, control, and style consistency.\n    image, editing, inpainting, mask, ideogram, v3\n\n    Use cases:\n    - Edit images with the latest Ideogram technology\n    - Apply high-fidelity masked edits\n    - Generate professional image modifications\n    - Create precise content-aware fills\n    - Refine image details with advanced controls",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramV3Edit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to fill the masked part of the image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "style_preset",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Preset",
          "description": "Style preset for generation. The chosen style preset will guide the generation."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Determine if MagicPrompt should be used in generating the request or not."
        },
        {
          "name": "rendering_speed",
          "type": {
            "type": "enum",
            "values": [
              "TURBO",
              "BALANCED",
              "QUALITY"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV3Edit.RenderingSpeed"
          },
          "default": "BALANCED",
          "title": "Rendering Speed",
          "description": "The rendering speed to use."
        },
        {
          "name": "style_codes",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Codes",
          "description": "A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style"
        },
        {
          "name": "color_palette",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Color Palette",
          "description": "A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. MUST have the exact same dimensions (width and height) as the mask image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "image_urls",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Urls",
          "description": "A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format"
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The mask URL to inpaint the image. MUST have the exact same dimensions (width and height) as the input image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image",
        "mask"
      ]
    },
    {
      "title": "Image 2 Pixel",
      "description": "Image2Pixel\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Image2Pixel",
      "properties": [
        {
          "name": "cleanup_morph",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Cleanup Morph",
          "description": "Apply morphological operations to remove noise."
        },
        {
          "name": "auto_color_detect",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Color Detect",
          "description": "Enable automatic detection of optimal number of colors."
        },
        {
          "name": "alpha_threshold",
          "type": {
            "type": "int"
          },
          "default": 128,
          "title": "Alpha Threshold",
          "description": "Alpha binarization threshold (0-255)."
        },
        {
          "name": "snap_grid",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Snap Grid",
          "description": "Align output to the pixel grid."
        },
        {
          "name": "fixed_palette",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Fixed Palette",
          "description": "Optional fixed color palette as hex strings (e.g., ['#000000', '#ffffff'])."
        },
        {
          "name": "scale",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Scale",
          "description": "Force a specific pixel scale. If None, auto-detect."
        },
        {
          "name": "cleanup_jaggy",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Cleanup Jaggy",
          "description": "Remove isolated diagonal pixels (jaggy edge cleanup)."
        },
        {
          "name": "trim_borders",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Trim Borders",
          "description": "Trim borders of the image."
        },
        {
          "name": "background_tolerance",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Background Tolerance",
          "description": "Background tolerance (0-255)."
        },
        {
          "name": "detect_method",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "runs",
              "edge"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Image2Pixel.DetectMethod"
          },
          "default": "auto",
          "title": "Detect Method",
          "description": "Scale detection method to use."
        },
        {
          "name": "transparent_background",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Transparent Background",
          "description": "Remove background of the image. This will check for contiguous color regions from the edges after correction and make them transparent."
        },
        {
          "name": "downscale_method",
          "type": {
            "type": "enum",
            "values": [
              "dominant",
              "median",
              "mode",
              "mean",
              "content-adaptive"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Image2Pixel.DownscaleMethod"
          },
          "default": "dominant",
          "title": "Downscale Method",
          "description": "Downscaling method to produce the pixel-art output."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to process into improved pixel art"
        },
        {
          "name": "background_mode",
          "type": {
            "type": "enum",
            "values": [
              "edges",
              "corners",
              "midpoints"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Image2Pixel.BackgroundMode"
          },
          "default": "corners",
          "title": "Background Mode",
          "description": "Controls where to flood-fill from when removing the background."
        },
        {
          "name": "max_colors",
          "type": {
            "type": "int"
          },
          "default": 32,
          "title": "Max Colors",
          "description": "Maximum number of colors in the output palette. Set None to disable limit."
        },
        {
          "name": "dominant_color_threshold",
          "type": {
            "type": "float"
          },
          "default": 0.05,
          "title": "Dominant Color Threshold",
          "description": "Dominant color threshold (0.0-1.0)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Age Modify",
      "description": "Age Modify\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2AgeModify",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Portrait image URL for age modification"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output (default: 3:4 for portraits)"
        },
        {
          "name": "preserve_identity",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Identity"
        },
        {
          "name": "target_age",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Target Age"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 City Teleport",
      "description": "City Teleport\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2CityTeleport",
      "properties": [
        {
          "name": "city_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "City Image Url",
          "description": "Optional city background image URL. When provided, the person will be blended into this custom scene."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "city_name",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "City Name",
          "description": "City name (used when city_image_url is not provided)"
        },
        {
          "name": "photo_shot",
          "type": {
            "type": "enum",
            "values": [
              "extreme_close_up",
              "close_up",
              "medium_close_up",
              "medium_shot",
              "medium_long_shot",
              "long_shot",
              "extreme_long_shot",
              "full_body"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2CityTeleport.PhotoShot"
          },
          "default": "medium_shot",
          "title": "Photo Shot",
          "description": "Type of photo shot"
        },
        {
          "name": "camera_angle",
          "type": {
            "type": "enum",
            "values": [
              "eye_level",
              "low_angle",
              "high_angle",
              "dutch_angle",
              "birds_eye_view",
              "worms_eye_view",
              "overhead",
              "side_angle"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2CityTeleport.CameraAngle"
          },
          "default": "eye_level",
          "title": "Camera Angle",
          "description": "Camera angle for the shot"
        },
        {
          "name": "person_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Person Image Url",
          "description": "Person photo URL"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Expression Change",
      "description": "Expression Change\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2ExpressionChange",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output (default: 3:4 for portraits)"
        },
        {
          "name": "target_expression",
          "type": {
            "type": "enum",
            "values": [
              "smile",
              "surprise",
              "glare",
              "panic",
              "shyness",
              "laugh",
              "cry",
              "angry",
              "sad",
              "happy",
              "excited",
              "shocked",
              "confused",
              "focused",
              "dreamy",
              "serious",
              "playful",
              "mysterious",
              "confident",
              "thoughtful"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2ExpressionChange.TargetExpression"
          },
          "default": "smile",
          "title": "Target Expression"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Portrait image URL for expression change"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Hair Change",
      "description": "Hair Change\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2HairChange",
      "properties": [
        {
          "name": "target_hairstyle",
          "type": {
            "type": "enum",
            "values": [
              "short_hair",
              "medium_long_hair",
              "long_hair",
              "curly_hair",
              "wavy_hair",
              "high_ponytail",
              "bun",
              "bob_cut",
              "pixie_cut",
              "braids",
              "straight_hair",
              "afro",
              "dreadlocks",
              "buzz_cut",
              "mohawk",
              "bangs",
              "side_part",
              "middle_part"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2HairChange.TargetHairstyle"
          },
          "default": "long_hair",
          "title": "Target Hairstyle"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output (default: 3:4 for portraits)"
        },
        {
          "name": "hair_color",
          "type": {
            "type": "enum",
            "values": [
              "black",
              "dark_brown",
              "light_brown",
              "blonde",
              "platinum_blonde",
              "red",
              "auburn",
              "gray",
              "silver",
              "blue",
              "green",
              "purple",
              "pink",
              "rainbow",
              "natural",
              "highlights",
              "ombre",
              "balayage"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2HairChange.HairColor"
          },
          "default": "natural",
          "title": "Hair Color"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Portrait image URL for hair change"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Headshot Photo",
      "description": "Headshot Generator\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2HeadshotPhoto",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output (default: 3:4 for portraits)"
        },
        {
          "name": "background_style",
          "type": {
            "type": "enum",
            "values": [
              "professional",
              "corporate",
              "clean",
              "gradient"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2HeadshotPhoto.BackgroundStyle"
          },
          "default": "professional",
          "title": "Background Style"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Portrait image URL to convert to professional headshot"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Makeup Application",
      "description": "Makeup Changer\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2MakeupApplication",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output (default: 3:4 for portraits)"
        },
        {
          "name": "intensity",
          "type": {
            "type": "enum",
            "values": [
              "light",
              "medium",
              "heavy",
              "dramatic"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2MakeupApplication.Intensity"
          },
          "default": "medium",
          "title": "Intensity"
        },
        {
          "name": "makeup_style",
          "type": {
            "type": "enum",
            "values": [
              "natural",
              "glamorous",
              "smoky_eyes",
              "bold_lips",
              "no_makeup",
              "remove_makeup",
              "dramatic",
              "bridal",
              "professional",
              "korean_style",
              "artistic"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2MakeupApplication.MakeupStyle"
          },
          "default": "natural",
          "title": "Makeup Style"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Portrait image URL for makeup application"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Object Removal",
      "description": "Object Removal\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2ObjectRemoval",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "object_to_remove",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Object To Remove",
          "description": "Object to remove"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL containing object to remove"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Outpaint",
      "description": "Image Outpaint\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2Outpaint",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional prompt to guide the outpainting. If provided, it will be appended to the base outpaint instruction. Example: 'with a beautiful sunset in the background'"
        },
        {
          "name": "expand_right",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Expand Right",
          "description": "Number of pixels to add as black margin on the right side (0-700)."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "zoom_out_percentage",
          "type": {
            "type": "float"
          },
          "default": 20,
          "title": "Zoom Out Percentage",
          "description": "Percentage to zoom out the image. If set, the image will be scaled down by this percentage and black margins will be added to maintain original size. Example: 50 means the image will be 50% of original size with black margins filling the rest."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "jpg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2Outpaint.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL to outpaint"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If True, the function will wait for the image to be generated and uploaded before returning the response. If False, the function will return immediately and the image will be generated asynchronously."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "expand_left",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Expand Left",
          "description": "Number of pixels to add as black margin on the left side (0-700)."
        },
        {
          "name": "expand_bottom",
          "type": {
            "type": "int"
          },
          "default": 400,
          "title": "Expand Bottom",
          "description": "Number of pixels to add as black margin on the bottom side (0-700)."
        },
        {
          "name": "expand_top",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Expand Top",
          "description": "Number of pixels to add as black margin on the top side (0-700)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Perspective",
      "description": "Perspective Change\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2Perspective",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "target_perspective",
          "type": {
            "type": "enum",
            "values": [
              "front",
              "left_side",
              "right_side",
              "back",
              "top_down",
              "bottom_up",
              "birds_eye",
              "three_quarter_left",
              "three_quarter_right"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2Perspective.TargetPerspective"
          },
          "default": "front",
          "title": "Target Perspective"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL for perspective change"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Photo Restoration",
      "description": "Photo Restoration\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2PhotoRestoration",
      "properties": [
        {
          "name": "enhance_resolution",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enhance Resolution"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output (default: 4:3 for classic photos)"
        },
        {
          "name": "remove_scratches",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Remove Scratches"
        },
        {
          "name": "fix_colors",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Fix Colors"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Old or damaged photo URL to restore"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Photography Effects",
      "description": "Photography Effects\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2PhotographyEffects",
      "properties": [
        {
          "name": "effect_type",
          "type": {
            "type": "enum",
            "values": [
              "film",
              "vintage_film",
              "portrait_photography",
              "fashion_photography",
              "street_photography",
              "sepia_tone",
              "film_grain",
              "light_leaks",
              "vignette_effect",
              "instant_camera",
              "golden_hour",
              "dramatic_lighting",
              "soft_focus",
              "bokeh_effect",
              "high_contrast",
              "double_exposure"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2PhotographyEffects.EffectType"
          },
          "default": "film",
          "title": "Effect Type"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL for photography effects"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Portrait Enhance",
      "description": "Portrait Enhance\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2PortraitEnhance",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output (default: 3:4 for portraits)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Portrait image URL to enhance"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Product Holding",
      "description": "Product Holding\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2ProductHolding",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "product_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Product Image Url",
          "description": "Image URL of the product to be held by the person"
        },
        {
          "name": "person_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Person Image Url",
          "description": "Image URL of the person who will hold the product"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Product Photography",
      "description": "Product Photography\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2ProductPhotography",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "product_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Product Image Url",
          "description": "Image URL of the product to create professional studio photography"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Relighting",
      "description": "Relighting\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2Relighting",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "lighting_style",
          "type": {
            "type": "enum",
            "values": [
              "natural",
              "studio",
              "golden_hour",
              "blue_hour",
              "dramatic",
              "soft",
              "hard",
              "backlight",
              "side_light",
              "front_light",
              "rim_light",
              "sunset",
              "sunrise",
              "neon",
              "candlelight",
              "moonlight",
              "spotlight",
              "ambient"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2Relighting.LightingStyle"
          },
          "default": "natural",
          "title": "Lighting Style"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL for relighting"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Style Transfer",
      "description": "Style Transfer\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2StyleTransfer",
      "properties": [
        {
          "name": "target_style",
          "type": {
            "type": "enum",
            "values": [
              "anime_character",
              "cartoon_3d",
              "hand_drawn_animation",
              "cyberpunk_future",
              "anime_game_style",
              "comic_book_animation",
              "animated_series",
              "cartoon_animation",
              "lofi_aesthetic",
              "cottagecore",
              "dark_academia",
              "y2k",
              "vaporwave",
              "liminal_space",
              "weirdcore",
              "dreamcore",
              "synthwave",
              "outrun",
              "photorealistic",
              "hyperrealistic",
              "digital_art",
              "concept_art",
              "impressionist",
              "anime",
              "pixel_art",
              "claymation"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2StyleTransfer.TargetStyle"
          },
          "default": "impressionist",
          "title": "Target Style"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "style_reference_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Style Reference Image Url",
          "description": "Optional reference image URL. When provided, the style will be inferred from this image instead of the selected preset style."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL for style transfer"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Texture Transform",
      "description": "Texture Transform\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2TextureTransform",
      "properties": [
        {
          "name": "target_texture",
          "type": {
            "type": "enum",
            "values": [
              "cotton",
              "denim",
              "wool",
              "felt",
              "wood",
              "leather",
              "velvet",
              "stone",
              "marble",
              "ceramic",
              "concrete",
              "brick",
              "clay",
              "foam",
              "glass",
              "metal",
              "silk",
              "fabric",
              "crystal",
              "rubber",
              "plastic",
              "lace"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2TextureTransform.TargetTexture"
          },
          "default": "marble",
          "title": "Target Texture"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL for texture transformation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Virtual Try On",
      "description": "Virtual Try-on\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2VirtualTryOn",
      "properties": [
        {
          "name": "preserve_pose",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Pose"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output (default: 3:4 for fashion)"
        },
        {
          "name": "clothing_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Clothing Image Url",
          "description": "Clothing photo URL"
        },
        {
          "name": "person_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Person Image Url",
          "description": "Person photo URL"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Realism",
      "description": "Image Editing\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingRealism",
      "properties": [
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.6,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to enhance with realism details."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Retouch",
      "description": "Image Editing\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingRetouch",
      "properties": [
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to retouch."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Image O1",
      "description": "Kling Image O1 provides advanced image generation and transformation with optimized quality.\n    image, generation, kling, o1, optimized\n\n    Use cases:\n    - Generate images with Kling O1\n    - Transform images with optimization\n    - Create optimized quality results\n    - Produce advanced image generations\n    - Generate with balanced quality-speed",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.KlingImageO1",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for image generation. Reference images using @Image1, @Image2, etc. (or @Image if only one image). Max 2500 characters."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate (1-9)."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "1K",
              "2K"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.KlingImageO1.Resolution"
          },
          "default": "1K",
          "title": "Resolution",
          "description": "Image generation resolution. 1K: standard, 2K: high-res."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1",
              "4:3",
              "3:4",
              "3:2",
              "2:3",
              "21:9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.KlingImageO1.KlingImageO1AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of generated images. 'auto' intelligently determines based on input content."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.KlingImageO1.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "elements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Elements",
          "description": "Elements (characters/objects) to include in the image. Reference in prompt as @Element1, @Element2, etc. Maximum 10 total (elements + reference images)."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of reference images. Reference images in prompt using @Image1, @Image2, etc. (1-indexed). Max 10 images."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kolors Image To Image",
      "description": "Kolors transforms images using an advanced diffusion model. High-quality image-to-image generation with natural color preservation and detail retention.\n    image, transformation, kolors, diffusion, quality\n\n    Use cases:\n    - Transform images with natural color handling\n    - Create variations with preserved color harmony\n    - Apply modifications with detail retention\n    - Generate style transfers with color consistency\n    - Produce high-fidelity image transformations",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.KolorsImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the desired transformation"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.KolorsImageToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for image to image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "EulerDiscreteScheduler",
              "EulerAncestralDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DPMSolverMultistepScheduler_SDE_karras",
              "UniPCMultistepScheduler",
              "DEISMultistepScheduler"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.KolorsImageToImage.Scheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the model."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "Strength of the transformation (0-1, higher = more change)"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable safety checker."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image",
        "strength"
      ]
    },
    {
      "title": "Longcat Image Edit",
      "description": "Longcat Image Edit transforms images with unique AI-powered modifications and creative control.\n    image, editing, longcat, creative\n\n    Use cases:\n    - Edit images with Longcat AI\n    - Apply creative modifications\n    - Create unique image variations\n    - Transform images creatively\n    - Produce artistic modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.LongcatImageEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image with."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LongcatImageEdit.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LongcatImageEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Lucidflux",
      "description": "Lucidflux\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Lucidflux",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "guidance",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance",
          "description": "The guidance to use for the diffusion process."
        },
        {
          "name": "target_height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Target Height",
          "description": "The height of the output image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to edit."
        },
        {
          "name": "target_width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Target Width",
          "description": "The width of the output image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Seed used for random number generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Moondream 3 Preview Segment",
      "description": "Moondream3 Preview [Segment]\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Moondream3PreviewSegment",
      "properties": [
        {
          "name": "spatial_references",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Spatial References",
          "description": "Spatial references to guide the segmentation. By feeding in references you can help the segmentation process. Must be either list of Point object with x and y members, or list of arrays containing either 2 floats (x,y) or 4 floats (x1,y1,x2,y2). **NOTE**: You can also use the [**point endpoint**](https://fal.ai/models/fal-ai/moondream3-preview/point) to get points for the objects, and pass them in here."
        },
        {
          "name": "settings",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Settings",
          "description": "Sampling settings for the segmentation model"
        },
        {
          "name": "object",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Object",
          "description": "Object to be segmented in the image"
        },
        {
          "name": "preview",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preview",
          "description": "Whether to preview the output and return a binary mask of the image"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be processed Max width: 7000px, Max height: 7000px, Timeout: 20.0s"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Nano Banana Edit",
      "description": "Nano Banana\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.NanoBananaEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image editing."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "21:9",
              "16:9",
              "3:2",
              "4:3",
              "5:4",
              "1:1",
              "4:5",
              "3:4",
              "2:3",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.NanoBananaEdit.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.NanoBananaEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to use for image-to-image generation or image editing."
        },
        {
          "name": "limit_generations",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Limit Generations",
          "description": "Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Nano Banana Pro Edit",
      "description": "Nano Banana Pro\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.NanoBananaProEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image editing."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "enable_web_search",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Web Search",
          "description": "Enable web search for the image generation task. This will allow the model to use the latest information from the web to generate the image."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "1K",
              "2K",
              "4K"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.NanoBananaProEdit.Resolution"
          },
          "default": "1K",
          "title": "Resolution",
          "description": "The resolution of the image to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.NanoBananaProEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.NanoBananaProEdit.SafetyTolerance"
          },
          "default": "4",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for content moderation. 1 is the most strict (blocks most content), 6 is the least strict."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "limit_generations",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Limit Generations",
          "description": "Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to use for image-to-image generation or image editing."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Nextstep 1",
      "description": "Nextstep 1\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Nextstep1",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to edit."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Color Tint",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingColorTint",
      "properties": [
        {
          "name": "tint_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Tint Strength",
          "description": "Tint strength"
        },
        {
          "name": "tint_mode",
          "type": {
            "type": "enum",
            "values": [
              "sepia",
              "red",
              "green",
              "blue",
              "cyan",
              "magenta",
              "yellow",
              "purple",
              "orange",
              "warm",
              "cool",
              "lime",
              "navy",
              "vintage",
              "rose",
              "teal",
              "maroon",
              "peach",
              "lavender",
              "olive"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessingColorTint.TintMode"
          },
          "default": "sepia",
          "title": "Tint Mode",
          "description": "Tint color mode"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Desaturate",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingDesaturate",
      "properties": [
        {
          "name": "desaturate_method",
          "type": {
            "type": "enum",
            "values": [
              "luminance (Rec.709)",
              "luminance (Rec.601)",
              "average",
              "lightness"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessingDesaturate.DesaturateMethod"
          },
          "default": "luminance (Rec.709)",
          "title": "Desaturate Method",
          "description": "Desaturation method"
        },
        {
          "name": "desaturate_factor",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Desaturate Factor",
          "description": "Desaturation factor"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Dissolve",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingDissolve",
      "properties": [
        {
          "name": "dissolve_factor",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Dissolve Factor",
          "description": "Dissolve blend factor"
        },
        {
          "name": "dissolve_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Dissolve Image Url",
          "description": "URL of second image for dissolve"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Dodge Burn",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingDodgeBurn",
      "properties": [
        {
          "name": "dodge_burn_mode",
          "type": {
            "type": "enum",
            "values": [
              "dodge",
              "burn",
              "dodge_and_burn",
              "burn_and_dodge",
              "color_dodge",
              "color_burn",
              "linear_dodge",
              "linear_burn"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessingDodgeBurn.DodgeBurnMode"
          },
          "default": "dodge",
          "title": "Dodge Burn Mode",
          "description": "Dodge and burn mode"
        },
        {
          "name": "dodge_burn_intensity",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Dodge Burn Intensity",
          "description": "Dodge and burn intensity"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Grain",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingGrain",
      "properties": [
        {
          "name": "grain_style",
          "type": {
            "type": "enum",
            "values": [
              "modern",
              "analog",
              "kodak",
              "fuji",
              "cinematic",
              "newspaper"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessingGrain.GrainStyle"
          },
          "default": "modern",
          "title": "Grain Style",
          "description": "Style of film grain to apply"
        },
        {
          "name": "grain_intensity",
          "type": {
            "type": "float"
          },
          "default": 0.4,
          "title": "Grain Intensity",
          "description": "Film grain intensity"
        },
        {
          "name": "grain_scale",
          "type": {
            "type": "float"
          },
          "default": 10,
          "title": "Grain Scale",
          "description": "Film grain scale"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Parabolize",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingParabolize",
      "properties": [
        {
          "name": "parabolize_coeff",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Parabolize Coeff",
          "description": "Parabolize coefficient"
        },
        {
          "name": "vertex_y",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Vertex Y",
          "description": "Vertex Y position"
        },
        {
          "name": "vertex_x",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Vertex X",
          "description": "Vertex X position"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Sharpen",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingSharpen",
      "properties": [
        {
          "name": "sharpen_mode",
          "type": {
            "type": "enum",
            "values": [
              "basic",
              "smart",
              "cas"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessingSharpen.SharpenMode"
          },
          "default": "basic",
          "title": "Sharpen Mode",
          "description": "Type of sharpening to apply"
        },
        {
          "name": "sharpen_alpha",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Sharpen Alpha",
          "description": "Sharpen strength (for basic mode)"
        },
        {
          "name": "noise_radius",
          "type": {
            "type": "int"
          },
          "default": 7,
          "title": "Noise Radius",
          "description": "Noise radius for smart sharpen"
        },
        {
          "name": "sharpen_radius",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Sharpen Radius",
          "description": "Sharpen radius (for basic mode)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        },
        {
          "name": "smart_sharpen_strength",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Smart Sharpen Strength",
          "description": "Smart sharpen strength"
        },
        {
          "name": "cas_amount",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Cas Amount",
          "description": "CAS sharpening amount"
        },
        {
          "name": "preserve_edges",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Preserve Edges",
          "description": "Edge preservation factor"
        },
        {
          "name": "smart_sharpen_ratio",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Smart Sharpen Ratio",
          "description": "Smart sharpen blend ratio"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Solarize",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingSolarize",
      "properties": [
        {
          "name": "solarize_threshold",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Solarize Threshold",
          "description": "Solarize threshold"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Vignette",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingVignette",
      "properties": [
        {
          "name": "vignette_strength",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Vignette Strength",
          "description": "Vignette strength"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit",
      "description": "Qwen Image Edit\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509",
      "description": "Qwen Image Edit 2509 provides powerful image editing with advanced AI capabilities and high-quality output.\n    image, editing, qwen, 2509, ai-editing\n\n    Use cases:\n    - Edit images with Qwen 2509 technology\n    - Apply sophisticated modifications to photos\n    - Create quality edits with AI assistance\n    - Transform images with advanced models\n    - Produce professional image changes",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to edit."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora",
      "description": "Qwen Image Edit 2509 with LoRA enables fine-tuned models for specialized image editing applications.\n    image, editing, qwen, lora, fine-tuned\n\n    Use cases:\n    - Edit images with fine-tuned models\n    - Apply custom modifications using LoRA\n    - Create specialized edits for specific domains\n    - Transform images with trained models\n    - Produce tailored image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509Lora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used to calculate the size of the output image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509Lora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509Lora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to edit."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Add Background",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryAddBackground",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Remove white background and add a realistic scene behind the object",
          "title": "Prompt",
          "description": "Describe the background/scene you want to add behind the object. The model will remove the white background and add the specified environment."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryAddBackground.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryAddBackground.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to edit. Provide an image with a white or clean background."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Face To Full Portrait",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryFaceToFullPortrait",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Photography. A portrait of the person in professional attire with natural lighting",
          "title": "Prompt",
          "description": "Describe the full portrait you want to generate from the face. Include clothing, setting, pose, and style details."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryFaceToFullPortrait.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryFaceToFullPortrait.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the cropped face image. Provide a close-up face photo."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Group Photo",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryGroupPhoto",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Two people standing next to each other outside with a landscape background",
          "title": "Prompt",
          "description": "Describe the group photo scene, setting, and style. The model will maintain character consistency and add vintage effects like grain, blur, and retro filters."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryGroupPhoto.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryGroupPhoto.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to combine into a group photo. Provide 2 or more individual portrait images."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Integrate Product",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryIntegrateProduct",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Blend and integrate the product into the background",
          "title": "Prompt",
          "description": "Describe how to blend and integrate the product/element into the background. The model will automatically correct perspective, lighting and shadows for natural integration."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryIntegrateProduct.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryIntegrateProduct.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image with product to integrate into background."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Lighting Restoration",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryLightingRestoration",
      "properties": [
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryLightingRestoration.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryLightingRestoration.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image to restore lighting for."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Multiple Angles",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryMultipleAngles",
      "properties": [
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "wide_angle_lens",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Wide Angle Lens",
          "description": "Enable wide-angle lens effect"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryMultipleAngles.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image to adjust camera angle for."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "vertical_angle",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Vertical Angle",
          "description": "Adjust vertical camera angle (-1=bird's-eye view/looking down, 0=neutral, 1=worm's-eye view/looking up)"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "move_forward",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Move Forward",
          "description": "Move camera forward (0=no movement, 10=close-up)"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryMultipleAngles.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "rotate_right_left",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Rotate Right Left",
          "description": "Rotate camera left (positive) or right (negative) in degrees. Positive values rotate left, negative values rotate right."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1.25,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the camera control effect."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Next Scene",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryNextScene",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Next Scene: The camera moves forward revealing more of the scene",
          "title": "Prompt",
          "description": "Describe the camera movement, framing change, or scene transition. Start with 'Next Scene:' for best results. Examples: camera movements (dolly, push-in, pull-back), framing changes (wide to close-up), new elements entering frame."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryNextScene.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryNextScene.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image to create the next scene from."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Remove Element",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryRemoveElement",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Remove the specified element from the scene",
          "title": "Prompt",
          "description": "Specify what element(s) to remove from the image (objects, people, text, etc.). The model will cleanly remove the element while maintaining consistency of the rest of the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryRemoveElement.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryRemoveElement.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image containing elements to remove."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Remove Lighting",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryRemoveLighting",
      "properties": [
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryRemoveLighting.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryRemoveLighting.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image with lighting/shadows to remove."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Shirt Design",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryShirtDesign",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Put this design on their shirt",
          "title": "Prompt",
          "description": "Describe what design to put on the shirt. The model will apply the design from your input image onto the person's shirt."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryShirtDesign.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryShirtDesign.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images: first image is the person wearing a shirt, second image is the design/logo to put on the shirt."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2511",
      "description": "Qwen Image Edit 2511 provides state-of-the-art image editing with latest AI advancements and improved quality.\n    image, editing, qwen, 2511, latest\n\n    Use cases:\n    - Edit images with latest Qwen technology\n    - Apply advanced modifications to photos\n    - Create high-quality edits with AI assistance\n    - Transform images with cutting-edge models\n    - Produce professional image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2511",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image with."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2511.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If None, uses the input image dimensions."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2511.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to edit."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate an image from."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2511 Lora",
      "description": "Qwen Image Edit 2511 with LoRA support enables custom-trained models for specialized editing tasks.\n    image, editing, qwen, lora, custom\n\n    Use cases:\n    - Edit images with custom-trained models\n    - Apply specialized modifications using LoRA\n    - Create domain-specific edits\n    - Transform images with fine-tuned models\n    - Produce customized image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2511Lora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image with."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2511Lora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If None, uses the input image dimensions."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2511Lora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to edit."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate an image from."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2511 Multiple Angles",
      "description": "Qwen Image Edit 2511 Multiple Angles generates images from different viewpoints based on a single input image.\n    image, editing, qwen, multi-angle, viewpoint\n\n    Use cases:\n    - Generate multiple viewpoints from single image\n    - Create product views from different angles\n    - Visualize objects from various perspectives\n    - Produce multi-angle image sets\n    - Transform images to show different sides",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2511MultipleAngles",
      "properties": [
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2511MultipleAngles.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the input image will be used."
        },
        {
          "name": "horizontal_angle",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Horizontal Angle",
          "description": "Horizontal rotation angle around the object in degrees. 0\u00b0=front view, 90\u00b0=right side, 180\u00b0=back view, 270\u00b0=left side, 360\u00b0=front view again."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image to adjust camera angle for."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "zoom",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Zoom",
          "description": "Camera zoom/distance. 0=wide shot (far away), 5=medium shot (normal), 10=close-up (very close)."
        },
        {
          "name": "vertical_angle",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Vertical Angle",
          "description": "Vertical camera angle in degrees. -30\u00b0=low-angle shot (looking up), 0\u00b0=eye-level, 30\u00b0=elevated, 60\u00b0=high-angle, 90\u00b0=bird's-eye view (looking down)."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the camera control effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2511MultipleAngles.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI."
        },
        {
          "name": "additional_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Additional Prompt",
          "description": "Additional text to append to the automatically generated prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Image To Image",
      "description": "Qwen Image Edit\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditImageToImage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditImageToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.94,
          "title": "Strength",
          "description": "Strength of the image-to-image transformation. Lower values preserve more of the original image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Inpaint",
      "description": "Qwen Image Edit\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditInpaint",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditInpaint.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditInpaint.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.93,
          "title": "Strength",
          "description": "Strength of noising process for inpainting"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "The URL of the mask for inpainting"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Lora",
      "description": "Qwen Image Edit Lora\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditLora.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus",
      "description": "Qwen Image Edit Plus\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlus",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlus.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlus.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to edit."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora",
      "description": "Qwen Image Edit Plus Lora\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used to calculate the size of the output image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to edit."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Add Background",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryAddBackground",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Remove white background and add a realistic scene behind the object",
          "title": "Prompt",
          "description": "Describe the background/scene you want to add behind the object. The model will remove the white background and add the specified environment."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryAddBackground.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryAddBackground.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to edit. Provide an image with a white or clean background."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Face To Full Portrait",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryFaceToFullPortrait",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Photography. A portrait of the person in professional attire with natural lighting",
          "title": "Prompt",
          "description": "Describe the full portrait you want to generate from the face. Include clothing, setting, pose, and style details."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryFaceToFullPortrait.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryFaceToFullPortrait.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the cropped face image. Provide a close-up face photo."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Group Photo",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryGroupPhoto",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Two people standing next to each other outside with a landscape background",
          "title": "Prompt",
          "description": "Describe the group photo scene, setting, and style. The model will maintain character consistency and add vintage effects like grain, blur, and retro filters."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryGroupPhoto.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryGroupPhoto.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to combine into a group photo. Provide 2 or more individual portrait images."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Integrate Product",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryIntegrateProduct",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Blend and integrate the product into the background",
          "title": "Prompt",
          "description": "Describe how to blend and integrate the product/element into the background. The model will automatically correct perspective, lighting and shadows for natural integration."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryIntegrateProduct.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryIntegrateProduct.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image with product to integrate into background."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Lighting Restoration",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryLightingRestoration",
      "properties": [
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryLightingRestoration.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryLightingRestoration.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image to restore lighting for."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Multiple Angles",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryMultipleAngles",
      "properties": [
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "wide_angle_lens",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Wide Angle Lens",
          "description": "Enable wide-angle lens effect"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryMultipleAngles.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image to adjust camera angle for."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "vertical_angle",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Vertical Angle",
          "description": "Adjust vertical camera angle (-1=bird's-eye view/looking down, 0=neutral, 1=worm's-eye view/looking up)"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "move_forward",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Move Forward",
          "description": "Move camera forward (0=no movement, 10=close-up)"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryMultipleAngles.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "rotate_right_left",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Rotate Right Left",
          "description": "Rotate camera left (positive) or right (negative) in degrees. Positive values rotate left, negative values rotate right."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1.25,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the camera control effect."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Next Scene",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryNextScene",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Next Scene: The camera moves forward revealing more of the scene",
          "title": "Prompt",
          "description": "Describe the camera movement, framing change, or scene transition. Start with 'Next Scene:' for best results. Examples: camera movements (dolly, push-in, pull-back), framing changes (wide to close-up), new elements entering frame."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryNextScene.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryNextScene.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image to create the next scene from."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Remove Element",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryRemoveElement",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Remove the specified element from the scene",
          "title": "Prompt",
          "description": "Specify what element(s) to remove from the image (objects, people, text, etc.). The model will cleanly remove the element while maintaining consistency of the rest of the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryRemoveElement.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryRemoveElement.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image containing elements to remove."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Remove Lighting",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryRemoveLighting",
      "properties": [
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryRemoveLighting.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryRemoveLighting.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image with lighting/shadows to remove."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Shirt Design",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryShirtDesign",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Put this design on their shirt",
          "title": "Prompt",
          "description": "Describe what design to put on the shirt. The model will apply the design from your input image onto the person's shirt."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryShirtDesign.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryShirtDesign.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images: first image is the person wearing a shirt, second image is the design/logo to put on the shirt."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Image To Image",
      "description": "Qwen Image\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageImageToImage.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular', 'high'. Higher acceleration increases speed. 'regular' balances speed and quality. 'high' is recommended for images without text."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. By default, we will use the provided image for determining the image_size."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "use_turbo",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Turbo",
          "description": "Enable turbo mode for faster generation with high quality. When enabled, uses optimized settings (10 steps, CFG=1.2)."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageImageToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The reference image to guide the generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.6,
          "title": "Strength",
          "description": "Denoising strength. 1.0 = fully remake; 0.0 = preserve original."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Layered",
      "description": "Qwen Image Layered provides layer-based image editing for complex compositions and precise control.\n    image, editing, qwen, layered, composition\n\n    Use cases:\n    - Edit images with layer-based control\n    - Create complex compositions\n    - Apply modifications to specific layers\n    - Build multi-layer image edits\n    - Produce sophisticated image compositions",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageLayered",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "A caption for the input image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageLayered.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "num_layers",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Layers",
          "description": "The number of layers to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageLayered.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the input image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate an image from."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Layered Lora",
      "description": "Qwen Image Layered with LoRA combines layer-based editing with custom-trained models for specialized tasks.\n    image, editing, qwen, layered, lora\n\n    Use cases:\n    - Edit layered images with custom models\n    - Create specialized layer compositions\n    - Apply fine-tuned modifications\n    - Build complex edits with trained models\n    - Produce custom layer-based results",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageLayeredLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "A caption for the input image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageLayeredLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "num_layers",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Layers",
          "description": "The number of layers to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageLayeredLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the input image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate an image from."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Max Edit",
      "description": "Qwen Image Max Edit provides powerful image editing capabilities with advanced AI understanding and high-quality results.\n    image, editing, qwen, max, ai-editing\n\n    Use cases:\n    - Edit images with advanced AI understanding\n    - Apply complex modifications to photos\n    - Transform images with high-quality results\n    - Create professional edits with natural prompts\n    - Modify images with precise control",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageMaxEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the desired image. Supports Chinese and English. Max 800 characters."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageMaxEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Enable LLM prompt optimization for better results."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility (0-2147483647)."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Reference images for editing (1-3 images required). Order matters: reference as 'image 1', 'image 2', 'image 3' in prompt. Resolution: 384-5000px each dimension. Max size: 10MB each. Formats: JPEG, JPG, PNG (no alpha), WEBP."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Content to avoid in the generated image. Max 500 characters."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable content moderation for input and output."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Recraft V3 Image To Image",
      "description": "Recraft V3 transforms images with advanced style control and quality preservation. Professional-grade image-to-image generation with fine-tuned artistic control.\n    image, transformation, recraft, style, professional\n\n    Use cases:\n    - Transform images with precise style control\n    - Create high-quality image variations\n    - Apply artistic modifications with consistency\n    - Generate professional design alternatives\n    - Produce style-coherent image transformations",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.RecraftV3ImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the desired transformation"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "any",
              "realistic_image",
              "digital_illustration",
              "vector_illustration",
              "realistic_image/b_and_w",
              "realistic_image/hard_flash",
              "realistic_image/hdr",
              "realistic_image/natural_light",
              "realistic_image/studio_portrait",
              "realistic_image/enterprise",
              "realistic_image/motion_blur",
              "realistic_image/evening_light",
              "realistic_image/faded_nostalgia",
              "realistic_image/forest_life",
              "realistic_image/mystic_naturalism",
              "realistic_image/natural_tones",
              "realistic_image/organic_calm",
              "realistic_image/real_life_glow",
              "realistic_image/retro_realism",
              "realistic_image/retro_snapshot",
              "realistic_image/urban_drama",
              "realistic_image/village_realism",
              "realistic_image/warm_folk",
              "digital_illustration/pixel_art",
              "digital_illustration/hand_drawn",
              "digital_illustration/grain",
              "digital_illustration/infantile_sketch",
              "digital_illustration/2d_art_poster",
              "digital_illustration/handmade_3d",
              "digital_illustration/hand_drawn_outline",
              "digital_illustration/engraving_color",
              "digital_illustration/2d_art_poster_2",
              "digital_illustration/antiquarian",
              "digital_illustration/bold_fantasy",
              "digital_illustration/child_book",
              "digital_illustration/child_books",
              "digital_illustration/cover",
              "digital_illustration/crosshatch",
              "digital_illustration/digital_engraving",
              "digital_illustration/expressionism",
              "digital_illustration/freehand_details",
              "digital_illustration/grain_20",
              "digital_illustration/graphic_intensity",
              "digital_illustration/hard_comics",
              "digital_illustration/long_shadow",
              "digital_illustration/modern_folk",
              "digital_illustration/multicolor",
              "digital_illustration/neon_calm",
              "digital_illustration/noir",
              "digital_illustration/nostalgic_pastel",
              "digital_illustration/outline_details",
              "digital_illustration/pastel_gradient",
              "digital_illustration/pastel_sketch",
              "digital_illustration/pop_art",
              "digital_illustration/pop_renaissance",
              "digital_illustration/street_art",
              "digital_illustration/tablet_sketch",
              "digital_illustration/urban_glow",
              "digital_illustration/urban_sketching",
              "digital_illustration/vanilla_dreams",
              "digital_illustration/young_adult_book",
              "digital_illustration/young_adult_book_2",
              "vector_illustration/bold_stroke",
              "vector_illustration/chemistry",
              "vector_illustration/colored_stencil",
              "vector_illustration/contour_pop_art",
              "vector_illustration/cosmics",
              "vector_illustration/cutout",
              "vector_illustration/depressive",
              "vector_illustration/editorial",
              "vector_illustration/emotional_flat",
              "vector_illustration/infographical",
              "vector_illustration/marker_outline",
              "vector_illustration/mosaic",
              "vector_illustration/naivector",
              "vector_illustration/roundish_flat",
              "vector_illustration/segmented_colors",
              "vector_illustration/sharp_contrast",
              "vector_illustration/thin",
              "vector_illustration/vector_photo",
              "vector_illustration/vivid_shapes",
              "vector_illustration/engraving",
              "vector_illustration/line_art",
              "vector_illustration/line_circuit",
              "vector_illustration/linocut"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.RecraftV3ImageToImage.RecraftV3ImageToImageStyle"
          },
          "default": "realistic_image",
          "title": "Style",
          "description": "The artistic style to apply"
        },
        {
          "name": "style_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Id",
          "description": "The ID of the custom style reference (optional)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to modify. Must be less than 5 MB in size, have resolution less than 16 MP and max dimension less than 4096 pixels."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Strength",
          "description": "Defines the difference with the original image, should lie in [0, 1], where 0 means almost identical, and 1 means miserable similarity"
        },
        {
          "name": "colors",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Colors",
          "description": "An array of preferable colors"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A text description of undesired elements on an image"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image",
        "style"
      ]
    },
    {
      "title": "Reve Edit",
      "description": "Reve\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ReveEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text description of how to edit the provided image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ReveEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the reference image to edit. Must be publicly accessible or base64 data URI. Supports PNG, JPEG, WebP, AVIF, and HEIF formats."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Reve Fast Edit",
      "description": "Reve\n    editing, transformation, image-to-image, img2img, fast\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ReveFastEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text description of how to edit the provided image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ReveFastEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the reference image to edit. Must be publicly accessible or base64 data URI. Supports PNG, JPEG, WebP, AVIF, and HEIF formats."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Reve Fast Remix",
      "description": "Reve\n    editing, transformation, image-to-image, img2img, fast\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ReveFastRemix",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text description of the desired image. May include XML img tags like <img>0</img> to refer to specific images by their index in the image_urls list."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "16:9",
              "9:16",
              "3:2",
              "2:3",
              "4:3",
              "3:4",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ReveFastRemix.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The desired aspect ratio of the generated image. If not provided, will be smartly chosen by the model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ReveFastRemix.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the generated image."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of URLs of reference images. Must provide between 1 and 6 images (inclusive). Each image must be less than 10 MB. Supports PNG, JPEG, WebP, AVIF, and HEIF formats."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Reve Remix",
      "description": "Reve\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ReveRemix",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text description of the desired image. May include XML img tags like <img>0</img> to refer to specific images by their index in the image_urls list."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "16:9",
              "9:16",
              "3:2",
              "2:3",
              "4:3",
              "3:4",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ReveRemix.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The desired aspect ratio of the generated image. If not provided, will be smartly chosen by the model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ReveRemix.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the generated image."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of URLs of reference images. Must provide between 1 and 6 images (inclusive). Each image must be less than 10 MB. Supports PNG, JPEG, WebP, AVIF, and HEIF formats."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Rife",
      "description": "RIFE\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Rife",
      "properties": [
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Rife.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the output images. Only applicable if output_type is 'images'."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Fps",
          "description": "Frames per second for the output video. Only applicable if output_type is 'video'."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "include_end",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Include End",
          "description": "Whether to include the end image in the output."
        },
        {
          "name": "include_start",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Include Start",
          "description": "Whether to include the start image in the output."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Frames",
          "description": "The number of frames to generate between the input images."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the second image to use as the ending point for interpolation."
        },
        {
          "name": "output_type",
          "type": {
            "type": "enum",
            "values": [
              "images",
              "video"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Rife.OutputType"
          },
          "default": "images",
          "title": "Output Type",
          "description": "The type of output to generate; either individual images or a video."
        },
        {
          "name": "start_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Start Image Url",
          "description": "The URL of the first image to use as the starting point for interpolation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "enum",
            "values": [
              "images",
              "video"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Rife.OutputType"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Sam 3 Image",
      "description": "Segment Anything Model 3\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Sam3Image",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "wheel",
          "title": "Prompt",
          "description": "Text prompt for segmentation"
        },
        {
          "name": "include_boxes",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Include Boxes",
          "description": "Whether to include bounding boxes for each mask (when available)."
        },
        {
          "name": "return_multiple_masks",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Multiple Masks",
          "description": "If True, upload and return multiple generated masks as defined by `max_masks`."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Sam3Image.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be segmented"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If True, the media will be returned as a data URI."
        },
        {
          "name": "point_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Point Prompts",
          "description": "List of point prompts"
        },
        {
          "name": "include_scores",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Include Scores",
          "description": "Whether to include mask confidence scores."
        },
        {
          "name": "max_masks",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "Max Masks",
          "description": "Maximum number of masks to return when `return_multiple_masks` is enabled."
        },
        {
          "name": "box_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Box Prompts",
          "description": "Box prompt coordinates (x_min, y_min, x_max, y_max). Multiple boxes supported - use object_id to group boxes for the same object or leave empty for separate objects."
        },
        {
          "name": "apply_mask",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Apply Mask",
          "description": "Apply the mask on the image."
        },
        {
          "name": "text_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Prompt",
          "description": "[DEPRECATED] Use 'prompt' instead. Kept for backward compatibility."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Sam 3 Image Rle",
      "description": "Sam 3\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Sam3ImageRle",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "wheel",
          "title": "Prompt",
          "description": "Text prompt for segmentation"
        },
        {
          "name": "include_boxes",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Include Boxes",
          "description": "Whether to include bounding boxes for each mask (when available)."
        },
        {
          "name": "return_multiple_masks",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Multiple Masks",
          "description": "If True, upload and return multiple generated masks as defined by `max_masks`."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Sam3ImageRle.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be segmented"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If True, the media will be returned as a data URI."
        },
        {
          "name": "point_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Point Prompts",
          "description": "List of point prompts"
        },
        {
          "name": "include_scores",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Include Scores",
          "description": "Whether to include mask confidence scores."
        },
        {
          "name": "max_masks",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "Max Masks",
          "description": "Maximum number of masks to return when `return_multiple_masks` is enabled."
        },
        {
          "name": "box_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Box Prompts",
          "description": "Box prompt coordinates (x_min, y_min, x_max, y_max). Multiple boxes supported - use object_id to group boxes for the same object or leave empty for separate objects."
        },
        {
          "name": "apply_mask",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Apply Mask",
          "description": "Apply the mask on the image."
        },
        {
          "name": "text_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Prompt",
          "description": "[DEPRECATED] Use 'prompt' instead. Kept for backward compatibility."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Seedvr Upscale Image",
      "description": "SeedVR2\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.SeedvrUpscaleImage",
      "properties": [
        {
          "name": "upscale_mode",
          "type": {
            "type": "enum",
            "values": [
              "target",
              "factor"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.SeedvrUpscaleImage.UpscaleMode"
          },
          "default": "factor",
          "title": "Upscale Mode",
          "description": "The mode to use for the upscale. If 'target', the upscale factor will be calculated based on the target resolution. If 'factor', the upscale factor will be used directly."
        },
        {
          "name": "noise_scale",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Noise Scale",
          "description": "The noise scale to use for the generation process."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.SeedvrUpscaleImage.OutputFormat"
          },
          "default": "jpg",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "target_resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "1440p",
              "2160p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.SeedvrUpscaleImage.TargetResolution"
          },
          "default": "1080p",
          "title": "Target Resolution",
          "description": "The target resolution to upscale to when `upscale_mode` is `target`."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The input image to be processed"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "upscale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Upscale Factor",
          "description": "Upscaling factor to be used. Will multiply the dimensions with this factor when `upscale_mode` is `factor`."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The random seed used for the generation process."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Stepx Edit 2",
      "description": "StepX Edit 2 provides multi-step image editing with progressive refinement and control.\n    image, editing, stepx, progressive, refinement\n\n    Use cases:\n    - Edit images with progressive steps\n    - Apply multi-stage modifications\n    - Create refined edits gradually\n    - Transform images with step control\n    - Produce progressively refined results",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.StepxEdit2",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_reflection_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Reflection Mode",
          "description": "Enable reflection mode. Reviews outputs, corrects unintended changes, and determines when editing is complete."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.StepxEdit2.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. Needs to match the dimensions of the mask."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 6,
          "title": "Guidance Scale",
          "description": "The true CFG scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform. Recommended: 50."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "enable_thinking_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Thinking Mode",
          "description": "Enable thinking mode. Uses multimodal language model knowledge to interpret abstract editing instructions."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Uso",
      "description": "Uso\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Uso",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for generation. Can be empty for pure style transfer."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate in parallel."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Uso.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output image format. PNG preserves transparency, JPEG is smaller."
        },
        {
          "name": "keep_size",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Keep Size",
          "description": "Preserve the layout and dimensions of the input content image. Useful for style transfer."
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "List of image URLs in order: [content_image, style_image, extra_style_image]."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If true, wait for generation and upload before returning. Increases latency but provides immediate access to images."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "How closely to follow the prompt. Higher values stick closer to the prompt."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps. More steps can improve quality but increase generation time."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation. Use same seed for consistent results."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "What you don't want in the image. Use it to exclude unwanted elements, styles, or artifacts."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable NSFW content detection and filtering."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Vidu Q2 Reference To Image",
      "description": "Vidu Q2 Reference-to-Image generates images based on reference images with style and content transfer.\n    image, generation, vidu, reference, style-transfer\n\n    Use cases:\n    - Generate images from references\n    - Transfer style and content\n    - Create reference-based variations\n    - Transform using reference images\n    - Produce style-transferred results",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ViduQ2ReferenceToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation, max 1500 characters"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ViduQ2ReferenceToImage.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the output video"
        },
        {
          "name": "reference_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Image Urls",
          "description": "URLs of the reference images to use for consistent subject appearance"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Vidu Reference To Image",
      "description": "Vidu\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ViduReferenceToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation, max 1500 characters"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ViduReferenceToImage.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the output video"
        },
        {
          "name": "reference_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Image Urls",
          "description": "URLs of the reference images to use for consistent subject appearance"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan 25 Preview Image To Image",
      "description": "Wan 2.5 Image to Image\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Wan25PreviewImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing how to edit the image. Max 2000 characters."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate. Values from 1 to 4."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square",
          "title": "Image Size",
          "description": "The size of the generated image. Width and height must be between 384 and 1440 pixels."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "URLs of images to edit. For single-image editing, provide 1 URL. For multi-reference generation, provide up to 2 URLs. If more than 2 URLs are provided, only the first 2 will be used."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to describe content to avoid. Max 500 characters."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan V26 Image To Image",
      "description": "Wan v2.6 image-to-image provides high-quality image transformations with advanced AI capabilities.\n    image, transformation, wan, v2.6, quality\n\n    Use cases:\n    - Transform images with Wan v2.6\n    - Apply quality modifications to photos\n    - Create high-quality variations\n    - Generate advanced transformations\n    - Produce quality image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.WanV26ImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the desired image. Supports Chinese and English. Max 2000 characters. Example: 'Generate an image using the style of image 1 and background of image 2'."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate (1-4). Directly affects billing cost."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "Output image size. Use presets like 'square_hd', 'landscape_16_9', 'portrait_9_16', or specify exact dimensions with ImageSize(width=1280, height=720). Total pixels must be between 768*768 and 1280*1280."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable content moderation for input and output."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility (0-2147483647). Same seed produces more consistent results."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Reference images for editing (1-3 images required). Order matters: reference as 'image 1', 'image 2', 'image 3' in prompt. Resolution: 384-5000px each dimension. Max size: 10MB each. Formats: JPEG, JPG, PNG (no alpha), BMP, WEBP."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Content to avoid in the generated image. Max 500 characters."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Enable LLM prompt optimization. Significantly improves results for simple prompts but adds 3-4 seconds processing time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan V2 2A 14B Image To Image",
      "description": "Wan\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.WanV2_2A14BImageToImage",
      "properties": [
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Shift"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide image generation."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.WanV2_2A14BImageToImage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "image_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.WanV2_2A14BImageToImage.ImageFormat"
          },
          "default": "jpeg",
          "title": "Image Format",
          "description": "The format of the output image."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.WanV2_2A14BImageToImage.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated image. If 'auto', the aspect ratio will be determined automatically based on the input image."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Strength",
          "description": "Denoising strength. 1.0 = fully remake; 0.0 = preserve original."
        },
        {
          "name": "guidance_scale_2",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale 2",
          "description": "Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "ZImage Turbo Controlnet",
      "description": "Z-Image Turbo ControlNet provides fast controlled image generation with structural guidance.\n    image, controlnet, z-image, turbo, controlled\n\n    Use cases:\n    - Generate images with fast structural control\n    - Apply quick controlled modifications\n    - Create rapid guided generations\n    - Transform images with fast ControlNet\n    - Produce speedy controlled outputs",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ZImageTurboControlnet",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboControlnet.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "control_end",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Control End",
          "description": "The end of the controlnet conditioning."
        },
        {
          "name": "control_start",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Control Start",
          "description": "The start of the controlnet conditioning."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboControlnet.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of Image for ControlNet generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "control_scale",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Control Scale",
          "description": "The scale of the controlnet conditioning."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "preprocess",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "canny",
              "depth",
              "pose"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboControlnet.Preprocess"
          },
          "default": "none",
          "title": "Preprocess",
          "description": "What kind of preprocessing to apply to the image, if any."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "ZImage Turbo Controlnet Lora",
      "description": "Z-Image Turbo ControlNet with LoRA combines fast controlled generation with custom models.\n    image, controlnet, z-image, turbo, lora\n\n    Use cases:\n    - Generate with fast custom ControlNet\n    - Apply quick specialized controlled generation\n    - Create rapid custom guided outputs\n    - Transform images with fast custom control\n    - Produce speedy fine-tuned controlled results",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ZImageTurboControlnetLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboControlnetLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "control_end",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Control End",
          "description": "The end of the controlnet conditioning."
        },
        {
          "name": "control_start",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Control Start",
          "description": "The start of the controlnet conditioning."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboControlnetLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of Image for ControlNet generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "control_scale",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Control Scale",
          "description": "The scale of the controlnet conditioning."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "preprocess",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "canny",
              "depth",
              "pose"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboControlnetLora.Preprocess"
          },
          "default": "none",
          "title": "Preprocess",
          "description": "What kind of preprocessing to apply to the image, if any."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "ZImage Turbo Image To Image",
      "description": "Z-Image Turbo image-to-image provides fast image transformations with quality output.\n    image, transformation, z-image, turbo, fast\n\n    Use cases:\n    - Transform images quickly with Z-Image\n    - Apply fast modifications to photos\n    - Create rapid image variations\n    - Generate speedy transformations\n    - Produce quick image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ZImageTurboImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboImageToImage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboImageToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of Image for Image-to-Image generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.6,
          "title": "Strength",
          "description": "The strength of the image-to-image conditioning."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "ZImage Turbo Image To Image Lora",
      "description": "Z-Image Turbo image-to-image with LoRA enables fast custom-trained model transformations.\n    image, transformation, z-image, turbo, lora\n\n    Use cases:\n    - Transform images with custom Z-Image models\n    - Apply fast specialized modifications\n    - Create rapid custom edits\n    - Generate quick customized transformations\n    - Produce fast fine-tuned modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ZImageTurboImageToImageLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboImageToImageLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboImageToImageLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of Image for Image-to-Image generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.6,
          "title": "Strength",
          "description": "The strength of the image-to-image conditioning."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "ZImage Turbo Inpaint",
      "description": "Z-Image Turbo Inpaint fills masked regions in images quickly with contextually appropriate content.\n    image, inpainting, z-image, turbo, fast\n\n    Use cases:\n    - Fill masked regions in images quickly\n    - Remove unwanted objects fast\n    - Repair image areas with turbo speed\n    - Generate quick inpainting results\n    - Produce rapid contextual fills",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ZImageTurboInpaint",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboInpaint.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "mask_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image Url",
          "description": "URL of Mask for Inpaint generation."
        },
        {
          "name": "control_end",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Control End",
          "description": "The end of the controlnet conditioning."
        },
        {
          "name": "control_start",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Control Start",
          "description": "The start of the controlnet conditioning."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboInpaint.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of Image for Inpaint generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Strength",
          "description": "The strength of the inpaint conditioning."
        },
        {
          "name": "control_scale",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Control Scale",
          "description": "The scale of the controlnet conditioning."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "ZImage Turbo Inpaint Lora",
      "description": "Z-Image Turbo Inpaint with LoRA provides fast custom-trained inpainting for specialized tasks.\n    image, inpainting, z-image, turbo, lora\n\n    Use cases:\n    - Inpaint with custom fast models\n    - Fill regions using specialized training\n    - Repair images with custom inpainting\n    - Generate quick custom fills\n    - Produce rapid specialized inpainting",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ZImageTurboInpaintLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboInpaintLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "mask_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image Url",
          "description": "URL of Mask for Inpaint generation."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "control_end",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Control End",
          "description": "The end of the controlnet conditioning."
        },
        {
          "name": "control_start",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Control Start",
          "description": "The start of the controlnet conditioning."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboInpaintLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of Image for Inpaint generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Strength",
          "description": "The strength of the inpaint conditioning."
        },
        {
          "name": "control_scale",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Control Scale",
          "description": "The scale of the controlnet conditioning."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Eleven Labs Scribe V2",
      "description": "ElevenLabs Scribe V2 provides blazingly fast speech-to-text transcription.\n    audio, transcription, stt, fast, elevenlabs, speech-to-text\n\n    Use cases:\n    - Fast audio transcription\n    - Real-time speech recognition\n    - Quick transcript generation\n    - High-speed audio processing\n    - Rapid speech-to-text conversion",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.ElevenLabsScribeV2",
      "properties": [
        {
          "name": "language_code",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Language Code",
          "description": "Language code of the audio"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to transcribe"
        },
        {
          "name": "diarize",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Diarize",
          "description": "Whether to annotate who is speaking"
        },
        {
          "name": "keyterms",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Keyterms",
          "description": "Words or sentences to bias the model towards transcribing. Up to 100 keyterms, max 50 characters each. Adds 30% premium over base transcription price."
        },
        {
          "name": "tag_audio_events",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Tag Audio Events",
          "description": "Tag audio events like laughter, applause, etc."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Eleven Labs Speech To Text",
      "description": "ElevenLabs Speech to Text transcribes audio to text with high accuracy.\n    audio, transcription, stt, elevenlabs, speech-to-text\n\n    Use cases:\n    - Transcribe audio files\n    - Convert speech to text\n    - Generate transcripts from audio\n    - Extract text from recordings\n    - Create captions from audio",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.ElevenLabsSpeechToText",
      "properties": [
        {
          "name": "language_code",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Language Code",
          "description": "Language code of the audio"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to transcribe"
        },
        {
          "name": "diarize",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Diarize",
          "description": "Whether to annotate who is speaking"
        },
        {
          "name": "tag_audio_events",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Tag Audio Events",
          "description": "Tag audio events like laughter, applause, etc."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Smart Turn",
      "description": "Pipecat's Smart Turn model provides native audio turn detection for conversations.\n    audio, turn-detection, conversation, pipecat, speech-analysis\n\n    Use cases:\n    - Detect conversation turns\n    - Identify speaker changes\n    - Analyze dialogue timing\n    - Detect speech boundaries\n    - Process conversational audio",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.SmartTurn",
      "properties": [
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Speech To Text",
      "description": "General-purpose speech-to-text model for accurate audio transcription.\n    audio, transcription, stt, speech-to-text\n\n    Use cases:\n    - General audio transcription\n    - Convert speech recordings to text\n    - Generate audio transcripts\n    - Process voice recordings\n    - Extract text from speech",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.SpeechToText",
      "properties": [
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "Local filesystem path (or remote URL) to a long audio file"
        },
        {
          "name": "use_pnc",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Pnc",
          "description": "Whether to use Canary's built-in punctuation & capitalization"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Speech To Text Stream",
      "description": "Streaming speech-to-text for real-time audio transcription.\n    audio, transcription, stt, streaming, real-time, speech-to-text\n\n    Use cases:\n    - Real-time transcription\n    - Live audio captioning\n    - Stream audio processing\n    - Continuous speech recognition\n    - Live speech-to-text conversion",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.SpeechToTextStream",
      "properties": [
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "Local filesystem path (or remote URL) to a long audio file"
        },
        {
          "name": "use_pnc",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Pnc",
          "description": "Whether to use Canary's built-in punctuation & capitalization"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio_stream"
      ]
    },
    {
      "title": "Speech To Text Turbo",
      "description": "High-speed speech-to-text model optimized for fast transcription.\n    audio, transcription, stt, turbo, fast, speech-to-text\n\n    Use cases:\n    - Fast audio transcription\n    - Quick speech recognition\n    - Rapid transcript generation\n    - High-speed processing\n    - Efficient speech-to-text",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.SpeechToTextTurbo",
      "properties": [
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "Local filesystem path (or remote URL) to a long audio file"
        },
        {
          "name": "use_pnc",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Pnc",
          "description": "Whether to use Canary's built-in punctuation & capitalization"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Speech To Text Turbo Stream",
      "description": "High-speed streaming speech-to-text for real-time fast transcription.\n    audio, transcription, stt, turbo, streaming, fast, speech-to-text\n\n    Use cases:\n    - Real-time fast transcription\n    - Live fast captioning\n    - High-speed streaming STT\n    - Rapid live transcription\n    - Efficient real-time processing",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.SpeechToTextTurboStream",
      "properties": [
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "Local filesystem path (or remote URL) to a long audio file"
        },
        {
          "name": "use_pnc",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Pnc",
          "description": "Whether to use Canary's built-in punctuation & capitalization"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio_stream"
      ]
    },
    {
      "title": "Whisper",
      "description": "OpenAI's Whisper model for robust multilingual speech recognition.\n    audio, transcription, stt, whisper, multilingual, speech-to-text\n\n    Use cases:\n    - Multilingual transcription\n    - Robust speech recognition\n    - Transcribe multiple languages\n    - Handle noisy audio\n    - International audio processing",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.Whisper",
      "properties": [
        {
          "name": "version",
          "type": {
            "type": "enum",
            "values": [
              "3"
            ],
            "type_name": "nodetool.nodes.fal.speech_to_text.Whisper.Version"
          },
          "default": "3",
          "title": "Version",
          "description": "Version of the model to use. All of the models are the Whisper large variant."
        },
        {
          "name": "batch_size",
          "type": {
            "type": "int"
          },
          "default": 64,
          "title": "Batch Size"
        },
        {
          "name": "language",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "af",
              "am",
              "ar",
              "as",
              "az",
              "ba",
              "be",
              "bg",
              "bn",
              "bo",
              "br",
              "bs",
              "ca",
              "cs",
              "cy",
              "da",
              "de",
              "el",
              "en",
              "es",
              "et",
              "eu",
              "fa",
              "fi",
              "fo",
              "fr",
              "gl",
              "gu",
              "ha",
              "haw",
              "he",
              "hi",
              "hr",
              "ht",
              "hu",
              "hy",
              "id",
              "is",
              "it",
              "ja",
              "jw",
              "ka",
              "kk",
              "km",
              "kn",
              "ko",
              "la",
              "lb",
              "ln",
              "lo",
              "lt",
              "lv",
              "mg",
              "mi",
              "mk",
              "ml",
              "mn",
              "mr",
              "ms",
              "mt",
              "my",
              "ne",
              "nl",
              "nn",
              "no",
              "oc",
              "pa",
              "pl",
              "ps",
              "pt",
              "ro",
              "ru",
              "sa",
              "sd",
              "si",
              "sk",
              "sl",
              "sn",
              "so",
              "sq",
              "sr",
              "su",
              "sv",
              "sw",
              "ta",
              "te",
              "tg",
              "th",
              "tk",
              "tl",
              "tr",
              "tt",
              "uk",
              "ur",
              "uz",
              "vi",
              "yi",
              "yo",
              "zh"
            ],
            "type_name": "nodetool.nodes.fal.speech_to_text.Whisper.Language"
          },
          "default": null,
          "title": "Language",
          "description": "Language of the audio file. If set to null, the language will be automatically detected. Defaults to null. If translate is selected as the task, the audio will be translated to English, regardless of the language selected."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to use for generation. Defaults to an empty string."
        },
        {
          "name": "num_speakers",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Speakers",
          "description": "Number of speakers in the audio file. Defaults to null. If not provided, the number of speakers will be automatically detected."
        },
        {
          "name": "task",
          "type": {
            "type": "enum",
            "values": [
              "transcribe",
              "translate"
            ],
            "type_name": "nodetool.nodes.fal.speech_to_text.Whisper.Task"
          },
          "default": "transcribe",
          "title": "Task",
          "description": "Task to perform on the audio file. Either transcribe or translate."
        },
        {
          "name": "chunk_level",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "segment",
              "word"
            ],
            "type_name": "nodetool.nodes.fal.speech_to_text.Whisper.ChunkLevel"
          },
          "default": "segment",
          "title": "Chunk Level",
          "description": "Level of the chunks to return. Either none, segment or word. `none` would imply that all of the audio will be transcribed without the timestamp tokens, we suggest to switch to `none` if you are not satisfied with the transcription quality, since it will usually improve the quality of the results. Switching to `none` will also provide minor speed ups in the transcription due to less amount of generated tokens. Notice that setting to none will produce **a single chunk with the whole transcription**."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm."
        },
        {
          "name": "diarize",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Diarize",
          "description": "Whether to diarize the audio file. Defaults to false. Setting to true will add costs proportional to diarization inference time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Wizper",
      "description": "Wizper provides fast and accurate speech-to-text transcription.\n    audio, transcription, stt, wizper, fast, speech-to-text\n\n    Use cases:\n    - Fast accurate transcription\n    - Quick speech recognition\n    - Efficient audio processing\n    - Rapid text extraction\n    - Speedy speech-to-text",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.Wizper",
      "properties": [
        {
          "name": "language",
          "type": {
            "type": "str"
          },
          "default": "en",
          "title": "Language",
          "description": "Language of the audio file. If translate is selected as the task, the audio will be translated to English, regardless of the language selected. If `None` is passed, the language will be automatically detected. This will also increase the inference time."
        },
        {
          "name": "version",
          "type": {
            "type": "str"
          },
          "default": "3",
          "title": "Version",
          "description": "Version of the model to use. All of the models are the Whisper large variant."
        },
        {
          "name": "max_segment_len",
          "type": {
            "type": "int"
          },
          "default": 29,
          "title": "Max Segment Len",
          "description": "Maximum speech segment duration in seconds before splitting."
        },
        {
          "name": "task",
          "type": {
            "type": "enum",
            "values": [
              "transcribe",
              "translate"
            ],
            "type_name": "nodetool.nodes.fal.speech_to_text.Wizper.Task"
          },
          "default": "transcribe",
          "title": "Task",
          "description": "Task to perform on the audio file. Either transcribe or translate."
        },
        {
          "name": "chunk_level",
          "type": {
            "type": "str"
          },
          "default": "segment",
          "title": "Chunk Level",
          "description": "Level of the chunks to return."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm."
        },
        {
          "name": "merge_chunks",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Merge Chunks",
          "description": "Whether to merge consecutive chunks. When enabled, chunks are merged if their combined duration does not exceed max_segment_len."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "DWPose",
      "description": "DWPose detects human poses and keypoints in images.\n    pose, detection, keypoints, human\n\n    Use cases:\n    - Detect human poses\n    - Extract body keypoints\n    - Enable pose-guided generation\n    - Analyze body positions\n    - Create pose references",
      "namespace": "fal.segmentation",
      "node_type": "fal.segmentation.DWPose",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to analyze"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "image"
        },
        {
          "type": {
            "type": "list"
          },
          "name": "poses"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Image Preprocessor Depth Anything V2",
      "description": "Depth Anything V2 generates high-quality depth maps from images.\n    depth, preprocessor, depth-map, estimation\n\n    Use cases:\n    - Generate accurate depth maps\n    - Enable depth-aware effects\n    - Create 3D visualizations\n    - Prepare ControlNet inputs\n    - Analyze image depth",
      "namespace": "fal.segmentation",
      "node_type": "fal.segmentation.ImagePreprocessorDepthAnythingV2",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Marigold Depth",
      "description": "Marigold Depth generates high-quality monocular depth maps.\n    depth, marigold, depth-map, estimation\n\n    Use cases:\n    - Generate precise depth maps\n    - Create depth visualizations\n    - Enable depth-based effects\n    - Prepare 3D conversions\n    - Analyze scene depth",
      "namespace": "fal.segmentation",
      "node_type": "fal.segmentation.MarigoldDepth",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "SAM2 Image",
      "description": "SAM 2 Image segments objects in images with high accuracy.\n    segmentation, sam, image, masks\n\n    Use cases:\n    - Segment objects in images\n    - Create object masks\n    - Enable object selection\n    - Generate cutouts\n    - Create selection masks",
      "namespace": "fal.segmentation",
      "node_type": "fal.segmentation.SAM2Image",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to segment"
        },
        {
          "name": "point_coords",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "list",
                "type_args": [
                  {
                    "type": "float"
                  }
                ]
              }
            ]
          },
          "default": [],
          "title": "Point Coords",
          "description": "Point coordinates for prompts [[x, y], ...]"
        },
        {
          "name": "point_labels",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "int"
              }
            ]
          },
          "default": [],
          "title": "Point Labels",
          "description": "Labels for points (1=foreground, 0=background)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list"
          },
          "name": "masks"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "SAM2 Video",
      "description": "SAM 2 Video segments and tracks objects across video frames.\n    segmentation, sam, video, tracking\n\n    Use cases:\n    - Track objects in videos\n    - Create video masks\n    - Segment moving objects\n    - Generate video cutouts\n    - Enable video object selection",
      "namespace": "fal.segmentation",
      "node_type": "fal.segmentation.SAM2Video",
      "properties": [
        {
          "name": "video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video",
          "description": "The video to segment"
        },
        {
          "name": "point_coords",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "list",
                "type_args": [
                  {
                    "type": "float"
                  }
                ]
              }
            ]
          },
          "default": [],
          "title": "Point Coords",
          "description": "Point coordinates for prompts [[x, y], ...]"
        },
        {
          "name": "point_labels",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "int"
              }
            ]
          },
          "default": [],
          "title": "Point Labels",
          "description": "Labels for points (1=foreground, 0=background)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "masks_video"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "SAM3 Image",
      "description": "SAM 3 Image provides advanced segmentation with improved accuracy.\n    segmentation, sam3, image, masks, advanced\n\n    Use cases:\n    - High-accuracy object segmentation\n    - Complex scene segmentation\n    - Precise mask generation\n    - Advanced object selection\n    - Detailed cutout creation",
      "namespace": "fal.segmentation",
      "node_type": "fal.segmentation.SAM3Image",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to segment"
        },
        {
          "name": "point_coords",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "list",
                "type_args": [
                  {
                    "type": "float"
                  }
                ]
              }
            ]
          },
          "default": [],
          "title": "Point Coords",
          "description": "Point coordinates for prompts [[x, y], ...]"
        },
        {
          "name": "point_labels",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "int"
              }
            ]
          },
          "default": [],
          "title": "Point Labels",
          "description": "Labels for points (1=foreground, 0=background)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list"
          },
          "name": "masks"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Fal AI",
      "description": "Dynamic FAL node for running any fal.ai endpoint.\n    fal, schema, dynamic, openapi, inference, runtime, model\n\n    Use cases:\n    - Call new fal.ai endpoints without adding new Python nodes\n    - Prototype workflows with experimental FAL models\n    - Run custom endpoints by sharing model info (llms.txt)\n    - Build flexible pipelines that depend on runtime model selection",
      "namespace": "fal.dynamic_schema",
      "node_type": "fal.dynamic_schema.FalAI",
      "properties": [
        {
          "name": "model_info",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model Info",
          "description": "Paste the full llms.txt from the fal.ai model page (e.g. fal.ai/models/... \u2192 copy all)."
        }
      ],
      "basic_fields": [
        "model_info"
      ],
      "is_dynamic": true,
      "supports_dynamic_outputs": true
    },
    {
      "title": "Era 3D",
      "description": "Era3D creates multi-view consistent 3D models from images.\n    3d, generation, image-to-3d, era3d, multi-view\n\n    Use cases:\n    - Generate multi-view 3D models\n    - Create consistent 3D assets\n    - Produce 3D content with multiple views\n    - Generate detailed 3D models\n    - Create multi-view 3D for games",
      "namespace": "fal.model3d",
      "node_type": "fal.model3d.Era3D",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to convert to 3D"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "Number of inference steps",
          "min": 10.0,
          "max": 100.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list"
          },
          "name": "mv_images"
        },
        {
          "type": {
            "type": "model_3d"
          },
          "name": "model"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Hunyuan 3DV 2",
      "description": "Hunyuan3D V2 generates high-quality 3D models from images.\n    3d, generation, image-to-3d, hunyuan\n\n    Use cases:\n    - Generate detailed 3D models\n    - Create 3D assets from photos\n    - Produce high-quality 3D content\n    - Create 3D visualizations\n    - Generate 3D for productions",
      "namespace": "fal.model3d",
      "node_type": "fal.model3d.Hunyuan3DV2",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to convert to 3D"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation",
          "min": 1.0,
          "max": 10.0
        },
        {
          "name": "octree_resolution",
          "type": {
            "type": "int"
          },
          "default": 256,
          "title": "Octree Resolution",
          "description": "Octree resolution for 3D structure"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "model_3d"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Trellis",
      "description": "Trellis generates 3D models from single images.\n    3d, generation, image-to-3d, trellis\n\n    Use cases:\n    - Generate 3D models from images\n    - Create 3D assets from photos\n    - Produce 3D content for games\n    - Create 3D visualizations\n    - Generate 3D for AR/VR",
      "namespace": "fal.model3d",
      "node_type": "fal.model3d.Trellis",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to convert to 3D"
        },
        {
          "name": "ss_guidance_strength",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Ss Guidance Strength",
          "description": "Guidance strength for sparse structure",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "ss_sampling_steps",
          "type": {
            "type": "int"
          },
          "default": 12,
          "title": "Ss Sampling Steps",
          "description": "Sampling steps for sparse structure",
          "min": 1.0,
          "max": 50.0
        },
        {
          "name": "slat_guidance_strength",
          "type": {
            "type": "float"
          },
          "default": 3.0,
          "title": "Slat Guidance Strength",
          "description": "Guidance strength for structured latent",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "slat_sampling_steps",
          "type": {
            "type": "int"
          },
          "default": 12,
          "title": "Slat Sampling Steps",
          "description": "Sampling steps for structured latent",
          "min": 1.0,
          "max": 50.0
        },
        {
          "name": "mesh_simplify",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Mesh Simplify",
          "description": "Mesh simplification ratio",
          "min": 0.9,
          "max": 0.98
        },
        {
          "name": "texture_size",
          "type": {
            "type": "enum",
            "values": [
              512,
              1024,
              2048
            ],
            "type_name": "nodetool.nodes.fal.model3d.TextureSizeEnum"
          },
          "default": 1024,
          "title": "Texture Size",
          "description": "Texture resolution"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "model_3d"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "texture_size"
      ]
    },
    {
      "title": "Tripo SR",
      "description": "TripoSR generates 3D models from images with fast processing.\n    3d, generation, image-to-3d, triposr, fast\n\n    Use cases:\n    - Quick 3D model generation\n    - Rapid prototyping\n    - Create 3D assets from photos\n    - Generate 3D content quickly\n    - Fast 3D for AR/VR",
      "namespace": "fal.model3d",
      "node_type": "fal.model3d.TripoSR",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The input image to convert to 3D"
        },
        {
          "name": "foreground_ratio",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Foreground Ratio",
          "description": "Foreground ratio for cropping",
          "min": 0.5,
          "max": 1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "model_3d"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "ACEStep",
      "description": "ACE-Step generates music with lyrics from text using advanced audio synthesis.\n    audio, generation, music, lyrics, ace-step, text-to-audio\n\n    Use cases:\n    - Generate songs with lyrics\n    - Create music with vocal tracks\n    - Produce complete songs from text\n    - Generate lyrical content\n    - Create vocal music compositions",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.ACEStep",
      "properties": [
        {
          "name": "number_of_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Number Of Steps",
          "description": "Number of steps to generate the audio."
        },
        {
          "name": "duration",
          "type": {
            "type": "float"
          },
          "default": 60,
          "title": "Duration",
          "description": "The duration of the generated audio in seconds."
        },
        {
          "name": "tags",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Tags",
          "description": "Comma-separated list of genre tags to control the style of the generated audio."
        },
        {
          "name": "minimum_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Minimum Guidance Scale",
          "description": "Minimum guidance scale for the generation after the decay."
        },
        {
          "name": "lyrics",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Lyrics",
          "description": "Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song."
        },
        {
          "name": "tag_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Tag Guidance Scale",
          "description": "Tag guidance scale for the generation."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "euler",
              "heun"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.ACEStep.Scheduler"
          },
          "default": "euler",
          "title": "Scheduler",
          "description": "Scheduler to use for the generation process."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Guidance Scale",
          "description": "Guidance scale for the generation."
        },
        {
          "name": "guidance_type",
          "type": {
            "type": "enum",
            "values": [
              "cfg",
              "apg",
              "cfg_star"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.ACEStep.GuidanceType"
          },
          "default": "apg",
          "title": "Guidance Type",
          "description": "Type of CFG to use for the generation process."
        },
        {
          "name": "lyric_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Lyric Guidance Scale",
          "description": "Lyric guidance scale for the generation."
        },
        {
          "name": "guidance_interval",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Guidance Interval",
          "description": "Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)"
        },
        {
          "name": "guidance_interval_decay",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Interval Decay",
          "description": "Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If not provided, a random seed will be used."
        },
        {
          "name": "granularity_scale",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Granularity Scale",
          "description": "Granularity scale for the generation process. Higher values can reduce artifacts."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "ACEStep Prompt To Audio",
      "description": "ACE-Step generates music from text prompts with high-quality audio synthesis.\n    audio, generation, music, ace-step, text-to-audio\n\n    Use cases:\n    - Generate music from text descriptions\n    - Create background music for videos\n    - Produce royalty-free music\n    - Generate audio soundtracks\n    - Create custom music compositions",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.ACEStepPromptToAudio",
      "properties": [
        {
          "name": "number_of_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Number Of Steps",
          "description": "Number of steps to generate the audio."
        },
        {
          "name": "duration",
          "type": {
            "type": "float"
          },
          "default": 60,
          "title": "Duration",
          "description": "The duration of the generated audio in seconds."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to control the style of the generated audio. This will be used to generate tags and lyrics."
        },
        {
          "name": "minimum_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Minimum Guidance Scale",
          "description": "Minimum guidance scale for the generation after the decay."
        },
        {
          "name": "tag_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Tag Guidance Scale",
          "description": "Tag guidance scale for the generation."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "euler",
              "heun"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.ACEStepPromptToAudio.Scheduler"
          },
          "default": "euler",
          "title": "Scheduler",
          "description": "Scheduler to use for the generation process."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Guidance Scale",
          "description": "Guidance scale for the generation."
        },
        {
          "name": "guidance_type",
          "type": {
            "type": "enum",
            "values": [
              "cfg",
              "apg",
              "cfg_star"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.ACEStepPromptToAudio.GuidanceType"
          },
          "default": "apg",
          "title": "Guidance Type",
          "description": "Type of CFG to use for the generation process."
        },
        {
          "name": "instrumental",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Instrumental",
          "description": "Whether to generate an instrumental version of the audio."
        },
        {
          "name": "lyric_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Lyric Guidance Scale",
          "description": "Lyric guidance scale for the generation."
        },
        {
          "name": "guidance_interval",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Guidance Interval",
          "description": "Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)"
        },
        {
          "name": "guidance_interval_decay",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Interval Decay",
          "description": "Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If not provided, a random seed will be used."
        },
        {
          "name": "granularity_scale",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Granularity Scale",
          "description": "Granularity scale for the generation process. Higher values can reduce artifacts."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Beatoven Music Generation",
      "description": "Music Generation\n    audio, generation, text-to-audio, tts\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.BeatovenMusicGeneration",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Describe the music you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "float"
          },
          "default": 90,
          "title": "Duration",
          "description": "Length of the generated music in seconds"
        },
        {
          "name": "refinement",
          "type": {
            "type": "int"
          },
          "default": 100,
          "title": "Refinement",
          "description": "Refinement level - higher values may improve quality but take longer"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducible results - leave empty for random generation"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Describe what you want to avoid in the music (instruments, styles, moods). Leave blank for none."
        },
        {
          "name": "creativity",
          "type": {
            "type": "float"
          },
          "default": 16,
          "title": "Creativity",
          "description": "Creativity level - higher values allow more creative interpretation of the prompt"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Beatoven Sound Effect Generation",
      "description": "Sound Effect Generation\n    audio, generation, text-to-audio, tts\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.BeatovenSoundEffectGeneration",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Describe the sound effect you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Duration",
          "description": "Length of the generated sound effect in seconds"
        },
        {
          "name": "refinement",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Refinement",
          "description": "Refinement level - Higher values may improve quality but take longer"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducible results - leave empty for random generation"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Describe the types of sounds you don't want to generate in the output, avoid double-negatives, compare with positive prompts"
        },
        {
          "name": "creativity",
          "type": {
            "type": "float"
          },
          "default": 16,
          "title": "Creativity",
          "description": "Creativity level - higher values allow more creative interpretation of the prompt"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "CSM1 B",
      "description": "CSM (Conversational Speech Model) generates natural conversational speech from text.\n    audio, speech, tts, conversational, text-to-speech\n\n    Use cases:\n    - Generate natural conversation audio\n    - Create dialogue for characters\n    - Produce conversational voice content\n    - Generate realistic speech\n    - Create interactive voice responses",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.CSM1B",
      "properties": [
        {
          "name": "scene",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Scene",
          "description": "The text to generate an audio from."
        },
        {
          "name": "context",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Context",
          "description": "The context to generate an audio from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Diff Rhythm",
      "description": "DiffRhythm generates rhythmic music and beats using diffusion models.\n    audio, generation, rhythm, beats, music, text-to-audio\n\n    Use cases:\n    - Generate rhythmic music\n    - Create drum beats\n    - Produce percussion tracks\n    - Generate rhythm patterns\n    - Create beat sequences",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.DiffRhythm",
      "properties": [
        {
          "name": "lyrics",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Lyrics",
          "description": "The prompt to generate the song from. Must have two sections. Sections start with either [chorus] or a [verse]."
        },
        {
          "name": "cfg_strength",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Cfg Strength",
          "description": "The CFG strength to use for the music generation."
        },
        {
          "name": "reference_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Reference Audio Url",
          "description": "The URL of the reference audio to use for the music generation."
        },
        {
          "name": "music_duration",
          "type": {
            "type": "enum",
            "values": [
              "95s",
              "285s"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.DiffRhythm.MusicDuration"
          },
          "default": "95s",
          "title": "Music Duration",
          "description": "The duration of the music to generate."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "euler",
              "midpoint",
              "rk4",
              "implicit_adams"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.DiffRhythm.Scheduler"
          },
          "default": "euler",
          "title": "Scheduler",
          "description": "The scheduler to use for the music generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 32,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for the music generation."
        },
        {
          "name": "style_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Prompt",
          "description": "The style prompt to use for the music generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Eleven Labs Music",
      "description": "ElevenLabs Music generates custom music compositions from text descriptions.\n    audio, music, generation, elevenlabs, text-to-audio\n\n    Use cases:\n    - Generate custom music\n    - Create background scores\n    - Produce original compositions\n    - Generate mood music\n    - Create cinematic soundtracks",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.ElevenLabsMusic",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the music to generate"
        },
        {
          "name": "composition_plan",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Composition Plan",
          "description": "The composition plan for the music"
        },
        {
          "name": "music_length_ms",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Music Length Ms",
          "description": "The length of the song to generate in milliseconds. Used only in conjunction with prompt. Must be between 3000ms and 600000ms. Optional - if not provided, the model will choose a length based on the prompt."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "mp3_22050_32",
              "mp3_44100_32",
              "mp3_44100_64",
              "mp3_44100_96",
              "mp3_44100_128",
              "mp3_44100_192",
              "pcm_8000",
              "pcm_16000",
              "pcm_22050",
              "pcm_24000",
              "pcm_44100",
              "pcm_48000",
              "ulaw_8000",
              "alaw_8000",
              "opus_48000_32",
              "opus_48000_64",
              "opus_48000_96",
              "opus_48000_128",
              "opus_48000_192"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.ElevenLabsMusic.OutputFormat"
          },
          "default": "mp3_44100_128",
          "title": "Output Format",
          "description": "Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the \u03bc-law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs."
        },
        {
          "name": "respect_sections_durations",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Respect Sections Durations",
          "description": "Controls how strictly section durations in the composition_plan are enforced. It will only have an effect if it is used with composition_plan. When set to true, the model will precisely respect each section's duration_ms from the plan. When set to false, the model may adjust individual section durations which will generally lead to better generation quality and improved latency, while always preserving the total song duration from the plan."
        },
        {
          "name": "force_instrumental",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Force Instrumental",
          "description": "If true, guarantees that the generated song will be instrumental. If false, the song may or may not be instrumental depending on the prompt. Can only be used with prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Eleven Labs Sound Effects V2",
      "description": "ElevenLabs Sound Effects v2 generates custom sound effects from text descriptions.\n    audio, sound-effects, sfx, elevenlabs, text-to-audio\n\n    Use cases:\n    - Generate custom sound effects\n    - Create audio effects for videos\n    - Produce game sound effects\n    - Generate environmental sounds\n    - Create audio atmosphere",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.ElevenLabsSoundEffectsV2",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text describing the sound effect to generate"
        },
        {
          "name": "loop",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Loop",
          "description": "Whether to create a sound effect that loops smoothly."
        },
        {
          "name": "prompt_influence",
          "type": {
            "type": "float"
          },
          "default": 0.3,
          "title": "Prompt Influence",
          "description": "How closely to follow the prompt (0-1). Higher values mean less variation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "mp3_22050_32",
              "mp3_44100_32",
              "mp3_44100_64",
              "mp3_44100_96",
              "mp3_44100_128",
              "mp3_44100_192",
              "pcm_8000",
              "pcm_16000",
              "pcm_22050",
              "pcm_24000",
              "pcm_44100",
              "pcm_48000",
              "ulaw_8000",
              "alaw_8000",
              "opus_48000_32",
              "opus_48000_64",
              "opus_48000_96",
              "opus_48000_128",
              "opus_48000_192"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.ElevenLabsSoundEffectsV2.OutputFormat"
          },
          "default": "mp3_44100_128",
          "title": "Output Format",
          "description": "Output format of the generated audio. Formatted as codec_sample_rate_bitrate."
        },
        {
          "name": "duration_seconds",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Duration Seconds",
          "description": "Duration in seconds (0.5-22). If None, optimal duration will be determined from prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Eleven Labs TTSMultilingual V2",
      "description": "ElevenLabs Multilingual TTS v2 generates natural speech in multiple languages.\n    audio, tts, speech, multilingual, elevenlabs, text-to-speech\n\n    Use cases:\n    - Generate multilingual speech\n    - Create voiceovers in multiple languages\n    - Produce localized audio content\n    - Generate international voice content\n    - Create translated audio",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.ElevenLabsTTSMultilingualV2",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to convert to speech"
        },
        {
          "name": "next_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Next Text",
          "description": "The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation."
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Speed",
          "description": "Speech speed (0.7-1.2). Values below 1.0 slow down the speech, above 1.0 speed it up. Extreme values may affect quality."
        },
        {
          "name": "style",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Style",
          "description": "Style exaggeration (0-1)"
        },
        {
          "name": "stability",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Stability",
          "description": "Voice stability (0-1)"
        },
        {
          "name": "timestamps",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Timestamps",
          "description": "Whether to return timestamps for each word in the generated speech"
        },
        {
          "name": "similarity_boost",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Similarity Boost",
          "description": "Similarity boost (0-1)"
        },
        {
          "name": "voice",
          "type": {
            "type": "str"
          },
          "default": "Rachel",
          "title": "Voice",
          "description": "The voice to use for speech generation"
        },
        {
          "name": "language_code",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Language Code",
          "description": "Language code (ISO 639-1) used to enforce a language for the model. An error will be returned if language code is not supported by the model."
        },
        {
          "name": "apply_text_normalization",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "on",
              "off"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.ElevenLabsTTSMultilingualV2.ApplyTextNormalization"
          },
          "default": "auto",
          "title": "Apply Text Normalization",
          "description": "This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped."
        },
        {
          "name": "previous_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Previous Text",
          "description": "The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text",
        "language"
      ]
    },
    {
      "title": "Eleven Labs TTSV3",
      "description": "ElevenLabs TTS v3 generates high-quality natural speech with advanced voice control.\n    audio, tts, speech, elevenlabs, text-to-speech\n\n    Use cases:\n    - Generate high-quality voiceovers\n    - Create natural speech audio\n    - Produce professional narration\n    - Generate expressive speech\n    - Create audiobook content",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.ElevenLabsTTSV3",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to convert to speech"
        },
        {
          "name": "stability",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Stability",
          "description": "Voice stability (0-1)"
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Speed",
          "description": "Speech speed (0.7-1.2). Values below 1.0 slow down the speech, above 1.0 speed it up. Extreme values may affect quality."
        },
        {
          "name": "style",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Style",
          "description": "Style exaggeration (0-1)"
        },
        {
          "name": "timestamps",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Timestamps",
          "description": "Whether to return timestamps for each word in the generated speech"
        },
        {
          "name": "similarity_boost",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Similarity Boost",
          "description": "Similarity boost (0-1)"
        },
        {
          "name": "voice",
          "type": {
            "type": "str"
          },
          "default": "Rachel",
          "title": "Voice",
          "description": "The voice to use for speech generation"
        },
        {
          "name": "language_code",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Language Code",
          "description": "Language code (ISO 639-1) used to enforce a language for the model."
        },
        {
          "name": "apply_text_normalization",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "on",
              "off"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.ElevenLabsTTSV3.ApplyTextNormalization"
          },
          "default": "auto",
          "title": "Apply Text Normalization",
          "description": "This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Eleven Labs Text To Dialogue V3",
      "description": "ElevenLabs Text to Dialogue v3 generates conversational dialogue with multiple speakers.\n    audio, dialogue, conversation, elevenlabs, text-to-speech\n\n    Use cases:\n    - Generate multi-speaker dialogue\n    - Create conversational audio\n    - Produce podcast-style content\n    - Generate character conversations\n    - Create interactive dialogues",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.ElevenLabsTextToDialogueV3",
      "properties": [
        {
          "name": "stability",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Stability",
          "description": "Determines how stable the voice is and the randomness between each generation. Lower values introduce broader emotional range for the voice. Higher values can result in a monotonous voice with limited emotion. Must be one of 0.0, 0.5, 1.0, else it will be rounded to the nearest value."
        },
        {
          "name": "inputs",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Inputs",
          "description": "A list of dialogue inputs, each containing text and a voice ID which will be converted into speech."
        },
        {
          "name": "language_code",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Language Code",
          "description": "Language code (ISO 639-1) used to enforce a language for the model. An error will be returned if language code is not supported by the model."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "use_speaker_boost",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Use Speaker Boost",
          "description": "This setting boosts the similarity to the original speaker. Using this setting requires a slightly higher computational load, which in turn increases latency."
        },
        {
          "name": "pronunciation_dictionary_locators",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Pronunciation Dictionary Locators",
          "description": "A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "F5 TTS",
      "description": "F5 TTS generates natural speech with fast inference and high quality.\n    audio, tts, speech, fast, text-to-speech\n\n    Use cases:\n    - Fast speech generation\n    - Real-time TTS applications\n    - Quick voiceover creation\n    - Efficient speech synthesis\n    - Rapid audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.F5TTS",
      "properties": [
        {
          "name": "ref_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Ref Text",
          "description": "The reference text to be used for TTS. If not provided, an ASR (Automatic Speech Recognition) model will be used to generate the reference text."
        },
        {
          "name": "remove_silence",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Remove Silence",
          "description": "Whether to remove the silence from the audio file."
        },
        {
          "name": "gen_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Gen Text",
          "description": "The text to be converted to speech."
        },
        {
          "name": "model_type",
          "type": {
            "type": "enum",
            "values": [
              "F5-TTS",
              "E2-TTS"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.F5TTS.ModelType"
          },
          "default": "",
          "title": "Model Type",
          "description": "The name of the model to be used for TTS."
        },
        {
          "name": "ref_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ref Audio Url",
          "description": "The URL of the reference audio file."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Kokoro",
      "description": "Kokoro generates expressive and emotional speech with advanced prosody control.\n    audio, tts, speech, expressive, emotional, text-to-speech\n\n    Use cases:\n    - Generate expressive speech\n    - Create emotional voiceovers\n    - Produce dramatic narration\n    - Generate character voices\n    - Create emotive audio content",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.Kokoro",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "af_heart",
              "af_alloy",
              "af_aoede",
              "af_bella",
              "af_jessica",
              "af_kore",
              "af_nicole",
              "af_nova",
              "af_river",
              "af_sarah",
              "af_sky",
              "am_adam",
              "am_echo",
              "am_eric",
              "am_fenrir",
              "am_liam",
              "am_michael",
              "am_onyx",
              "am_puck",
              "am_santa"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.Kokoro.Voice"
          },
          "default": "af_heart",
          "title": "Voice",
          "description": "Voice ID for the desired voice."
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Speed",
          "description": "Speed of the generated audio. Default is 1.0."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Minimax Music V1 5",
      "description": "MiniMax (Hailuo AI) Music v1.5\n    audio, generation, text-to-audio, tts, professional\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.MinimaxMusicV1_5",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Lyrics, supports [intro][verse][chorus][bridge][outro] sections. 10-600 characters."
        },
        {
          "name": "lyrics_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Lyrics Prompt",
          "description": "Control music generation. 10-3000 characters."
        },
        {
          "name": "audio_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Audio Setting",
          "description": "Audio configuration settings"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Minimax Music V2",
      "description": "Minimax Music\n    audio, generation, text-to-audio, tts, professional\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.MinimaxMusicV2",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "A description of the music, specifying style, mood, and scenario. 10-300 characters."
        },
        {
          "name": "lyrics_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Lyrics Prompt",
          "description": "Lyrics of the song. Use n to separate lines. You may add structure tags like [Intro], [Verse], [Chorus], [Bridge], [Outro] to enhance the arrangement. 10-3000 characters."
        },
        {
          "name": "audio_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Audio Setting",
          "description": "Audio configuration settings"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Sonauto V2 Inpaint",
      "description": "Sonauto V2\n    audio, generation, text-to-audio, tts\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.SonautoV2Inpaint",
      "properties": [
        {
          "name": "lyrics_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Lyrics Prompt",
          "description": "The lyrics sung in the generated song. An empty string will generate an instrumental track."
        },
        {
          "name": "tags",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Tags",
          "description": "Tags/styles of the music to generate. You can view a list of all available tags at https://sonauto.ai/tag-explorer."
        },
        {
          "name": "prompt_strength",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Prompt Strength",
          "description": "Controls how strongly your prompt influences the output. Greater values adhere more to the prompt but sound less natural. (This is CFG.)"
        },
        {
          "name": "output_bit_rate",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Output Bit Rate",
          "description": "The bit rate to use for mp3 and m4a formats. Not available for other formats."
        },
        {
          "name": "num_songs",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Songs",
          "description": "Generating 2 songs costs 1.5x the price of generating 1 song. Also, note that using the same seed may not result in identical songs if the number of songs generated is changed."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "flac",
              "mp3",
              "wav",
              "ogg",
              "m4a"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.SonautoV2Inpaint.OutputFormat"
          },
          "default": "wav",
          "title": "Output Format"
        },
        {
          "name": "selection_crop",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Selection Crop",
          "description": "Crop to the selected region"
        },
        {
          "name": "sections",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Sections",
          "description": "List of sections to inpaint. Currently, only one section is supported so the list length must be 1."
        },
        {
          "name": "balance_strength",
          "type": {
            "type": "float"
          },
          "default": 0.7,
          "title": "Balance Strength",
          "description": "Greater means more natural vocals. Lower means sharper instrumentals. We recommend 0.7."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file to alter. Must be a valid publicly accessible URL."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed to use for generation. Will pick a random seed if not provided. Repeating a request with identical parameters (must use lyrics and tags, not prompt) and the same seed will generate the same song."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Stable Audio",
      "description": "Stable Audio generates high-quality audio from text with consistent results.\n    audio, generation, stable, music, text-to-audio\n\n    Use cases:\n    - Generate consistent audio\n    - Create reliable soundtracks\n    - Produce predictable audio\n    - Generate stable music\n    - Create dependable audio content",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.StableAudio",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate audio from"
        },
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 100,
          "title": "Steps",
          "description": "The number of steps to denoise the audio for"
        },
        {
          "name": "seconds_total",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Seconds Total",
          "description": "The duration of the audio clip to generate"
        },
        {
          "name": "seconds_start",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seconds Start",
          "description": "The start point of the audio clip to generate"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Stable Audio 25 Text To Audio",
      "description": "Stable Audio 2.5\n    audio, generation, text-to-audio, tts\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.StableAudio25TextToAudio",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate audio from"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seconds_total",
          "type": {
            "type": "int"
          },
          "default": 190,
          "title": "Seconds Total",
          "description": "The duration of the audio clip to generate"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of steps to denoise the audio for"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "XTTS",
      "description": "XTTS generates expressive speech with voice cloning capabilities.\n    audio, tts, speech, voice-cloning, expressive, text-to-speech\n\n    Use cases:\n    - Clone and generate voices\n    - Create personalized speech\n    - Produce voice-matched content\n    - Generate custom voice audio\n    - Create voice replications",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.XTTS",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt you would like to convert to speech."
        },
        {
          "name": "repetition_penalty",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Repetition Penalty",
          "description": "The repetition penalty to use for generation. Defaults to 5.0."
        },
        {
          "name": "language",
          "type": {
            "type": "enum",
            "values": [
              "English",
              "Spanish",
              "French",
              "German",
              "Italian",
              "Portuguese",
              "Polish",
              "Turkish",
              "Russian",
              "Dutch",
              "Czech",
              "Arabic",
              "Chinese",
              "Japanese",
              "Hungarian",
              "Korean",
              "Hindi"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.XTTS.Language"
          },
          "default": "English",
          "title": "Language",
          "description": "The language to use for generation. Defaults to English."
        },
        {
          "name": "gpt_cond_len",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Gpt Cond Len",
          "description": "The length of the GPT conditioning. Defaults to 30."
        },
        {
          "name": "gpt_cond_chunk_len",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Gpt Cond Chunk Len",
          "description": "The length of the GPT conditioning chunks. Defaults to 4."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the voice file to match"
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Temperature",
          "description": "The temperature to use for generation. Higher is more creative. Defaults to 0.75."
        },
        {
          "name": "sample_rate",
          "type": {
            "type": "int"
          },
          "default": 24000,
          "title": "Sample Rate",
          "description": "The sample rate of the audio. Defaults to 24000."
        },
        {
          "name": "max_ref_length",
          "type": {
            "type": "int"
          },
          "default": 60,
          "title": "Max Ref Length",
          "description": "The maximum length of the reference. Defaults to 60."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Animate Diff Sparse Ctrl LCM",
      "description": "AnimateDiff SparseCtrl LCM animates drawings with latent consistency models for fast generation.\n    video, generation, animatediff, sparsectrl, lcm, animation, text-to-video\n\n    Use cases:\n    - Animate hand-drawn sketches\n    - Bring drawings to life\n    - Create animated illustrations\n    - Generate animations from concept art\n    - Produce animation from sparse frames",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.AnimateDiffSparseCtrlLCM",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "controlnet_type",
          "type": {
            "type": "enum",
            "values": [
              "scribble",
              "rgb"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.AnimateDiffSparseCtrlLCM.ControlnetType"
          },
          "default": "scribble",
          "title": "Controlnet Type",
          "description": "The type of controlnet to use for generating the video. The controlnet determines how the video will be animated."
        },
        {
          "name": "keyframe_2_index",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Keyframe 2 Index",
          "description": "The frame index of the third keyframe to use for the generation."
        },
        {
          "name": "keyframe_0_index",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Keyframe 0 Index",
          "description": "The frame index of the first keyframe to use for the generation."
        },
        {
          "name": "keyframe_1_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Keyframe 1 Image Url",
          "description": "The URL of the second keyframe to use for the generation."
        },
        {
          "name": "keyframe_1_index",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Keyframe 1 Index",
          "description": "The frame index of the second keyframe to use for the generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "Increasing the amount of steps tells Stable Diffusion that it should take more steps to generate your final result which can increase the amount of detail in your image."
        },
        {
          "name": "keyframe_2_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Keyframe 2 Image Url",
          "description": "The URL of the third keyframe to use for the generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to specify what you don't want."
        },
        {
          "name": "keyframe_0_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Keyframe 0 Image Url",
          "description": "The URL of the first keyframe to use for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Animate Diff Text To Video",
      "description": "AnimateDiff generates smooth animations from text prompts using diffusion models.\n    video, generation, animatediff, animation, text-to-video, txt2vid\n\n    Use cases:\n    - Animate ideas from text descriptions\n    - Create animated content quickly\n    - Generate motion graphics from prompts\n    - Produce animated concept art\n    - Create video loops and sequences",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.AnimateDiffTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the video. Be as descriptive as possible for best results."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Fps",
          "description": "Number of frames per second to extract from the video."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "square",
          "title": "Video Size",
          "description": "The size of the video to generate."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Num Frames",
          "description": "The number of frames to generate for the video."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(bad quality, worst quality:1.2), ugly faces, bad anime",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "motions",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Motions",
          "description": "The motions to apply to the video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Animate Diff Turbo Text To Video",
      "description": "AnimateDiff Turbo generates animations at lightning speed with reduced steps.\n    video, generation, animatediff, turbo, fast, text-to-video, txt2vid\n\n    Use cases:\n    - Rapidly prototype video animations\n    - Create quick video previews\n    - Generate animations with minimal latency\n    - Iterate on video concepts quickly\n    - Produce real-time animation effects",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.AnimateDiffTurboTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the video. Be as descriptive as possible for best results."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Fps",
          "description": "Number of frames per second to extract from the video."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "square",
          "title": "Video Size",
          "description": "The size of the video to generate."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Num Frames",
          "description": "The number of frames to generate for the video."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform. 4-12 is recommended for turbo mode."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(bad quality, worst quality:1.2), ugly faces, bad anime",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "motions",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Motions",
          "description": "The motions to apply to the video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Argil Avatars Text To Video",
      "description": "Argil Avatars creates realistic talking avatar videos from text descriptions.\n    video, generation, avatar, talking-head, argil, text-to-video\n\n    Use cases:\n    - Generate avatar spokesperson videos\n    - Create virtual presenter content\n    - Produce automated video announcements\n    - Generate character-based narratives\n    - Create social media avatar videos",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.ArgilAvatarsTextToVideo",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text"
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "Rachel",
              "Clyde",
              "Roger",
              "Sarah",
              "Laura",
              "Thomas",
              "Charlie",
              "George",
              "Callum",
              "River",
              "Harry",
              "Liam",
              "Alice",
              "Matilda",
              "Will",
              "Jessica",
              "Lilly",
              "Bill",
              "Oxley",
              "Luna"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo.Voice"
          },
          "default": "",
          "title": "Voice"
        },
        {
          "name": "remove_background",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Remove Background",
          "description": "Enabling the remove background feature will result in a 50% increase in the price."
        },
        {
          "name": "avatar",
          "type": {
            "type": "enum",
            "values": [
              "Mia outdoor (UGC)",
              "Lara (Masterclass)",
              "Ines (UGC)",
              "Maria (Masterclass)",
              "Emma (UGC)",
              "Sienna (Masterclass)",
              "Elena (UGC)",
              "Jasmine (Masterclass)",
              "Amara (Masterclass)",
              "Ryan podcast (UGC)",
              "Tyler (Masterclass)",
              "Jayse (Masterclass)",
              "Paul (Masterclass)",
              "Matteo (UGC)",
              "Daniel car (UGC)",
              "Dario (Masterclass)",
              "Viva (Masterclass)",
              "Chen (Masterclass)",
              "Alex (Masterclass)",
              "Vanessa (UGC)",
              "Laurent (UGC)",
              "Noemie car (UGC)",
              "Brandon (UGC)",
              "Byron (Masterclass)",
              "Calista (Masterclass)",
              "Milo (Masterclass)",
              "Fabien (Masterclass)",
              "Rose (UGC)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo.Avatar"
          },
          "default": "",
          "title": "Avatar"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Cog Video X5 B",
      "description": "CogVideoX-5B is a powerful open-source text-to-video generation model with 5 billion parameters.\n    video, generation, cogvideo, text-to-video, txt2vid\n\n    Use cases:\n    - Generate detailed videos from text prompts\n    - Create animated storytelling content\n    - Produce concept videos for pitches\n    - Generate video storyboards\n    - Create educational demonstrations",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.CogVideoX5B",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "use_rife",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Rife",
          "description": "Use RIFE for video interpolation"
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. We currently support one lora."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related video to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "export_fps",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Export Fps",
          "description": "The target FPS of the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate video from"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Video",
      "description": "Hunyuan Video is Tencent's advanced text-to-video model for high-quality video generation.\n    video, generation, hunyuan, text-to-video, txt2vid\n\n    Use cases:\n    - Generate cinematic videos from text descriptions\n    - Create marketing videos from product descriptions\n    - Produce educational video content\n    - Generate creative video concepts\n    - Create animated scenes from stories",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.HunyuanVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.HunyuanVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video to generate."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.HunyuanVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the video to generate."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to run. Lower gets faster results, higher gets better results."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for generating the video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "enum",
            "values": [
              "129",
              "85"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.HunyuanVideo.NumFrames"
          },
          "default": 129,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "pro_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Pro Mode",
          "description": "By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Video V1 5 Text To Video",
      "description": "Hunyuan Video V1.5\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.HunyuanVideoV1_5TextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.HunyuanVideoV1_5TextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.HunyuanVideoV1_5TextToVideo.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Enable prompt expansion to enhance the input prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what not to generate."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Infinitalk Single Text",
      "description": "Infinitalk\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.InfinitalkSingleText",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the video to generate. Must be either 480p or 720p."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for generation."
        },
        {
          "name": "text_input",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Input",
          "description": "The text input to guide video generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "Aria",
              "Roger",
              "Sarah",
              "Laura",
              "Charlie",
              "George",
              "Callum",
              "River",
              "Liam",
              "Charlotte",
              "Alice",
              "Matilda",
              "Will",
              "Jessica",
              "Eric",
              "Chris",
              "Brian",
              "Daniel",
              "Lily",
              "Bill"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Voice"
          },
          "default": "",
          "title": "Voice",
          "description": "The voice to use for speech generation"
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 145,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 41 to 721."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Infinity Star Text To Video",
      "description": "Infinity Star\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.InfinityStarTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for generating the video"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.InfinityStarTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated output"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enhance Prompt",
          "description": "Whether to use an LLM to enhance the prompt."
        },
        {
          "name": "use_apg",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Apg",
          "description": "Whether to use APG"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. Leave empty for random generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to guide what to avoid in generation"
        },
        {
          "name": "tau_video",
          "type": {
            "type": "float"
          },
          "default": 0.4,
          "title": "Tau Video",
          "description": "Tau value for video scale"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kandinsky 5 Pro Text To Video",
      "description": "Kandinsky5 Pro\n    video, generation, text-to-video, txt2vid, professional\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Kandinsky5ProTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "512P",
              "1024P"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Resolution"
          },
          "default": "512P",
          "title": "Resolution",
          "description": "Video resolution: 512p or 1024p."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for faster generation."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "3:2",
              "1:1",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.AspectRatio"
          },
          "default": "3:2",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video. One of (3:2, 1:1, 2:3)."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5s"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Duration"
          },
          "default": "5s",
          "title": "Duration",
          "description": "The length of the video to generate (5s or 10s)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kandinsky 5 Text To Video",
      "description": "Kandinsky5\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Kandinsky5TextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "768x512"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.Resolution"
          },
          "default": "768x512",
          "title": "Resolution",
          "description": "Resolution of the generated video in W:H format. Will be calculated based on the aspect ratio(768x512, 512x512, 512x768)."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5s",
              "10s"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.Duration"
          },
          "default": "5s",
          "title": "Duration",
          "description": "The length of the video to generate (5s or 10s)"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "3:2",
              "1:1",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.AspectRatio"
          },
          "default": "3:2",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video. One of (3:2, 1:1, 2:3)."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kandinsky 5 Text To Video Distill",
      "description": "Kandinsky5\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Kandinsky5TextToVideoDistill",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5s",
              "10s"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.Duration"
          },
          "default": "5s",
          "title": "Duration",
          "description": "The length of the video to generate (5s or 10s)"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "3:2",
              "1:1",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.AspectRatio"
          },
          "default": "3:2",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video. One of (3:2, 1:1, 2:3)."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "768x512"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.Resolution"
          },
          "default": "768x512",
          "title": "Resolution",
          "description": "Resolution of the generated video in W:H format. Will be calculated based on the aspect ratio(768x512, 512x512, 512x768)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kling Video V1 Standard Text To Video",
      "description": "Kling Video v1 Standard generates videos from text with balanced quality and speed.\n    video, generation, kling, text-to-video, txt2vid\n\n    Use cases:\n    - Generate standard quality videos\n    - Create video content efficiently\n    - Produce videos for web use\n    - Generate video previews\n    - Create video concepts",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KlingVideoV1StandardTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame"
        },
        {
          "name": "advanced_camera_control",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Advanced Camera Control",
          "description": "Advanced Camera control parameters"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "camera_control",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "down_back",
              "forward_up",
              "right_turn_forward",
              "left_turn_forward"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.CameraControl"
          },
          "default": null,
          "title": "Camera Control",
          "description": "Camera control parameters"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kling Video V2 6 Pro Text To Video",
      "description": "Kling Video v2.6 Text to Video\n    video, generation, text-to-video, txt2vid, professional\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KlingVideoV2_6ProTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV2_6ProTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV2_6ProTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Krea Wan 14B Text To Video",
      "description": "Krea Wan 14b- Text to Video\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KreaWan14BTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt for the video-to-video generation."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 78,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be a multiple of 12 plus 6, for example 6, 18, 30, 42, etc."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the video-to-video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "LTXVideo",
      "description": "LTX Video generates high-quality videos from text prompts with advanced temporal consistency.\n    video, generation, ltx, text-to-video, txt2vid\n\n    Use cases:\n    - Generate temporally consistent videos\n    - Create smooth video sequences\n    - Produce high-quality video content\n    - Generate professional video clips\n    - Create cinematic video scenes",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LTXVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for random number generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to take."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Longcat Video Distilled Text To Video 480P",
      "description": "LongCat Video Distilled\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LongcatVideoDistilledTextToVideo480P",
      "properties": [
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 15,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 162,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 12,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Longcat Video Distilled Text To Video 720P",
      "description": "LongCat Video Distilled\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LongcatVideoDistilledTextToVideo720P",
      "properties": [
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "num_refine_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 12,
          "title": "Num Refine Inference Steps",
          "description": "The number of inference steps to use for refinement."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 162,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 12,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Longcat Video Text To Video 480P",
      "description": "LongCat Video\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LongcatVideoTextToVideo480P",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for the video generation."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 15,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the video generation."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 162,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "The negative prompt to use for the video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Longcat Video Text To Video 720P",
      "description": "LongCat Video\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LongcatVideoTextToVideo720P",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for the video generation."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "num_refine_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Refine Inference Steps",
          "description": "The number of inference steps to use for refinement."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the video generation."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 162,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "The negative prompt to use for the video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for the video generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ltx 219B Distilled Text To Video",
      "description": "LTX-2 19B Distilled\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Ltx219BDistilledTextToVideo",
      "properties": [
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ltx 219B Distilled Text To Video Lora",
      "description": "LTX-2 19B Distilled\n    video, generation, text-to-video, txt2vid, lora\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Ltx219BDistilledTextToVideoLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ltx 219B Text To Video",
      "description": "LTX-2 19B\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Ltx219BTextToVideo",
      "properties": [
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ltx 219B Text To Video Lora",
      "description": "LTX-2 19B\n    video, generation, text-to-video, txt2vid, lora\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Ltx219BTextToVideoLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Luma Dream Machine Text To Video",
      "description": "Luma Dream Machine generates creative videos from text with dreamlike aesthetics.\n    video, generation, luma, dream-machine, text-to-video, txt2vid\n\n    Use cases:\n    - Generate dreamlike video content\n    - Create surreal video sequences\n    - Produce artistic video interpretations\n    - Generate creative video concepts\n    - Create imaginative video art",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LumaDreamMachineTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "21:9",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LumaDreamMachineTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "loop",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Loop",
          "description": "Whether the video should loop (end of video is blended with the beginning)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Luma Photon",
      "description": "Luma Photon generates photorealistic videos from text with high visual fidelity.\n    video, generation, luma, photon, photorealistic, text-to-video\n\n    Use cases:\n    - Generate photorealistic video content\n    - Create realistic video simulations\n    - Produce lifelike video scenes\n    - Generate high-fidelity video outputs\n    - Create realistic visual content",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LumaPhoton",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1",
              "4:3",
              "3:4",
              "21:9",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LumaPhoton.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Minimax Hailuo 2 3 Pro Text To Video",
      "description": "MiniMax Hailuo 2.3 [Pro] (Text to Video)\n    video, generation, text-to-video, txt2vid, professional\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.MinimaxHailuo2_3ProTextToVideo",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Minimax Hailuo 2 3 Standard Text To Video",
      "description": "MiniMax Hailuo 2.3 [Standard] (Text to Video)\n    video, generation, text-to-video, txt2vid, professional\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.MinimaxHailuo2_3StandardTextToVideo",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "6",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.MinimaxHailuo2_3StandardTextToVideo.Duration"
          },
          "default": "6",
          "title": "Duration",
          "description": "The duration of the video in seconds."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Mochi V1",
      "description": "Mochi v1 generates creative videos from text with unique artistic style.\n    video, generation, mochi, artistic, text-to-video, txt2vid\n\n    Use cases:\n    - Generate artistic video content\n    - Create stylized animations\n    - Produce creative video art\n    - Generate experimental videos\n    - Create unique visual content",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.MochiV1",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a video from."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for generating the video."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt for the video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Moonvalley Marey T2 V",
      "description": "Marey Realism V1.5\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.MoonvalleyMareyT2V",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a video from"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5s",
              "10s"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V.Duration"
          },
          "default": "5s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "dimensions",
          "type": {
            "type": "enum",
            "values": [
              "1920x1080",
              "1152x1152",
              "1536x1152",
              "1152x1536"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V.Dimensions"
          },
          "default": "1920x1080",
          "title": "Dimensions",
          "description": "The dimensions of the generated video in width x height format."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Guidance Scale",
          "description": "Controls how strongly the generation is guided by the prompt (0-20). Higher values follow the prompt more closely."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for random number generation. Use -1 for random seed each run."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "<synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts",
          "title": "Negative Prompt",
          "description": "Negative prompt used to guide the model away from undesirable features."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ovi",
      "description": "Ovi Text to Video\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Ovi",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "512x992",
              "992x512",
              "960x512",
              "512x960",
              "720x720",
              "448x1120",
              "1120x448"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ovi.Resolution"
          },
          "default": "992x512",
          "title": "Resolution",
          "description": "Resolution of the generated video in W:H format. One of (512x992, 992x512, 960x512, 512x960, 720x720, or 448x1120)."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps."
        },
        {
          "name": "audio_negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "robotic, muffled, echo, distorted",
          "title": "Audio Negative Prompt",
          "description": "Negative prompt for audio generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "jitter, bad hands, blur, distortion",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Pixverse V5 Text To Video",
      "description": "Pixverse\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.PixverseV5TextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Pixverse V5 5 Text To Video",
      "description": "Pixverse\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.PixverseV5_5TextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "thinking_type",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "enabled",
              "disabled",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.ThinkingType"
          },
          "default": null,
          "title": "Thinking Type",
          "description": "Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision"
        },
        {
          "name": "generate_multi_clip_switch",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Generate Multi Clip Switch",
          "description": "Enable multi-clip generation with dynamic camera changes"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds"
        },
        {
          "name": "generate_audio_switch",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Generate Audio Switch",
          "description": "Enable audio generation (BGM, SFX, dialogue)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Pixverse V5 6 Text To Video",
      "description": "Pixverse\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.PixverseV5_6TextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "thinking_type",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "enabled",
              "disabled",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.ThinkingType"
          },
          "default": null,
          "title": "Thinking Type",
          "description": "Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. 1080p videos are limited to 5 or 8 seconds"
        },
        {
          "name": "generate_audio_switch",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Generate Audio Switch",
          "description": "Enable audio generation (BGM, SFX, dialogue)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Sana Video",
      "description": "Sana Video\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.SanaVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the video to generate"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.SanaVideo.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the output video"
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Fps",
          "description": "Frames per second for the output video"
        },
        {
          "name": "motion_score",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Motion Score",
          "description": "Motion intensity score (higher = more motion)"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 6,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation (higher = more prompt adherence)"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation. If not provided, a random seed will be used."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "A chaotic sequence with misshapen, deformed limbs in heavy motion blur, sudden disappearance, jump cuts, jerky movements, rapid shot changes, frames out of sync, inconsistent character shapes, temporal artifacts, jitter, and ghosting effects, creating a disorienting visual experience.",
          "title": "Negative Prompt",
          "description": "The negative prompt describing what to avoid in the generation"
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "See Dance V15 Pro Text To Video",
      "description": "SeeDance v1.5 Pro from ByteDance generates high-quality dance videos from text prompts.\n    video, generation, dance, seedance, bytedance, text-to-video\n\n    Use cases:\n    - Generate dance choreography videos\n    - Create dance performance visualizations\n    - Produce music video concepts\n    - Generate dance training content\n    - Create dance animation prototypes",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.SeeDanceV15ProTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the video in seconds"
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "camera_fixed",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Camera Fixed",
          "description": "Whether to fix the camera position"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control video generation. Use -1 for random."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "See Dance V1 Pro Fast Text To Video",
      "description": "SeeDance v1 Pro Fast generates dance videos quickly from text with reduced generation time.\n    video, generation, dance, seedance, fast, bytedance, text-to-video\n\n    Use cases:\n    - Rapidly prototype dance videos\n    - Create quick dance previews\n    - Generate dance concepts efficiently\n    - Iterate on choreography ideas\n    - Produce dance storyboards",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.SeeDanceV1ProFastTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the video"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "2",
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the video in seconds"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.Resolution"
          },
          "default": "1080p",
          "title": "Resolution",
          "description": "Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "camera_fixed",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Camera Fixed",
          "description": "Whether to fix the camera position"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control video generation. Use -1 for random."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Stable Video",
      "description": "Stable Video generates consistent and stable video sequences from text prompts.\n    video, generation, stable, text-to-video, txt2vid\n\n    Use cases:\n    - Generate stable video sequences\n    - Create consistent video content\n    - Produce reliable video outputs\n    - Generate predictable video scenes\n    - Create controlled video generation",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.StableVideo",
      "properties": [
        {
          "name": "motion_bucket_id",
          "type": {
            "type": "int"
          },
          "default": 127,
          "title": "Motion Bucket Id",
          "description": "The motion bucket id determines the motion of the generated video. The higher the number, the more motion there will be."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "cond_aug",
          "type": {
            "type": "float"
          },
          "default": 0.02,
          "title": "Cond Aug",
          "description": "The conditoning augmentation determines the amount of noise that will be added to the conditioning frame. The higher the number, the more noise there will be, and the less the video will look like the initial image. Increase it for more motion."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "T2 VTurbo",
      "description": "T2V Turbo generates videos from text at high speed with optimized performance.\n    video, generation, turbo, fast, text-to-video, txt2vid\n\n    Use cases:\n    - Generate videos with minimal latency\n    - Create rapid video prototypes\n    - Produce quick video previews\n    - Generate real-time video content\n    - Create efficient video workflows",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.T2VTurbo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate images from"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The guidance scale"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed to use for the random number generator"
        },
        {
          "name": "export_fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Export Fps",
          "description": "The FPS of the exported video"
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Num Frames",
          "description": "The number of frames to generate"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of steps to sample"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Veed Avatars Text To Video",
      "description": "VEED Avatars generates talking avatar videos from text using realistic AI-powered characters.\n    video, generation, avatar, talking-head, veed, text-to-video\n\n    Use cases:\n    - Create talking avatar presentations\n    - Generate spokesperson videos\n    - Produce educational talking head videos\n    - Create personalized video messages\n    - Generate multilingual avatar content",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.VeedAvatarsTextToVideo",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text"
        },
        {
          "name": "avatar_id",
          "type": {
            "type": "enum",
            "values": [
              "emily_vertical_primary",
              "emily_vertical_secondary",
              "marcus_vertical_primary",
              "marcus_vertical_secondary",
              "mira_vertical_primary",
              "mira_vertical_secondary",
              "jasmine_vertical_primary",
              "jasmine_vertical_secondary",
              "jasmine_vertical_walking",
              "aisha_vertical_walking",
              "elena_vertical_primary",
              "elena_vertical_secondary",
              "any_male_vertical_primary",
              "any_female_vertical_primary",
              "any_male_vertical_secondary",
              "any_female_vertical_secondary",
              "any_female_vertical_walking",
              "emily_primary",
              "emily_side",
              "marcus_primary",
              "marcus_side",
              "aisha_walking",
              "elena_primary",
              "elena_side",
              "any_male_primary",
              "any_female_primary",
              "any_male_side",
              "any_female_side"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.VeedAvatarsTextToVideo.AvatarId"
          },
          "default": "",
          "title": "Avatar Id",
          "description": "The avatar to use for the video"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Veed Fabric 10 Text",
      "description": "VEED Fabric 1.0 generates video content from text using advanced video synthesis.\n    video, generation, fabric, veed, text-to-video, txt2vid\n\n    Use cases:\n    - Generate marketing videos from text\n    - Create explainer video content\n    - Produce video ads from copy\n    - Generate social media videos\n    - Create branded video content",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.VeedFabric10Text",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "480p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.VeedFabric10Text.Resolution"
          },
          "default": "",
          "title": "Resolution",
          "description": "Resolution"
        },
        {
          "name": "voice_description",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Voice Description",
          "description": "Optional additional voice description. The primary voice description is auto-generated from the image. You can use simple descriptors like 'British accent' or 'Confident' or provide a detailed description like 'Confident male voice, mid-20s, with notes of...'"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Veo 3 1",
      "description": "Veo 3.1\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Veo3_1",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the video you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4s",
              "6s",
              "8s"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Veo3_1.Duration"
          },
          "default": "8s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Veo3_1.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video"
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "4k"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Veo3_1.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Veo 3 1 Fast",
      "description": "Veo 3.1 Fast\n    video, generation, text-to-video, txt2vid, fast\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Veo3_1Fast",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the video you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4s",
              "6s",
              "8s"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Veo3_1Fast.Duration"
          },
          "default": "8s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Veo3_1Fast.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video"
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "4k"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Veo3_1Fast.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan 25 Preview Text To Video",
      "description": "Wan 2.5 Text to Video\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Wan25PreviewTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt for video generation. Supports Chinese and English, max 800 characters."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the generated video in seconds. Choose between 5 or 10 seconds."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.Resolution"
          },
          "default": "1080p",
          "title": "Resolution",
          "description": "Video resolution tier"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "URL of the audio to use as the background music. Must be publicly accessible. Limit handling: If the audio duration exceeds the duration value (5 or 10 seconds), the audio is truncated to the first 5 or 10 seconds, and the rest is discarded. If the audio is shorter than the video, the remaining part of the video will be silent. For example, if the audio is 3 seconds long and the video duration is 5 seconds, the first 3 seconds of the output video will have sound, and the last 2 seconds will be silent. - Format: WAV, MP3. - Duration: 3 to 30 s. - File size: Up to 15 MB."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to describe content to avoid. Max 500 characters."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan Alpha",
      "description": "Wan Alpha\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.WanAlpha",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 10.5,
          "title": "Shift",
          "description": "The shift of the generated video."
        },
        {
          "name": "mask_clamp_upper",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Mask Clamp Upper",
          "description": "The upper bound of the mask clamping."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "mask_clamp_lower",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Mask Clamp Lower",
          "description": "The lower bound of the mask clamping."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "mask_binarization_threshold",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Mask Binarization Threshold",
          "description": "The threshold for mask binarization. When binarize_mask is True, this threshold will be used to binarize the mask. This will also be used for transparency when the output type is `.webm`."
        },
        {
          "name": "sampler",
          "type": {
            "type": "enum",
            "values": [
              "unipc",
              "dpmPP",
              "euler"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanAlpha.Sampler"
          },
          "default": "euler",
          "title": "Sampler",
          "description": "The sampler to use."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanAlpha.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanAlpha.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanAlpha.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanAlpha.VideoOutputType"
          },
          "default": "VP9 (.webm)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "binarize_mask",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Binarize Mask",
          "description": "Whether to binarize the mask."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanAlpha.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan V2 6 Text To Video",
      "description": "Wan v2.6 Text to Video\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.WanV2_6TextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt for video generation. Supports Chinese and English, max 800 characters. For multi-shot videos, use format: 'Overall description. First shot [0-3s] content. Second shot [3-5s] content.'"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10",
              "15"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV2_6TextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the generated video in seconds. Choose between 5, 10, or 15 seconds."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV2_6TextToVideo.Resolution"
          },
          "default": "1080p",
          "title": "Resolution",
          "description": "Video resolution tier. Wan 2.6 T2V only supports 720p and 1080p (no 480p)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1",
              "4:3",
              "3:4"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV2_6TextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video. Wan 2.6 supports additional ratios."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "URL of the audio to use as the background music. Must be publicly accessible. Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds), the audio is truncated to the first N seconds, and the rest is discarded. If the audio is shorter than the video, the remaining part of the video will be silent. For example, if the audio is 3 seconds long and the video duration is 5 seconds, the first 3 seconds of the output video will have sound, and the last 2 seconds will be silent. - Format: WAV, MP3. - Duration: 3 to 30 s. - File size: Up to 15 MB."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "multi_shots",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Multi Shots",
          "description": "When true, enables intelligent multi-shot segmentation for coherent narrative videos. Only active when enable_prompt_expansion is True. Set to false for single-shot generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to describe content to avoid. Max 500 characters."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 4B Base Trainer",
      "description": "Flux 2 Klein 4B Base Trainer\n    flux, training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.Flux2Klein4BBaseTrainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Total number of training steps."
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better. The zip can also contain a text file for each image. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 5e-05,
          "title": "Learning Rate",
          "description": "Learning rate applied to trainable parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        },
        {
          "name": "output_lora_format",
          "type": {
            "type": "enum",
            "values": [
              "fal",
              "comfy"
            ],
            "type_name": "nodetool.nodes.fal.training.Flux2Klein4BBaseTrainer.OutputLoraFormat"
          },
          "default": "fal",
          "title": "Output Lora Format",
          "description": "Dictates the naming scheme for the output weights"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption",
        "output_lora_format"
      ]
    },
    {
      "title": "Flux 2 Klein 4B Base Trainer Edit",
      "description": "Flux 2 Klein 4B Base Trainer\n    flux, training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.Flux2Klein4BBaseTrainerEdit",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Total number of training steps."
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain up to four reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 5e-05,
          "title": "Learning Rate",
          "description": "Learning rate applied to trainable parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        },
        {
          "name": "output_lora_format",
          "type": {
            "type": "enum",
            "values": [
              "fal",
              "comfy"
            ],
            "type_name": "nodetool.nodes.fal.training.Flux2Klein4BBaseTrainerEdit.OutputLoraFormat"
          },
          "default": "fal",
          "title": "Output Lora Format",
          "description": "Dictates the naming scheme for the output weights"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption",
        "output_lora_format"
      ]
    },
    {
      "title": "Flux 2 Klein 9B Base Trainer",
      "description": "Flux 2 Klein 9B Base Trainer\n    flux, training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.Flux2Klein9BBaseTrainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Total number of training steps."
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better. The zip can also contain a text file for each image. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 5e-05,
          "title": "Learning Rate",
          "description": "Learning rate applied to trainable parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        },
        {
          "name": "output_lora_format",
          "type": {
            "type": "enum",
            "values": [
              "fal",
              "comfy"
            ],
            "type_name": "nodetool.nodes.fal.training.Flux2Klein9BBaseTrainer.OutputLoraFormat"
          },
          "default": "fal",
          "title": "Output Lora Format",
          "description": "Dictates the naming scheme for the output weights"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption",
        "output_lora_format"
      ]
    },
    {
      "title": "Flux 2 Klein 9B Base Trainer Edit",
      "description": "Flux 2 Klein 9B Base Trainer\n    flux, training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.Flux2Klein9BBaseTrainerEdit",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Total number of training steps."
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain up to four reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 5e-05,
          "title": "Learning Rate",
          "description": "Learning rate applied to trainable parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        },
        {
          "name": "output_lora_format",
          "type": {
            "type": "enum",
            "values": [
              "fal",
              "comfy"
            ],
            "type_name": "nodetool.nodes.fal.training.Flux2Klein9BBaseTrainerEdit.OutputLoraFormat"
          },
          "default": "fal",
          "title": "Output Lora Format",
          "description": "Dictates the naming scheme for the output weights"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption",
        "output_lora_format"
      ]
    },
    {
      "title": "Flux 2 Trainer V2",
      "description": "Flux 2 Trainer V2\n    flux, training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.Flux2TrainerV2",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Total number of training steps."
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better. The zip can also contain a text file for each image. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 5e-05,
          "title": "Learning Rate",
          "description": "Learning rate applied to trainable parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        },
        {
          "name": "output_lora_format",
          "type": {
            "type": "enum",
            "values": [
              "fal",
              "comfy"
            ],
            "type_name": "nodetool.nodes.fal.training.Flux2TrainerV2.OutputLoraFormat"
          },
          "default": "fal",
          "title": "Output Lora Format",
          "description": "Dictates the naming scheme for the output weights"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption",
        "output_lora_format"
      ]
    },
    {
      "title": "Flux 2 Trainer V2 Edit",
      "description": "Flux 2 Trainer V2\n    flux, training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.Flux2TrainerV2Edit",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Total number of training steps."
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain up to four reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 5e-05,
          "title": "Learning Rate",
          "description": "Learning rate applied to trainable parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        },
        {
          "name": "output_lora_format",
          "type": {
            "type": "enum",
            "values": [
              "fal",
              "comfy"
            ],
            "type_name": "nodetool.nodes.fal.training.Flux2TrainerV2Edit.OutputLoraFormat"
          },
          "default": "fal",
          "title": "Output Lora Format",
          "description": "Dictates the naming scheme for the output weights"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption",
        "output_lora_format"
      ]
    },
    {
      "title": "Qwen Image 2512 Trainer",
      "description": "Qwen Image 2512 Trainer\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.QwenImage2512Trainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Number of steps to train for"
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive for text-to-image training. The zip should contain images with their corresponding text captions: image.EXT and image.txt For example: photo.jpg and photo.txt The text file contains the caption/prompt describing the target image. If no text file is provided for an image, the default_caption will be used. If no default_caption is provided and a text file is missing, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0005,
          "title": "Learning Rate",
          "description": "Learning rate for LoRA parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption"
      ]
    },
    {
      "title": "Qwen Image 2512 Trainer V2",
      "description": "Qwen Image 2512 Trainer V2\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.QwenImage2512TrainerV2",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 2000,
          "title": "Steps",
          "description": "Number of steps to train for"
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images and corresponding captions. The images should be named: ROOT.EXT. For example: 001.jpg The corresponding captions should be named: ROOT.txt. For example: 001.txt If no text file is provided for an image, the default_caption will be used."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0005,
          "title": "Learning Rate",
          "description": "Learning rate."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Trainer",
      "description": "Qwen Image Edit 2509 Trainer\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.QwenImageEdit2509Trainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Number of steps to train for"
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain more than one reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The Reference Image Count field should be set to the number of reference images. The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0001,
          "title": "Learning Rate",
          "description": "Learning rate for LoRA parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption"
      ]
    },
    {
      "title": "Qwen Image Edit 2511 Trainer",
      "description": "Qwen Image Edit 2511 Trainer\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.QwenImageEdit2511Trainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Number of steps to train for"
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain more than one reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The Reference Image Count field should be set to the number of reference images. The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0001,
          "title": "Learning Rate",
          "description": "Learning rate for LoRA parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption"
      ]
    },
    {
      "title": "Qwen Image Layered Trainer",
      "description": "Qwen Image Layered Trainer\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.QwenImageLayeredTrainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Number of steps to train for"
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain groups of images. The images should be named: ROOT_start.EXT, ROOT_end.EXT, ROOT_end2.EXT, ..., ROOT_endN.EXT For example: photo_start.png, photo_end.png, photo_end2.png, ..., photo_endN.png The start image is the base image that will be decomposed into layers. The end images are the layers that will be added to the base image. ROOT_end.EXT is the first layer, ROOT_end2.EXT is the second layer, and so on. You can have up to 8 layers. All image groups must have the same number of output layers. The end images can contain transparent regions. Only PNG and WebP images are supported since these are the only formats that support transparency. The zip can also contain a text file for each image group. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify a description of the base image. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0001,
          "title": "Learning Rate",
          "description": "Learning rate for LoRA parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption"
      ]
    },
    {
      "title": "ZImage Base Trainer",
      "description": "Z-Image Trainer\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.ZImageBaseTrainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 2000,
          "title": "Steps",
          "description": "Number of steps to train for"
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images and corresponding captions. The images should be named: ROOT.EXT. For example: 001.jpg The corresponding captions should be named: ROOT.txt. For example: 001.txt If no text file is provided for an image, the default_caption will be used."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0005,
          "title": "Learning Rate",
          "description": "Learning rate."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption"
      ]
    },
    {
      "title": "ZImage Turbo Trainer V2",
      "description": "Z Image Turbo Trainer V2\n    training, fine-tuning, lora, model-training, fast\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.ZImageTurboTrainerV2",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 2000,
          "title": "Steps",
          "description": "Number of steps to train for"
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images and corresponding captions. The images should be named: ROOT.EXT. For example: 001.jpg The corresponding captions should be named: ROOT.txt. For example: 001.txt If no text file is provided for an image, the default_caption will be used."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0005,
          "title": "Learning Rate",
          "description": "Learning rate."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption"
      ]
    },
    {
      "title": "Index Tts 2 Text To Speech",
      "description": "Index TTS 2 generates natural-sounding speech from text with advanced neural synthesis.\n    speech, synthesis, text-to-speech, tts, neural\n\n    Use cases:\n    - Generate natural speech from text\n    - Create voice narration\n    - Produce audio books\n    - Generate voice-overs\n    - Create speech content",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.IndexTts2TextToSpeech",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The speech prompt to generate"
        },
        {
          "name": "emotional_strengths",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Emotional Strengths",
          "description": "The strengths of individual emotions for fine-grained control."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Strength",
          "description": "The strength of the emotional style transfer. Higher values result in stronger emotional influence."
        },
        {
          "name": "emotional_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Emotional Audio Url",
          "description": "The emotional reference audio file to extract the style from."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The audio file to generate the speech from."
        },
        {
          "name": "emotion_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Emotion Prompt",
          "description": "The emotional prompt to influence the emotional style. Must be used together with should_use_prompt_for_emotion."
        },
        {
          "name": "should_use_prompt_for_emotion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Should Use Prompt For Emotion",
          "description": "Whether to use the `prompt` to calculate emotional strengths, if enabled it will overwrite the `emotional_strengths` values. If `emotion_prompt` is provided, it will be used to instead of `prompt` to extract the emotional style."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Maya",
      "description": "Maya generates high-quality natural speech from text with advanced voice synthesis capabilities.\n    audio, tts, maya, high-quality, text-to-speech\n\n    Use cases:\n    - Generate high-quality speech from text\n    - Create professional voice-overs\n    - Produce premium audio narration\n    - Generate natural-sounding speech\n    - Create professional audio content",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.Maya",
      "properties": [
        {
          "name": "repetition_penalty",
          "type": {
            "type": "float"
          },
          "default": 1.1,
          "title": "Repetition Penalty",
          "description": "Penalty for repeating tokens. Higher values reduce repetition artifacts."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Description of the voice/character. Includes attributes like age, accent, pitch, timbre, pacing, tone, and intensity. See examples for format."
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Top P",
          "description": "Nucleus sampling parameter. Controls diversity of token selection."
        },
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to synthesize into speech. You can embed emotion tags anywhere in the text using the format <emotion_name>. Available emotions: laugh, laugh_harder, sigh, chuckle, gasp, angry, excited, whisper, cry, scream, sing, snort, exhale, gulp, giggle, sarcastic, curious. Example: 'Hello world! <excited> This is amazing!' or 'I can't believe this <sigh> happened again.'"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "wav",
              "mp3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.Maya.OutputFormat"
          },
          "default": "wav",
          "title": "Output Format",
          "description": "Output audio format for the generated speech"
        },
        {
          "name": "max_tokens",
          "type": {
            "type": "int"
          },
          "default": 2000,
          "title": "Max Tokens",
          "description": "Maximum number of SNAC tokens to generate (7 tokens per frame). Controls maximum audio length."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.4,
          "title": "Temperature",
          "description": "Sampling temperature. Lower values (0.2-0.5) produce more stable/consistent audio. Higher values add variation."
        },
        {
          "name": "sample_rate",
          "type": {
            "type": "enum",
            "values": [
              "48 kHz",
              "24 kHz"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.Maya.SampleRate"
          },
          "default": "48 kHz",
          "title": "Sample Rate",
          "description": "Output audio sample rate. 48 kHz provides higher quality audio, 24 kHz is faster."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Maya Batch",
      "description": "Maya Batch TTS generates high-quality speech in batch mode for efficient processing.\n    speech, synthesis, text-to-speech, tts, batch, maya\n\n    Use cases:\n    - Generate speech for multiple texts\n    - Batch process narration\n    - Create bulk voice-overs\n    - Efficient audio content creation\n    - Generate multiple speech files",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.MayaBatch",
      "properties": [
        {
          "name": "repetition_penalty",
          "type": {
            "type": "float"
          },
          "default": 1.1,
          "title": "Repetition Penalty",
          "description": "Repetition penalty for all generations."
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Top P",
          "description": "Nucleus sampling parameter for all generations."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "wav",
              "mp3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MayaBatch.OutputFormat"
          },
          "default": "wav",
          "title": "Output Format",
          "description": "Output audio format for all generated speech files"
        },
        {
          "name": "texts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Texts",
          "description": "List of texts to synthesize into speech. You can embed emotion tags in each text using the format <emotion_name>."
        },
        {
          "name": "prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Prompts",
          "description": "List of voice descriptions for each text. Must match the length of texts list. Each describes the voice/character attributes."
        },
        {
          "name": "max_tokens",
          "type": {
            "type": "int"
          },
          "default": 2000,
          "title": "Max Tokens",
          "description": "Maximum SNAC tokens per generation."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.4,
          "title": "Temperature",
          "description": "Sampling temperature for all generations."
        },
        {
          "name": "sample_rate",
          "type": {
            "type": "enum",
            "values": [
              "48 kHz",
              "24 kHz"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MayaBatch.SampleRate"
          },
          "default": "48 kHz",
          "title": "Sample Rate",
          "description": "Output audio sample rate for all generations. 48 kHz provides higher quality, 24 kHz is faster."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Maya Stream",
      "description": "Maya Stream TTS generates high-quality speech in streaming mode for real-time applications.\n    speech, synthesis, text-to-speech, tts, streaming, maya\n\n    Use cases:\n    - Generate speech in real-time\n    - Stream narration dynamically\n    - Create live voice-overs\n    - Real-time audio synthesis\n    - Generate streaming speech",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.MayaStream",
      "properties": [
        {
          "name": "repetition_penalty",
          "type": {
            "type": "float"
          },
          "default": 1.1,
          "title": "Repetition Penalty",
          "description": "Penalty for repeating tokens. Higher values reduce repetition artifacts."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Description of the voice/character. Includes attributes like age, accent, pitch, timbre, pacing, tone, and intensity. See examples for format."
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Top P",
          "description": "Nucleus sampling parameter. Controls diversity of token selection."
        },
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to synthesize into speech. You can embed emotion tags anywhere in the text using the format <emotion_name>. Available emotions: laugh, laugh_harder, sigh, chuckle, gasp, angry, excited, whisper, cry, scream, sing, snort, exhale, gulp, giggle, sarcastic, curious. Example: 'Hello world! <excited> This is amazing!' or 'I can't believe this <sigh> happened again.'"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "mp3",
              "wav",
              "pcm"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MayaStream.OutputFormat"
          },
          "default": "mp3",
          "title": "Output Format",
          "description": "Output audio format. 'mp3' for browser-playable audio, 'wav' for uncompressed audio, 'pcm' for raw PCM (lowest latency, requires client-side decoding)."
        },
        {
          "name": "max_tokens",
          "type": {
            "type": "int"
          },
          "default": 2000,
          "title": "Max Tokens",
          "description": "Maximum number of SNAC tokens to generate (7 tokens per frame). Controls maximum audio length."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.4,
          "title": "Temperature",
          "description": "Sampling temperature. Lower values (0.2-0.5) produce more stable/consistent audio. Higher values add variation."
        },
        {
          "name": "sample_rate",
          "type": {
            "type": "enum",
            "values": [
              "48 kHz",
              "24 kHz"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MayaStream.SampleRate"
          },
          "default": "24 kHz",
          "title": "Sample Rate",
          "description": "Output audio sample rate. 48 kHz uses upsampling for higher quality audio, 24 kHz is native SNAC output (faster, lower latency)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Minimax Speech 26 Hd",
      "description": "Minimax Speech 2.6 HD generates high-definition speech from text with superior audio quality.\n    audio, tts, minimax, 2.6, hd, high-quality\n\n    Use cases:\n    - Generate HD quality speech from text\n    - Create premium voice-overs\n    - Produce high-fidelity audio narration\n    - Generate superior audio quality speech\n    - Create broadcast-quality audio",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.MinimaxSpeech26Hd",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text to convert to speech. Paragraph breaks should be marked with newline characters. **NOTE**: You can customize speech pauses by adding markers in the form `<#x#>`, where `x` is the pause duration in seconds. Valid range: `[0.01, 99.99]`, up to two decimal places. Pause markers must be placed between speakable text segments and cannot be used consecutively."
        },
        {
          "name": "language_boost",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "Chinese",
              "Chinese,Yue",
              "English",
              "Arabic",
              "Russian",
              "Spanish",
              "French",
              "Portuguese",
              "German",
              "Turkish",
              "Dutch",
              "Ukrainian",
              "Vietnamese",
              "Indonesian",
              "Japanese",
              "Italian",
              "Korean",
              "Thai",
              "Polish",
              "Romanian",
              "Greek",
              "Czech",
              "Finnish",
              "Hindi",
              "Bulgarian",
              "Danish",
              "Hebrew",
              "Malay",
              "Slovak",
              "Swedish",
              "Croatian",
              "Hungarian",
              "Norwegian",
              "Slovenian",
              "Catalan",
              "Nynorsk",
              "Afrikaans",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MinimaxSpeech26Hd.LanguageBoost"
          },
          "default": null,
          "title": "Language Boost",
          "description": "Enhance recognition of specified languages and dialects"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "url",
              "hex"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MinimaxSpeech26Hd.MinimaxSpeech26HdOutputFormat"
          },
          "default": "hex",
          "title": "Output Format",
          "description": "Format of the output content (non-streaming only)"
        },
        {
          "name": "pronunciation_dict",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Pronunciation Dict",
          "description": "Custom pronunciation dictionary for text replacement"
        },
        {
          "name": "voice_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Voice Setting",
          "description": "Voice configuration settings"
        },
        {
          "name": "normalization_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Normalization Setting",
          "description": "Loudness normalization settings for the audio"
        },
        {
          "name": "audio_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Audio Setting",
          "description": "Audio configuration settings"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Minimax Speech 26 Turbo",
      "description": "Minimax Speech 2.6 Turbo generates speech from text with optimized speed and good quality.\n    audio, tts, minimax, 2.6, turbo, fast\n\n    Use cases:\n    - Generate speech quickly from text\n    - Create fast voice-overs\n    - Produce rapid audio narration\n    - Generate speech with turbo speed\n    - Create efficient audio content",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.MinimaxSpeech26Turbo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text to convert to speech. Paragraph breaks should be marked with newline characters. **NOTE**: You can customize speech pauses by adding markers in the form `<#x#>`, where `x` is the pause duration in seconds. Valid range: `[0.01, 99.99]`, up to two decimal places. Pause markers must be placed between speakable text segments and cannot be used consecutively."
        },
        {
          "name": "language_boost",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "Chinese",
              "Chinese,Yue",
              "English",
              "Arabic",
              "Russian",
              "Spanish",
              "French",
              "Portuguese",
              "German",
              "Turkish",
              "Dutch",
              "Ukrainian",
              "Vietnamese",
              "Indonesian",
              "Japanese",
              "Italian",
              "Korean",
              "Thai",
              "Polish",
              "Romanian",
              "Greek",
              "Czech",
              "Finnish",
              "Hindi",
              "Bulgarian",
              "Danish",
              "Hebrew",
              "Malay",
              "Slovak",
              "Swedish",
              "Croatian",
              "Hungarian",
              "Norwegian",
              "Slovenian",
              "Catalan",
              "Nynorsk",
              "Afrikaans",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MinimaxSpeech26Turbo.LanguageBoost"
          },
          "default": null,
          "title": "Language Boost",
          "description": "Enhance recognition of specified languages and dialects"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "url",
              "hex"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MinimaxSpeech26Turbo.MinimaxSpeech26TurboOutputFormat"
          },
          "default": "hex",
          "title": "Output Format",
          "description": "Format of the output content (non-streaming only)"
        },
        {
          "name": "pronunciation_dict",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Pronunciation Dict",
          "description": "Custom pronunciation dictionary for text replacement"
        },
        {
          "name": "voice_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Voice Setting",
          "description": "Voice configuration settings"
        },
        {
          "name": "normalization_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Normalization Setting",
          "description": "Loudness normalization settings for the audio"
        },
        {
          "name": "audio_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Audio Setting",
          "description": "Audio configuration settings"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Qwen 3 Tts Text To Speech 06B",
      "description": "Qwen-3 TTS 0.6B generates speech from text efficiently using the compact 600-million parameter model.\n    audio, tts, qwen, 0.6b, efficient, text-to-speech\n\n    Use cases:\n    - Generate speech efficiently from text\n    - Create fast voice-overs\n    - Produce quick audio narration\n    - Generate spoken content with low latency\n    - Create efficient text-to-speech",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.Qwen3TtsTextToSpeech06B",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional prompt to guide the style of the generated speech. This prompt will be ignored if a speaker embedding is provided."
        },
        {
          "name": "speaker_voice_embedding_file_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Speaker Voice Embedding File Url",
          "description": "URL to a speaker embedding file in safetensors format, from `fal-ai/qwen-3-tts/clone-voice/0.6b` endpoint. If provided, the TTS model will use the cloned voice for synthesis instead of the predefined voices."
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Top P",
          "description": "Top-p sampling parameter."
        },
        {
          "name": "repetition_penalty",
          "type": {
            "type": "float"
          },
          "default": 1.05,
          "title": "Repetition Penalty",
          "description": "Penalty to reduce repeated tokens/codes."
        },
        {
          "name": "subtalker_temperature",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Subtalker Temperature",
          "description": "Temperature for sub-talker sampling."
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Top K",
          "description": "Top-k sampling parameter."
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "Vivian",
              "Serena",
              "Uncle_Fu",
              "Dylan",
              "Eric",
              "Ryan",
              "Aiden",
              "Ono_Anna",
              "Sohee"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.Qwen3TtsTextToSpeech06B.Voice"
          },
          "default": null,
          "title": "Voice",
          "description": "The voice to be used for speech synthesis, will be ignored if a speaker embedding is provided. Check out the **[documentation](https://github.com/QwenLM/Qwen3-TTS/tree/main?tab=readme-ov-file#custom-voice-generate)** for each voice's details and which language they primarily support."
        },
        {
          "name": "reference_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Reference Text",
          "description": "Optional reference text that was used when creating the speaker embedding. Providing this can improve synthesis quality when using a cloned voice."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Temperature",
          "description": "Sampling temperature; higher => more random."
        },
        {
          "name": "language",
          "type": {
            "type": "enum",
            "values": [
              "Auto",
              "English",
              "Chinese",
              "Spanish",
              "French",
              "German",
              "Italian",
              "Japanese",
              "Korean",
              "Portuguese",
              "Russian"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.Qwen3TtsTextToSpeech06B.Language"
          },
          "default": "Auto",
          "title": "Language",
          "description": "The language of the voice."
        },
        {
          "name": "subtalker_top_k",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Subtalker Top K",
          "description": "Top-k for sub-talker sampling."
        },
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to be converted to speech."
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 200,
          "title": "Max New Tokens",
          "description": "Maximum number of new codec tokens to generate."
        },
        {
          "name": "subtalker_dosample",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Subtalker Dosample",
          "description": "Sampling switch for the sub-talker."
        },
        {
          "name": "subtalker_top_p",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Subtalker Top P",
          "description": "Top-p for sub-talker sampling."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Qwen 3 Tts Text To Speech 17B",
      "description": "Qwen-3 TTS 1.7B generates natural-sounding speech from text using the large 1.7-billion parameter model.\n    audio, tts, qwen, 1.7b, text-to-speech, speech-synthesis\n\n    Use cases:\n    - Generate natural-sounding speech from text\n    - Create voice-overs for videos\n    - Produce audiobook narration\n    - Generate spoken content for applications\n    - Create text-to-speech for accessibility",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.Qwen3TtsTextToSpeech17B",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional prompt to guide the style of the generated speech. This prompt will be ignored if a speaker embedding is provided."
        },
        {
          "name": "speaker_voice_embedding_file_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Speaker Voice Embedding File Url",
          "description": "URL to a speaker embedding file in safetensors format, from `fal-ai/qwen-3-tts/clone-voice` endpoint. If provided, the TTS model will use the cloned voice for synthesis instead of the predefined voices."
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Top P",
          "description": "Top-p sampling parameter."
        },
        {
          "name": "repetition_penalty",
          "type": {
            "type": "float"
          },
          "default": 1.05,
          "title": "Repetition Penalty",
          "description": "Penalty to reduce repeated tokens/codes."
        },
        {
          "name": "subtalker_temperature",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Subtalker Temperature",
          "description": "Temperature for sub-talker sampling."
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Top K",
          "description": "Top-k sampling parameter."
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "Vivian",
              "Serena",
              "Uncle_Fu",
              "Dylan",
              "Eric",
              "Ryan",
              "Aiden",
              "Ono_Anna",
              "Sohee"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.Qwen3TtsTextToSpeech17B.Voice"
          },
          "default": null,
          "title": "Voice",
          "description": "The voice to be used for speech synthesis, will be ignored if a speaker embedding is provided. Check out the **[documentation](https://github.com/QwenLM/Qwen3-TTS/tree/main?tab=readme-ov-file#custom-voice-generate)** for each voice's details and which language they primarily support."
        },
        {
          "name": "reference_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Reference Text",
          "description": "Optional reference text that was used when creating the speaker embedding. Providing this can improve synthesis quality when using a cloned voice."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Temperature",
          "description": "Sampling temperature; higher => more random."
        },
        {
          "name": "language",
          "type": {
            "type": "enum",
            "values": [
              "Auto",
              "English",
              "Chinese",
              "Spanish",
              "French",
              "German",
              "Italian",
              "Japanese",
              "Korean",
              "Portuguese",
              "Russian"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.Qwen3TtsTextToSpeech17B.Language"
          },
          "default": "Auto",
          "title": "Language",
          "description": "The language of the voice."
        },
        {
          "name": "subtalker_top_k",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Subtalker Top K",
          "description": "Top-k for sub-talker sampling."
        },
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to be converted to speech."
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 200,
          "title": "Max New Tokens",
          "description": "Maximum number of new codec tokens to generate."
        },
        {
          "name": "subtalker_dosample",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Subtalker Dosample",
          "description": "Sampling switch for the sub-talker."
        },
        {
          "name": "subtalker_top_p",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Subtalker Top P",
          "description": "Top-p for sub-talker sampling."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Qwen 3 Tts Voice Design 17B",
      "description": "Qwen-3 TTS Voice Design 1.7B creates custom voice characteristics for personalized speech synthesis.\n    audio, tts, qwen, voice-design, custom, 1.7b\n\n    Use cases:\n    - Design custom voice characteristics\n    - Create personalized speech synthesis\n    - Generate unique voice styles\n    - Produce custom voice-overs\n    - Create tailored speech synthesis",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.Qwen3TtsVoiceDesign17B",
      "properties": [
        {
          "name": "repetition_penalty",
          "type": {
            "type": "float"
          },
          "default": 1.05,
          "title": "Repetition Penalty",
          "description": "Penalty to reduce repeated tokens/codes."
        },
        {
          "name": "subtalker_top_k",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Subtalker Top K",
          "description": "Top-k for sub-talker sampling."
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Top P",
          "description": "Top-p sampling parameter."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional prompt to guide the style of the generated speech."
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 200,
          "title": "Max New Tokens",
          "description": "Maximum number of new codec tokens to generate."
        },
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to be converted to speech."
        },
        {
          "name": "language",
          "type": {
            "type": "enum",
            "values": [
              "Auto",
              "English",
              "Chinese",
              "Spanish",
              "French",
              "German",
              "Italian",
              "Japanese",
              "Korean",
              "Portuguese",
              "Russian"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.Qwen3TtsVoiceDesign17B.Language"
          },
          "default": "Auto",
          "title": "Language",
          "description": "The language of the voice to be designed."
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Top K",
          "description": "Top-k sampling parameter."
        },
        {
          "name": "subtalker_dosample",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Subtalker Dosample",
          "description": "Sampling switch for the sub-talker."
        },
        {
          "name": "subtalker_temperature",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Subtalker Temperature",
          "description": "Temperature for sub-talker sampling."
        },
        {
          "name": "subtalker_top_p",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Subtalker Top P",
          "description": "Top-p for sub-talker sampling."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Temperature",
          "description": "Sampling temperature; higher => more random."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Vibevoice 05B",
      "description": "VibeVoice 0.5B generates expressive and emotive speech from text with natural vocal characteristics.\n    audio, tts, vibevoice, 0.5b, expressive, text-to-speech\n\n    Use cases:\n    - Generate expressive speech from text\n    - Create emotive voice-overs\n    - Produce natural vocal narration\n    - Generate speech with personality\n    - Create engaging audio content",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.Vibevoice05B",
      "properties": [
        {
          "name": "script",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Script",
          "description": "The script to convert to speech."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation."
        },
        {
          "name": "speaker",
          "type": {
            "type": "enum",
            "values": [
              "Frank",
              "Wayne",
              "Carter",
              "Emma",
              "Grace",
              "Mike"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.Vibevoice05B.Speaker"
          },
          "default": "",
          "title": "Speaker",
          "description": "Voice to use for speaking."
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 1.3,
          "title": "Cfg Scale",
          "description": "CFG (Classifier-Free Guidance) scale for generation. Higher values increase adherence to text."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Open Router",
      "description": "OpenRouter provides unified access to any LLM (Large Language Model) through a single API.\n    llm, chat, openrouter, multimodel, language-model\n\n    Use cases:\n    - Run any LLM through unified interface\n    - Switch between models seamlessly\n    - Access multiple LLM providers\n    - Flexible model selection\n    - Unified LLM API access",
      "namespace": "fal.llm",
      "node_type": "fal.llm.OpenRouter",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model",
          "description": "Name of the model to use. Charged based on actual token usage."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to be used for the chat completion"
        },
        {
          "name": "max_tokens",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Max Tokens",
          "description": "This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Temperature",
          "description": "This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input."
        },
        {
          "name": "reasoning",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reasoning",
          "description": "Should reasoning be the part of the final answer."
        },
        {
          "name": "system_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "System Prompt",
          "description": "System prompt to provide context or instructions to the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "model"
      ]
    },
    {
      "title": "Open Router Chat Completions",
      "description": "OpenRouter Chat Completions provides OpenAI-compatible interface for any LLM.\n    llm, chat, openai-compatible, openrouter, chat-completions\n\n    Use cases:\n    - OpenAI-compatible LLM access\n    - Drop-in replacement for OpenAI API\n    - Multi-model chat completions\n    - Standardized chat interface\n    - Universal LLM chat API",
      "namespace": "fal.llm",
      "node_type": "fal.llm.OpenRouterChatCompletions",
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "messages",
        "model"
      ]
    },
    {
      "title": "Qwen 3 Guard",
      "description": "Qwen 3 Guard provides content safety and moderation using Qwen's LLM.\n    llm, safety, moderation, qwen, guard\n\n    Use cases:\n    - Content safety checking\n    - Moderation of text content\n    - Safety filtering for outputs\n    - Content policy enforcement\n    - Text safety analysis",
      "namespace": "fal.llm",
      "node_type": "fal.llm.Qwen3Guard",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The input text to be classified"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Face Swap Video",
      "description": "Swap faces in videos using a source face image. Replaces faces in the target video with the source face while maintaining natural motion and expressions.\n    face-swap, video-editing, face-replacement, deep-fake, video-manipulation\n\n    Use cases:\n    - Create face-swapped video content\n    - Generate creative video edits\n    - Produce entertainment content\n    - Test different faces in video footage\n    - Create video memes and parodies",
      "namespace": "fal.video_processing",
      "node_type": "fal.video_processing.FaceSwapVideo",
      "properties": [
        {
          "name": "source_face",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Source Face",
          "description": "Source face image to swap into video"
        },
        {
          "name": "target_video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Target Video",
          "description": "Target video to swap face in (max 25 minutes)"
        },
        {
          "name": "enable_occlusion_prevention",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Occlusion Prevention",
          "description": "Enable occlusion prevention for faces covered by hands/objects (costs 2x more)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "source_face",
        "target_video"
      ]
    },
    {
      "title": "Live Avatar",
      "description": "Live Avatar creates animated avatars from images and audio.\n    video, avatar, animation, audio-driven\n\n    Use cases:\n    - Create talking avatars\n    - Generate animated presentations\n    - Produce video content from photos\n    - Create virtual presenters\n    - Generate video messages",
      "namespace": "fal.video_processing",
      "node_type": "fal.video_processing.LiveAvatar",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The avatar image"
        },
        {
          "name": "audio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Audio",
          "description": "URL to the driving audio"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "audio"
      ]
    },
    {
      "title": "RIFE",
      "description": "RIFE (Real-time Intermediate Flow Estimation) interpolates frames for smooth video.\n    video, interpolation, frame-rate, smoothing\n\n    Use cases:\n    - Increase video frame rate\n    - Create smooth slow motion\n    - Improve video fluidity\n    - Generate intermediate frames\n    - Enhance animation smoothness",
      "namespace": "fal.video_processing",
      "node_type": "fal.video_processing.RIFE",
      "properties": [
        {
          "name": "start_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Start Image",
          "description": "The start frame"
        },
        {
          "name": "end_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image",
          "description": "The end frame"
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 2,
          "title": "Num Frames",
          "description": "Number of intermediate frames",
          "min": 1.0,
          "max": 16.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "start_image",
        "end_image",
        "num_frames"
      ]
    },
    {
      "title": "RIFEVideo",
      "description": "RIFE Video interpolates video frames for increased frame rate.\n    video, interpolation, frame-rate, enhancement\n\n    Use cases:\n    - Double video frame rate\n    - Create slow motion videos\n    - Improve video smoothness\n    - Enhance low-fps footage\n    - Generate high-fps content",
      "namespace": "fal.video_processing",
      "node_type": "fal.video_processing.RIFEVideo",
      "properties": [
        {
          "name": "video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video",
          "description": "The video to interpolate"
        },
        {
          "name": "multiplier",
          "type": {
            "type": "int"
          },
          "default": 2,
          "title": "Multiplier",
          "description": "Frame rate multiplier",
          "min": 2.0,
          "max": 4.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "multiplier"
      ]
    },
    {
      "title": "Sync Lipsync V2",
      "description": "Sync Lipsync V2 synchronizes lip movements to audio.\n    video, lipsync, audio, synchronization\n\n    Use cases:\n    - Sync lips to new audio\n    - Create talking head videos\n    - Dub videos in other languages\n    - Generate speaking animations\n    - Create video voice-overs",
      "namespace": "fal.video_processing",
      "node_type": "fal.video_processing.SyncLipsyncV2",
      "properties": [
        {
          "name": "video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video",
          "description": "The video with the face"
        },
        {
          "name": "audio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Audio",
          "description": "URL to the audio file for lipsync"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "audio"
      ]
    },
    {
      "title": "Topaz Video Upscale",
      "description": "Topaz Video Upscale enhances video quality using advanced AI.\n    video, upscaling, enhancement, topaz, professional\n\n    Use cases:\n    - Professional video upscaling\n    - Restore archival footage\n    - Enhance video for broadcast\n    - Improve video quality\n    - Prepare videos for 4K display",
      "namespace": "fal.video_processing",
      "node_type": "fal.video_processing.TopazVideoUpscale",
      "properties": [
        {
          "name": "video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video",
          "description": "The video to upscale"
        },
        {
          "name": "scale",
          "type": {
            "type": "int"
          },
          "default": 2,
          "title": "Scale",
          "description": "Upscaling factor",
          "min": 2.0,
          "max": 4.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "scale"
      ]
    },
    {
      "title": "Video Upscaler",
      "description": "Video Upscaler enhances video resolution using AI.\n    video, upscaling, enhancement, super-resolution\n\n    Use cases:\n    - Upscale low-resolution videos\n    - Enhance video quality\n    - Improve video for larger displays\n    - Restore old videos\n    - Prepare videos for high-res output",
      "namespace": "fal.video_processing",
      "node_type": "fal.video_processing.VideoUpscaler",
      "properties": [
        {
          "name": "video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video",
          "description": "The video to upscale"
        },
        {
          "name": "scale",
          "type": {
            "type": "float"
          },
          "default": 2.0,
          "title": "Scale",
          "description": "Upscaling factor (1-8)",
          "min": 1.0,
          "max": 8.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "scale"
      ]
    },
    {
      "title": "Bria Fibo Generate",
      "description": "Fibo\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.BriaFiboGenerate",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt for image generation."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "2:3",
              "3:2",
              "3:4",
              "4:3",
              "4:5",
              "5:4",
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.BriaFiboGenerate.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "Aspect ratio. Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9"
        },
        {
          "name": "steps_num",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Steps Num",
          "description": "Number of inference steps."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Reference image (file or URL)."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If true, returns the image directly in the response (increases latency)."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for text."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 5555,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for image generation."
        },
        {
          "name": "structured_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Structured Prompt",
          "description": "The structured prompt to generate an image from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Lite Generate",
      "description": "Fibo Lite\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.BriaFiboLiteGenerate",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt for image generation."
        },
        {
          "name": "steps_num",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Steps Num",
          "description": "Number of inference steps for Fibo Lite."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "2:3",
              "3:2",
              "3:4",
              "4:3",
              "4:5",
              "5:4",
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.BriaFiboLiteGenerate.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "Aspect ratio. Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Reference image (file or URL)."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If true, returns the image directly in the response (increases latency)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 5555,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "structured_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Structured Prompt",
          "description": "The structured prompt to generate an image from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bytedance Dreamina V3 1 Text To Image",
      "description": "Bytedance\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.BytedanceDreaminaV3_1TextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Width and height must be between 512 and 2048."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control the stochasticity of image generation."
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to use an LLM to enhance the prompt"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bytedance Seedream V3 Text To Image",
      "description": "Bytedance\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.BytedanceSeedreamV3TextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "Use for finer control over the output image size. Will be used over aspect_ratio, if both are provided. Width and height must be between 512 and 2048."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "Controls how closely the output image aligns with the input prompt. Higher values mean stronger prompt correlation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control the stochasticity of image generation."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bytedance Seedream V45 Text To Image",
      "description": "ByteDance SeeDream v4.5 generates advanced images from text with cutting-edge AI technology.\n    image, generation, bytedance, seedream, v4.5, text-to-image\n\n    Use cases:\n    - Generate images with SeeDream v4.5\n    - Create cutting-edge visual content\n    - Produce advanced AI artwork\n    - Generate images with latest tech\n    - Create modern AI visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.BytedanceSeedreamV45TextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of separate model generations to be run with the prompt."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Width and height must be between 1920 and 4096, or total number of pixels must be between 2560*1440 and 4096*4096."
        },
        {
          "name": "max_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Max Images",
          "description": "If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control the stochasticity of image generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bytedance Seedream V4 Text To Image",
      "description": "Bytedance Seedream v4\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.BytedanceSeedreamV4TextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of separate model generations to be run with the prompt."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Total pixels must be between 960x960 and 4096x4096."
        },
        {
          "name": "max_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Max Images",
          "description": "If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`."
        },
        {
          "name": "enhance_prompt_mode",
          "type": {
            "type": "enum",
            "values": [
              "standard",
              "fast"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.BytedanceSeedreamV4TextToImage.EnhancePromptMode"
          },
          "default": "standard",
          "title": "Enhance Prompt Mode",
          "description": "The mode to use for enhancing prompt enhancement. Standard mode provides higher quality results but takes longer to generate. Fast mode provides average quality results but takes less time to generate."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control the stochasticity of image generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Emu 3 5 Image Text To Image",
      "description": "Emu 3.5 Image\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Emu3_5ImageTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to create the image."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Emu3_5ImageTextToImage.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the output image."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Emu3_5ImageTextToImage.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the output image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Emu3_5ImageTextToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "Whether to return the image in sync mode."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the inference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 1 Krea",
      "description": "FLUX.1 Krea [dev]\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux1Krea",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux1Krea.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux1Krea.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 1 Srpo",
      "description": "FLUX.1 SRPO [dev]\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux1Srpo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux1Srpo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux1Srpo.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Flash",
      "description": "FLUX.2 Flash is an ultra-fast variant of FLUX.2 designed for instant image generation with minimal latency.\n    image, generation, flux, ultra-fast, flash, text-to-image, txt2img\n\n    Use cases:\n    - Instant preview generation for user interfaces\n    - Real-time collaborative design tools\n    - Lightning-fast concept exploration\n    - High-speed batch processing\n    - Interactive gaming and entertainment applications",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Flash",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Flash.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "If set to true, the prompt will be expanded for better results."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image_size"
      ]
    },
    {
      "title": "Flux 2 Flex",
      "description": "Flux 2 Flex\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Flex",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Flex.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Flex.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to expand the prompt using the model's own knowledge."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 4B",
      "description": "FLUX-2 Klein 4B generates images with the efficient 4-billion parameter model for balanced quality and speed.\n    image, generation, flux-2, klein, 4b, text-to-image\n\n    Use cases:\n    - Generate images with 4B model\n    - Create balanced quality-speed content\n    - Produce efficient visual artwork\n    - Generate images with good performance\n    - Create optimized visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Klein4B",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the image to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein4B.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 4B Base",
      "description": "FLUX-2 Klein 4B Base provides foundation model generation with 4-billion parameters.\n    image, generation, flux-2, klein, 4b, base\n\n    Use cases:\n    - Generate with base 4B model\n    - Create foundation quality content\n    - Produce standard visual artwork\n    - Generate images with base model\n    - Create baseline visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Klein4BBase",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the image to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein4BBase.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for image generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein4BBase.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for classifier-free guidance. Describes what to avoid in the image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 4B Base Lora",
      "description": "FLUX-2 Klein 4B Base with LoRA enables custom-trained 4B models for specialized generation.\n    image, generation, flux-2, klein, 4b, base, lora\n\n    Use cases:\n    - Generate with custom 4B base model\n    - Create specialized foundation content\n    - Produce domain-specific visuals\n    - Generate with fine-tuned 4B model\n    - Create customized baseline visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Klein4BBaseLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the image to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein4BBaseLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for image generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein4BBaseLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for classifier-free guidance. Describes what to avoid in the image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 9B",
      "description": "FLUX-2 Klein 9B generates high-quality images with the powerful 9-billion parameter model.\n    image, generation, flux-2, klein, 9b, text-to-image\n\n    Use cases:\n    - Generate high-quality images with 9B model\n    - Create superior visual content\n    - Produce detailed artwork\n    - Generate images with powerful model\n    - Create premium quality visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Klein9B",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the image to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein9B.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 9B Base",
      "description": "FLUX-2 Klein 9B Base provides foundation generation with the full 9-billion parameter model.\n    image, generation, flux-2, klein, 9b, base\n\n    Use cases:\n    - Generate with base 9B model\n    - Create high-quality foundation content\n    - Produce superior baseline artwork\n    - Generate images with powerful base\n    - Create premium baseline visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Klein9BBase",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the image to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein9BBase.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for image generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein9BBase.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for classifier-free guidance. Describes what to avoid in the image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 9B Base Lora",
      "description": "FLUX-2 Klein 9B Base with LoRA combines powerful generation with custom-trained models.\n    image, generation, flux-2, klein, 9b, base, lora\n\n    Use cases:\n    - Generate with custom 9B base model\n    - Create specialized high-quality content\n    - Produce custom superior visuals\n    - Generate with fine-tuned 9B model\n    - Create advanced customized visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Klein9BBaseLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the image to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein9BBaseLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for image generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein9BBaseLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for classifier-free guidance. Describes what to avoid in the image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Ballpoint Pen Sketch",
      "description": "Flux 2 Lora Gallery\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2LoraGalleryBallpointPenSketch",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a ballpoint pen sketch style image. Use 'b4llp01nt' trigger word for best results."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGalleryBallpointPenSketch.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the ballpoint pen sketch effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGalleryBallpointPenSketch.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Digital Comic Art",
      "description": "Flux 2 Lora Gallery\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2LoraGalleryDigitalComicArt",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a digital comic art style image. Use 'd1g1t4l' trigger word for best results."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGalleryDigitalComicArt.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the digital comic art effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGalleryDigitalComicArt.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Hdr Style",
      "description": "Flux 2 Lora Gallery\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2LoraGalleryHdrStyle",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an HDR style image. The trigger word 'Hyp3rRe4list1c' will be automatically prepended."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGalleryHdrStyle.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the HDR style effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGalleryHdrStyle.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Realism",
      "description": "Flux 2 Lora Gallery\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2LoraGalleryRealism",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a realistic image with natural lighting and authentic details."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGalleryRealism.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the realism effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGalleryRealism.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Satellite View Style",
      "description": "Flux 2 Lora Gallery\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2LoraGallerySatelliteViewStyle",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a satellite/aerial view style image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGallerySatelliteViewStyle.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the satellite view style effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGallerySatelliteViewStyle.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Sepia Vintage",
      "description": "Flux 2 Lora Gallery\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2LoraGallerySepiaVintage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a sepia vintage photography style image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGallerySepiaVintage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the sepia vintage photography effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGallerySepiaVintage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Max",
      "description": "FLUX-2 Max generates maximum quality images with the most advanced FLUX-2 model for premium results.\n    image, generation, flux-2, max, premium, text-to-image\n\n    Use cases:\n    - Generate maximum quality images\n    - Create premium visual content\n    - Produce professional-grade artwork\n    - Generate images with best model\n    - Create superior quality visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Max",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Max.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Max.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Turbo",
      "description": "FLUX.2 Turbo is a blazing-fast image generation model optimized for speed without sacrificing quality, ideal for real-time applications.\n    image, generation, flux, fast, turbo, text-to-image, txt2img\n\n    Use cases:\n    - Real-time image generation for interactive apps\n    - Rapid prototyping of visual concepts\n    - Generate multiple variations instantly\n    - Live visual effects and augmented reality\n    - High-throughput batch image processing",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Turbo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Turbo.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "If set to true, the prompt will be expanded for better results."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image_size",
        "num_images"
      ]
    },
    {
      "title": "Flux Dev",
      "description": "FLUX.1 [dev] is a powerful open-weight text-to-image model with 12 billion parameters. Optimized for prompt following and visual quality.\n    image, generation, flux, text-to-image, txt2img\n\n    Use cases:\n    - Generate high-quality images from text prompts\n    - Create detailed illustrations with precise control\n    - Produce professional artwork and designs\n    - Generate multiple variations from one prompt\n    - Create safe-for-work content with built-in safety checker",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxDev",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxDev.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxDev.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable safety checker to filter unsafe content"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "How strictly to follow the prompt. Higher values are more literal"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps. More steps typically improve quality"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image_size",
        "num_inference_steps"
      ]
    },
    {
      "title": "Flux Kontext Lora Text To Image",
      "description": "Flux Kontext Lora\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxKontextLoraTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxKontextLoraTextToImage.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxKontextLoraTextToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux Krea",
      "description": "FLUX.1 Krea [dev]\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxKrea",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxKrea.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxKrea.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux Krea Lora",
      "description": "FLUX.1 Krea [dev] with LoRAs\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxKreaLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxKreaLora.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux Krea Lora Stream",
      "description": "Flux Krea Lora\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxKreaLoraStream",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxKreaLoraStream.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux Lora",
      "description": "FLUX with LoRA support enables fine-tuned image generation using custom LoRA models for specific styles or subjects.\n    image, generation, flux, lora, fine-tuning, text-to-image, txt2img\n\n    Use cases:\n    - Generate images with custom artistic styles\n    - Create consistent characters across images\n    - Apply brand-specific visual styles\n    - Generate images with specialized subjects\n    - Combine multiple LoRA models for unique results",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxLora.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA models to apply with their weights"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "How strictly to follow the prompt"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable safety checker to filter unsafe content"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "loras",
        "image_size"
      ]
    },
    {
      "title": "Flux Pro New",
      "description": "FLUX.1 Pro New is the latest version of the professional FLUX model with enhanced capabilities and improved output quality.\n    image, generation, flux, professional, text-to-image, txt2img\n\n    Use cases:\n    - Generate professional-grade marketing visuals\n    - Create high-quality product renders\n    - Produce detailed architectural visualizations\n    - Design premium brand assets\n    - Generate photorealistic commercial imagery",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxProNew",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxProNew.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxProNew.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image_size"
      ]
    },
    {
      "title": "Flux Schnell",
      "description": "FLUX.1 [schnell] is a fast distilled version of FLUX.1 optimized for speed. Can generate high-quality images in 1-4 steps.\n    image, generation, flux, fast, text-to-image, txt2img\n\n    Use cases:\n    - Generate images quickly for rapid iteration\n    - Create concept art with minimal latency\n    - Produce preview images before final generation\n    - Generate multiple variations efficiently\n    - Real-time image generation applications",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxSchnell",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxSchnell.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxSchnell.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable safety checker to filter unsafe content"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps (1-4 recommended for schnell)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image_size",
        "num_inference_steps"
      ]
    },
    {
      "title": "Flux Srpo",
      "description": "FLUX.1 SRPO [dev]\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxSrpo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxSrpo.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxSrpo.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux V1 Pro",
      "description": "FLUX.1 Pro is a state-of-the-art image generation model with superior prompt following and image quality.\n    image, generation, flux, pro, text-to-image, txt2img\n\n    Use cases:\n    - Generate professional-grade images for commercial use\n    - Create highly detailed artwork with complex prompts\n    - Produce marketing materials and brand assets\n    - Generate photorealistic images\n    - Create custom visual content with precise control",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxV1Pro",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxV1Pro.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "Output image format (jpeg or png)"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxV1Pro.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "Safety checker tolerance level (1-6). Higher is more permissive"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable safety checker to filter unsafe content"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image_size",
        "guidance_scale"
      ]
    },
    {
      "title": "Flux V1 Pro Ultra",
      "description": "FLUX.1 Pro Ultra delivers the highest quality image generation with enhanced detail and realism.\n    image, generation, flux, pro, ultra, text-to-image, txt2img\n\n    Use cases:\n    - Generate ultra-high quality photorealistic images\n    - Create professional photography-grade visuals\n    - Produce detailed product renders\n    - Generate premium marketing materials\n    - Create artistic masterpieces with fine details",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxV1ProUltra",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for the generated image"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxV1ProUltra.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxV1ProUltra.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "image_prompt_strength",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Image Prompt Strength",
          "description": "Strength of image prompt influence (0-1)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "raw",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Raw",
          "description": "Generate less processed, more natural results"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image_size",
        "aspect_ratio"
      ]
    },
    {
      "title": "Gemini 25 Flash Image",
      "description": "Gemini 2.5 Flash Image\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Gemini25FlashImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to generate an image from."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "3:2",
              "4:3",
              "5:4",
              "1:1",
              "4:5",
              "3:4",
              "2:3",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Gemini25FlashImage.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Gemini25FlashImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "limit_generations",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Limit Generations",
          "description": "Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Gemini 3 Pro Image Preview",
      "description": "Gemini 3 Pro Image Preview\n    generation, text-to-image, txt2img, ai-art, professional\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Gemini3ProImagePreview",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "enable_web_search",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Web Search",
          "description": "Enable web search for the image generation task. This will allow the model to use the latest information from the web to generate the image."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image. Use \"auto\" to let the model decide based on the prompt."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "1K",
              "2K",
              "4K"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Gemini3ProImagePreview.Resolution"
          },
          "default": "1K",
          "title": "Resolution",
          "description": "The resolution of the image to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Gemini3ProImagePreview.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Gemini3ProImagePreview.SafetyTolerance"
          },
          "default": "4",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for content moderation. 1 is the most strict (blocks most content), 6 is the least strict."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "limit_generations",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Limit Generations",
          "description": "Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Glm Image",
      "description": "GLM Image generates images from text with advanced AI understanding and quality output.\n    image, generation, glm, ai, text-to-image\n\n    Use cases:\n    - Generate images with GLM AI\n    - Create intelligent visual content\n    - Produce AI-powered artwork\n    - Generate images with understanding\n    - Create smart visuals from text",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.GlmImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for image generation."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "Output image size."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GlmImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "Output image format."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If True, the image will be returned as a base64 data URI instead of a URL."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values make the model follow the prompt more closely."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. The same seed with the same prompt will produce the same image."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "If True, the prompt will be enhanced using an LLM for more detailed and higher quality results."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of diffusion denoising steps. More steps generally produce higher quality images."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable NSFW safety checking on the generated images."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Gpt Image 15",
      "description": "GPT Image 1.5 generates images from text with GPT-powered language understanding and visual creation.\n    image, generation, gpt, language-ai, text-to-image\n\n    Use cases:\n    - Generate images with GPT understanding\n    - Create language-aware visual content\n    - Produce intelligent artwork\n    - Generate images with natural language\n    - Create GPT-powered visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.GptImage15",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "1024x1024",
              "1536x1024",
              "1024x1536"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage15.ImageSize"
          },
          "default": "1024x1024",
          "title": "Image Size",
          "description": "Aspect ratio for the generated image"
        },
        {
          "name": "background",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "transparent",
              "opaque"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage15.Background"
          },
          "default": "auto",
          "title": "Background",
          "description": "Background for the generated image"
        },
        {
          "name": "quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage15.Quality"
          },
          "default": "high",
          "title": "Quality",
          "description": "Quality for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage15.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the images"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Gpt Image 1 Mini",
      "description": "GPT Image 1 Mini\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.GptImage1Mini",
      "properties": [
        {
          "name": "background",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "transparent",
              "opaque"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage1Mini.Background"
          },
          "default": "auto",
          "title": "Background",
          "description": "Background for the generated image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "1024x1024",
              "1536x1024",
              "1024x1536"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage1Mini.ImageSize"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "Aspect ratio for the generated image"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation"
        },
        {
          "name": "quality",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "low",
              "medium",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage1Mini.Quality"
          },
          "default": "auto",
          "title": "Quality",
          "description": "Quality for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage1Mini.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the images"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Image V2 1 Text To Image",
      "description": "Hunyuan Image\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.HunyuanImageV2_1TextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The desired size of the generated image."
        },
        {
          "name": "use_reprompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Reprompt",
          "description": "Enable prompt enhancement for potentially better results."
        },
        {
          "name": "use_refiner",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Refiner",
          "description": "Enable the refiner model for improved image quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.HunyuanImageV2_1TextToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Controls how much the model adheres to the prompt. Higher values mean stricter adherence."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible results. If None, a random seed is used."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide the image generation away from certain concepts."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Image V3 Instruct Text To Image",
      "description": "Hunyuan Image v3 Instruct generates high-quality images from text with advanced instruction understanding.\n    image, generation, hunyuan, v3, instruct, text-to-image\n\n    Use cases:\n    - Generate images with detailed instructions\n    - Create artwork with precise text control\n    - Produce high-quality visual content\n    - Generate images with advanced understanding\n    - Create professional visuals from text",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.HunyuanImageV3InstructTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The desired size of the generated image. If auto, image size will be determined by the model."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.HunyuanImageV3InstructTextToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible results. If None, a random seed is used."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Controls how much the model adheres to the prompt. Higher values mean stricter adherence."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Image V3 Text To Image",
      "description": "Hunyuan Image\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.HunyuanImageV3TextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt for image-to-image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The desired size of the generated image."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.HunyuanImageV3TextToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Controls how much the model adheres to the prompt. Higher values mean stricter adherence."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible results. If None, a random seed is used."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide the image generation away from certain concepts."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ideogram V2",
      "description": "Ideogram V2 is a state-of-the-art image generation model optimized for commercial and creative use, featuring exceptional typography handling and realistic outputs.\n    image, generation, ai, typography, realistic, text-to-image, txt2img\n\n    Use cases:\n    - Create commercial artwork and designs\n    - Generate realistic product visualizations\n    - Design marketing materials with text\n    - Produce high-quality illustrations\n    - Create brand assets and logos",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.IdeogramV2",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "10:16",
              "16:10",
              "9:16",
              "16:9",
              "4:3",
              "3:4",
              "1:1",
              "1:3",
              "3:1",
              "3:2",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.IdeogramV2.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "general",
              "realistic",
              "design",
              "render_3D",
              "anime"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.IdeogramV2.Style"
          },
          "default": "auto",
          "title": "Style",
          "description": "The style of the generated image"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt with MagicPrompt functionality"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to avoid in the generated image"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "aspect_ratio",
        "style"
      ]
    },
    {
      "title": "Ideogram V2 Turbo",
      "description": "Ideogram V2 Turbo offers faster image generation with the same exceptional quality and typography handling as V2.\n    image, generation, ai, typography, realistic, fast, text-to-image, txt2img\n\n    Use cases:\n    - Rapidly generate commercial designs\n    - Quick iteration on marketing materials\n    - Fast prototyping of visual concepts\n    - Real-time design exploration\n    - Efficient batch generation of branded content",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.IdeogramV2Turbo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "10:16",
              "16:10",
              "9:16",
              "16:9",
              "4:3",
              "3:4",
              "1:1",
              "1:3",
              "3:1",
              "3:2",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.IdeogramV2Turbo.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "general",
              "realistic",
              "design",
              "render_3D",
              "anime"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.IdeogramV2Turbo.Style"
          },
          "default": "auto",
          "title": "Style",
          "description": "The style of the generated image"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt with MagicPrompt functionality"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to avoid in the generated image"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "aspect_ratio",
        "style"
      ]
    },
    {
      "title": "Ideogram V3",
      "description": "Ideogram V3 is the latest generation with enhanced text rendering, superior image quality, and expanded creative controls.\n    image, generation, ideogram, typography, text-rendering, text-to-image, txt2img\n\n    Use cases:\n    - Create professional graphics with embedded text\n    - Design social media posts with perfect typography\n    - Generate logos and brand identities\n    - Produce marketing materials with text overlays\n    - Create educational content with clear text",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.IdeogramV3",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The resolution of the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style",
          "description": "The style preset for the generated image"
        },
        {
          "name": "style_preset",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Preset",
          "description": "Style preset for generation. The chosen style preset will guide the generation."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Automatically enhance the prompt for better results"
        },
        {
          "name": "rendering_speed",
          "type": {
            "type": "enum",
            "values": [
              "TURBO",
              "BALANCED",
              "QUALITY"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.IdeogramV3.RenderingSpeed"
          },
          "default": "BALANCED",
          "title": "Rendering Speed",
          "description": "The rendering speed to use."
        },
        {
          "name": "style_codes",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Codes",
          "description": "A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style"
        },
        {
          "name": "color_palette",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Color Palette",
          "description": "A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        },
        {
          "name": "image_urls",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Urls",
          "description": "A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "aspect_ratio",
        "style"
      ]
    },
    {
      "title": "Imagineart Imagineart 1 5 Preview Text To Image",
      "description": "Imagineart 1.5 Preview\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.ImagineartImagineart1_5PreviewTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the desired image"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "3:1",
              "1:3",
              "3:2",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ImagineartImagineart1_5PreviewTextToImage.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "Image aspect ratio: 1:1, 3:1, 1:3, 16:9, 9:16, 4:3, 3:4, 3:2, 2:3"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the image generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Imagineart Imagineart 1 5 Pro Preview Text To Image",
      "description": "ImagineArt 1.5 Pro Preview\n    generation, text-to-image, txt2img, ai-art, professional\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.ImagineartImagineart1_5ProPreviewTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the desired image"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "3:1",
              "1:3",
              "3:2",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ImagineartImagineart1_5ProPreviewTextToImage.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "Image aspect ratio: 1:1, 3:1, 1:3, 16:9, 9:16, 4:3, 3:4, 3:2, 2:3"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the image generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Longcat Image",
      "description": "Longcat Image generates creative and unique images from text with distinctive AI characteristics.\n    image, generation, longcat, creative, text-to-image\n\n    Use cases:\n    - Generate creative images\n    - Create unique visual content\n    - Produce distinctive artwork\n    - Generate images with character\n    - Create artistic visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.LongcatImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.LongcatImage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.LongcatImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Nano Banana",
      "description": "Nano Banana\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.NanoBanana",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to generate an image from."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "3:2",
              "4:3",
              "5:4",
              "1:1",
              "4:5",
              "3:4",
              "2:3",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.NanoBanana.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.NanoBanana.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "limit_generations",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Limit Generations",
          "description": "Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Nano Banana Pro",
      "description": "Nano Banana Pro\n    generation, text-to-image, txt2img, ai-art, professional\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.NanoBananaPro",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "enable_web_search",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Web Search",
          "description": "Enable web search for the image generation task. This will allow the model to use the latest information from the web to generate the image."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image. Use \"auto\" to let the model decide based on the prompt."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "1K",
              "2K",
              "4K"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.NanoBananaPro.Resolution"
          },
          "default": "1K",
          "title": "Resolution",
          "description": "The resolution of the image to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.NanoBananaPro.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.NanoBananaPro.SafetyTolerance"
          },
          "default": "4",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for content moderation. 1 is the most strict (blocks most content), 6 is the least strict."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "limit_generations",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Limit Generations",
          "description": "Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Omni Gen V1",
      "description": "OmniGen V1 is a versatile unified model for multi-modal image generation and editing with text, supporting complex compositional tasks.\n    image, generation, multi-modal, editing, unified, text-to-image, txt2img\n\n    Use cases:\n    - Generate images with multiple input modalities\n    - Edit existing images with text instructions\n    - Create complex compositional scenes\n    - Combine text and image inputs for generation\n    - Perform advanced image manipulations",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.OmniGenV1",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate or edit an image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "img_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.6,
          "title": "Img Guidance Scale",
          "description": "The Image Guidance scale is a measure of how close you want the model to stick to your input image when looking for a related image to show you."
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "URL of images to use while generating the image, Use <img><|image_1|></img> for the first image and so on."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.OmniGenV1.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "How strictly to follow the prompt and inputs"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps for generation quality"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "guidance_scale",
        "num_inference_steps"
      ]
    },
    {
      "title": "Omnigen V2",
      "description": "Omnigen V2\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.OmnigenV2",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate or edit an image. Use specific language like 'Add the bird from image 1 to the desk in image 2' for better results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "euler",
              "dpmsolver"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.OmnigenV2.Scheduler"
          },
          "default": "euler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "cfg_range_end",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Cfg Range End",
          "description": "CFG range end value."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(((deformed))), blurry, over saturation, bad anatomy, disfigured, poorly drawn face, mutation, mutated, (extra_limb), (ugly), (poorly drawn hands), fused fingers, messy drawing, broken legs censor, censored, censor_bar",
          "title": "Negative Prompt",
          "description": "Negative prompt to guide what should not be in the image."
        },
        {
          "name": "text_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Text Guidance Scale",
          "description": "The Text Guidance scale controls how closely the model follows the text prompt. Higher values make the model stick more closely to the prompt."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Image Guidance Scale",
          "description": "The Image Guidance scale controls how closely the model follows the input images. For image editing: 1.3-2.0, for in-context generation: 2.0-3.0"
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "URLs of input images to use for image editing or multi-image generation. Support up to 3 images."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.OmnigenV2.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "cfg_range_start",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Cfg Range Start",
          "description": "CFG range start value."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ovis Image",
      "description": "Ovis Image\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.OvisImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.OvisImage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.OvisImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate an image from."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Piflow",
      "description": "Piflow\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Piflow",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image. You can choose between some presets or custom height and width that **must be multiples of 8**."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Piflow.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation. If set to None, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Qwen Image",
      "description": "Qwen Image\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.QwenImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.QwenImage.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular', 'high'. Higher acceleration increases speed. 'regular' balances speed and quality. 'high' is recommended for images without text."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.QwenImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "use_turbo",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Turbo",
          "description": "Enable turbo mode for faster generation with high quality. When enabled, uses optimized settings (10 steps, CFG=1.2)."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Qwen Image 2512",
      "description": "Qwen Image 2512 generates high-resolution images from text with excellent quality and detail.\n    image, generation, qwen, 2512, high-resolution, text-to-image\n\n    Use cases:\n    - Generate high-resolution images\n    - Create detailed visual content\n    - Produce quality artwork from text\n    - Generate images with fine details\n    - Create high-quality visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.QwenImage2512",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.QwenImage2512.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.QwenImage2512.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate an image from."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Qwen Image 2512 Lora",
      "description": "Qwen Image 2512 with LoRA support enables custom-trained models for specialized image generation.\n    image, generation, qwen, 2512, lora, custom\n\n    Use cases:\n    - Generate images with custom models\n    - Create specialized visual content\n    - Produce domain-specific artwork\n    - Generate images with fine-tuned models\n    - Create customized visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.QwenImage2512Lora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.QwenImage2512Lora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.QwenImage2512Lora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate an image from."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Max Text To Image",
      "description": "Qwen Image Max generates premium quality images from text with superior detail and accuracy.\n    image, generation, qwen, max, premium, text-to-image\n\n    Use cases:\n    - Generate premium quality images\n    - Create detailed artwork from text\n    - Produce high-fidelity visual content\n    - Generate professional-grade images\n    - Create superior quality visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.QwenImageMaxTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the desired image. Supports Chinese and English. Max 800 characters."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.QwenImageMaxTextToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Enable LLM prompt optimization for better results."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility (0-2147483647)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable content moderation for input and output."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Content to avoid in the generated image. Max 500 characters."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Recraft V3",
      "description": "Recraft V3 is a powerful image generation model with exceptional control over style and colors, ideal for brand consistency and design work.\n    image, generation, design, branding, style, text-to-image, txt2img\n\n    Use cases:\n    - Create brand-consistent visual assets\n    - Generate designs with specific color palettes\n    - Produce stylized illustrations and artwork\n    - Design marketing materials with brand colors\n    - Create cohesive visual content series",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.RecraftV3",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "any",
              "realistic_image",
              "digital_illustration",
              "vector_illustration",
              "realistic_image/b_and_w",
              "realistic_image/hard_flash",
              "realistic_image/hdr",
              "realistic_image/natural_light",
              "realistic_image/studio_portrait",
              "realistic_image/enterprise",
              "realistic_image/motion_blur",
              "realistic_image/evening_light",
              "realistic_image/faded_nostalgia",
              "realistic_image/forest_life",
              "realistic_image/mystic_naturalism",
              "realistic_image/natural_tones",
              "realistic_image/organic_calm",
              "realistic_image/real_life_glow",
              "realistic_image/retro_realism",
              "realistic_image/retro_snapshot",
              "realistic_image/urban_drama",
              "realistic_image/village_realism",
              "realistic_image/warm_folk",
              "digital_illustration/pixel_art",
              "digital_illustration/hand_drawn",
              "digital_illustration/grain",
              "digital_illustration/infantile_sketch",
              "digital_illustration/2d_art_poster",
              "digital_illustration/handmade_3d",
              "digital_illustration/hand_drawn_outline",
              "digital_illustration/engraving_color",
              "digital_illustration/2d_art_poster_2",
              "digital_illustration/antiquarian",
              "digital_illustration/bold_fantasy",
              "digital_illustration/child_book",
              "digital_illustration/child_books",
              "digital_illustration/cover",
              "digital_illustration/crosshatch",
              "digital_illustration/digital_engraving",
              "digital_illustration/expressionism",
              "digital_illustration/freehand_details",
              "digital_illustration/grain_20",
              "digital_illustration/graphic_intensity",
              "digital_illustration/hard_comics",
              "digital_illustration/long_shadow",
              "digital_illustration/modern_folk",
              "digital_illustration/multicolor",
              "digital_illustration/neon_calm",
              "digital_illustration/noir",
              "digital_illustration/nostalgic_pastel",
              "digital_illustration/outline_details",
              "digital_illustration/pastel_gradient",
              "digital_illustration/pastel_sketch",
              "digital_illustration/pop_art",
              "digital_illustration/pop_renaissance",
              "digital_illustration/street_art",
              "digital_illustration/tablet_sketch",
              "digital_illustration/urban_glow",
              "digital_illustration/urban_sketching",
              "digital_illustration/vanilla_dreams",
              "digital_illustration/young_adult_book",
              "digital_illustration/young_adult_book_2",
              "vector_illustration/bold_stroke",
              "vector_illustration/chemistry",
              "vector_illustration/colored_stencil",
              "vector_illustration/contour_pop_art",
              "vector_illustration/cosmics",
              "vector_illustration/cutout",
              "vector_illustration/depressive",
              "vector_illustration/editorial",
              "vector_illustration/emotional_flat",
              "vector_illustration/infographical",
              "vector_illustration/marker_outline",
              "vector_illustration/mosaic",
              "vector_illustration/naivector",
              "vector_illustration/roundish_flat",
              "vector_illustration/segmented_colors",
              "vector_illustration/sharp_contrast",
              "vector_illustration/thin",
              "vector_illustration/vector_photo",
              "vector_illustration/vivid_shapes",
              "vector_illustration/engraving",
              "vector_illustration/line_art",
              "vector_illustration/line_circuit",
              "vector_illustration/linocut"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.RecraftV3.RecraftV3Style"
          },
          "default": "realistic_image",
          "title": "Style",
          "description": "Visual style preset for the generated image"
        },
        {
          "name": "colors",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Colors",
          "description": "Specific color palette to use in the generation"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "style_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Id",
          "description": "Custom style ID for brand-specific styles"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "style",
        "colors"
      ]
    },
    {
      "title": "Reve Text To Image",
      "description": "Reve\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.ReveTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text description of the desired image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "3:2",
              "2:3",
              "4:3",
              "3:4",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ReveTextToImage.AspectRatio"
          },
          "default": "3:2",
          "title": "Aspect Ratio",
          "description": "The desired aspect ratio of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ReveTextToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the generated image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Sana",
      "description": "Sana is an efficient high-resolution image generation model that balances quality and speed for practical applications.\n    image, generation, efficient, high-resolution, text-to-image, txt2img\n\n    Use cases:\n    - Generate high-resolution images efficiently\n    - Create detailed artwork with good performance\n    - Produce quality visuals with limited compute\n    - Generate images for web and mobile applications\n    - Balanced quality-speed image production",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Sana",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "style_name",
          "type": {
            "type": "enum",
            "values": [
              "(No style)",
              "Cinematic",
              "Photographic",
              "Anime",
              "Manga",
              "Digital Art",
              "Pixel art",
              "Fantasy art",
              "Neonpunk",
              "3D Model"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Sana.StyleName"
          },
          "default": "(No style)",
          "title": "Style Name",
          "description": "The style to generate the image in."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Sana.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "How strictly to follow the prompt"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 18,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Elements to avoid in the generated image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image_size",
        "guidance_scale"
      ]
    },
    {
      "title": "Sky Raccoon",
      "description": "Sky Raccoon\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.SkyRaccoon",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "turbo_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Turbo Mode",
          "description": "If true, the video will be generated faster with no noticeable degradation in the visual quality."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Stable Diffusion V35 Large",
      "description": "Stable Diffusion 3.5 Large is a powerful open-weight model with excellent prompt adherence and diverse output capabilities.\n    image, generation, stable-diffusion, open-source, text-to-image, txt2img\n\n    Use cases:\n    - Generate diverse artistic styles\n    - Create high-quality illustrations\n    - Produce photorealistic images\n    - Generate concept art and designs\n    - Create custom visual content",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.StableDiffusionV35Large",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Defaults to landscape_4_3 if no controlnet has been passed, otherwise defaults to the size of the controlnet conditioning image."
        },
        {
          "name": "controlnet",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Controlnet",
          "description": "ControlNet for inference."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.StableDiffusionV35Large.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "ip_adapter",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Ip Adapter",
          "description": "IP-Adapter to use during inference."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Elements to avoid in the generated image"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "negative_prompt",
        "aspect_ratio"
      ]
    },
    {
      "title": "Vidu Q2 Text To Image",
      "description": "Vidu Q2 generates quality images from text with optimized performance and consistent results.\n    image, generation, vidu, q2, optimized, text-to-image\n\n    Use cases:\n    - Generate optimized quality images\n    - Create consistent visual content\n    - Produce balanced artwork\n    - Generate images efficiently\n    - Create reliable visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.ViduQ2TextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation, max 1500 characters"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ViduQ2TextToImage.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the output video"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan 25 Preview Text To Image",
      "description": "Wan 2.5 Text to Image\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Wan25PreviewTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation. Supports Chinese and English, max 2000 characters."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate. Values from 1 to 4."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square",
          "title": "Image Size",
          "description": "The size of the generated image. Can use preset names like 'square', 'landscape_16_9', etc., or specific dimensions. Total pixels must be between 768\u00d7768 and 1440\u00d71440, with aspect ratio between [1:4, 4:1]."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to describe content to avoid. Max 500 characters."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan V26 Text To Image",
      "description": "Wan v2.6 generates high-quality images from text with advanced capabilities and consistent results.\n    image, generation, wan, v2.6, quality, text-to-image\n\n    Use cases:\n    - Generate quality images with Wan v2.6\n    - Create consistent visual content\n    - Produce reliable artwork from text\n    - Generate images with advanced model\n    - Create high-quality visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.WanV26TextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the desired image. Supports Chinese and English. Max 2000 characters."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "Output image size. If not set: matches input image size (up to 1280*1280). Use presets like 'square_hd', 'landscape_16_9', or specify exact dimensions."
        },
        {
          "name": "max_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Max Images",
          "description": "Maximum number of images to generate (1-5). Actual count may be less depending on model inference."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional reference image (0 or 1). When provided, can be used for style guidance. Resolution: 384-5000px each dimension. Max size: 10MB. Formats: JPEG, JPG, PNG (no alpha), BMP, WEBP."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable content moderation for input and output."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility (0-2147483647)."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Content to avoid in the generated image. Max 500 characters."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan V2 25B Text To Image",
      "description": "Wan\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.WanV2_25BTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide image generation."
        },
        {
          "name": "image_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.WanV2_25BTextToImage.ImageFormat"
          },
          "default": "jpeg",
          "title": "Image Format",
          "description": "The format of the output image."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Shift",
          "description": "Shift value for the image. Must be between 1.0 and 10.0."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan V2 2A 14B Text To Image",
      "description": "Wan\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.WanV2_2A14BTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide image generation."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Shift",
          "description": "Shift value for the image. Must be between 1.0 and 10.0."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.WanV2_2A14BTextToImage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "guidance_scale_2",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale 2",
          "description": "Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan V2 2A 14B Text To Image Lora",
      "description": "Wan v2.2 A14B Text-to-Image A14B with LoRAs\n    generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.WanV2_2A14BTextToImageLora",
      "properties": [
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Shift",
          "description": "Shift value for the image. Must be between 1.0 and 10.0."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide image generation."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.WanV2_2A14BTextToImageLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'."
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "If true, the video will be reversed."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to be used in the inference."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "image_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.WanV2_2A14BTextToImageLora.ImageFormat"
          },
          "default": "jpeg",
          "title": "Image Format",
          "description": "The format of the output image."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "guidance_scale_2",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale 2",
          "description": "Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "ZImage Base",
      "description": "Z-Image Base generates quality images from text with efficient processing and good results.\n    image, generation, z-image, base, efficient, text-to-image\n\n    Use cases:\n    - Generate images efficiently\n    - Create quality artwork from text\n    - Produce visual content quickly\n    - Generate images with good performance\n    - Create efficient visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.ZImageBase",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ZImageBase.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ZImageBase.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use for the image generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "ZImage Base Lora",
      "description": "Z-Image Base with LoRA enables efficient custom-trained models for specialized generation tasks.\n    image, generation, z-image, base, lora, custom\n\n    Use cases:\n    - Generate images with custom efficient models\n    - Create specialized content quickly\n    - Produce domain-specific visuals\n    - Generate with fine-tuned base model\n    - Create efficient custom visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.ZImageBaseLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ZImageBaseLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ZImageBaseLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use for the image generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "ZImage Turbo",
      "description": "Z-Image Turbo generates images from text with maximum speed for rapid iteration and prototyping.\n    image, generation, z-image, turbo, fast, text-to-image\n\n    Use cases:\n    - Generate images at maximum speed\n    - Create rapid prototypes from text\n    - Produce quick visual iterations\n    - Generate images for fast workflows\n    - Create instant visual content",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.ZImageTurbo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ZImageTurbo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ZImageTurbo.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "ZImage Turbo Lora",
      "description": "Z-Image Turbo with LoRA combines maximum speed with custom models for fast specialized generation.\n    image, generation, z-image, turbo, lora, fast\n\n    Use cases:\n    - Generate custom images at turbo speed\n    - Create specialized content rapidly\n    - Produce quick domain-specific visuals\n    - Generate with fast fine-tuned models\n    - Create instant custom visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.ZImageTurboLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ZImageTurboLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ZImageTurboLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "AIFace Swap Video",
      "description": "AI Face Swap replaces faces in videos with target faces while preserving expressions and movements.\n    video, face-swap, deepfake, face-replacement, video-to-video\n\n    Use cases:\n    - Replace faces in video content\n    - Create personalized video content\n    - Swap actors in video scenes\n    - Generate face replacement effects\n    - Create video with different faces",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.AIFaceSwapVideo",
      "properties": [
        {
          "name": "enable_occlusion_prevention",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Occlusion Prevention",
          "description": "Enable occlusion prevention for handling faces covered by hands/objects. Warning: Enabling this runs an occlusion-aware model which costs 2x more."
        },
        {
          "name": "source_face_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Source Face Url",
          "description": "Source face image. Allowed items: bmp, jpeg, png, tiff, webp"
        },
        {
          "name": "target_video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Target Video Url",
          "description": "Target video URL (max 25 minutes, will be truncated if longer; FPS capped at 25). Allowed items: avi, m4v, mkv, mp4, mpeg, mov, mxf, webm, wmv"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "target_face"
      ]
    },
    {
      "title": "AMTInterpolation",
      "description": "AMT (Any-to-Many Temporal) Interpolation creates smooth transitions between video frames.\n    video, interpolation, frame-generation, amt, video-to-video\n\n    Use cases:\n    - Increase video frame rate smoothly\n    - Create slow-motion effects\n    - Smooth out choppy video\n    - Generate intermediate frames\n    - Enhance video playback quality",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.AMTInterpolation",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video to be processed"
        },
        {
          "name": "recursive_interpolation_passes",
          "type": {
            "type": "int"
          },
          "default": 2,
          "title": "Recursive Interpolation Passes",
          "description": "Number of recursive interpolation passes"
        },
        {
          "name": "output_fps",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Output Fps",
          "description": "Output frames per second"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Animate Diff Turbo Video To Video",
      "description": "AnimateDiff Turbo re-animates videos quickly with reduced generation time.\n    video, style-transfer, animatediff, turbo, fast, video-to-video\n\n    Use cases:\n    - Quickly restyle videos\n    - Rapid video transformations\n    - Fast video effect application\n    - Efficient video processing\n    - Real-time video styling",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.AnimateDiffTurboVideoToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video."
        },
        {
          "name": "first_n_seconds",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "First N Seconds",
          "description": "The first N number of seconds of video to animate."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Fps",
          "description": "Number of frames per second to extract from the video."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.7,
          "title": "Strength",
          "description": "The strength of the input video in the final output."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform. 4-12 is recommended for turbo mode."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(bad quality, worst quality:1.2), ugly faces, bad anime",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "motions",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Motions",
          "description": "The motions to apply to the video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "prompt"
      ]
    },
    {
      "title": "Animate Diff Video To Video",
      "description": "AnimateDiff re-animates videos with new styles and effects using diffusion models.\n    video, style-transfer, animatediff, re-animation, video-to-video\n\n    Use cases:\n    - Restyle existing videos\n    - Apply artistic effects to videos\n    - Transform video aesthetics\n    - Create stylized video versions\n    - Generate video variations",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.AnimateDiffVideoToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video."
        },
        {
          "name": "first_n_seconds",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "First N Seconds",
          "description": "The first N number of seconds of video to animate."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Fps",
          "description": "Number of frames per second to extract from the video."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.7,
          "title": "Strength",
          "description": "The strength of the input video in the final output."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(bad quality, worst quality:1.2), ugly faces, bad anime",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "motions",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Motions",
          "description": "The motions to apply to the video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "prompt"
      ]
    },
    {
      "title": "Auto Caption",
      "description": "Auto Caption automatically generates and adds captions to videos with speech recognition.\n    video, captions, subtitles, speech-to-text, video-to-video\n\n    Use cases:\n    - Add subtitles to videos automatically\n    - Generate captions for accessibility\n    - Create multilingual subtitles\n    - Transcribe video speech\n    - Add text overlays to videos",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.AutoCaption",
      "properties": [
        {
          "name": "txt_font",
          "type": {
            "type": "str"
          },
          "default": "Standard",
          "title": "Txt Font",
          "description": "Font for generated captions. Choose one in 'Arial','Standard','Garamond', 'Times New Roman','Georgia', or pass a url to a .ttf file"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the .mp4 video with audio. Only videos of size <100MB are allowed."
        },
        {
          "name": "top_align",
          "type": {
            "type": "str"
          },
          "default": "center",
          "title": "Top Align",
          "description": "Top-to-bottom alignment of the text. Can be a string ('top', 'center', 'bottom') or a float (0.0-1.0)"
        },
        {
          "name": "txt_color",
          "type": {
            "type": "str"
          },
          "default": "white",
          "title": "Txt Color",
          "description": "Colour of the text. Can be a RGB tuple, a color name, or an hexadecimal notation."
        },
        {
          "name": "stroke_width",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Stroke Width",
          "description": "Width of the text strokes in pixels"
        },
        {
          "name": "refresh_interval",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Refresh Interval",
          "description": "Number of seconds the captions should stay on screen. A higher number will also result in more text being displayed at once."
        },
        {
          "name": "font_size",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Font Size",
          "description": "Size of text in generated captions."
        },
        {
          "name": "left_align",
          "type": {
            "type": "str"
          },
          "default": "center",
          "title": "Left Align",
          "description": "Left-to-right alignment of the text. Can be a string ('left', 'center', 'right') or a float (0.0-1.0)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ben V2 Video",
      "description": "Ben v2 Video enhances and processes video content with advanced AI techniques.\n    video, enhancement, processing, ben, video-to-video\n\n    Use cases:\n    - Enhance video quality\n    - Process video content\n    - Improve video clarity\n    - Apply video enhancements\n    - Optimize video output",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BenV2Video",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of video to be used for background removal."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation."
        },
        {
          "name": "background_color",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Background Color",
          "description": "Optional RGB values (0-255) for the background color. If not provided, the background will be transparent. For ex: [0, 0, 0]"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Bi Ref Net V2 Video",
      "description": "BiRefNet v2 Video performs background removal from videos with high accuracy.\n    video, background-removal, segmentation, birefnet, video-to-video\n\n    Use cases:\n    - Remove backgrounds from videos\n    - Create transparent video backgrounds\n    - Isolate video subjects\n    - Generate video mattes\n    - Prepare videos for compositing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BiRefNetV2Video",
      "properties": [
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BiRefNetV2Video.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BiRefNetV2Video.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "operating_resolution",
          "type": {
            "type": "enum",
            "values": [
              "1024x1024",
              "2048x2048",
              "2304x2304"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BiRefNetV2Video.OperatingResolution"
          },
          "default": "1024x1024",
          "title": "Operating Resolution",
          "description": "The resolution to operate on. The higher the resolution, the more accurate the output will be for high res input images. The '2304x2304' option is only available for the 'General Use (Dynamic)' model."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video to remove background from"
        },
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "General Use (Light)",
              "General Use (Light 2K)",
              "General Use (Heavy)",
              "Matting",
              "Portrait",
              "General Use (Dynamic)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BiRefNetV2Video.Model"
          },
          "default": "General Use (Light)",
          "title": "Model",
          "description": "Model to use for background removal. The 'General Use (Light)' model is the original model used in the BiRefNet repository. The 'General Use (Light 2K)' model is the original model used in the BiRefNet repository but trained with 2K images. The 'General Use (Heavy)' model is a slower but more accurate model. The 'Matting' model is a model trained specifically for matting images. The 'Portrait' model is a model trained specifically for portrait images. The 'General Use (Dynamic)' model supports dynamic resolutions from 256x256 to 2304x2304. The 'General Use (Light)' model is recommended for most use cases. The corresponding models are as follows: - 'General Use (Light)': BiRefNet - 'General Use (Light 2K)': BiRefNet_lite-2K - 'General Use (Heavy)': BiRefNet_lite - 'Matting': BiRefNet-matting - 'Portrait': BiRefNet-portrait - 'General Use (Dynamic)': BiRefNet_dynamic"
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BiRefNetV2Video.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_mask",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Output Mask",
          "description": "Whether to output the mask used to remove the background"
        },
        {
          "name": "refine_foreground",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Refine Foreground",
          "description": "Whether to refine the foreground using the estimated mask"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Bria Video Erase Keypoints",
      "description": "Video\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BriaVideoEraseKeypoints",
      "properties": [
        {
          "name": "preserve_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Audio",
          "description": "If true, audio will be preserved in the output video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Input video to erase object from. duration must be less than 5s."
        },
        {
          "name": "output_container_and_codec",
          "type": {
            "type": "enum",
            "values": [
              "mp4_h265",
              "mp4_h264",
              "webm_vp9",
              "gif",
              "mov_h264",
              "mov_h265",
              "mov_proresks",
              "mkv_h264",
              "mkv_h265",
              "mkv_vp9",
              "mkv_mpeg4"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BriaVideoEraseKeypoints.OutputContainerAndCodec"
          },
          "default": "mp4_h264",
          "title": "Output Container And Codec",
          "description": "Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4."
        },
        {
          "name": "keypoints",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Keypoints",
          "description": "Input keypoints [x,y] to erase or keep from the video. Format like so: {'x':100, 'y':100, 'type':'positive/negative'}"
        },
        {
          "name": "auto_trim",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Auto Trim",
          "description": "auto trim the video, to working duration ( 5s )"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Bria Video Erase Mask",
      "description": "Video\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BriaVideoEraseMask",
      "properties": [
        {
          "name": "preserve_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Audio",
          "description": "If true, audio will be preserved in the output video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Input video to erase object from. duration must be less than 5s."
        },
        {
          "name": "output_container_and_codec",
          "type": {
            "type": "enum",
            "values": [
              "mp4_h265",
              "mp4_h264",
              "webm_vp9",
              "gif",
              "mov_h264",
              "mov_h265",
              "mov_proresks",
              "mkv_h264",
              "mkv_h265",
              "mkv_vp9",
              "mkv_mpeg4"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BriaVideoEraseMask.OutputContainerAndCodec"
          },
          "default": "mp4_h264",
          "title": "Output Container And Codec",
          "description": "Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4."
        },
        {
          "name": "mask_video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Mask Video Url",
          "description": "Input video to mask erase object from. duration must be less than 5s."
        },
        {
          "name": "auto_trim",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Auto Trim",
          "description": "auto trim the video, to working duration ( 5s )"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Bria Video Erase Prompt",
      "description": "Video\n    video, editing, video-to-video, vid2vid, professional\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BriaVideoErasePrompt",
      "properties": [
        {
          "name": "preserve_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Audio",
          "description": "If true, audio will be preserved in the output video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Input video to erase object from. duration must be less than 5s."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Input prompt to detect object to erase"
        },
        {
          "name": "output_container_and_codec",
          "type": {
            "type": "enum",
            "values": [
              "mp4_h265",
              "mp4_h264",
              "webm_vp9",
              "gif",
              "mov_h264",
              "mov_h265",
              "mov_proresks",
              "mkv_h264",
              "mkv_h265",
              "mkv_vp9",
              "mkv_mpeg4"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BriaVideoErasePrompt.OutputContainerAndCodec"
          },
          "default": "mp4_h264",
          "title": "Output Container And Codec",
          "description": "Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4."
        },
        {
          "name": "auto_trim",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Auto Trim",
          "description": "auto trim the video, to working duration ( 5s )"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Bria Video Eraser Keypoints",
      "description": "Bria Video Eraser removes objects from videos using keypoint-based selection.\n    video, object-removal, eraser, keypoints, bria, video-to-video\n\n    Use cases:\n    - Remove objects using keypoint selection\n    - Erase specific areas from videos\n    - Targeted video content removal\n    - Precision video editing\n    - Remove elements with point markers",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BriaVideoEraserKeypoints",
      "properties": [
        {
          "name": "preserve_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Audio",
          "description": "If true, audio will be preserved in the output video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Input video to erase object from. duration must be less than 5s."
        },
        {
          "name": "output_container_and_codec",
          "type": {
            "type": "enum",
            "values": [
              "mp4_h265",
              "mp4_h264",
              "webm_vp9",
              "gif",
              "mov_h264",
              "mov_h265",
              "mov_proresks",
              "mkv_h264",
              "mkv_h265",
              "mkv_vp9",
              "mkv_mpeg4"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BriaVideoEraserKeypoints.OutputContainerAndCodec"
          },
          "default": "mp4_h264",
          "title": "Output Container And Codec",
          "description": "Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4."
        },
        {
          "name": "keypoints",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Keypoints",
          "description": "Input keypoints [x,y] to erase or keep from the video. Format like so: {'x':100, 'y':100, 'type':'positive/negative'}"
        },
        {
          "name": "auto_trim",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Auto Trim",
          "description": "auto trim the video, to working duration ( 5s )"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "keypoints"
      ]
    },
    {
      "title": "Bria Video Eraser Mask",
      "description": "Bria Video Eraser removes objects from videos using mask-based selection.\n    video, object-removal, eraser, inpainting, bria, video-to-video\n\n    Use cases:\n    - Remove unwanted objects from videos\n    - Erase people or items from footage\n    - Clean up video backgrounds\n    - Remove watermarks from videos\n    - Edit video content seamlessly",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BriaVideoEraserMask",
      "properties": [
        {
          "name": "preserve_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Audio",
          "description": "If true, audio will be preserved in the output video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Input video to erase object from. duration must be less than 5s."
        },
        {
          "name": "output_container_and_codec",
          "type": {
            "type": "enum",
            "values": [
              "mp4_h265",
              "mp4_h264",
              "webm_vp9",
              "gif",
              "mov_h264",
              "mov_h265",
              "mov_proresks",
              "mkv_h264",
              "mkv_h265",
              "mkv_vp9",
              "mkv_mpeg4"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BriaVideoEraserMask.OutputContainerAndCodec"
          },
          "default": "mp4_h264",
          "title": "Output Container And Codec",
          "description": "Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4."
        },
        {
          "name": "mask_video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Mask Video Url",
          "description": "Input video to mask erase object from. duration must be less than 5s."
        },
        {
          "name": "auto_trim",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Auto Trim",
          "description": "auto trim the video, to working duration ( 5s )"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "mask"
      ]
    },
    {
      "title": "Bria Video Eraser Prompt",
      "description": "Bria Video Eraser removes objects from videos using text prompt descriptions.\n    video, object-removal, eraser, prompt, bria, video-to-video\n\n    Use cases:\n    - Remove objects by describing them\n    - Text-based video editing\n    - Natural language video cleanup\n    - Prompt-driven object removal\n    - Semantic video editing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BriaVideoEraserPrompt",
      "properties": [
        {
          "name": "preserve_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Audio",
          "description": "If true, audio will be preserved in the output video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Input video to erase object from. duration must be less than 5s."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Input prompt to detect object to erase"
        },
        {
          "name": "output_container_and_codec",
          "type": {
            "type": "enum",
            "values": [
              "mp4_h265",
              "mp4_h264",
              "webm_vp9",
              "gif",
              "mov_h264",
              "mov_h265",
              "mov_proresks",
              "mkv_h264",
              "mkv_h265",
              "mkv_vp9",
              "mkv_mpeg4"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BriaVideoEraserPrompt.OutputContainerAndCodec"
          },
          "default": "mp4_h264",
          "title": "Output Container And Codec",
          "description": "Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4."
        },
        {
          "name": "auto_trim",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Auto Trim",
          "description": "auto trim the video, to working duration ( 5s )"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "prompt"
      ]
    },
    {
      "title": "Bytedance Upscaler Upscale Video",
      "description": "Bytedance Upscaler\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BytedanceUpscalerUpscaleVideo",
      "properties": [
        {
          "name": "target_fps",
          "type": {
            "type": "enum",
            "values": [
              "30fps",
              "60fps"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BytedanceUpscalerUpscaleVideo.TargetFps"
          },
          "default": "30fps",
          "title": "Target Fps",
          "description": "The target FPS of the video to upscale."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to upscale."
        },
        {
          "name": "target_resolution",
          "type": {
            "type": "enum",
            "values": [
              "1080p",
              "2k",
              "4k"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BytedanceUpscalerUpscaleVideo.TargetResolution"
          },
          "default": "1080p",
          "title": "Target Resolution",
          "description": "The target resolution of the video to upscale."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "CCSR",
      "description": "CCSR (Controllable Color Style Restoration) restores and enhances video colors.\n    video, color-restoration, enhancement, ccsr, video-to-video\n\n    Use cases:\n    - Restore video colors\n    - Enhance video color quality\n    - Fix color issues in videos\n    - Improve video color grading\n    - Restore faded video footage",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.CCSR",
      "properties": [
        {
          "name": "color_fix_type",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "wavelet",
              "adain"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.CCSR.ColorFixType"
          },
          "default": "adain",
          "title": "Color Fix Type",
          "description": "Type of color correction for samples."
        },
        {
          "name": "tile_diffusion_size",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Tile Diffusion Size",
          "description": "Size of patch."
        },
        {
          "name": "tile_vae_decoder_size",
          "type": {
            "type": "int"
          },
          "default": 226,
          "title": "Tile Vae Decoder Size",
          "description": "Size of VAE patch."
        },
        {
          "name": "tile_vae_encoder_size",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Tile Vae Encoder Size",
          "description": "Size of latent image"
        },
        {
          "name": "t_min",
          "type": {
            "type": "float"
          },
          "default": 0.3333,
          "title": "T Min",
          "description": "The starting point of uniform sampling strategy."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL or data URI of the image to upscale."
        },
        {
          "name": "tile_diffusion_stride",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Tile Diffusion Stride",
          "description": "Stride of sliding patch."
        },
        {
          "name": "tile_vae",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Tile Vae",
          "description": "If specified, a patch-based sampling strategy will be used for VAE decoding."
        },
        {
          "name": "scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Scale",
          "description": "The scale of the output image. The higher the scale, the bigger the output image will be."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducibility. Different seeds will make slightly different results."
        },
        {
          "name": "t_max",
          "type": {
            "type": "float"
          },
          "default": 0.6667,
          "title": "T Max",
          "description": "The ending point of uniform sampling strategy."
        },
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Steps",
          "description": "The number of steps to run the model for. The higher the number the better the quality and longer it will take to generate."
        },
        {
          "name": "tile_diffusion",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "mix",
              "gaussian"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.CCSR.TileDiffusion"
          },
          "default": "none",
          "title": "Tile Diffusion",
          "description": "If specified, a patch-based sampling strategy will be used for sampling."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Clarityai Crystal Video Upscaler",
      "description": "Crystal Upscaler [Video]\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.ClarityaiCrystalVideoUpscaler",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the input video."
        },
        {
          "name": "scale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Scale Factor",
          "description": "Scale factor. The scale factor must be chosen such that the upscaled video does not exceed 5K resolution."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Cog Video X5 BVideo To Video",
      "description": "CogVideoX-5B transforms existing videos with new styles and effects.\n    video, transformation, cogvideo, style-transfer, video-to-video\n\n    Use cases:\n    - Transform video styles\n    - Apply effects to existing videos\n    - Restyle video content\n    - Generate video variations\n    - Create artistic video versions",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.CogVideoX5BVideoToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The video to generate the video from."
        },
        {
          "name": "use_rife",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Rife",
          "description": "Use RIFE for video interpolation"
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. We currently support one lora."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "The strength to use for Video to Video. 1.0 completely remakes the video while 0.0 preserves the original."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related video to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "export_fps",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Export Fps",
          "description": "The target FPS of the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate video from"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "prompt"
      ]
    },
    {
      "title": "Decart Lucy Edit Fast",
      "description": "Lucy Edit [Fast]\n    video, editing, video-to-video, vid2vid, fast\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.DecartLucyEditFast",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the video to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the video directly in the response without going through the CDN."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video to edit"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text description of the desired video content"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Decart Lucy Restyle",
      "description": "Lucy Restyle\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.DecartLucyRestyle",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the video to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the video directly in the response without going through the CDN."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video to edit"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.DecartLucyRestyle.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text description of the desired video content"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for video generation"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Editto",
      "description": "Editto\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Editto",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the source video file. Required for inpainting."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "str"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between the original frames. A value of 0 means no interpolation."
        },
        {
          "name": "temporal_downsample_factor",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Temporal Downsample Factor",
          "description": "Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "str"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true."
        },
        {
          "name": "match_input_num_frames",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Num Frames",
          "description": "If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 241 (inclusive)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "sampler",
          "type": {
            "type": "enum",
            "values": [
              "unipc",
              "dpmPP",
              "euler"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Editto.Sampler"
          },
          "default": "unipc",
          "title": "Sampler",
          "description": "Sampler to use for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Editto.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Editto.Resolution"
          },
          "default": "auto",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Editto.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "If true, also return a ZIP file containing all generated frames."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Editto.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "match_input_frames_per_second",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Frames Per Second",
          "description": "If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "enable_auto_downsample",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Auto Downsample",
          "description": "If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Flashvsr Upscale Video",
      "description": "Flashvsr\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.FlashvsrUpscaleVideo",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The input video to be upscaled"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration mode for VAE decoding. Options: regular (best quality), high (balanced), full (fastest). More accerleation means longer duration videos can be processed too."
        },
        {
          "name": "quality",
          "type": {
            "type": "int"
          },
          "default": 70,
          "title": "Quality",
          "description": "Quality level for tile blending (0-100). Controls overlap between tiles to prevent grid artifacts. Higher values provide better quality with more overlap. Recommended: 70-85 for high-res videos, 50-70 for faster processing."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideo.OutputFormat"
          },
          "default": "X264 (.mp4)",
          "title": "Output Format",
          "description": "The format of the output video."
        },
        {
          "name": "color_fix",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Color Fix",
          "description": "Color correction enabled."
        },
        {
          "name": "output_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideo.OutputWriteMode"
          },
          "default": "balanced",
          "title": "Output Write Mode",
          "description": "The write mode of the output video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned inline and not stored in history."
        },
        {
          "name": "output_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideo.OutputQuality"
          },
          "default": "high",
          "title": "Output Quality",
          "description": "The quality of the output video."
        },
        {
          "name": "upscale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Upscale Factor",
          "description": "Upscaling factor to be used."
        },
        {
          "name": "preserve_audio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preserve Audio",
          "description": "Copy the original audio tracks into the upscaled video using FFmpeg when possible."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The random seed used for the generation process."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Hunyuan Video To Video",
      "description": "Hunyuan Video transforms existing videos with advanced AI-powered effects.\n    video, transformation, hunyuan, video-to-video\n\n    Use cases:\n    - Transform video content\n    - Apply AI effects to videos\n    - Restyle existing footage\n    - Generate video variations\n    - Create enhanced video versions",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.HunyuanVideoToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.HunyuanVideoToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video to generate."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.HunyuanVideoToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the video to generate."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video input."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "Strength for Video-to-Video"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to run. Lower gets faster results, higher gets better results."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for generating the video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "enum",
            "values": [
              "129",
              "85"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.HunyuanVideoToVideo.NumFrames"
          },
          "default": 129,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "pro_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Pro Mode",
          "description": "By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "prompt"
      ]
    },
    {
      "title": "Infinitalk Video To Video",
      "description": "Infinitalk\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.InfinitalkVideoToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.InfinitalkVideoToVideo.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the video to generate. Must be either 480p or 720p."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.InfinitalkVideoToVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 145,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Kling Video O1 Standard Video To Video Edit",
      "description": "Kling O1 Edit Video [Standard]\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.KlingVideoO1StandardVideoToVideoEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB. Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s"
        },
        {
          "name": "elements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Elements",
          "description": "Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video."
        },
        {
          "name": "keep_audio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Keep Audio",
          "description": "Whether to keep the original audio from the video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Kling Video O1 Standard Video To Video Reference",
      "description": "Kling O1 Reference Video to Video [Standard]\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.KlingVideoO1StandardVideoToVideoReference",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.KlingVideoO1StandardVideoToVideoReference.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame. If 'auto', the aspect ratio will be determined automatically based on the input video, and the closest aspect ratio to the input video will be used."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB. Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.KlingVideoO1StandardVideoToVideoReference.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Video duration in seconds."
        },
        {
          "name": "keep_audio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Keep Audio",
          "description": "Whether to keep the original audio from the video."
        },
        {
          "name": "elements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Elements",
          "description": "Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Kling Video O1 Video To Video Edit",
      "description": "Kling O1 Edit Video [Pro]\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.KlingVideoO1VideoToVideoEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB. Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s"
        },
        {
          "name": "elements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Elements",
          "description": "Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video."
        },
        {
          "name": "keep_audio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Keep Audio",
          "description": "Whether to keep the original audio from the video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Kling Video O1 Video To Video Reference",
      "description": "Kling O1 Reference Video to Video [Pro]\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.KlingVideoO1VideoToVideoReference",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.KlingVideoO1VideoToVideoReference.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame. If 'auto', the aspect ratio will be determined automatically based on the input video, and the closest aspect ratio to the input video will be used."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB. Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.KlingVideoO1VideoToVideoReference.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Video duration in seconds."
        },
        {
          "name": "keep_audio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Keep Audio",
          "description": "Whether to keep the original audio from the video."
        },
        {
          "name": "elements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Elements",
          "description": "Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Kling Video V2 6 Pro Motion Control",
      "description": "Kling Video v2.6 Motion Control [Pro]\n    video, editing, video-to-video, vid2vid, professional\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.KlingVideoV2_6ProMotionControl",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "video_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Video Url",
          "description": "Reference video URL. The character actions in the generated video will be consistent with this reference video. Should contain a realistic style character with entire body or upper body visible, including head, without obstruction. Duration limit depends on character_orientation: 10s max for 'image', 30s max for 'video'."
        },
        {
          "name": "character_orientation",
          "type": {
            "type": "enum",
            "values": [
              "image",
              "video"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.KlingVideoV2_6ProMotionControl.CharacterOrientation"
          },
          "default": "",
          "title": "Character Orientation",
          "description": "Controls whether the output character's orientation matches the reference image or video. 'video': orientation matches reference video - better for complex motions (max 30s). 'image': orientation matches reference image - better for following camera movements (max 10s)."
        },
        {
          "name": "keep_original_sound",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Keep Original Sound",
          "description": "Whether to keep the original sound from the reference video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Reference image URL. The characters, backgrounds, and other elements in the generated video are based on this reference image. Characters should have clear body proportions, avoid occlusion, and occupy more than 5% of the image area."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Kling Video V2 6 Standard Motion Control",
      "description": "Kling Video v2.6 Motion Control [Standard]\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.KlingVideoV2_6StandardMotionControl",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "video_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Video Url",
          "description": "Reference video URL. The character actions in the generated video will be consistent with this reference video. Should contain a realistic style character with entire body or upper body visible, including head, without obstruction. Duration limit depends on character_orientation: 10s max for 'image', 30s max for 'video'."
        },
        {
          "name": "character_orientation",
          "type": {
            "type": "enum",
            "values": [
              "image",
              "video"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.KlingVideoV2_6StandardMotionControl.CharacterOrientation"
          },
          "default": "",
          "title": "Character Orientation",
          "description": "Controls whether the output character's orientation matches the reference image or video. 'video': orientation matches reference video - better for complex motions (max 30s). 'image': orientation matches reference image - better for following camera movements (max 10s)."
        },
        {
          "name": "keep_original_sound",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Keep Original Sound",
          "description": "Whether to keep the original sound from the reference video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Reference image URL. The characters, backgrounds, and other elements in the generated video are based on this reference image. Characters should have clear body proportions, avoid occlusion, and occupy more than 5% of the image area."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Krea Wan 14B Video To Video",
      "description": "Krea Wan 14B\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.KreaWan14BVideoToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt for the video-to-video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video. Currently, only outputs of 16:9 aspect ratio and 480p resolution are supported. Video duration should be less than 1000 frames at 16fps, and output frames will be 6 plus a multiple of 12, for example 18, 30, 42, etc."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "Denoising strength for the video-to-video generation. 0.0 preserves the original, 1.0 completely remakes the video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the video-to-video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Lightx Recamera",
      "description": "Lightx\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.LightxRecamera",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional text prompt. If omitted, Light-X will auto-caption the video."
        },
        {
          "name": "trajectory",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Trajectory",
          "description": "Camera trajectory parameters (required for recamera mode)."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video."
        },
        {
          "name": "camera",
          "type": {
            "type": "enum",
            "values": [
              "traj",
              "target"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LightxRecamera.Camera"
          },
          "default": "traj",
          "title": "Camera",
          "description": "Camera control mode."
        },
        {
          "name": "target_pose",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "float"
              }
            ]
          },
          "default": [],
          "title": "Target Pose",
          "description": "Target camera pose [theta, phi, radius, x, y] (required when camera='target')."
        },
        {
          "name": "mode",
          "type": {
            "type": "enum",
            "values": [
              "gradual",
              "bullet",
              "direct",
              "dolly-zoom"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LightxRecamera.Mode"
          },
          "default": "gradual",
          "title": "Mode",
          "description": "Camera motion mode."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Lightx Relight",
      "description": "Lightx\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.LightxRelight",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional text prompt. If omitted, Light-X will auto-caption the video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video."
        },
        {
          "name": "relight_parameters",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Relight Parameters",
          "description": "Relighting parameters (required for relight_condition_type='ic'). Not used for 'bg' (which expects a background image URL instead)."
        },
        {
          "name": "ref_id",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Ref Id",
          "description": "Frame index to use as referencen to relight the video with reference."
        },
        {
          "name": "relit_cond_img_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Relit Cond Img Url",
          "description": "URL of conditioning image. Required for relight_condition_type='ref'/'hdr'. Also required for relight_condition_type='bg' (background image)."
        },
        {
          "name": "relit_cond_type",
          "type": {
            "type": "enum",
            "values": [
              "ic",
              "ref",
              "hdr",
              "bg"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LightxRelight.RelitCondType"
          },
          "default": "ic",
          "title": "Relit Cond Type",
          "description": "Relight condition type."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 219B Distilled Extend Video",
      "description": "LTX-2 19B Distilled\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx219BDistilledExtendVideo",
      "properties": [
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to extend."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideo.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the extended video."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "extend_direction",
          "type": {
            "type": "enum",
            "values": [
              "forward",
              "backward"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideo.ExtendDirection"
          },
          "default": "forward",
          "title": "Extend Direction",
          "description": "Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "video_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Video Strength",
          "description": "Video conditioning strength. Lower values represent more freedom given to the model to change the video content."
        },
        {
          "name": "num_context_frames",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Context Frames",
          "description": "The number of frames to use as context for the extension."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "match_input_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Fps",
          "description": "When true, match the output FPS to the input video's FPS instead of using the default target FPS."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 219B Distilled Extend Video Lora",
      "description": "LTX-2 19B Distilled\n    video, editing, video-to-video, vid2vid, lora\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx219BDistilledExtendVideoLora",
      "properties": [
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to extend."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLora.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the extended video."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "extend_direction",
          "type": {
            "type": "enum",
            "values": [
              "forward",
              "backward"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLora.ExtendDirection"
          },
          "default": "forward",
          "title": "Extend Direction",
          "description": "Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "video_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Video Strength",
          "description": "Video conditioning strength. Lower values represent more freedom given to the model to change the video content."
        },
        {
          "name": "num_context_frames",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Context Frames",
          "description": "The number of frames to use as context for the extension."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "match_input_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Fps",
          "description": "When true, match the output FPS to the input video's FPS instead of using the default target FPS."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 219B Distilled Video To Video",
      "description": "LTX-2 19B Distilled\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx219BDistilledVideoToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to generate the video from."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "ic_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Ic Lora Scale",
          "description": "The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Video Strength",
          "description": "Video conditioning strength. Lower values represent more freedom given to the model to change the video content."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "An optional URL of an image to use as the first frame of the video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "match_video_length",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Video Length",
          "description": "When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideo.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "preprocessor",
          "type": {
            "type": "enum",
            "values": [
              "depth",
              "canny",
              "pose",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideo.Preprocessor"
          },
          "default": "none",
          "title": "Preprocessor",
          "description": "The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "ic_lora",
          "type": {
            "type": "enum",
            "values": [
              "match_preprocessor",
              "canny",
              "depth",
              "pose",
              "detailer",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideo.IcLora"
          },
          "default": "match_preprocessor",
          "title": "Ic Lora",
          "description": "The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "match_input_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Fps",
          "description": "When true, match the output FPS to the input video's FPS instead of using the default target FPS."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 219B Distilled Video To Video Lora",
      "description": "LTX-2 19B Distilled\n    video, editing, video-to-video, vid2vid, lora\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx219BDistilledVideoToVideoLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to generate the video from."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "ic_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Ic Lora Scale",
          "description": "The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Video Strength",
          "description": "Video conditioning strength. Lower values represent more freedom given to the model to change the video content."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "An optional URL of an image to use as the first frame of the video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "match_video_length",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Video Length",
          "description": "When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLora.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "preprocessor",
          "type": {
            "type": "enum",
            "values": [
              "depth",
              "canny",
              "pose",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLora.Preprocessor"
          },
          "default": "none",
          "title": "Preprocessor",
          "description": "The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "ic_lora",
          "type": {
            "type": "enum",
            "values": [
              "match_preprocessor",
              "canny",
              "depth",
              "pose",
              "detailer",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLora.IcLora"
          },
          "default": "match_preprocessor",
          "title": "Ic Lora",
          "description": "The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "match_input_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Fps",
          "description": "When true, match the output FPS to the input video's FPS instead of using the default target FPS."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 219B Extend Video",
      "description": "LTX-2 19B\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx219BExtendVideo",
      "properties": [
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to extend."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the extended video."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "extend_direction",
          "type": {
            "type": "enum",
            "values": [
              "forward",
              "backward"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideo.ExtendDirection"
          },
          "default": "forward",
          "title": "Extend Direction",
          "description": "Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "video_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Video Strength",
          "description": "Video conditioning strength. Lower values represent more freedom given to the model to change the video content."
        },
        {
          "name": "num_context_frames",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Context Frames",
          "description": "The number of frames to use as context for the extension."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "match_input_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Fps",
          "description": "When true, match the output FPS to the input video's FPS instead of using the default target FPS."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 219B Extend Video Lora",
      "description": "LTX-2 19B\n    video, editing, video-to-video, vid2vid, lora\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx219BExtendVideoLora",
      "properties": [
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to extend."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the extended video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "video_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Video Strength",
          "description": "Video conditioning strength. Lower values represent more freedom given to the model to change the video content."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "extend_direction",
          "type": {
            "type": "enum",
            "values": [
              "forward",
              "backward"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLora.ExtendDirection"
          },
          "default": "forward",
          "title": "Extend Direction",
          "description": "Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "num_context_frames",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Context Frames",
          "description": "The number of frames to use as context for the extension."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "match_input_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Fps",
          "description": "When true, match the output FPS to the input video's FPS instead of using the default target FPS."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 219B Video To Video",
      "description": "LTX-2 19B\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx219BVideoToVideo",
      "properties": [
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to generate the video from."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "ic_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Ic Lora Scale",
          "description": "The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Video Strength",
          "description": "Video conditioning strength. Lower values represent more freedom given to the model to change the video content."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "An optional URL of an image to use as the first frame of the video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "match_video_length",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Video Length",
          "description": "When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "preprocessor",
          "type": {
            "type": "enum",
            "values": [
              "depth",
              "canny",
              "pose",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideo.Preprocessor"
          },
          "default": "none",
          "title": "Preprocessor",
          "description": "The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "ic_lora",
          "type": {
            "type": "enum",
            "values": [
              "match_preprocessor",
              "canny",
              "depth",
              "pose",
              "detailer",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideo.IcLora"
          },
          "default": "match_preprocessor",
          "title": "Ic Lora",
          "description": "The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "match_input_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Fps",
          "description": "When true, match the output FPS to the input video's FPS instead of using the default target FPS."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 219B Video To Video Lora",
      "description": "LTX-2 19B\n    video, editing, video-to-video, vid2vid, lora\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx219BVideoToVideoLora",
      "properties": [
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to generate the video from."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "ic_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Ic Lora Scale",
          "description": "The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Video Strength",
          "description": "Video conditioning strength. Lower values represent more freedom given to the model to change the video content."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "An optional URL of an image to use as the first frame of the video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "match_video_length",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Video Length",
          "description": "When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "preprocessor",
          "type": {
            "type": "enum",
            "values": [
              "depth",
              "canny",
              "pose",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLora.Preprocessor"
          },
          "default": "none",
          "title": "Preprocessor",
          "description": "The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "ic_lora",
          "type": {
            "type": "enum",
            "values": [
              "match_preprocessor",
              "canny",
              "depth",
              "pose",
              "detailer",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLora.IcLora"
          },
          "default": "match_preprocessor",
          "title": "Ic Lora",
          "description": "The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "match_input_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Fps",
          "description": "When true, match the output FPS to the input video's FPS instead of using the default target FPS."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 2 Retake Video",
      "description": "LTX Video 2.0 Retake\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx2RetakeVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to retake the video with"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to retake"
        },
        {
          "name": "start_time",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Start Time",
          "description": "The start time of the video to retake in seconds"
        },
        {
          "name": "duration",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Duration",
          "description": "The duration of the video to retake in seconds"
        },
        {
          "name": "retake_mode",
          "type": {
            "type": "enum",
            "values": [
              "replace_audio",
              "replace_video",
              "replace_audio_and_video"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx2RetakeVideo.RetakeMode"
          },
          "default": "replace_audio_and_video",
          "title": "Retake Mode",
          "description": "The retake mode to use for the retake"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Mirelo Ai Sfx V1 5 Video To Video",
      "description": "Mirelo SFX V1.5\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.MireloAiSfxV1_5VideoToVideo",
      "properties": [
        {
          "name": "num_samples",
          "type": {
            "type": "str"
          },
          "default": 2,
          "title": "Num Samples",
          "description": "The number of samples to generate from the model"
        },
        {
          "name": "duration",
          "type": {
            "type": "str"
          },
          "default": 10,
          "title": "Duration",
          "description": "The duration of the generated audio in seconds"
        },
        {
          "name": "start_offset",
          "type": {
            "type": "str"
          },
          "default": 0,
          "title": "Start Offset",
          "description": "The start offset in seconds to start the audio generation from"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "A video url that can accessed from the API to process and add sound effects"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": 8069,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used"
        },
        {
          "name": "text_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Prompt",
          "description": "Additional description to guide the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "One To All Animation 14B",
      "description": "One To All Animation\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.OneToAllAnimation14B",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.OneToAllAnimation14B.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the video to generate."
        },
        {
          "name": "image_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Image Guidance Scale",
          "description": "The image guidance scale to use for the video generation."
        },
        {
          "name": "pose_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Pose Guidance Scale",
          "description": "The pose guidance scale to use for the video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to use as a reference for the video generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a reference for the video generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "One To All Animation 1 3B",
      "description": "One To All Animation\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.OneToAllAnimation1_3B",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.OneToAllAnimation1_3B.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the video to generate."
        },
        {
          "name": "image_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Image Guidance Scale",
          "description": "The image guidance scale to use for the video generation."
        },
        {
          "name": "pose_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Pose Guidance Scale",
          "description": "The pose guidance scale to use for the video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to use as a reference for the video generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a reference for the video generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Sam 3 Video",
      "description": "Sam 3\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Sam3Video",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth')."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to be segmented."
        },
        {
          "name": "detection_threshold",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detection Threshold",
          "description": "Detection confidence threshold (0.0-1.0). Lower = more detections but less precise."
        },
        {
          "name": "box_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Box Prompts",
          "description": "List of box prompt coordinates (x_min, y_min, x_max, y_max)."
        },
        {
          "name": "point_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Point Prompts",
          "description": "List of point prompts"
        },
        {
          "name": "apply_mask",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Apply Mask",
          "description": "Apply the mask on the video."
        },
        {
          "name": "text_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Prompt",
          "description": "[DEPRECATED] Use 'prompt' instead. Kept for backward compatibility."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Sam 3 Video Rle",
      "description": "Sam 3\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Sam3VideoRle",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth')."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to be segmented."
        },
        {
          "name": "detection_threshold",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detection Threshold",
          "description": "Detection confidence threshold (0.0-1.0). Lower = more detections but less precise. Defaults: 0.5 for existing, 0.7 for new objects. Try 0.2-0.3 if text prompts fail."
        },
        {
          "name": "box_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Box Prompts",
          "description": "List of box prompts with optional frame_index."
        },
        {
          "name": "point_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Point Prompts",
          "description": "List of point prompts with frame indices."
        },
        {
          "name": "boundingbox_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Boundingbox Zip",
          "description": "Return per-frame bounding box overlays as a zip archive."
        },
        {
          "name": "frame_index",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Frame Index",
          "description": "Frame index used for initial interaction when mask_url is provided."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "The URL of the mask to be applied initially."
        },
        {
          "name": "apply_mask",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Apply Mask",
          "description": "Apply the mask on the video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Scail",
      "description": "Scail\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Scail",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to guide video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to use as a reference for the video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "512p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Scail.Resolution"
          },
          "default": "512p",
          "title": "Resolution",
          "description": "Output resolution. Outputs 896x512 (landscape) or 512x896 (portrait) based on the input image aspect ratio."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for the video generation."
        },
        {
          "name": "multi_character",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Multi Character",
          "description": "Enable multi-character mode. Use when driving video has multiple people."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a reference for the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Seedvr Upscale Video",
      "description": "SeedVR2\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.SeedvrUpscaleVideo",
      "properties": [
        {
          "name": "upscale_mode",
          "type": {
            "type": "enum",
            "values": [
              "target",
              "factor"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideo.UpscaleMode"
          },
          "default": "factor",
          "title": "Upscale Mode",
          "description": "The mode to use for the upscale. If 'target', the upscale factor will be calculated based on the target resolution. If 'factor', the upscale factor will be used directly."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The input video to be processed"
        },
        {
          "name": "noise_scale",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Noise Scale",
          "description": "The noise scale to use for the generation process."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideo.OutputFormat"
          },
          "default": "X264 (.mp4)",
          "title": "Output Format",
          "description": "The format of the output video."
        },
        {
          "name": "output_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideo.OutputWriteMode"
          },
          "default": "balanced",
          "title": "Output Write Mode",
          "description": "The write mode of the output video."
        },
        {
          "name": "target_resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "1440p",
              "2160p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideo.TargetResolution"
          },
          "default": "1080p",
          "title": "Target Resolution",
          "description": "The target resolution to upscale to when `upscale_mode` is `target`."
        },
        {
          "name": "output_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideo.OutputQuality"
          },
          "default": "high",
          "title": "Output Quality",
          "description": "The quality of the output video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "upscale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Upscale Factor",
          "description": "Upscaling factor to be used. Will multiply the dimensions with this factor when `upscale_mode` is `factor`."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The random seed used for the generation process."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Sora 2 Video To Video Remix",
      "description": "Sora 2\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Sora2VideoToVideoRemix",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Updated text prompt that directs the remix generation"
        },
        {
          "name": "video_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Video Id",
          "description": "The video_id from a previous Sora 2 generation. Note: You can only remix videos that were generated by Sora (via text-to-video or image-to-video endpoints), not arbitrary uploaded videos."
        },
        {
          "name": "delete_video",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Delete Video",
          "description": "Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Steady Dancer",
      "description": "Steady Dancer\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.SteadyDancer",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A person dancing with smooth and natural movements.",
          "title": "Prompt",
          "description": "Text prompt describing the desired animation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "image"
          },
          "default": "https://v3b.fal.media/files/b/0a84de68/jXDWywjhagRfR-GuZjoRs_video.mp4",
          "title": "Video Url",
          "description": "URL of the driving pose video. The motion from this video will be transferred to the reference image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "light",
              "moderate",
              "aggressive"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SteadyDancer.Acceleration"
          },
          "default": "aggressive",
          "title": "Acceleration",
          "description": "Acceleration levels."
        },
        {
          "name": "pose_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Pose Guidance Scale",
          "description": "Pose guidance scale for pose control strength."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "pose_guidance_end",
          "type": {
            "type": "float"
          },
          "default": 0.4,
          "title": "Pose Guidance End",
          "description": "End ratio for pose guidance. Controls when pose guidance ends."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 24. If not specified, uses the FPS from the input video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale for prompt adherence."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Frames",
          "description": "Number of frames to generate. If not specified, uses the frame count from the input video (capped at 241). Will be adjusted to nearest valid value (must satisfy 4k+1 pattern)."
        },
        {
          "name": "use_turbo",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Turbo",
          "description": "If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized (num_inference_steps=6, guidance_scale=1.0) and uses the LightX2V distillation LoRA."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurred, distorted face, bad anatomy, extra limbs, poorly drawn hands, poorly drawn feet, disfigured, out of frame, duplicate, watermark, signature, text",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SteadyDancer.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video. If 'auto', will be determined from the reference image."
        },
        {
          "name": "pose_guidance_start",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Pose Guidance Start",
          "description": "Start ratio for pose guidance. Controls when pose guidance begins."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "576p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SteadyDancer.Resolution"
          },
          "default": "576p",
          "title": "Resolution",
          "description": "Resolution of the generated video. 576p is default, 720p for higher quality. 480p is lower quality."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": "https://v3b.fal.media/files/b/0a85edaa/GDUCMPrdvOMcI5JpEcU7f.png",
          "title": "Image Url",
          "description": "URL of the reference image to animate. This is the person/character whose appearance will be preserved."
        },
        {
          "name": "preserve_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Audio",
          "description": "If enabled, copies audio from the input driving video to the output video."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Sync Lipsync React 1",
      "description": "Sync React-1\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.SyncLipsyncReact1",
      "properties": [
        {
          "name": "emotion",
          "type": {
            "type": "enum",
            "values": [
              "happy",
              "angry",
              "sad",
              "neutral",
              "disgusted",
              "surprised"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SyncLipsyncReact1.Emotion"
          },
          "default": "",
          "title": "Emotion",
          "description": "Emotion prompt for the generation. Currently supports single-word emotions only."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the input video. Must be **15 seconds or shorter**."
        },
        {
          "name": "lipsync_mode",
          "type": {
            "type": "enum",
            "values": [
              "cut_off",
              "loop",
              "bounce",
              "silence",
              "remap"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SyncLipsyncReact1.LipsyncMode"
          },
          "default": "bounce",
          "title": "Lipsync Mode",
          "description": "Lipsync mode when audio and video durations are out of sync."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL to the input audio. Must be **15 seconds or shorter**."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Temperature",
          "description": "Controls the expresiveness of the lipsync."
        },
        {
          "name": "model_mode",
          "type": {
            "type": "enum",
            "values": [
              "lips",
              "face",
              "head"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SyncLipsyncReact1.ModelMode"
          },
          "default": "face",
          "title": "Model Mode",
          "description": "Controls the edit region and movement scope for the model. Available options: - `lips`: Only lipsync using react-1 (minimal facial changes). - `face`: Lipsync + facial expressions without head movements. - `head`: Lipsync + facial expressions + natural talking head movements."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Veed Video Background Removal",
      "description": "Video Background Removal\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.VeedVideoBackgroundRemoval",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url"
        },
        {
          "name": "subject_is_person",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Subject Is Person",
          "description": "Set to False if the subject is not a person."
        },
        {
          "name": "output_codec",
          "type": {
            "type": "enum",
            "values": [
              "vp9",
              "h264"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemoval.OutputCodec"
          },
          "default": "vp9",
          "title": "Output Codec",
          "description": "Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality."
        },
        {
          "name": "refine_foreground_edges",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Refine Foreground Edges",
          "description": "Improves the quality of the extracted object's edges."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Veed Video Background Removal Fast",
      "description": "Video Background Removal\n    video, editing, video-to-video, vid2vid, fast\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.VeedVideoBackgroundRemovalFast",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url"
        },
        {
          "name": "subject_is_person",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Subject Is Person",
          "description": "Set to False if the subject is not a person."
        },
        {
          "name": "output_codec",
          "type": {
            "type": "enum",
            "values": [
              "vp9",
              "h264"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemovalFast.OutputCodec"
          },
          "default": "vp9",
          "title": "Output Codec",
          "description": "Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality."
        },
        {
          "name": "refine_foreground_edges",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Refine Foreground Edges",
          "description": "Improves the quality of the extracted object's edges."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Veed Video Background Removal Green Screen",
      "description": "Video Background Removal\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.VeedVideoBackgroundRemovalGreenScreen",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url"
        },
        {
          "name": "output_codec",
          "type": {
            "type": "enum",
            "values": [
              "vp9",
              "h264"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemovalGreenScreen.OutputCodec"
          },
          "default": "vp9",
          "title": "Output Codec",
          "description": "Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality."
        },
        {
          "name": "spill_suppression_strength",
          "type": {
            "type": "str"
          },
          "default": 0.8,
          "title": "Spill Suppression Strength",
          "description": "Increase the value if green spots remain in the video, decrease if color changes are noticed on the extracted subject."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Veo 3 1 Extend Video",
      "description": "Veo 3.1\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Veo3_1ExtendVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing how the video should be extended"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "7s"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Veo3_1ExtendVideo.Duration"
          },
          "default": "7s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Veo3_1ExtendVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Veo3_1ExtendVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Veo 3 1 Fast Extend Video",
      "description": "Veo 3.1 Fast\n    video, editing, video-to-video, vid2vid, fast\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Veo3_1FastExtendVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing how the video should be extended"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "7s"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Veo3_1FastExtendVideo.Duration"
          },
          "default": "7s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Veo3_1FastExtendVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Veo3_1FastExtendVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Video As Prompt",
      "description": "Video As Prompt\n    video, editing, video-to-video, vid2vid, professional\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.VideoAsPrompt",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.VideoAsPrompt.AspectRatio"
          },
          "default": "9:16",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.VideoAsPrompt.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "reference video to generate effect video from."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Input image to generate the effect video for."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Fps",
          "description": "Frames per second for the output video. Only applicable if output_type is 'video'."
        },
        {
          "name": "video_description",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Video Description",
          "description": "A brief description of the input video content."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducible generation. If set none, a random seed will be used."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 49,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Video Upscaler",
      "description": "Video Upscaler enhances video resolution and quality using AI.\n    video, upscaling, enhancement, resolution, video-to-video\n\n    Use cases:\n    - Upscale low resolution videos\n    - Enhance video quality\n    - Increase video resolution\n    - Improve video clarity\n    - Restore old video footage",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.VideoUpscaler",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to upscale"
        },
        {
          "name": "scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Scale",
          "description": "The scale factor"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan V2 6 Reference To Video",
      "description": "Wan v2.6 Reference to Video\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanV2_6ReferenceToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Use @Video1, @Video2, @Video3 to reference subjects from your videos. Works for people, animals, or objects. For multi-shot prompts: '[0-3s] Shot 1. [3-6s] Shot 2.' Max 800 characters."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV2_6ReferenceToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the generated video in seconds. R2V supports only 5 or 10 seconds (no 15s)."
        },
        {
          "name": "video_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Video Urls",
          "description": "Reference videos for subject consistency (1-3 videos). Videos' FPS must be at least 16 FPS.Reference in prompt as @Video1, @Video2, @Video3. Works for people, animals, or objects."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV2_6ReferenceToVideo.Resolution"
          },
          "default": "1080p",
          "title": "Resolution",
          "description": "Video resolution tier. R2V only supports 720p and 1080p (no 480p)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1",
              "4:3",
              "3:4"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV2_6ReferenceToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt rewriting using LLM."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "multi_shots",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Multi Shots",
          "description": "When true (default), enables intelligent multi-shot segmentation for coherent narrative videos with multiple shots. When false, generates single continuous shot. Only active when enable_prompt_expansion is True."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to describe content to avoid. Max 500 characters."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan Vace Apps Long Reframe",
      "description": "Wan 2.1 VACE Long Reframe\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanVaceAppsLongReframe",
      "properties": [
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the source video file. This video will be used as a reference for the reframe task."
        },
        {
          "name": "zoom_factor",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Zoom Factor",
          "description": "Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom."
        },
        {
          "name": "paste_back",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Paste Back",
          "description": "Whether to paste back the reframed scene to the original video."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "low",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframe.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation. Optional for reframing."
        },
        {
          "name": "scene_threshold",
          "type": {
            "type": "float"
          },
          "default": 30,
          "title": "Scene Threshold",
          "description": "Threshold for scene detection sensitivity (0-100). Lower values detect more scenes."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "auto_downsample_min_fps",
          "type": {
            "type": "float"
          },
          "default": 6,
          "title": "Auto Downsample Min Fps",
          "description": "Minimum FPS for auto downsample."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "sampler",
          "type": {
            "type": "enum",
            "values": [
              "unipc",
              "dpmPP",
              "euler"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframe.Sampler"
          },
          "default": "unipc",
          "title": "Sampler",
          "description": "Sampler to use for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframe.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "If true, also return a ZIP file containing all generated frames."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframe.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframe.Resolution"
          },
          "default": "auto",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "transparency_mode",
          "type": {
            "type": "enum",
            "values": [
              "content_aware",
              "white",
              "black"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframe.TransparencyMode"
          },
          "default": "content_aware",
          "title": "Transparency Mode",
          "description": "The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled."
        },
        {
          "name": "trim_borders",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Trim Borders",
          "description": "Whether to trim borders from the video."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframe.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "rife",
              "film"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframe.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. Options are 'rife' or 'film'."
        },
        {
          "name": "enable_auto_downsample",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Auto Downsample",
          "description": "Whether to enable auto downsample."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan Vace Apps Video Edit",
      "description": "Wan VACE Video Edit\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanVaceAppsVideoEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to edit the video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "low",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEdit.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEdit.Resolution"
          },
          "default": "auto",
          "title": "Resolution",
          "description": "Resolution of the edited video."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "Whether to include a ZIP archive containing all generated frames."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEdit.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the edited video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "video_type",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "general",
              "human"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEdit.VideoType"
          },
          "default": "auto",
          "title": "Video Type",
          "description": "The type of video you're editing. Use 'general' for most videos, and 'human' for videos emphasizing human subjects and motions. The default value 'auto' means the model will guess based on the first frame of the video."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "URLs of the input images to use as a reference for the generation."
        },
        {
          "name": "enable_auto_downsample",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Auto Downsample",
          "description": "Whether to enable automatic downsampling. If your video has a high frame rate or is long, enabling longer sequences to be generated. The video will be interpolated back to the original frame rate after generation."
        },
        {
          "name": "auto_downsample_min_fps",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Auto Downsample Min Fps",
          "description": "The minimum frames per second to downsample the video to."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan Vision Enhancer",
      "description": "Wan Vision Enhancer\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanVisionEnhancer",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional prompt to prepend to the VLM-generated description. Leave empty to use only the auto-generated description from the video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to enhance with Wan Video. Maximum 200MB file size. Videos longer than 500 frames will have only the first 500 frames processed (~8-21 seconds depending on fps)."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If not provided, a random seed will be used."
        },
        {
          "name": "target_resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVisionEnhancer.TargetResolution"
          },
          "default": "720p",
          "title": "Target Resolution",
          "description": "Target output resolution for the enhanced video. 720p (native, fast) or 1080p (upscaled, slower). Processing is always done at 720p, then upscaled if 1080p selected."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "oversaturated, overexposed, static, blurry details, subtitles, stylized, artwork, painting, still frame, overall gray, worst quality, low quality, JPEG artifacts, ugly, mutated, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fused fingers, static motion, cluttered background, three legs, crowded background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt to avoid unwanted features."
        },
        {
          "name": "creativity",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Creativity",
          "description": "Controls how much the model enhances/changes the video. 0 = Minimal change (preserves original), 1 = Subtle enhancement (default), 2 = Medium enhancement, 3 = Strong enhancement, 4 = Maximum enhancement."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Workflow Utilities Auto Subtitle",
      "description": "Workflow Utilities\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WorkflowUtilitiesAutoSubtitle",
      "properties": [
        {
          "name": "font_weight",
          "type": {
            "type": "enum",
            "values": [
              "normal",
              "bold",
              "black"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitle.FontWeight"
          },
          "default": "bold",
          "title": "Font Weight",
          "description": "Font weight (TikTok style typically uses bold or black)"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video file to add automatic subtitles to Max file size: 95.4MB, Timeout: 30.0s"
        },
        {
          "name": "stroke_width",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "Stroke Width",
          "description": "Text stroke/outline width in pixels (0 for no stroke)"
        },
        {
          "name": "font_color",
          "type": {
            "type": "enum",
            "values": [
              "white",
              "black",
              "red",
              "green",
              "blue",
              "yellow",
              "orange",
              "purple",
              "pink",
              "brown",
              "gray",
              "cyan",
              "magenta"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitle.FontColor"
          },
          "default": "white",
          "title": "Font Color",
          "description": "Subtitle text color for non-active words"
        },
        {
          "name": "font_size",
          "type": {
            "type": "int"
          },
          "default": 100,
          "title": "Font Size",
          "description": "Font size for subtitles (TikTok style uses larger text)"
        },
        {
          "name": "language",
          "type": {
            "type": "str"
          },
          "default": "en",
          "title": "Language",
          "description": "Language code for transcription (e.g., 'en', 'es', 'fr', 'de', 'it', 'pt', 'nl', 'ja', 'zh', 'ko') or 3-letter ISO code (e.g., 'eng', 'spa', 'fra')"
        },
        {
          "name": "y_offset",
          "type": {
            "type": "int"
          },
          "default": 75,
          "title": "Y Offset",
          "description": "Vertical offset in pixels (positive = move down, negative = move up)"
        },
        {
          "name": "background_opacity",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Background Opacity",
          "description": "Background opacity (0.0 = fully transparent, 1.0 = fully opaque)"
        },
        {
          "name": "stroke_color",
          "type": {
            "type": "enum",
            "values": [
              "black",
              "white",
              "red",
              "green",
              "blue",
              "yellow",
              "orange",
              "purple",
              "pink",
              "brown",
              "gray",
              "cyan",
              "magenta"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitle.StrokeColor"
          },
          "default": "black",
          "title": "Stroke Color",
          "description": "Text stroke/outline color"
        },
        {
          "name": "highlight_color",
          "type": {
            "type": "enum",
            "values": [
              "white",
              "black",
              "red",
              "green",
              "blue",
              "yellow",
              "orange",
              "purple",
              "pink",
              "brown",
              "gray",
              "cyan",
              "magenta"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitle.HighlightColor"
          },
          "default": "purple",
          "title": "Highlight Color",
          "description": "Color for the currently speaking word (karaoke-style highlight)"
        },
        {
          "name": "enable_animation",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Animation",
          "description": "Enable animation effects for subtitles (bounce style entrance)"
        },
        {
          "name": "font_name",
          "type": {
            "type": "str"
          },
          "default": "Montserrat",
          "title": "Font Name",
          "description": "Any Google Font name from fonts.google.com (e.g., 'Montserrat', 'Poppins', 'BBH Sans Hegarty')"
        },
        {
          "name": "position",
          "type": {
            "type": "enum",
            "values": [
              "top",
              "center",
              "bottom"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitle.Position"
          },
          "default": "bottom",
          "title": "Position",
          "description": "Vertical position of subtitles"
        },
        {
          "name": "words_per_subtitle",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "Words Per Subtitle",
          "description": "Maximum number of words per subtitle segment. Use 1 for single-word display, 2-3 for short phrases, or 8-12 for full sentences."
        },
        {
          "name": "background_color",
          "type": {
            "type": "enum",
            "values": [
              "black",
              "white",
              "red",
              "green",
              "blue",
              "yellow",
              "orange",
              "purple",
              "pink",
              "brown",
              "gray",
              "cyan",
              "magenta",
              "none",
              "transparent"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitle.BackgroundColor"
          },
          "default": "none",
          "title": "Background Color",
          "description": "Background color behind text ('none' or 'transparent' for no background)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "AIAvatar",
      "description": "MultiTalk generates talking avatar videos from images and audio files.\n    video, avatar, talking-head, multitalk, image-to-video\n\n    Use cases:\n    - Create talking avatar videos\n    - Animate portrait photos with audio\n    - Generate spokesperson videos\n    - Produce avatar presentations\n    - Create personalized video messages",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.AIAvatar",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatar.AIAvatarResolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the video to generate. Must be either 480p or 720p."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatar.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 145,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "audio"
      ]
    },
    {
      "title": "AIAvatar Multi",
      "description": "MultiTalk generates multi-speaker avatar videos with audio synchronization.\n    video, avatar, multi-speaker, talking-head, image-to-video\n\n    Use cases:\n    - Create multi-speaker videos with audio\n    - Generate synchronized dialogue\n    - Produce conversation videos\n    - Create interactive characters\n    - Generate multi-avatar content",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.AIAvatarMulti",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarMulti.AIAvatarMultiResolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the video to generate. Must be either 480p or 720p."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarMulti.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for generation."
        },
        {
          "name": "first_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "First Audio Url",
          "description": "The URL of the Person 1 audio file."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "second_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Second Audio Url",
          "description": "The URL of the Person 2 audio file."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "use_only_first_audio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Only First Audio",
          "description": "Whether to use only the first audio file."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 181,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "images",
        "audio"
      ]
    },
    {
      "title": "AIAvatar Multi Text",
      "description": "MultiTalk generates multi-speaker avatar videos from images and text.\n    video, avatar, multi-speaker, talking-head, image-to-video\n\n    Use cases:\n    - Create multi-speaker conversations\n    - Generate dialogue between avatars\n    - Produce interactive presentations\n    - Create conversational content\n    - Generate multi-character scenes",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.AIAvatarMultiText",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "second_text_input",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Second Text Input",
          "description": "The text input to guide video generation."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarMultiText.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarMultiText.AIAvatarMultiTextResolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the video to generate. Must be either 480p or 720p."
        },
        {
          "name": "first_text_input",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "First Text Input",
          "description": "The text input to guide video generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "voice2",
          "type": {
            "type": "enum",
            "values": [
              "Aria",
              "Roger",
              "Sarah",
              "Laura",
              "Charlie",
              "George",
              "Callum",
              "River",
              "Liam",
              "Charlotte",
              "Alice",
              "Matilda",
              "Will",
              "Jessica",
              "Eric",
              "Chris",
              "Brian",
              "Daniel",
              "Lily",
              "Bill"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarMultiText.Voice2"
          },
          "default": "Roger",
          "title": "Voice2",
          "description": "The second person's voice to use for speech generation"
        },
        {
          "name": "voice1",
          "type": {
            "type": "enum",
            "values": [
              "Aria",
              "Roger",
              "Sarah",
              "Laura",
              "Charlie",
              "George",
              "Callum",
              "River",
              "Liam",
              "Charlotte",
              "Alice",
              "Matilda",
              "Will",
              "Jessica",
              "Eric",
              "Chris",
              "Brian",
              "Daniel",
              "Lily",
              "Bill"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarMultiText.Voice1"
          },
          "default": "Sarah",
          "title": "Voice1",
          "description": "The first person's voice to use for speech generation"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 191,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "images",
        "texts"
      ]
    },
    {
      "title": "AIAvatar Single Text",
      "description": "MultiTalk generates talking avatar videos from an image and text input.\n    video, avatar, talking-head, text-to-speech, image-to-video\n\n    Use cases:\n    - Create avatar videos from text\n    - Generate talking heads with TTS\n    - Produce text-driven avatars\n    - Create virtual presenters\n    - Generate automated spokesperson videos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.AIAvatarSingleText",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarSingleText.AIAvatarSingleTextResolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the video to generate. Must be either 480p or 720p."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarSingleText.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for generation."
        },
        {
          "name": "text_input",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Input",
          "description": "The text input to guide video generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "Aria",
              "Roger",
              "Sarah",
              "Laura",
              "Charlie",
              "George",
              "Callum",
              "River",
              "Liam",
              "Charlotte",
              "Alice",
              "Matilda",
              "Will",
              "Jessica",
              "Eric",
              "Chris",
              "Brian",
              "Daniel",
              "Lily",
              "Bill"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarSingleText.Voice"
          },
          "default": "",
          "title": "Voice",
          "description": "The voice to use for speech generation"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 136,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "text"
      ]
    },
    {
      "title": "AMTFrame Interpolation",
      "description": "AMT Frame Interpolation creates smooth transitions between image frames.\n    video, interpolation, frame-generation, amt, image-to-video\n\n    Use cases:\n    - Create smooth transitions between images\n    - Generate intermediate frames\n    - Animate image sequences\n    - Create video from image pairs\n    - Produce smooth motion effects",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.AMTFrameInterpolation",
      "properties": [
        {
          "name": "frames",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Frames",
          "description": "Frames to interpolate"
        },
        {
          "name": "recursive_interpolation_passes",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Recursive Interpolation Passes",
          "description": "Number of recursive interpolation passes"
        },
        {
          "name": "output_fps",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Output Fps",
          "description": "Output frames per second"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Byte Dance Video Stylize",
      "description": "ByteDance Video Stylize applies artistic styles to image-based video generation.\n    video, style-transfer, artistic, bytedance, image-to-video\n\n    Use cases:\n    - Apply artistic styles to videos\n    - Create stylized video content\n    - Generate artistic animations\n    - Produce style-transferred videos\n    - Create visually unique content",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.ByteDanceVideoStylize",
      "properties": [
        {
          "name": "style",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style",
          "description": "The style for your character in the video. Please use a short description."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to make the stylized video from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "style"
      ]
    },
    {
      "title": "Bytedance Lynx",
      "description": "Lynx\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.BytedanceLynx",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide video generation"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.BytedanceLynx.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p, 580p, or 720p)"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.BytedanceLynx.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video (16:9, 9:16, or 1:1)"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "guidance_scale_2",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Guidance Scale 2",
          "description": "Image guidance scale. Controls how closely the generated video follows the reference image. Higher values increase adherence to the reference image but may decrease quality."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Strength",
          "description": "Reference image scale. Controls the influence of the reference image on the generated video."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 30."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the subject image to be used for video generation"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames in the generated video. Must be between 9 to 100."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Bright tones, overexposed, blurred background, static, subtitles, style, works, paintings, images, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt to guide what should not appear in the generated video"
        },
        {
          "name": "ip_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Ip Scale",
          "description": "Identity preservation scale. Controls how closely the generated video preserves the subject's identity from the reference image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Cog Video X5 BImage To Video",
      "description": "CogVideoX-5B generates high-quality videos from images with advanced motion.\n    video, generation, cogvideo, image-to-video, img2vid\n\n    Use cases:\n    - Generate videos from images\n    - Create dynamic image animations\n    - Produce high-quality video content\n    - Animate static images\n    - Generate motion from photos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.CogVideoX5BImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "use_rife",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Rife",
          "description": "Use RIFE for video interpolation"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL to the image to generate the video from."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. We currently support one lora."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related video to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "export_fps",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Export Fps",
          "description": "The target FPS of the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate video from"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Creatify Aurora",
      "description": "Creatify Aurora generates creative and visually stunning videos from images with unique effects.\n    video, generation, creatify, aurora, creative, effects\n\n    Use cases:\n    - Generate creative visual effects videos\n    - Create stunning video animations\n    - Produce artistic video content\n    - Generate unique video effects\n    - Create visually impressive videos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.CreatifyAurora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "A text prompt to guide the video generation process."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.CreatifyAurora.CreatifyAuroraResolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "Guidance scale to be used for text prompt adherence."
        },
        {
          "name": "audio_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Audio Guidance Scale",
          "description": "Guidance scale to be used for audio adherence."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file to be used for video generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image file to be used for video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Decart Lucy 14B Image To Video",
      "description": "Decart Lucy 14b\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.DecartLucy14BImageToVideo",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.DecartLucy14BImageToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text description of the desired video content"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.DecartLucy14BImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Video V15 Image To Video",
      "description": "Hunyuan Video v1.5 generates high-quality videos from images with advanced AI capabilities.\n    video, generation, hunyuan, v1.5, advanced\n\n    Use cases:\n    - Generate advanced quality videos\n    - Create sophisticated animations\n    - Produce high-fidelity video content\n    - Generate videos with AI excellence\n    - Create cutting-edge video animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.HunyuanVideoV15ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.HunyuanVideoV15ImageToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.HunyuanVideoV15ImageToVideo.HunyuanVideoV15Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the reference image for image-to-video generation."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Enable prompt expansion to enhance the input prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what not to generate."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kandinsky 5 Pro Image To Video",
      "description": "Kandinsky5 Pro generates professional quality videos from images with artistic style and control.\n    video, generation, kandinsky, pro, artistic\n\n    Use cases:\n    - Generate artistic videos from images\n    - Create stylized video animations\n    - Produce creative video content\n    - Generate videos with artistic flair\n    - Create professional artistic videos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Kandinsky5ProImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "512P",
              "1024P"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Kandinsky5ProImageToVideo.Kandinsky5ProResolution"
          },
          "default": "512P",
          "title": "Resolution",
          "description": "Video resolution: 512p or 1024p."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Kandinsky5ProImageToVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for faster generation."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5s"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Kandinsky5ProImageToVideo.Kandinsky5ProDuration"
          },
          "default": "5s",
          "title": "Duration",
          "description": "Video duration."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a reference for the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video Ai Avatar V2 Pro",
      "description": "Kling Video AI Avatar v2 Pro creates professional quality animated talking avatars with enhanced realism.\n    video, avatar, kling, v2, pro, talking-head\n\n    Use cases:\n    - Create professional talking avatars\n    - Animate portraits with high quality\n    - Generate realistic avatar videos\n    - Produce premium speaking characters\n    - Create pro-grade AI avatars",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoAiAvatarV2Pro",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": ".",
          "title": "Prompt",
          "description": "The prompt to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as your avatar"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video Ai Avatar V2 Standard",
      "description": "Kling Video AI Avatar v2 Standard creates animated talking avatars with standard quality.\n    video, avatar, kling, v2, standard, talking-head\n\n    Use cases:\n    - Create standard quality talking avatars\n    - Animate portraits with speech\n    - Generate avatar presentations\n    - Produce speaking character videos\n    - Create AI-driven avatars",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoAiAvatarV2Standard",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": ".",
          "title": "Prompt",
          "description": "The prompt to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as your avatar"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video O1 Image To Video",
      "description": "Kling O1 First Frame Last Frame to Video [Pro]\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoO1ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Use @Image1 to reference the start frame, @Image2 to reference the end frame."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoO1ImageToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Video duration in seconds."
        },
        {
          "name": "start_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Start Image Url",
          "description": "Image to use as the first frame of the video. Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s"
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "Image to use as the last frame of the video. Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video O1 Reference To Video",
      "description": "Kling O1 Reference Image to Video [Pro]\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoO1ReferenceToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Take @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoO1ReferenceToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoO1ReferenceToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Video duration in seconds."
        },
        {
          "name": "elements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Elements",
          "description": "Elements (characters/objects) to include in the video. Reference in prompt as @Element1, @Element2, etc. Maximum 7 total (elements + reference images + start image)."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Additional reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 7 total (elements + reference images + start image)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video O1 Standard Image To Video",
      "description": "Kling Video O1 Standard generates videos with optimized standard quality from images.\n    video, generation, kling, o1, standard\n\n    Use cases:\n    - Generate standard O1 quality videos\n    - Create optimized video animations\n    - Produce efficient video content\n    - Generate balanced quality videos\n    - Create standard tier animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoO1StandardImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Use @Image1 to reference the start frame, @Image2 to reference the end frame."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoO1StandardImageToVideo.KlingVideoO1StandardDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Video duration in seconds."
        },
        {
          "name": "start_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Start Image Url",
          "description": "Image to use as the first frame of the video. Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s"
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "Image to use as the last frame of the video. Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video O1 Standard Reference To Video",
      "description": "Kling Video O1 Standard generates videos using reference images for style consistency.\n    video, generation, kling, o1, standard, reference\n\n    Use cases:\n    - Generate videos from reference images\n    - Create style-consistent animations\n    - Produce reference-guided content\n    - Generate videos matching examples\n    - Create standardized reference videos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoO1StandardReferenceToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Take @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoO1StandardReferenceToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoO1StandardReferenceToVideo.KlingVideoO1StandardReferenceToVideoDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Video duration in seconds."
        },
        {
          "name": "elements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Elements",
          "description": "Elements (characters/objects) to include in the video. Reference in prompt as @Element1, @Element2, etc. Maximum 7 total (elements + reference images + start image)."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Additional reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 7 total (elements + reference images + start image)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video V1 Pro Ai Avatar",
      "description": "Kling AI Avatar Pro\n    video, animation, image-to-video, img2vid, professional\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoV1ProAiAvatar",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": ".",
          "title": "Prompt",
          "description": "The prompt to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as your avatar"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video V1 Standard Ai Avatar",
      "description": "Kling AI Avatar\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoV1StandardAiAvatar",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": ".",
          "title": "Prompt",
          "description": "The prompt to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as your avatar"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video V1 Standard Image To Video",
      "description": "Kling Video v1 Standard generates videos from images with balanced quality.\n    video, generation, kling, standard, image-to-video\n\n    Use cases:\n    - Generate standard quality videos\n    - Create balanced video animations\n    - Produce efficient video content\n    - Generate videos for web use\n    - Create moderate quality outputs",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoV1StandardImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for the video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV1StandardImageToVideo.KlingVideoV1StandardDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "tail_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Tail Image Url",
          "description": "URL of the image to be used for the end of the video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be used for the video"
        },
        {
          "name": "static_mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Static Mask Url",
          "description": "URL of the image for Static Brush Application Area (Mask image created by users using the motion brush)"
        },
        {
          "name": "dynamic_masks",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Dynamic Masks",
          "description": "List of dynamic masks"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video V26 Pro Image To Video",
      "description": "Kling Video v2.6 Pro generates professional quality videos with latest model improvements.\n    video, generation, kling, v2.6, pro\n\n    Use cases:\n    - Generate professional v2.6 videos\n    - Create latest quality animations\n    - Produce premium video content\n    - Generate advanced videos\n    - Create pro-tier animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoV26ProImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV26ProImageToVideo.KlingVideoV26ProDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "voice_ids",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Voice Ids",
          "description": "Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice"
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase."
        },
        {
          "name": "start_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Start Image Url",
          "description": "URL of the image to be used for the video"
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the image to be used for the end of the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video V2 5 Turbo Standard Image To Video",
      "description": "Kling Video\n    video, animation, image-to-video, img2vid, fast\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoV2_5TurboStandardImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV2_5TurboStandardImageToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be used for the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "LTXImage To Video",
      "description": "LTX Video generates temporally consistent videos from images.\n    video, generation, ltx, temporal, image-to-video\n\n    Use cases:\n    - Generate temporally consistent videos\n    - Create smooth image animations\n    - Produce coherent video sequences\n    - Animate with temporal awareness\n    - Generate fluid motion videos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LTXImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for random number generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to take."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate the video from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Live Avatar",
      "description": "Live Avatar creates animated talking avatars from portrait images with realistic lip-sync and expressions.\n    video, avatar, talking-head, animation, portrait\n\n    Use cases:\n    - Create talking avatar videos\n    - Animate portrait images\n    - Generate lip-synced avatars\n    - Produce speaking character videos\n    - Create animated presenters",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LiveAvatar",
      "properties": [
        {
          "name": "frames_per_clip",
          "type": {
            "type": "int"
          },
          "default": 48,
          "title": "Frames Per Clip",
          "description": "Number of frames per clip. Must be a multiple of 4. Higher values = smoother but slower generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "A text prompt describing the scene and character. Helps guide the video generation style and context."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "light",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LiveAvatar.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "Acceleration level for faster video decoding"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the reference image for avatar generation. The character in this image will be animated."
        },
        {
          "name": "num_clips",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Num Clips",
          "description": "Number of video clips to generate. Each clip is approximately 3 seconds. Set higher for longer videos."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the driving audio file (WAV or MP3). The avatar will be animated to match this audio."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values follow the prompt more closely."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable safety checker for content moderation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Longcat Video Distilled Image To Video 480P",
      "description": "LongCat Video Distilled\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LongcatVideoDistilledImageToVideo480P",
      "properties": [
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo480P.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo480P.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 15,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate a video from."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo480P.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 12,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 162,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Longcat Video Distilled Image To Video 720P",
      "description": "LongCat Video Distilled\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LongcatVideoDistilledImageToVideo720P",
      "properties": [
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo720P.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo720P.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "num_refine_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 12,
          "title": "Num Refine Inference Steps",
          "description": "The number of inference steps to use for refinement."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate a video from."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 12,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 162,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo720P.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Longcat Video Image To Video 480P",
      "description": "LongCat Video\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LongcatVideoImageToVideo480P",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for the video generation."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 15,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the video generation."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 162,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "The negative prompt to use for the video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate a video from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for the video generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Longcat Video Image To Video 720P",
      "description": "LongCat Video\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LongcatVideoImageToVideo720P",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for the video generation."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "num_refine_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Refine Inference Steps",
          "description": "The number of inference steps to use for refinement."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the video generation."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 162,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "The negative prompt to use for the video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate a video from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for the video generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ltx 219B Distilled Image To Video",
      "description": "LTX-2 19B Distilled generates videos efficiently using knowledge distillation from the 19B model.\n    video, generation, ltx-2, 19b, distilled, efficient\n\n    Use cases:\n    - Generate videos efficiently with distilled model\n    - Create fast quality video animations\n    - Produce optimized video content\n    - Generate videos with good performance\n    - Create balanced quality-speed videos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Ltx219BDistilledImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt used for the generation."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate the video from."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "interpolation_direction",
          "type": {
            "type": "enum",
            "values": [
              "forward",
              "backward"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.InterpolationDirection"
          },
          "default": "forward",
          "title": "Interpolation Direction",
          "description": "The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ltx 219B Distilled Image To Video Lora",
      "description": "LTX-2 19B Distilled with LoRA combines efficient generation with custom-trained models.\n    video, generation, ltx-2, 19b, distilled, lora\n\n    Use cases:\n    - Generate videos with custom distilled model\n    - Create efficient specialized content\n    - Produce fast domain-specific videos\n    - Generate with optimized custom model\n    - Create quick customized animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Ltx219BDistilledImageToVideoLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt used for the generation."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate the video from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "interpolation_direction",
          "type": {
            "type": "enum",
            "values": [
              "forward",
              "backward"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.InterpolationDirection"
          },
          "default": "forward",
          "title": "Interpolation Direction",
          "description": "The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ltx 219B Image To Video",
      "description": "LTX-2 19B generates high-quality videos from images using the powerful 19-billion parameter model.\n    video, generation, ltx-2, 19b, large-model\n\n    Use cases:\n    - Generate high-quality videos with large model\n    - Create detailed video animations\n    - Produce superior video content\n    - Generate videos with powerful AI\n    - Create premium video animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Ltx219BImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt used for the generation."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate the video from."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "interpolation_direction",
          "type": {
            "type": "enum",
            "values": [
              "forward",
              "backward"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.InterpolationDirection"
          },
          "default": "forward",
          "title": "Interpolation Direction",
          "description": "The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ltx 219B Image To Video Lora",
      "description": "LTX-2 19B with LoRA enables custom-trained 19B models for specialized video generation.\n    video, generation, ltx-2, 19b, lora, custom\n\n    Use cases:\n    - Generate videos with custom 19B model\n    - Create specialized video content\n    - Produce domain-specific animations\n    - Generate with fine-tuned large model\n    - Create customized video animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Ltx219BImageToVideoLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt used for the generation."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate the video from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "interpolation_direction",
          "type": {
            "type": "enum",
            "values": [
              "forward",
              "backward"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.InterpolationDirection"
          },
          "default": "forward",
          "title": "Interpolation Direction",
          "description": "The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Luma Dream Machine",
      "description": "Generate video clips from your images using Luma Dream Machine v1.5. Supports various aspect ratios and optional end-frame blending.\n    video, generation, animation, blending, aspect-ratio, img2vid, image-to-video\n\n    Use cases:\n    - Create seamless video loops\n    - Generate video transitions\n    - Transform images into animations\n    - Create motion graphics\n    - Produce video content",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LumaDreamMachine",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "21:9",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LumaDreamMachine.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "loop",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Loop",
          "description": "Whether the video should loop (end of video is blended with the beginning)"
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "An image to blend the end of the video with"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt",
        "resolution"
      ]
    },
    {
      "title": "Minimax Hailuo 2 3 Fast Pro Image To Video",
      "description": "MiniMax Hailuo 2.3 Fast [Pro] (Image to Video)\n    video, animation, image-to-video, img2vid, fast, professional\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.MinimaxHailuo2_3FastProImageToVideo",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Minimax Hailuo 2 3 Fast Standard Image To Video",
      "description": "MiniMax Hailuo 2.3 Fast [Standard] (Image to Video)\n    video, animation, image-to-video, img2vid, fast, professional\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.MinimaxHailuo2_3FastStandardImageToVideo",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "6",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.MinimaxHailuo2_3FastStandardImageToVideo.Duration"
          },
          "default": "6",
          "title": "Duration",
          "description": "The duration of the video in seconds."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Minimax Hailuo 2 3 Standard Image To Video",
      "description": "MiniMax Hailuo 2.3 [Standard] (Image to Video)\n    video, animation, image-to-video, img2vid, professional\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.MinimaxHailuo2_3StandardImageToVideo",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "6",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.MinimaxHailuo2_3StandardImageToVideo.Duration"
          },
          "default": "6",
          "title": "Duration",
          "description": "The duration of the video in seconds."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Omni Human V15",
      "description": "OmniHuman v1.5 generates realistic human videos from images.\n    video, human, realistic, bytedance, image-to-video\n\n    Use cases:\n    - Generate realistic human videos\n    - Create human motion animations\n    - Produce lifelike character videos\n    - Generate human performances\n    - Create realistic human content",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.OmniHumanV15",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to guide the video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.OmniHumanV15.OmniHumanV15Resolution"
          },
          "default": "1080p",
          "title": "Resolution",
          "description": "The resolution of the generated video. Defaults to 1080p. 720p generation is faster and higher in quality. 1080p generation is limited to 30s audio and 720p generation is limited to 60s audio."
        },
        {
          "name": "turbo_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Turbo Mode",
          "description": "Generate a video at a faster rate with a slight quality trade-off."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file to generate the video. Audio must be under 30s long for 1080p generation and under 60s long for 720p generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image used to generate the video"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ovi Image To Video",
      "description": "Ovi\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.OviImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps."
        },
        {
          "name": "audio_negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "robotic, muffled, echo, distorted",
          "title": "Audio Negative Prompt",
          "description": "Negative prompt for audio generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "jitter, bad hands, blur, distortion",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to guide video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pika V2 2 Pikaframes",
      "description": "Pika\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PikaV2_2Pikaframes",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Default prompt for all transitions. Individual transition prompts override this."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PikaV2_2Pikaframes.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "transitions",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Transitions",
          "description": "Configuration for each transition. Length must be len(image_urls) - 1. Total duration of all transitions must not exceed 25 seconds. If not provided, uses default 5-second transitions with the global prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator"
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "URLs of keyframe images (2-5 images) to create transitions between"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse Swap",
      "description": "Pixverse\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseSwap",
      "properties": [
        {
          "name": "original_sound_switch",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Original Sound Switch",
          "description": "Whether to keep the original audio"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the external video to swap"
        },
        {
          "name": "keyframe_id",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Keyframe Id",
          "description": "The keyframe ID (from 1 to the last frame position)"
        },
        {
          "name": "mode",
          "type": {
            "type": "enum",
            "values": [
              "person",
              "object",
              "background"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseSwap.Mode"
          },
          "default": "person",
          "title": "Mode",
          "description": "The swap mode to use"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseSwap.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The output resolution (1080p not supported)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the target image for swapping"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V56 Image To Video",
      "description": "Generate high-quality videos from images with Pixverse v5.6.\n    video, generation, pixverse, v5.6, image-to-video, img2vid\n\n    Use cases:\n    - Animate photos into professional video clips\n    - Create dynamic product showcase videos\n    - Generate stylized video content from artwork\n    - Produce high-resolution social media animations\n    - Transform static images with various visual styles",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV56ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the desired video motion"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution quality of the output video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56Style"
          },
          "default": null,
          "title": "Style",
          "description": "Optional visual style for the video"
        },
        {
          "name": "thinking_type",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "enabled",
              "disabled",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56ThinkingType"
          },
          "default": null,
          "title": "Thinking Type",
          "description": "Thinking mode for video generation"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to transform into a video"
        },
        {
          "name": "generate_audio_switch",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Generate Audio Switch",
          "description": "Whether to generate audio for the video"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Optional seed for reproducible generation"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "What to avoid in the generated video"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt",
        "resolution"
      ]
    },
    {
      "title": "Pixverse V56 Transition",
      "description": "Pixverse v5.6 Transition creates smooth video transitions between two images with professional effects.\n    video, transition, pixverse, v5.6, effects\n\n    Use cases:\n    - Create smooth transitions between images\n    - Generate professional video effects\n    - Produce seamless image morphing\n    - Create transition animations\n    - Generate video connecting two scenes",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV56Transition",
      "properties": [
        {
          "name": "first_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "First Image Url",
          "description": "URL of the image to use as the first frame"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56Transition.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56Transition.PixverseV56TransitionResolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56Transition.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "thinking_type",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "enabled",
              "disabled",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56Transition.ThinkingType"
          },
          "default": null,
          "title": "Thinking Type",
          "description": "Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for the transition"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56Transition.PixverseV56TransitionDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. 1080p videos are limited to 5 or 8 seconds"
        },
        {
          "name": "generate_audio_switch",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Generate Audio Switch",
          "description": "Enable audio generation (BGM, SFX, dialogue)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the image to use as the last frame"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V5 5 Effects",
      "description": "Pixverse\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV5_5Effects",
      "properties": [
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5_5Effects.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5_5Effects.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "thinking_type",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "enabled",
              "disabled",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5_5Effects.ThinkingType"
          },
          "default": null,
          "title": "Thinking Type",
          "description": "Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision"
        },
        {
          "name": "effect",
          "type": {
            "type": "enum",
            "values": [
              "Kiss Me AI",
              "Kiss",
              "Muscle Surge",
              "Warmth of Jesus",
              "Anything, Robot",
              "The Tiger Touch",
              "Hug",
              "Holy Wings",
              "Microwave",
              "Zombie Mode",
              "Squid Game",
              "Baby Face",
              "Black Myth: Wukong",
              "Long Hair Magic",
              "Leggy Run",
              "Fin-tastic Mermaid",
              "Punch Face",
              "Creepy Devil Smile",
              "Thunder God",
              "Eye Zoom Challenge",
              "Who's Arrested?",
              "Baby Arrived",
              "Werewolf Rage",
              "Bald Swipe",
              "BOOM DROP",
              "Huge Cutie",
              "Liquid Metal",
              "Sharksnap!",
              "Dust Me Away",
              "3D Figurine Factor",
              "Bikini Up",
              "My Girlfriends",
              "My Boyfriends",
              "Subject 3 Fever",
              "Earth Zoom",
              "Pole Dance",
              "Vroom Dance",
              "GhostFace Terror",
              "Dragon Evoker",
              "Skeletal Bae",
              "Summoning succubus",
              "Halloween Voodoo Doll",
              "3D Naked-Eye AD",
              "Package Explosion",
              "Dishes Served",
              "Ocean ad",
              "Supermarket AD",
              "Tree doll",
              "Come Feel My Abs",
              "The Bicep Flex",
              "London Elite Vibe",
              "Flora Nymph Gown",
              "Christmas Costume",
              "It's Snowy",
              "Reindeer Cruiser",
              "Snow Globe Maker",
              "Pet Christmas Outfit",
              "Adopt a Polar Pal",
              "Cat Christmas Box",
              "Starlight Gift Box",
              "Xmas Poster",
              "Pet Christmas Tree",
              "City Santa Hat",
              "Stocking Sweetie",
              "Christmas Night",
              "Xmas Front Page Karma",
              "Grinch's Xmas Hijack",
              "Giant Product",
              "Truck Fashion Shoot",
              "Beach AD",
              "Shoal Surround",
              "Mechanical Assembly",
              "Lighting AD",
              "Billboard AD",
              "Product close-up",
              "Parachute Delivery",
              "Dreamlike Cloud",
              "Macaron Machine",
              "Poster AD",
              "Truck AD",
              "Graffiti AD",
              "3D Figurine Factory",
              "The Exclusive First Class",
              "Art Zoom Challenge",
              "I Quit",
              "Hitchcock Dolly Zoom",
              "Smell the Lens",
              "I believe I can fly",
              "Strikout Dance",
              "Pixel World",
              "Mint in Box",
              "Hands up, Hand",
              "Flora Nymph Go",
              "Somber Embrace",
              "Beam me up",
              "Suit Swagger"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5_5Effects.Effect"
          },
          "default": "",
          "title": "Effect",
          "description": "The effect to apply to the video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of the image to use as the first frame. If not provided, generates from text"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V5 5 Image To Video",
      "description": "Pixverse\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV5_5ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5_5ImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5_5ImageToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5_5ImageToVideo.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "thinking_type",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "enabled",
              "disabled",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5_5ImageToVideo.ThinkingType"
          },
          "default": null,
          "title": "Thinking Type",
          "description": "Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision"
        },
        {
          "name": "generate_multi_clip_switch",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Generate Multi Clip Switch",
          "description": "Enable multi-clip generation with dynamic camera changes"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        },
        {
          "name": "generate_audio_switch",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Generate Audio Switch",
          "description": "Enable audio generation (BGM, SFX, dialogue)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V5 5 Transition",
      "description": "Pixverse\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV5_5Transition",
      "properties": [
        {
          "name": "first_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "First Image Url",
          "description": "URL of the image to use as the first frame"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5_5Transition.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5_5Transition.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5_5Transition.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "thinking_type",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "enabled",
              "disabled",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5_5Transition.ThinkingType"
          },
          "default": null,
          "title": "Thinking Type",
          "description": "Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for the transition"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5_5Transition.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds"
        },
        {
          "name": "generate_audio_switch",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Generate Audio Switch",
          "description": "Enable audio generation (BGM, SFX, dialogue)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the image to use as the last frame"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "See Dance V15 Pro Image To Video",
      "description": "SeeDance v1.5 Pro generates high-quality dance videos from images.\n    video, dance, animation, seedance, bytedance, image-to-video\n\n    Use cases:\n    - Animate photos into dance videos\n    - Create dance choreography from images\n    - Generate dance performances\n    - Produce music video content\n    - Create dance training materials",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.SeeDanceV15ProImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV15ProImageToVideo.SeeDanceV15ProResolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV15ProImageToVideo.SeeDanceV15ProDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the video in seconds"
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV15ProImageToVideo.SeeDanceV15ProAspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image used to generate video"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "camera_fixed",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Camera Fixed",
          "description": "Whether to fix the camera position"
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image the video ends with. Defaults to None."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control video generation. Use -1 for random."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "See Dance V1 Lite Reference To Video",
      "description": "SeeDance v1 Lite generates lightweight dance videos using reference images.\n    video, dance, lite, reference, seedance, image-to-video\n\n    Use cases:\n    - Generate efficient dance videos\n    - Create reference-based animations\n    - Produce lightweight dance content\n    - Generate quick dance outputs\n    - Create optimized dance videos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.SeeDanceV1LiteReferenceToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV1LiteReferenceToVideo.SeeDanceV1LiteResolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Video resolution - 480p for faster generation, 720p for higher quality"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "2",
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV1LiteReferenceToVideo.SeeDanceV1LiteDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the video in seconds"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV1LiteReferenceToVideo.SeeDanceV1LiteAspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "reference_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Image Urls",
          "description": "Reference images to generate the video with."
        },
        {
          "name": "camera_fixed",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Camera Fixed",
          "description": "Whether to fix the camera position"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control video generation. Use -1 for random."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "reference"
      ]
    },
    {
      "title": "See Dance V1 Pro Fast Image To Video",
      "description": "SeeDance v1 Pro Fast generates dance videos quickly from images.\n    video, dance, fast, seedance, bytedance, image-to-video\n\n    Use cases:\n    - Rapidly generate dance videos\n    - Quick dance animation\n    - Fast dance prototypes\n    - Create dance previews\n    - Efficient dance video generation",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.SeeDanceV1ProFastImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV1ProFastImageToVideo.SeeDanceV1ProFastResolution"
          },
          "default": "1080p",
          "title": "Resolution",
          "description": "Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "2",
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV1ProFastImageToVideo.SeeDanceV1ProFastDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the video in seconds"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV1ProFastImageToVideo.SeeDanceV1ProFastAspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image used to generate video"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "camera_fixed",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Camera Fixed",
          "description": "Whether to fix the camera position"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control video generation. Use -1 for random."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Stable Video Image To Video",
      "description": "Stable Video generates consistent video animations from images.\n    video, generation, stable, consistent, image-to-video\n\n    Use cases:\n    - Generate stable video animations\n    - Create consistent motion\n    - Produce reliable video outputs\n    - Animate images consistently\n    - Generate predictable videos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.StableVideoImageToVideo",
      "properties": [
        {
          "name": "motion_bucket_id",
          "type": {
            "type": "int"
          },
          "default": 127,
          "title": "Motion Bucket Id",
          "description": "The motion bucket id determines the motion of the generated video. The higher the number, the more motion there will be."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "cond_aug",
          "type": {
            "type": "float"
          },
          "default": 0.02,
          "title": "Cond Aug",
          "description": "The conditoning augmentation determines the amount of noise that will be added to the conditioning frame. The higher the number, the more noise there will be, and the less the video will look like the initial image. Increase it for more motion."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Veed Fabric 1 0",
      "description": "Fabric 1.0\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.VeedFabric1_0",
      "properties": [
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "480p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.VeedFabric1_0.Resolution"
          },
          "default": "",
          "title": "Resolution",
          "description": "Resolution"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Veed Fabric 1 0 Fast",
      "description": "Fabric 1.0 Fast\n    video, animation, image-to-video, img2vid, fast\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.VeedFabric1_0Fast",
      "properties": [
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "480p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.VeedFabric1_0Fast.Resolution"
          },
          "default": "",
          "title": "Resolution",
          "description": "Resolution"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Veo 3 1 Fast First Last Frame To Video",
      "description": "Veo 3.1 Fast\n    video, animation, image-to-video, img2vid, fast\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Veo3_1FastFirstLastFrameToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the video you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4s",
              "6s",
              "8s"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3_1FastFirstLastFrameToVideo.Duration"
          },
          "default": "8s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3_1FastFirstLastFrameToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "4k"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3_1FastFirstLastFrameToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "first_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "First Frame Url",
          "description": "URL of the first frame of the video"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "last_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Last Frame Url",
          "description": "URL of the last frame of the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Veo 3 1 Fast Image To Video",
      "description": "Veo 3.1 Fast\n    video, animation, image-to-video, img2vid, fast\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Veo3_1FastImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the video you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4s",
              "6s",
              "8s"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3_1FastImageToVideo.Duration"
          },
          "default": "8s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3_1FastImageToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video. Only 16:9 and 9:16 are supported."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "4k"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3_1FastImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Veo 3 1 First Last Frame To Video",
      "description": "Veo 3.1\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Veo3_1FirstLastFrameToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the video you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4s",
              "6s",
              "8s"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3_1FirstLastFrameToVideo.Duration"
          },
          "default": "8s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3_1FirstLastFrameToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "4k"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3_1FirstLastFrameToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "first_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "First Frame Url",
          "description": "URL of the first frame of the video"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "last_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Last Frame Url",
          "description": "URL of the last frame of the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Veo 3 1 Image To Video",
      "description": "Veo 3.1\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Veo3_1ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the video you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4s",
              "6s",
              "8s"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3_1ImageToVideo.Duration"
          },
          "default": "8s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3_1ImageToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video. Only 16:9 and 9:16 are supported."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "4k"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3_1ImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Veo 3 1 Reference To Video",
      "description": "Veo 3.1\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Veo3_1ReferenceToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the video you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "8s"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3_1ReferenceToVideo.Duration"
          },
          "default": "8s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3_1ReferenceToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "4k"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3_1ReferenceToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "URLs of the reference images to use for consistent subject appearance"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Vidu Q2 Reference To Video Pro",
      "description": "Vidu Q2 Reference-to-Video Pro generates professional quality videos using reference images for style and content.\n    video, generation, vidu, q2, pro, reference\n\n    Use cases:\n    - Generate pro videos from references\n    - Create style-consistent animations\n    - Produce reference-guided videos\n    - Generate videos matching examples\n    - Create professional reference-based content",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.ViduQ2ReferenceToVideoPro",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation, max 2000 characters"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.ViduQ2ReferenceToVideoPro.ViduQ2ReferenceToVideoProResolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Output video resolution"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the output video (e.g., auto, 16:9, 9:16, 1:1, or any W:H)"
        },
        {
          "name": "duration",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Duration",
          "description": "Duration of the video in seconds (0 for automatic duration)"
        },
        {
          "name": "reference_video_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Video Urls",
          "description": "URLs of the reference videos for video editing or motion reference. Supports up to 2 videos."
        },
        {
          "name": "bgm",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Bgm",
          "description": "Whether to add background music to the generated video"
        },
        {
          "name": "reference_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Image Urls",
          "description": "URLs of the reference images for subject appearance. If videos are provided, up to 4 images are allowed; otherwise up to 7 images."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "movement_amplitude",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "small",
              "medium",
              "large"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.ViduQ2ReferenceToVideoPro.MovementAmplitude"
          },
          "default": "auto",
          "title": "Movement Amplitude",
          "description": "The movement amplitude of objects in the frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan Ati",
      "description": "Wan Ati\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.WanAti",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanAti.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p, 580p, 720p)."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image."
        },
        {
          "name": "track",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "list",
                "type_args": [
                  {
                    "type": "str"
                  }
                ]
              }
            ]
          },
          "default": [],
          "title": "Track",
          "description": "Motion tracks to guide video generation. Each track is a sequence of points defining a motion trajectory. Multiple tracks can control different elements or objects in the video. Expected format: array of tracks, where each track is an array of points with 'x' and 'y' coordinates (up to 121 points per track). Points will be automatically padded to 121 if fewer are provided. Coordinates should be within the image dimensions."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan Move",
      "description": "Wan Move generates videos with natural motion and movement from static images.\n    video, generation, wan, motion, animation\n\n    Use cases:\n    - Add natural motion to images\n    - Create animated movements\n    - Produce dynamic video content\n    - Generate moving scenes from stills\n    - Create motion animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.WanMove",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide the video generation."
        },
        {
          "name": "trajectories",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "list",
                "type_args": [
                  {
                    "type": "str"
                  }
                ]
              }
            ]
          },
          "default": [],
          "title": "Trajectories",
          "description": "A list of trajectories. Each trajectory list means the movement of one object."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70",
          "title": "Negative Prompt",
          "description": "Negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan V26 Image To Video",
      "description": "Wan v2.6 generates high-quality videos from images with balanced quality and performance.\n    video, generation, wan, v2.6, image-to-video\n\n    Use cases:\n    - Generate quality videos from images\n    - Create balanced video animations\n    - Produce reliable video content\n    - Generate consistent videos\n    - Create professional animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.WanV26ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the desired video motion. Max 800 characters."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10",
              "15"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV26ImageToVideo.WanV26Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the generated video in seconds. Choose between 5, 10 or 15 seconds."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV26ImageToVideo.WanV26Resolution"
          },
          "default": "1080p",
          "title": "Resolution",
          "description": "Video resolution. Valid values: 720p, 1080p"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame. Must be publicly accessible or base64 data URI. Image dimensions must be between 240 and 7680."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "URL of the audio to use as the background music. Must be publicly accessible. Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds), the audio is truncated to the first N seconds, and the rest is discarded. If the audio is shorter than the video, the remaining part of the video will be silent. For example, if the audio is 3 seconds long and the video duration is 5 seconds, the first 3 seconds of the output video will have sound, and the last 2 seconds will be silent. - Format: WAV, MP3. - Duration: 3 to 30 s. - File size: Up to 15 MB."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "multi_shots",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Multi Shots",
          "description": "When true, enables intelligent multi-shot segmentation. Only active when enable_prompt_expansion is True. Set to false for single-shot generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to describe content to avoid. Max 500 characters."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt rewriting using LLM."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan V26 Image To Video Flash",
      "description": "Wan v2.6 Flash generates videos from images with ultra-fast processing for rapid iteration.\n    video, generation, wan, v2.6, flash, fast\n\n    Use cases:\n    - Generate videos at maximum speed\n    - Create rapid video prototypes\n    - Produce instant video previews\n    - Generate quick video iterations\n    - Create fast video animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.WanV26ImageToVideoFlash",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the desired video motion. Max 800 characters."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10",
              "15"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV26ImageToVideoFlash.WanV26FlashDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the generated video in seconds. Choose between 5, 10 or 15 seconds."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV26ImageToVideoFlash.WanV26FlashResolution"
          },
          "default": "1080p",
          "title": "Resolution",
          "description": "Video resolution. Valid values: 720p, 1080p"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame. Must be publicly accessible or base64 data URI. Image dimensions must be between 240 and 7680."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "URL of the audio to use as the background music. Must be publicly accessible. Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds), the audio is truncated to the first N seconds, and the rest is discarded. If the audio is shorter than the video, the remaining part of the video will be silent. For example, if the audio is 3 seconds long and the video duration is 5 seconds, the first 3 seconds of the output video will have sound, and the last 2 seconds will be silent. - Format: WAV, MP3. - Duration: 3 to 30 s. - File size: Up to 15 MB."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "multi_shots",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Multi Shots",
          "description": "When true, enables intelligent multi-shot segmentation. Only active when enable_prompt_expansion is True. Set to false for single-shot generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to describe content to avoid. Max 500 characters."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt rewriting using LLM."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "AIDetector Image",
      "description": "AI Detector analyzes images to determine if they were generated by AI or are real photos.\n    vision, ai-detection, analysis, classification\n\n    Use cases:\n    - Detect AI-generated images\n    - Verify image authenticity\n    - Filter synthetic content\n    - Content moderation for AI images\n    - Analyze image provenance",
      "namespace": "fal.vision",
      "node_type": "fal.vision.AIDetectorImage",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL pointing to an image to analyze for AI generation.(Max: 3000 characters)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Arbiter Image",
      "description": "Arbiter provides comprehensive image analysis and quality metrics.\n    vision, analysis, quality, metrics, image-evaluation\n\n    Use cases:\n    - Analyze image quality\n    - Extract image metrics\n    - Evaluate visual properties\n    - Assess image characteristics\n    - Generate quality reports",
      "namespace": "fal.vision",
      "node_type": "fal.vision.ArbiterImage",
      "properties": [
        {
          "name": "measurements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Measurements",
          "description": "The measurements to use for the measurement."
        },
        {
          "name": "inputs",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Inputs",
          "description": "The inputs to use for the measurement."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Arbiter Image Image",
      "description": "Arbiter measures similarity and alignment between reference images.\n    vision, similarity, comparison, image-matching, analysis\n\n    Use cases:\n    - Compare image similarity\n    - Measure visual alignment\n    - Find duplicate images\n    - Rank image variations\n    - Evaluate image consistency",
      "namespace": "fal.vision",
      "node_type": "fal.vision.ArbiterImageImage",
      "properties": [
        {
          "name": "measurements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Measurements",
          "description": "The measurements to use for the measurement."
        },
        {
          "name": "inputs",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Inputs",
          "description": "The inputs to use for the measurement."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image1",
        "image2"
      ]
    },
    {
      "title": "Arbiter Image Text",
      "description": "Arbiter measures semantic alignment between images and text descriptions.\n    vision, alignment, similarity, text-image, analysis\n\n    Use cases:\n    - Measure image-text alignment\n    - Verify prompt accuracy\n    - Quality control for generated images\n    - Rank images by text relevance\n    - Evaluate caption accuracy",
      "namespace": "fal.vision",
      "node_type": "fal.vision.ArbiterImageText",
      "properties": [
        {
          "name": "measurements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Measurements",
          "description": "The measurements to use for the measurement."
        },
        {
          "name": "inputs",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Inputs",
          "description": "The inputs to use for the measurement."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "text"
      ]
    },
    {
      "title": "Florence 2 Caption",
      "description": "Florence-2 Large generates concise, accurate captions for images.\n    vision, captioning, description, florence, analysis\n\n    Use cases:\n    - Generate image captions\n    - Create alt text for images\n    - Describe images concisely\n    - Automate image descriptions\n    - Produce accessibility captions",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Florence2Caption",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Florence 2 Detailed Caption",
      "description": "Florence-2 Large generates detailed captions with rich contextual information.\n    vision, captioning, detailed-description, florence, analysis\n\n    Use cases:\n    - Generate detailed captions\n    - Create rich image descriptions\n    - Produce comprehensive captions\n    - Analyze image context\n    - Generate informative descriptions",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Florence2DetailedCaption",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Florence 2 More Detailed Caption",
      "description": "Florence-2 Large generates highly detailed, comprehensive image captions.\n    vision, captioning, detailed-description, florence, analysis\n\n    Use cases:\n    - Generate detailed image descriptions\n    - Create comprehensive captions\n    - Produce rich image narratives\n    - Analyze image content thoroughly\n    - Generate long-form descriptions",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Florence2MoreDetailedCaption",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Florence 2OCR",
      "description": "Florence-2 Large performs optical character recognition to extract text from images.\n    vision, ocr, text-extraction, florence, reading\n\n    Use cases:\n    - Extract text from images\n    - Read document images\n    - Digitize printed text\n    - Parse image text content\n    - Convert images to text",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Florence2OCR",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Florence 2 Region To Category",
      "description": "Florence-2 Large classifies image regions into semantic categories.\n    vision, classification, region-analysis, florence, categorization\n\n    Use cases:\n    - Classify image regions\n    - Categorize image areas\n    - Label image segments\n    - Identify region types\n    - Semantic region analysis",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Florence2RegionToCategory",
      "properties": [
        {
          "name": "region",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Region",
          "description": "The user input coordinates"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "region"
      ]
    },
    {
      "title": "Florence 2 Region To Description",
      "description": "Florence-2 Large generates detailed descriptions of specific image regions.\n    vision, captioning, region-description, florence, ocr\n\n    Use cases:\n    - Describe specific image regions\n    - Generate region captions\n    - Extract region information\n    - Annotate image areas\n    - Create detailed region descriptions",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Florence2RegionToDescription",
      "properties": [
        {
          "name": "region",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Region",
          "description": "The user input coordinates"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "region"
      ]
    },
    {
      "title": "Moondream 3 Preview Caption",
      "description": "Moondream3 Preview [Caption]\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Moondream3PreviewCaption",
      "properties": [
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Top P",
          "description": "Nucleus sampling probability mass to use, between 0 and 1."
        },
        {
          "name": "length",
          "type": {
            "type": "enum",
            "values": [
              "short",
              "normal",
              "long"
            ],
            "type_name": "nodetool.nodes.fal.vision.Moondream3PreviewCaption.Length"
          },
          "default": "normal",
          "title": "Length",
          "description": "Length of the caption to generate"
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Temperature",
          "description": "Sampling temperature to use, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If not set, defaults to 0."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be processed Max width: 7000px, Max height: 7000px, Timeout: 20.0s"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Moondream 3 Preview Detect",
      "description": "Moondream3 Preview [Detect]\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Moondream3PreviewDetect",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Object to be detected in the image"
        },
        {
          "name": "preview",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preview",
          "description": "Whether to preview the output"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be processed Max width: 7000px, Max height: 7000px, Timeout: 20.0s"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Moondream 3 Preview Point",
      "description": "Moondream3 Preview [Point]\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Moondream3PreviewPoint",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Object to be located in the image"
        },
        {
          "name": "preview",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preview",
          "description": "Whether to preview the output"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be processed Max width: 7000px, Max height: 7000px, Timeout: 20.0s"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Moondream 3 Preview Query",
      "description": "Moondream 3 Preview [Query]\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Moondream3PreviewQuery",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Query to be asked in the image"
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Top P",
          "description": "Nucleus sampling probability mass to use, between 0 and 1."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Temperature",
          "description": "Sampling temperature to use, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If not set, defaults to 0."
        },
        {
          "name": "reasoning",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Reasoning",
          "description": "Whether to include detailed reasoning behind the answer"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be processed Max width: 7000px, Max height: 7000px, Timeout: 20.0s"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Openrouter Router Vision",
      "description": "OpenRouter [Vision]\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.vision",
      "node_type": "fal.vision.OpenrouterRouterVision",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to be used for the image"
        },
        {
          "name": "reasoning",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reasoning",
          "description": "Should reasoning be the part of the final answer."
        },
        {
          "name": "system_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "System Prompt",
          "description": "System prompt to provide context or instructions to the model"
        },
        {
          "name": "model",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model",
          "description": "Name of the model to use. Charged based on actual token usage."
        },
        {
          "name": "max_tokens",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Max Tokens",
          "description": "This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Temperature",
          "description": "This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of image URLs to be processed"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Perceptron Isaac 01 Openai V1 Chat Completions",
      "description": "Isaac 0.1 [OpenAI Compatible Endpoint]\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.vision",
      "node_type": "fal.vision.PerceptronIsaac01OpenaiV1ChatCompletions",
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Sam 3 Image Embed",
      "description": "Sam 3\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Sam3ImageEmbed",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to embed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Elevenlabs Dubbing",
      "description": "ElevenLabs Dubbing\n    video, generation, audio-to-video, visualization\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.ElevenlabsDubbing",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video file to dub. Either audio_url or video_url must be provided. If both are provided, video_url takes priority."
        },
        {
          "name": "highest_resolution",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Highest Resolution",
          "description": "Whether to use the highest resolution for dubbing."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to dub. Either audio_url or video_url must be provided."
        },
        {
          "name": "target_lang",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Target Lang",
          "description": "Target language code for dubbing (ISO 639-1)"
        },
        {
          "name": "num_speakers",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Num Speakers",
          "description": "Number of speakers in the audio. If not provided, will be auto-detected."
        },
        {
          "name": "source_lang",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Source Lang",
          "description": "Source language code. If not provided, will be auto-detected."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video_url",
        "highest_resolution",
        "audio_url",
        "target_lang",
        "num_speakers"
      ]
    },
    {
      "title": "Longcat Multi Avatar Image Audio To Video",
      "description": "Longcat Multi Avatar\n    video, generation, audio-to-video, visualization\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.LongcatMultiAvatarImageAudioToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Two people are having a conversation with natural expressions and movements.",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "audio_url_person2",
          "type": {
            "type": "str"
          },
          "default": "https://raw.githubusercontent.com/meituan-longcat/LongCat-Video/refs/heads/main/assets/avatar/multi/sing_woman.WAV",
          "title": "Audio Url Person2",
          "description": "The URL of the audio file for person 2 (right side)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "bbox_person1",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Bbox Person1",
          "description": "Bounding box for person 1. If not provided, defaults to left half of image."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Close-up, Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "The negative prompt to avoid in the video generation."
        },
        {
          "name": "text_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Text Guidance Scale",
          "description": "The text guidance scale for classifier-free guidance."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.LongcatMultiAvatarImageAudioToVideo.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p or 720p). Billing is per video-second (16 frames): 480p is 1 unit per second and 720p is 4 units per second."
        },
        {
          "name": "audio_type",
          "type": {
            "type": "enum",
            "values": [
              "para",
              "add"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.LongcatMultiAvatarImageAudioToVideo.AudioType"
          },
          "default": "para",
          "title": "Audio Type",
          "description": "How to combine the two audio tracks. 'para' (parallel) plays both simultaneously, 'add' (sequential) plays person 1 first then person 2."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image containing two speakers."
        },
        {
          "name": "audio_url_person1",
          "type": {
            "type": "str"
          },
          "default": "https://raw.githubusercontent.com/meituan-longcat/LongCat-Video/refs/heads/main/assets/avatar/multi/sing_man.WAV",
          "title": "Audio Url Person1",
          "description": "The URL of the audio file for person 1 (left side)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "audio_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Audio Guidance Scale",
          "description": "The audio guidance scale. Higher values may lead to exaggerated mouth movements."
        },
        {
          "name": "bbox_person2",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Bbox Person2",
          "description": "Bounding box for person 2. If not provided, defaults to right half of image."
        },
        {
          "name": "num_segments",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Segments",
          "description": "Number of video segments to generate. Each segment adds ~5 seconds of video. First segment is ~5.8s, additional segments are 5s each."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "num_inference_steps",
        "audio_url_person2",
        "enable_safety_checker",
        "bbox_person1"
      ]
    },
    {
      "title": "Ltx 219B Audio To Video",
      "description": "LTX-2 19B\n    video, generation, audio-to-video, visualization\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.Ltx219BAudioToVideo",
      "properties": [
        {
          "name": "match_audio_length",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Audio Length",
          "description": "When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Video Size",
          "description": "The size of the generated video. Use 'auto' to match the input image dimensions if provided."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "preprocess_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preprocess Audio",
          "description": "Whether to preprocess the audio before using it as conditioning."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of an image to use as the first frame of the video."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio to generate the video from."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "match_audio_length",
        "prompt",
        "acceleration",
        "use_multiscale",
        "num_inference_steps"
      ]
    },
    {
      "title": "Ltx 219B Audio To Video Lora",
      "description": "LTX-2 19B\n    video, generation, audio-to-video, visualization, lora\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.Ltx219BAudioToVideoLora",
      "properties": [
        {
          "name": "match_audio_length",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Audio Length",
          "description": "When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Video Size",
          "description": "The size of the generated video. Use 'auto' to match the input image dimensions if provided."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "preprocess_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preprocess Audio",
          "description": "Whether to preprocess the audio before using it as conditioning."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of an image to use as the first frame of the video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio to generate the video from."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "match_audio_length",
        "prompt",
        "acceleration",
        "use_multiscale",
        "num_inference_steps"
      ]
    },
    {
      "title": "Ltx 219B Distilled Audio To Video",
      "description": "LTX-2 19B Distilled\n    video, generation, audio-to-video, visualization\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.Ltx219BDistilledAudioToVideo",
      "properties": [
        {
          "name": "match_audio_length",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Audio Length",
          "description": "When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Video Size",
          "description": "The size of the generated video. Use 'auto' to match the input image dimensions if provided."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "preprocess_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preprocess Audio",
          "description": "Whether to preprocess the audio before using it as conditioning."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of an image to use as the first frame of the video."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio to generate the video from."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "match_audio_length",
        "prompt",
        "acceleration",
        "use_multiscale",
        "fps"
      ]
    },
    {
      "title": "Ltx 219B Distilled Audio To Video Lora",
      "description": "LTX-2 19B Distilled\n    video, generation, audio-to-video, visualization, lora\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.Ltx219BDistilledAudioToVideoLora",
      "properties": [
        {
          "name": "match_audio_length",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Audio Length",
          "description": "When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Video Size",
          "description": "The size of the generated video. Use 'auto' to match the input image dimensions if provided."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "preprocess_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preprocess Audio",
          "description": "Whether to preprocess the audio before using it as conditioning."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of an image to use as the first frame of the video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio to generate the video from."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "match_audio_length",
        "prompt",
        "acceleration",
        "use_multiscale",
        "fps"
      ]
    },
    {
      "title": "Deepfilternet 3",
      "description": "DeepFilterNet3 removes noise and improves audio quality with advanced deep learning filtering.\n    audio, noise-reduction, filtering, cleaning, audio-to-audio\n\n    Use cases:\n    - Remove noise from audio\n    - Clean audio recordings\n    - Filter unwanted sounds\n    - Improve audio clarity\n    - Generate clean audio",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.Deepfilternet3",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "audio_format",
          "type": {
            "type": "enum",
            "values": [
              "mp3",
              "aac",
              "m4a",
              "ogg",
              "opus",
              "flac",
              "wav"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.Deepfilternet3.AudioFormat"
          },
          "default": "mp3",
          "title": "Audio Format",
          "description": "The format for the output audio."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio to enhance."
        },
        {
          "name": "bitrate",
          "type": {
            "type": "str"
          },
          "default": "192k",
          "title": "Bitrate",
          "description": "The bitrate of the output audio."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Demucs",
      "description": "Demucs separates music into vocals, drums, bass, and other instruments with high quality.\n    audio, music-separation, stems, demucs, audio-to-audio\n\n    Use cases:\n    - Separate music into stems\n    - Extract vocals from songs\n    - Isolate instruments in music\n    - Create karaoke tracks\n    - Generate individual audio stems",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.Demucs",
      "properties": [
        {
          "name": "segment_length",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Segment Length",
          "description": "Length in seconds of each segment for processing. Smaller values use less memory but may reduce quality. Default is model-specific."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "wav",
              "mp3"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.Demucs.OutputFormat"
          },
          "default": "mp3",
          "title": "Output Format",
          "description": "Output audio format for the separated stems"
        },
        {
          "name": "stems",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Stems",
          "description": "Specific stems to extract. If None, extracts all available stems. Available stems depend on model: vocals, drums, bass, other, guitar, piano (for 6s model)"
        },
        {
          "name": "overlap",
          "type": {
            "type": "float"
          },
          "default": 0.25,
          "title": "Overlap",
          "description": "Overlap between segments (0.0 to 1.0). Higher values may improve quality but increase processing time."
        },
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "htdemucs",
              "htdemucs_ft",
              "htdemucs_6s",
              "hdemucs_mmi",
              "mdx",
              "mdx_extra",
              "mdx_q",
              "mdx_extra_q"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.Demucs.Model"
          },
          "default": "htdemucs_6s",
          "title": "Model",
          "description": "Demucs model to use for separation"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to separate into stems"
        },
        {
          "name": "shifts",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Shifts",
          "description": "Number of random shifts for equivariant stabilization. Higher values improve quality but increase processing time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Elevenlabs Voice Changer",
      "description": "ElevenLabs Voice Changer transforms voice characteristics in audio with AI-powered voice conversion.\n    audio, voice-change, elevenlabs, transformation, audio-to-audio\n\n    Use cases:\n    - Change voice characteristics in audio\n    - Transform vocal qualities\n    - Create voice variations\n    - Modify speaker identity\n    - Generate voice-changed audio",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.ElevenlabsVoiceChanger",
      "properties": [
        {
          "name": "voice",
          "type": {
            "type": "str"
          },
          "default": "Rachel",
          "title": "Voice",
          "description": "The voice to use for speech generation"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The input audio file"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "mp3_22050_32",
              "mp3_44100_32",
              "mp3_44100_64",
              "mp3_44100_96",
              "mp3_44100_128",
              "mp3_44100_192",
              "pcm_8000",
              "pcm_16000",
              "pcm_22050",
              "pcm_24000",
              "pcm_44100",
              "pcm_48000",
              "ulaw_8000",
              "alaw_8000",
              "opus_48000_32",
              "opus_48000_64",
              "opus_48000_96",
              "opus_48000_128",
              "opus_48000_192"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.ElevenlabsVoiceChanger.OutputFormat"
          },
          "default": "mp3_44100_128",
          "title": "Output Format",
          "description": "Output format of the generated audio. Formatted as codec_sample_rate_bitrate."
        },
        {
          "name": "remove_background_noise",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Remove Background Noise",
          "description": "If set, will remove the background noise from your audio input using our audio isolation model."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Ffmpeg Api Merge Audios",
      "description": "FFmpeg API Merge Audios combines multiple audio files into a single output.\n    audio, processing, audio-to-audio, merging, ffmpeg\n\n    Use cases:\n    - Combine multiple audio tracks\n    - Merge audio segments\n    - Create audio compilations\n    - Join split audio files\n    - Generate combined audio output",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.FfmpegApiMergeAudios",
      "properties": [
        {
          "name": "audio_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Audio Urls",
          "description": "List of audio URLs to merge in order. The 0th stream of the audio will be considered as the merge candidate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Output Format",
          "description": "Output format of the combined audio. If not used, will be determined automatically using FFMPEG. Formatted as codec_sample_rate_bitrate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Nova Sr",
      "description": "Nova SR enhances audio quality through super-resolution processing for clearer and richer sound.\n    audio, enhancement, super-resolution, quality, audio-to-audio\n\n    Use cases:\n    - Enhance audio quality\n    - Improve sound clarity\n    - Upscale audio resolution\n    - Restore degraded audio\n    - Generate high-quality audio",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.NovaSr",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "bitrate",
          "type": {
            "type": "str"
          },
          "default": "192k",
          "title": "Bitrate",
          "description": "The bitrate of the output audio."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file to enhance."
        },
        {
          "name": "audio_format",
          "type": {
            "type": "enum",
            "values": [
              "mp3",
              "aac",
              "m4a",
              "ogg",
              "opus",
              "flac",
              "wav"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.NovaSr.AudioFormat"
          },
          "default": "mp3",
          "title": "Audio Format",
          "description": "The format for the output audio."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Sam Audio Separate",
      "description": "SAM Audio Separate isolates and extracts different audio sources from mixed recordings.\n    audio, separation, source-extraction, isolation, audio-to-audio\n\n    Use cases:\n    - Separate audio sources\n    - Extract vocals from music\n    - Isolate instruments\n    - Remove background sounds\n    - Generate separated audio tracks",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.SamAudioSeparate",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the sound to isolate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "quality"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.SamAudioSeparate.Acceleration"
          },
          "default": "balanced",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to process (WAV, MP3, FLAC supported)"
        },
        {
          "name": "predict_spans",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Predict Spans",
          "description": "Automatically predict temporal spans where the target sound occurs."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "wav",
              "mp3"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.SamAudioSeparate.OutputFormat"
          },
          "default": "wav",
          "title": "Output Format",
          "description": "Output audio format."
        },
        {
          "name": "reranking_candidates",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Reranking Candidates",
          "description": "Number of candidates to generate and rank. Higher improves quality but increases latency and cost."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Sam Audio Span Separate",
      "description": "SAM Audio Span Separate isolates audio sources across time spans with precise temporal control.\n    audio, separation, temporal, span, audio-to-audio\n\n    Use cases:\n    - Separate audio by time spans\n    - Extract sources in specific periods\n    - Isolate temporal audio segments\n    - Remove sounds in time ranges\n    - Generate time-based separations",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.SamAudioSpanSeparate",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the sound to isolate. Optional but recommended - helps the model identify what type of sound to extract from the span."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "quality"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.SamAudioSpanSeparate.Acceleration"
          },
          "default": "balanced",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "spans",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Spans",
          "description": "Time spans where the target sound occurs which should be isolated."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "wav",
              "mp3"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.SamAudioSpanSeparate.OutputFormat"
          },
          "default": "wav",
          "title": "Output Format",
          "description": "Output audio format."
        },
        {
          "name": "trim_to_span",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Trim To Span",
          "description": "Trim output audio to only include the specified span time range. If False, returns the full audio length with the target sound isolated throughout."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to process."
        },
        {
          "name": "reranking_candidates",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Reranking Candidates",
          "description": "Number of candidates to generate and rank. Higher improves quality but increases latency and cost. Requires text prompt; ignored for span-only separation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Stable Audio 25 Audio To Audio",
      "description": "Stable Audio 2.5 transforms and modifies audio with AI-powered processing and effects.\n    audio, transformation, stable-audio, 2.5, audio-to-audio\n\n    Use cases:\n    - Transform audio characteristics\n    - Apply AI-powered audio effects\n    - Modify audio properties\n    - Generate audio variations\n    - Create processed audio",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.StableAudio25AudioToAudio",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to guide the audio generation"
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Sometimes referred to as denoising, this parameter controls how much influence the `audio_url` parameter has on the generated audio. A value of 0 would yield audio that is identical to the input. A value of 1 would be as if you passed in no audio at all."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The audio clip to transform"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of steps to denoise the audio for"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed"
        },
        {
          "name": "total_seconds",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Total Seconds",
          "description": "The duration of the audio clip to generate. If not provided, it will be set to the duration of the input audio."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio",
        "prompt"
      ]
    }
  ]
}