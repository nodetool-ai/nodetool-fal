{
  "name": "nodetool-fal",
  "description": "Nodetool FAL nodes",
  "version": "0.6.3-rc.17",
  "authors": [
    "Matthias Georgi <matti.georgi@gmail.com>"
  ],
  "repo_id": "",
  "nodes": [
    {
      "title": "Bytedance Seed 3D Image To 3D",
      "description": "Bytedance\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.BytedanceSeed3DImageTo3D",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image for the 3D asset generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image_url"
      ]
    },
    {
      "title": "Hunyuan 3DV 3 Image To 3D",
      "description": "Hunyuan3d V3\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Hunyuan3DV3ImageTo3D",
      "properties": [
        {
          "name": "input_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image Url",
          "description": "URL of image to use while generating the 3D model."
        },
        {
          "name": "polygon_type",
          "type": {
            "type": "enum",
            "values": [
              "triangle",
              "quadrilateral"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hunyuan3DV3ImageTo3D.PolygonType"
          },
          "default": "triangle",
          "title": "Polygon Type",
          "description": "Polygon type. Only takes effect when GenerateType is LowPoly."
        },
        {
          "name": "face_count",
          "type": {
            "type": "int"
          },
          "default": 500000,
          "title": "Face Count",
          "description": "Target face count. Range: 40000-1500000"
        },
        {
          "name": "right_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Right Image Url",
          "description": "Optional right view image URL for better 3D reconstruction."
        },
        {
          "name": "back_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Back Image Url",
          "description": "Optional back view image URL for better 3D reconstruction."
        },
        {
          "name": "enable_pbr",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Pbr",
          "description": "Whether to enable PBR material generation. Does not take effect when generate_type is Geometry."
        },
        {
          "name": "generate_type",
          "type": {
            "type": "enum",
            "values": [
              "Normal",
              "LowPoly",
              "Geometry"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hunyuan3DV3ImageTo3D.GenerateType"
          },
          "default": "Normal",
          "title": "Generate Type",
          "description": "Generation type. Normal: textured model. LowPoly: polygon reduction. Geometry: white model without texture."
        },
        {
          "name": "left_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Left Image Url",
          "description": "Optional left view image URL for better 3D reconstruction."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "input_image_url",
        "polygon_type",
        "face_count",
        "right_image_url",
        "back_image_url"
      ]
    },
    {
      "title": "Hunyuan 3DV 3 Sketch To 3D",
      "description": "Hunyuan3d V3\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Hunyuan3DV3SketchTo3D",
      "properties": [
        {
          "name": "input_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image Url",
          "description": "URL of sketch or line art image to transform into a 3D model. Image resolution must be between 128x128 and 5000x5000 pixels."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the 3D content attributes such as color, category, and material."
        },
        {
          "name": "face_count",
          "type": {
            "type": "int"
          },
          "default": 500000,
          "title": "Face Count",
          "description": "Target face count. Range: 40000-1500000"
        },
        {
          "name": "enable_pbr",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Pbr",
          "description": "Whether to enable PBR material generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "input_image_url",
        "prompt",
        "face_count",
        "enable_pbr"
      ]
    },
    {
      "title": "Hunyuan 3 d V2",
      "description": "Generate 3D models from your images using Hunyuan 3D. A native 3D generative model enabling versatile and high-quality 3D asset creation.\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - 3D model generation from photos\n    - Product 3D visualization\n    - AR/VR content creation\n    - Game asset generation\n    - Architectural visualization",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Hunyuan3dV2",
      "properties": [
        {
          "name": "input_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image Url",
          "description": "URL of image to use while generating the 3D model."
        },
        {
          "name": "octree_resolution",
          "type": {
            "type": "int"
          },
          "default": 256,
          "title": "Octree Resolution",
          "description": "Octree resolution for the model."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for the model."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps to perform."
        },
        {
          "name": "textured_mesh",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Textured Mesh",
          "description": "If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "input_image_url",
        "octree_resolution",
        "guidance_scale",
        "seed",
        "num_inference_steps"
      ]
    },
    {
      "title": "Hunyuan 3 d V21",
      "description": "Hunyuan3D-2.1 is a scalable 3D asset creation system that advances state-of-the-art 3D generation through Physically-Based Rendering (PBR).\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - 3D model generation from photos\n    - Product 3D visualization\n    - AR/VR content creation\n    - Game asset generation\n    - Architectural visualization",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Hunyuan3dV21",
      "properties": [
        {
          "name": "input_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image Url",
          "description": "URL of image to use while generating the 3D model."
        },
        {
          "name": "octree_resolution",
          "type": {
            "type": "int"
          },
          "default": 256,
          "title": "Octree Resolution",
          "description": "Octree resolution for the model."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for the model."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps to perform."
        },
        {
          "name": "textured_mesh",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Textured Mesh",
          "description": "If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "input_image_url",
        "octree_resolution",
        "guidance_scale",
        "seed",
        "num_inference_steps"
      ]
    },
    {
      "title": "Hunyuan 3 d V2 Mini",
      "description": "Generate 3D models from your images using Hunyuan 3D. A native 3D generative model enabling versatile and high-quality 3D asset creation.\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - 3D model generation from photos\n    - Product 3D visualization\n    - AR/VR content creation\n    - Game asset generation\n    - Architectural visualization",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Hunyuan3dV2Mini",
      "properties": [
        {
          "name": "input_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image Url",
          "description": "URL of image to use while generating the 3D model."
        },
        {
          "name": "octree_resolution",
          "type": {
            "type": "int"
          },
          "default": 256,
          "title": "Octree Resolution",
          "description": "Octree resolution for the model."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for the model."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps to perform."
        },
        {
          "name": "textured_mesh",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Textured Mesh",
          "description": "If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "input_image_url",
        "octree_resolution",
        "guidance_scale",
        "seed",
        "num_inference_steps"
      ]
    },
    {
      "title": "Hunyuan 3 d V2 Mini Turbo",
      "description": "Generate 3D models from your images using Hunyuan 3D. A native 3D generative model enabling versatile and high-quality 3D asset creation.\n    3d, generation, image-to-3d, modeling, fast\n\n    Use cases:\n    - 3D model generation from photos\n    - Product 3D visualization\n    - AR/VR content creation\n    - Game asset generation\n    - Architectural visualization",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Hunyuan3dV2MiniTurbo",
      "properties": [
        {
          "name": "input_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image Url",
          "description": "URL of image to use while generating the 3D model."
        },
        {
          "name": "octree_resolution",
          "type": {
            "type": "int"
          },
          "default": 256,
          "title": "Octree Resolution",
          "description": "Octree resolution for the model."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for the model."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps to perform."
        },
        {
          "name": "textured_mesh",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Textured Mesh",
          "description": "If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "input_image_url",
        "octree_resolution",
        "guidance_scale",
        "seed",
        "num_inference_steps"
      ]
    },
    {
      "title": "Hunyuan 3 d V2 Multi View",
      "description": "Generate 3D models from your images using Hunyuan 3D. A native 3D generative model enabling versatile and high-quality 3D asset creation.\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - 3D model generation from photos\n    - Product 3D visualization\n    - AR/VR content creation\n    - Game asset generation\n    - Architectural visualization",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Hunyuan3dV2MultiView",
      "properties": [
        {
          "name": "front_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Front Image Url",
          "description": "URL of image to use while generating the 3D model."
        },
        {
          "name": "octree_resolution",
          "type": {
            "type": "int"
          },
          "default": 256,
          "title": "Octree Resolution",
          "description": "Octree resolution for the model."
        },
        {
          "name": "back_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Back Image Url",
          "description": "URL of image to use while generating the 3D model."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for the model."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps to perform."
        },
        {
          "name": "textured_mesh",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Textured Mesh",
          "description": "If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "left_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Left Image Url",
          "description": "URL of image to use while generating the 3D model."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "front_image_url",
        "octree_resolution",
        "back_image_url",
        "guidance_scale",
        "num_inference_steps"
      ]
    },
    {
      "title": "Hunyuan 3 d V2 Multi View Turbo",
      "description": "Generate 3D models from your images using Hunyuan 3D. A native 3D generative model enabling versatile and high-quality 3D asset creation.\n    3d, generation, image-to-3d, modeling, fast\n\n    Use cases:\n    - 3D model generation from photos\n    - Product 3D visualization\n    - AR/VR content creation\n    - Game asset generation\n    - Architectural visualization",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Hunyuan3dV2MultiViewTurbo",
      "properties": [
        {
          "name": "front_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Front Image Url",
          "description": "URL of image to use while generating the 3D model."
        },
        {
          "name": "octree_resolution",
          "type": {
            "type": "int"
          },
          "default": 256,
          "title": "Octree Resolution",
          "description": "Octree resolution for the model."
        },
        {
          "name": "back_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Back Image Url",
          "description": "URL of image to use while generating the 3D model."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for the model."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps to perform."
        },
        {
          "name": "textured_mesh",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Textured Mesh",
          "description": "If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "left_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Left Image Url",
          "description": "URL of image to use while generating the 3D model."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "front_image_url",
        "octree_resolution",
        "back_image_url",
        "guidance_scale",
        "num_inference_steps"
      ]
    },
    {
      "title": "Hunyuan 3 d V2 Turbo",
      "description": "Generate 3D models from your images using Hunyuan 3D. A native 3D generative model enabling versatile and high-quality 3D asset creation.\n    3d, generation, image-to-3d, modeling, fast\n\n    Use cases:\n    - 3D model generation from photos\n    - Product 3D visualization\n    - AR/VR content creation\n    - Game asset generation\n    - Architectural visualization",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Hunyuan3dV2Turbo",
      "properties": [
        {
          "name": "input_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image Url",
          "description": "URL of image to use while generating the 3D model."
        },
        {
          "name": "octree_resolution",
          "type": {
            "type": "int"
          },
          "default": 256,
          "title": "Octree Resolution",
          "description": "Octree resolution for the model."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for the model."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps to perform."
        },
        {
          "name": "textured_mesh",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Textured Mesh",
          "description": "If set true, textured mesh will be generated and the price charged would be 3 times that of white mesh."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "input_image_url",
        "octree_resolution",
        "guidance_scale",
        "seed",
        "num_inference_steps"
      ]
    },
    {
      "title": "Hunyuan World Image To World",
      "description": "Hunyuan World\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Hunyuan_WorldImageToWorld",
      "properties": [
        {
          "name": "classes",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Classes",
          "description": "Classes to use for the world generation."
        },
        {
          "name": "export_drc",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Export Drc",
          "description": "Whether to export DRC (Dynamic Resource Configuration)."
        },
        {
          "name": "labels_fg1",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Labels Fg1",
          "description": "Labels for the first foreground object."
        },
        {
          "name": "labels_fg2",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Labels Fg2",
          "description": "Labels for the second foreground object."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to convert to a world."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "classes",
        "export_drc",
        "labels_fg1",
        "labels_fg2",
        "image_url"
      ]
    },
    {
      "title": "Hyper 3D Rodin V2",
      "description": "Hyper3d\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Hyper3DRodinV2",
      "properties": [
        {
          "name": "quality_mesh_option",
          "type": {
            "type": "enum",
            "values": [
              "4K Quad",
              "8K Quad",
              "18K Quad",
              "50K Quad",
              "2K Triangle",
              "20K Triangle",
              "150K Triangle",
              "500K Triangle"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.QualityMeshOption"
          },
          "default": "500K Triangle",
          "title": "Quality Mesh Option",
          "description": "Combined quality and mesh type selection. Quad = smooth surfaces, Triangle = detailed geometry. These corresponds to `mesh_mode` (if the option contains 'Triangle', mesh_mode is 'Raw', otherwise 'Quad') and `quality_override` (the numeric part of the option) parameters in Hyper3D API."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "A textual prompt to guide model generation. Optional for Image-to-3D mode - if empty, AI will generate a prompt based on your images."
        },
        {
          "name": "preview_render",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preview Render",
          "description": "Generate a preview render image of the 3D model along with the model files."
        },
        {
          "name": "bbox_condition",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "int"
              }
            ]
          },
          "default": [],
          "title": "Bbox Condition",
          "description": "An array that specifies the bounding box dimensions [width, height, length]."
        },
        {
          "name": "TAPose",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Tapose",
          "description": "Generate characters in T-pose or A-pose format, making them easier to rig and animate in 3D software."
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "URL of images to use while generating the 3D model. Required for Image-to-3D mode. Up to 5 images allowed."
        },
        {
          "name": "use_original_alpha",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Original Alpha",
          "description": "When enabled, preserves the transparency channel from input images during 3D generation."
        },
        {
          "name": "geometry_file_format",
          "type": {
            "type": "enum",
            "values": [
              "glb",
              "usdz",
              "fbx",
              "obj",
              "stl"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.GeometryFileFormat"
          },
          "default": "glb",
          "title": "Geometry File Format",
          "description": "Format of the geometry file. Possible values: glb, usdz, fbx, obj, stl. Default is glb."
        },
        {
          "name": "addons",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "HighPack"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.Addons"
          },
          "default": null,
          "title": "Addons",
          "description": "The HighPack option will provide 4K resolution textures instead of the default 1K, as well as models with high-poly. It will cost **triple the billable units**."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed value for randomization, ranging from 0 to 65535. Optional."
        },
        {
          "name": "material",
          "type": {
            "type": "enum",
            "values": [
              "PBR",
              "Shaded",
              "All"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.Material"
          },
          "default": "All",
          "title": "Material",
          "description": "Material type. PBR: Physically-based materials with realistic lighting. Shaded: Simple materials with baked lighting. All: Both types included."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "quality_mesh_option",
        "prompt",
        "preview_render",
        "bbox_condition",
        "TAPose"
      ]
    },
    {
      "title": "Hyper 3 d Rodin",
      "description": "Rodin by Hyper3D generates realistic and production ready 3D models from text or images.\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - 3D model generation from photos\n    - Product 3D visualization\n    - AR/VR content creation\n    - Game asset generation\n    - Architectural visualization",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Hyper3dRodin",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "A textual prompt to guide model generation. Required for Text-to-3D mode. Optional for Image-to-3D mode."
        },
        {
          "name": "condition_mode",
          "type": {
            "type": "enum",
            "values": [
              "fuse",
              "concat"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hyper3dRodin.ConditionMode"
          },
          "default": "concat",
          "title": "Condition Mode",
          "description": "For fuse mode, One or more images are required.It will generate a model by extracting and fusing features of objects from multiple images.For concat mode, need to upload multiple multi-view images of the same object and generate the model. (You can upload multi-view images in any order, regardless of the order of view.)"
        },
        {
          "name": "bbox_condition",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "int"
              }
            ]
          },
          "default": [],
          "title": "Bbox Condition",
          "description": "An array that specifies the dimensions and scaling factor of the bounding box. Typically, this array contains 3 elements, Length(X-axis), Width(Y-axis) and Height(Z-axis)."
        },
        {
          "name": "tier",
          "type": {
            "type": "enum",
            "values": [
              "Regular",
              "Sketch"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hyper3dRodin.Tier"
          },
          "default": "Regular",
          "title": "Tier",
          "description": "Tier of generation. For Rodin Sketch, set to Sketch. For Rodin Regular, set to Regular."
        },
        {
          "name": "quality",
          "type": {
            "type": "enum",
            "values": [
              "high",
              "medium",
              "low",
              "extra-low"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hyper3dRodin.Quality"
          },
          "default": "medium",
          "title": "Quality",
          "description": "Generation quality. Possible values: high, medium, low, extra-low. Default is medium."
        },
        {
          "name": "TAPose",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Tapose",
          "description": "When generating the human-like model, this parameter control the generation result to T/A Pose."
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "URL of images to use while generating the 3D model. Required for Image-to-3D mode. Optional for Text-to-3D mode."
        },
        {
          "name": "geometry_file_format",
          "type": {
            "type": "enum",
            "values": [
              "glb",
              "usdz",
              "fbx",
              "obj",
              "stl"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hyper3dRodin.GeometryFileFormat"
          },
          "default": "glb",
          "title": "Geometry File Format",
          "description": "Format of the geometry file. Possible values: glb, usdz, fbx, obj, stl. Default is glb."
        },
        {
          "name": "use_hyper",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Hyper",
          "description": "Whether to export the model using hyper mode. Default is false."
        },
        {
          "name": "addons",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "HighPack"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hyper3dRodin.Addons"
          },
          "default": null,
          "title": "Addons",
          "description": "Generation add-on features. Default is []. Possible values are HighPack. The HighPack option will provide 4K resolution textures instead of the default 1K, as well as models with high-poly. It will cost triple the billable units."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed value for randomization, ranging from 0 to 65535. Optional."
        },
        {
          "name": "material",
          "type": {
            "type": "enum",
            "values": [
              "PBR",
              "Shaded"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Hyper3dRodin.Material"
          },
          "default": "PBR",
          "title": "Material",
          "description": "Material type. Possible values: PBR, Shaded. Default is PBR."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "condition_mode",
        "bbox_condition",
        "tier",
        "quality"
      ]
    },
    {
      "title": "Meshy V5 Multi Image To 3D",
      "description": "Meshy 5 Multi\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.MeshyV5MultiImageTo3D",
      "properties": [
        {
          "name": "enable_pbr",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Pbr",
          "description": "Generate PBR Maps (metallic, roughness, normal) in addition to base color. Requires should_texture to be true."
        },
        {
          "name": "should_texture",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Should Texture",
          "description": "Whether to generate textures. False provides mesh without textures for 5 credits, True adds texture generation for additional 10 credits."
        },
        {
          "name": "target_polycount",
          "type": {
            "type": "int"
          },
          "default": 30000,
          "title": "Target Polycount",
          "description": "Target number of polygons in the generated model"
        },
        {
          "name": "is_a_t_pose",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Is A T Pose",
          "description": "Whether to generate the model in an A/T pose"
        },
        {
          "name": "texture_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Texture Image Url",
          "description": "2D image to guide the texturing process. Requires should_texture to be true."
        },
        {
          "name": "topology",
          "type": {
            "type": "enum",
            "values": [
              "quad",
              "triangle"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.MeshyV5MultiImageTo3D.Topology"
          },
          "default": "triangle",
          "title": "Topology",
          "description": "Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "symmetry_mode",
          "type": {
            "type": "enum",
            "values": [
              "off",
              "auto",
              "on"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.MeshyV5MultiImageTo3D.SymmetryMode"
          },
          "default": "auto",
          "title": "Symmetry Mode",
          "description": "Controls symmetry behavior during model generation."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "1 to 4 images for 3D model creation. All images should depict the same object from different angles. Supports .jpg, .jpeg, .png formats, and AVIF/HEIF which will be automatically converted. If more than 4 images are provided, only the first 4 will be used."
        },
        {
          "name": "texture_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Texture Prompt",
          "description": "Text prompt to guide the texturing process. Requires should_texture to be true."
        },
        {
          "name": "should_remesh",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Should Remesh",
          "description": "Whether to enable the remesh phase. When false, returns triangular mesh ignoring topology and target_polycount."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "enable_pbr",
        "should_texture",
        "target_polycount",
        "is_a_t_pose",
        "texture_image_url"
      ]
    },
    {
      "title": "Meshy V6 Preview Image To 3D",
      "description": "Meshy 6 Preview\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.MeshyV6PreviewImageTo3D",
      "properties": [
        {
          "name": "enable_pbr",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Pbr",
          "description": "Generate PBR Maps (metallic, roughness, normal) in addition to base color"
        },
        {
          "name": "is_a_t_pose",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Is A T Pose",
          "description": "Whether to generate the model in an A/T pose"
        },
        {
          "name": "target_polycount",
          "type": {
            "type": "int"
          },
          "default": 30000,
          "title": "Target Polycount",
          "description": "Target number of polygons in the generated model"
        },
        {
          "name": "should_texture",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Should Texture",
          "description": "Whether to generate textures"
        },
        {
          "name": "texture_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Texture Image Url",
          "description": "2D image to guide the texturing process"
        },
        {
          "name": "topology",
          "type": {
            "type": "enum",
            "values": [
              "quad",
              "triangle"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.MeshyV6PreviewImageTo3D.Topology"
          },
          "default": "triangle",
          "title": "Topology",
          "description": "Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL or base64 data URI for 3D model creation. Supports .jpg, .jpeg, and .png formats. Also supports AVIF and HEIF formats which will be automatically converted."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "symmetry_mode",
          "type": {
            "type": "enum",
            "values": [
              "off",
              "auto",
              "on"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.MeshyV6PreviewImageTo3D.SymmetryMode"
          },
          "default": "auto",
          "title": "Symmetry Mode",
          "description": "Controls symmetry behavior during model generation. Off disables symmetry, Auto determines it automatically, On enforces symmetry."
        },
        {
          "name": "texture_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Texture Prompt",
          "description": "Text prompt to guide the texturing process"
        },
        {
          "name": "should_remesh",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Should Remesh",
          "description": "Whether to enable the remesh phase"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "enable_pbr",
        "is_a_t_pose",
        "target_polycount",
        "should_texture",
        "texture_image_url"
      ]
    },
    {
      "title": "Omnipart",
      "description": "Omnipart\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Omnipart",
      "properties": [
        {
          "name": "input_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image Url",
          "description": "URL of image to use while generating the 3D model."
        },
        {
          "name": "parts",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Parts",
          "description": "Specify which segments to merge (e.g., '0,1;3,4' merges segments 0&1 together and 3&4 together)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 765464,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "minimum_segment_size",
          "type": {
            "type": "int"
          },
          "default": 2000,
          "title": "Minimum Segment Size",
          "description": "Minimum segment size (pixels) for the model."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for the model."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "input_image_url",
        "parts",
        "seed",
        "minimum_segment_size",
        "guidance_scale"
      ]
    },
    {
      "title": "Pshuman",
      "description": "Pshuman\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Pshuman",
      "properties": [
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "Guidance scale for the diffusion process. Controls how much the output adheres to the generated views."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducibility. If None, a random seed will be used."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "A direct URL to the input image of a person."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "guidance_scale",
        "seed",
        "image_url"
      ]
    },
    {
      "title": "Sam 33D Body",
      "description": "Sam 3\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Sam33DBody",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image containing humans"
        },
        {
          "name": "include_3d_keypoints",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Include 3D Keypoints",
          "description": "Include 3D keypoint markers (spheres) in the GLB mesh for visualization"
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "Optional URL of a binary mask image (white=person, black=background). When provided, skips auto human detection and uses this mask instead. Bbox is auto-computed from the mask."
        },
        {
          "name": "export_meshes",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Export Meshes",
          "description": "Export individual mesh files (.ply) per person"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image_url",
        "include_3d_keypoints",
        "mask_url",
        "export_meshes"
      ]
    },
    {
      "title": "Sam 33D Objects",
      "description": "Sam 3\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Sam33DObjects",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "car",
          "title": "Prompt",
          "description": "Text prompt for auto-segmentation when no masks provided (e.g., 'chair', 'lamp')"
        },
        {
          "name": "export_textured_glb",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Export Textured Glb",
          "description": "If True, exports GLB with baked texture and UVs instead of vertex colors."
        },
        {
          "name": "detection_threshold",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Detection Threshold",
          "description": "Detection confidence threshold (0.1-1.0). Lower = more detections but less precise. If not set, uses the model's default."
        },
        {
          "name": "pointmap_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Pointmap Url",
          "description": "Optional URL to external pointmap/depth data (NPY or NPZ format) for improved 3D reconstruction depth estimation"
        },
        {
          "name": "box_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Box Prompts",
          "description": "Box prompts for auto-segmentation when no masks provided. Multiple boxes supported - each produces a separate object mask for 3D reconstruction."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to reconstruct in 3D"
        },
        {
          "name": "mask_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Mask Urls",
          "description": "Optional list of mask URLs (one per object). If not provided, use prompt/point_prompts/box_prompts to auto-segment, or entire image will be used."
        },
        {
          "name": "point_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Point Prompts",
          "description": "Point prompts for auto-segmentation when no masks provided"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "export_textured_glb",
        "detection_threshold",
        "pointmap_url",
        "box_prompts"
      ]
    },
    {
      "title": "Tripo 3 d Tripo V25 Image To 3 d",
      "description": "State of the art Image to 3D Object generation. Generate 3D model from a single image!\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - 3D model generation from photos\n    - Product 3D visualization\n    - AR/VR content creation\n    - Game asset generation\n    - Architectural visualization",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Tripo3dTripoV25ImageTo3d",
      "properties": [
        {
          "name": "face_limit",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Face Limit",
          "description": "Limits the number of faces on the output model. If this option is not set, the face limit will be adaptively determined."
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "person:person2cartoon",
              "object:clay",
              "object:steampunk",
              "animal:venom",
              "object:barbie",
              "object:christmas",
              "gold",
              "ancient_bronze"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Tripo3dTripoV25ImageTo3d.Style"
          },
          "default": null,
          "title": "Style",
          "description": "[DEPRECATED] Defines the artistic style or transformation to be applied to the 3D model, altering its appearance according to preset options (extra $0.05 per generation). Omit this option to keep the original style and apperance."
        },
        {
          "name": "pbr",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Pbr",
          "description": "A boolean option to enable pbr. The default value is True, set False to get a model without pbr. If this option is set to True, texture will be ignored and used as True."
        },
        {
          "name": "texture_alignment",
          "type": {
            "type": "enum",
            "values": [
              "original_image",
              "geometry"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Tripo3dTripoV25ImageTo3d.TextureAlignment"
          },
          "default": "original_image",
          "title": "Texture Alignment",
          "description": "Determines the prioritization of texture alignment in the 3D model. The default value is original_image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use for model generation."
        },
        {
          "name": "texture",
          "type": {
            "type": "enum",
            "values": [
              "no",
              "standard",
              "HD"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Tripo3dTripoV25ImageTo3d.Texture"
          },
          "default": "standard",
          "title": "Texture",
          "description": "An option to enable texturing. Default is 'standard', set 'no' to get a model without any textures, and set 'HD' to get a model with hd quality textures."
        },
        {
          "name": "auto_size",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Size",
          "description": "Automatically scale the model to real-world dimensions, with the unit in meters. The default value is False."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "This is the random seed for model generation. The seed controls the geometry generation process, ensuring identical models when the same seed is used. This parameter is an integer and is randomly chosen if not set."
        },
        {
          "name": "quad",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Quad",
          "description": "Set True to enable quad mesh output (extra $0.05 per generation). If quad=True and face_limit is not set, the default face_limit will be 10000. Note: Enabling this option will force the output to be an FBX model."
        },
        {
          "name": "orientation",
          "type": {
            "type": "enum",
            "values": [
              "default",
              "align_image"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Tripo3dTripoV25ImageTo3d.Orientation"
          },
          "default": "default",
          "title": "Orientation",
          "description": "Set orientation=align_image to automatically rotate the model to align the original image. The default value is default."
        },
        {
          "name": "texture_seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Texture Seed",
          "description": "This is the random seed for texture generation. Using the same seed will produce identical textures. This parameter is an integer and is randomly chosen if not set. If you want a model with different textures, please use same seed and different texture_seed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "face_limit",
        "style",
        "pbr",
        "texture_alignment",
        "image_url"
      ]
    },
    {
      "title": "Tripo 3 d Tripo V25 Multiview To 3 d",
      "description": "State of the art Multiview to 3D Object generation. Generate 3D models from multiple images!\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - 3D model generation from photos\n    - Product 3D visualization\n    - AR/VR content creation\n    - Game asset generation\n    - Architectural visualization",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Tripo3dTripoV25MultiviewTo3d",
      "properties": [
        {
          "name": "face_limit",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Face Limit",
          "description": "Limits the number of faces on the output model. If this option is not set, the face limit will be adaptively determined."
        },
        {
          "name": "right_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Right Image Url",
          "description": "Right view image of the object."
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "person:person2cartoon",
              "object:clay",
              "object:steampunk",
              "animal:venom",
              "object:barbie",
              "object:christmas",
              "gold",
              "ancient_bronze"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Tripo3dTripoV25MultiviewTo3d.Style"
          },
          "default": null,
          "title": "Style",
          "description": "[DEPRECATED] Defines the artistic style or transformation to be applied to the 3D model, altering its appearance according to preset options (extra $0.05 per generation). Omit this option to keep the original style and apperance."
        },
        {
          "name": "quad",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Quad",
          "description": "Set True to enable quad mesh output (extra $0.05 per generation). If quad=True and face_limit is not set, the default face_limit will be 10000. Note: Enabling this option will force the output to be an FBX model."
        },
        {
          "name": "front_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Front Image Url",
          "description": "Front view image of the object."
        },
        {
          "name": "texture_seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Texture Seed",
          "description": "This is the random seed for texture generation. Using the same seed will produce identical textures. This parameter is an integer and is randomly chosen if not set. If you want a model with different textures, please use same seed and different texture_seed."
        },
        {
          "name": "back_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Back Image Url",
          "description": "Back view image of the object."
        },
        {
          "name": "pbr",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Pbr",
          "description": "A boolean option to enable pbr. The default value is True, set False to get a model without pbr. If this option is set to True, texture will be ignored and used as True."
        },
        {
          "name": "texture_alignment",
          "type": {
            "type": "enum",
            "values": [
              "original_image",
              "geometry"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Tripo3dTripoV25MultiviewTo3d.TextureAlignment"
          },
          "default": "original_image",
          "title": "Texture Alignment",
          "description": "Determines the prioritization of texture alignment in the 3D model. The default value is original_image."
        },
        {
          "name": "texture",
          "type": {
            "type": "enum",
            "values": [
              "no",
              "standard",
              "HD"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Tripo3dTripoV25MultiviewTo3d.Texture"
          },
          "default": "standard",
          "title": "Texture",
          "description": "An option to enable texturing. Default is 'standard', set 'no' to get a model without any textures, and set 'HD' to get a model with hd quality textures."
        },
        {
          "name": "auto_size",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Size",
          "description": "Automatically scale the model to real-world dimensions, with the unit in meters. The default value is False."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "This is the random seed for model generation. The seed controls the geometry generation process, ensuring identical models when the same seed is used. This parameter is an integer and is randomly chosen if not set."
        },
        {
          "name": "orientation",
          "type": {
            "type": "enum",
            "values": [
              "default",
              "align_image"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Tripo3dTripoV25MultiviewTo3d.Orientation"
          },
          "default": "default",
          "title": "Orientation",
          "description": "Set orientation=align_image to automatically rotate the model to align the original image. The default value is default."
        },
        {
          "name": "left_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Left Image Url",
          "description": "Left view image of the object."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "face_limit",
        "right_image_url",
        "style",
        "quad",
        "front_image_url"
      ]
    },
    {
      "title": "Triposr",
      "description": "State of the art Image to 3D Object generation\n    3d, generation, image-to-3d, modeling\n\n    Use cases:\n    - 3D model generation from photos\n    - Product 3D visualization\n    - AR/VR content creation\n    - Game asset generation\n    - Architectural visualization",
      "namespace": "fal.image_to_3d",
      "node_type": "fal.image_to_3d.Triposr",
      "properties": [
        {
          "name": "mc_resolution",
          "type": {
            "type": "int"
          },
          "default": 256,
          "title": "Mc Resolution",
          "description": "Resolution of the marching cubes. Above 512 is not recommended."
        },
        {
          "name": "do_remove_background",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Do Remove Background",
          "description": "Whether to remove the background from the input image."
        },
        {
          "name": "foreground_ratio",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Foreground Ratio",
          "description": "Ratio of the foreground image to the original image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "glb",
              "obj"
            ],
            "type_name": "nodetool.nodes.fal.image_to_3d.Triposr.OutputFormat"
          },
          "default": "glb",
          "title": "Output Format",
          "description": "Output format for the 3D model."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Path for the image file to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "mc_resolution",
        "do_remove_background",
        "foreground_ratio",
        "output_format",
        "image_url"
      ]
    },
    {
      "title": "Nemotron Asr",
      "description": "Use the fast speed and pin point accuracy of nemotron to transcribe your texts.\n    speech, recognition, transcription, audio-analysis\n\n    Use cases:\n    - Speech recognition\n    - Audio transcription\n    - Speaker diarization\n    - Voice activity detection\n    - Meeting transcription",
      "namespace": "fal.audio_to_text",
      "node_type": "fal.audio_to_text.NemotronAsr",
      "properties": [
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "low",
              "medium",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_text.NemotronAsr.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "Controls the speed/accuracy trade-off. 'none' = best accuracy (1.12s chunks, ~7.16% WER), 'low' = balanced (0.56s chunks, ~7.22% WER), 'medium' = faster (0.16s chunks, ~7.84% WER), 'high' = fastest (0.08s chunks, ~8.53% WER)."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "acceleration",
        "audio_url"
      ]
    },
    {
      "title": "Nemotron Asr Stream",
      "description": "Use the fast speed and pin point accuracy of nemotron to transcribe your texts.\n    speech, recognition, transcription, audio-analysis\n\n    Use cases:\n    - Speech recognition\n    - Audio transcription\n    - Speaker diarization\n    - Voice activity detection\n    - Meeting transcription",
      "namespace": "fal.audio_to_text",
      "node_type": "fal.audio_to_text.NemotronAsrStream",
      "properties": [
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "low",
              "medium",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_text.NemotronAsrStream.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "Controls the speed/accuracy trade-off. 'none' = best accuracy (1.12s chunks, ~7.16% WER), 'low' = balanced (0.56s chunks, ~7.22% WER), 'medium' = faster (0.16s chunks, ~7.84% WER), 'high' = fastest (0.08s chunks, ~8.53% WER)."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "acceleration",
        "audio_url"
      ]
    },
    {
      "title": "Silero Vad",
      "description": "Detect speech presence and timestamps with accuracy and speed using the ultra-lightweight Silero VAD model\n    speech, recognition, transcription, audio-analysis\n\n    Use cases:\n    - Speech recognition\n    - Audio transcription\n    - Speaker diarization\n    - Voice activity detection\n    - Meeting transcription",
      "namespace": "fal.audio_to_text",
      "node_type": "fal.audio_to_text.SileroVad",
      "properties": [
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio to get speech timestamps from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio_url"
      ]
    },
    {
      "title": "Hunyuan Part",
      "description": "Use the capabilities of hunyuan part to generate point clouds from your 3D files.\n    3d, editing, transformation, modeling\n\n    Use cases:\n    - 3D model editing and refinement\n    - Mesh optimization\n    - Texture application\n    - 3D format conversion\n    - Model retopology",
      "namespace": "fal.3d_to_3d",
      "node_type": "fal.3d_to_3d.HunyuanPart",
      "properties": [
        {
          "name": "point_prompt_x",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Point Prompt X",
          "description": "X coordinate of the point prompt for segmentation (normalized space -1 to 1)."
        },
        {
          "name": "point_prompt_z",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Point Prompt Z",
          "description": "Z coordinate of the point prompt for segmentation (normalized space -1 to 1)."
        },
        {
          "name": "use_normal",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Normal",
          "description": "Whether to use normal information for segmentation."
        },
        {
          "name": "noise_std",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Noise Std",
          "description": "Standard deviation of noise to add to sampled points."
        },
        {
          "name": "point_num",
          "type": {
            "type": "int"
          },
          "default": 100000,
          "title": "Point Num",
          "description": "Number of points to sample from the mesh."
        },
        {
          "name": "model_file_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model File Url",
          "description": "URL of the 3D model file (.glb or .obj) to process for segmentation."
        },
        {
          "name": "point_prompt_y",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Point Prompt Y",
          "description": "Y coordinate of the point prompt for segmentation (normalized space -1 to 1)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and input will produce the same segmentation results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "point_prompt_x",
        "point_prompt_z",
        "use_normal",
        "noise_std",
        "point_num"
      ]
    },
    {
      "title": "Meshy V5 Remesh",
      "description": "Meshy-5 remesh allows you to remesh and export existing 3D models into various formats\n    3d, editing, transformation, modeling\n\n    Use cases:\n    - 3D model editing and refinement\n    - Mesh optimization\n    - Texture application\n    - 3D format conversion\n    - Model retopology",
      "namespace": "fal.3d_to_3d",
      "node_type": "fal.3d_to_3d.MeshyV5Remesh",
      "properties": [
        {
          "name": "resize_height",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Resize Height",
          "description": "Resize the model to a certain height measured in meters. Set to 0 for no resizing."
        },
        {
          "name": "topology",
          "type": {
            "type": "enum",
            "values": [
              "quad",
              "triangle"
            ],
            "type_name": "nodetool.nodes.fal.3d_to_3d.MeshyV5Remesh.Topology"
          },
          "default": "triangle",
          "title": "Topology",
          "description": "Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry."
        },
        {
          "name": "target_polycount",
          "type": {
            "type": "int"
          },
          "default": 30000,
          "title": "Target Polycount",
          "description": "Target number of polygons in the generated model. Actual count may vary based on geometry complexity."
        },
        {
          "name": "model_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model Url",
          "description": "URL or base64 data URI of a 3D model to remesh. Supports .glb, .gltf, .obj, .fbx, .stl formats. Can be a publicly accessible URL or data URI with MIME type application/octet-stream."
        },
        {
          "name": "origin_at",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "bottom",
              "center"
            ],
            "type_name": "nodetool.nodes.fal.3d_to_3d.MeshyV5Remesh.OriginAt"
          },
          "default": null,
          "title": "Origin At",
          "description": "Position of the origin. None means no effect."
        },
        {
          "name": "target_formats",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Target Formats",
          "description": "List of target formats for the remeshed model."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "resize_height",
        "topology",
        "target_polycount",
        "model_url",
        "origin_at"
      ]
    },
    {
      "title": "Meshy V5 Retexture",
      "description": "Meshy-5 retexture applies new, high-quality textures to existing 3D models using either text prompts or reference images. It supports PBR material generation for realistic, production-ready results.\n    3d, editing, transformation, modeling\n\n    Use cases:\n    - 3D model editing and refinement\n    - Mesh optimization\n    - Texture application\n    - 3D format conversion\n    - Model retopology",
      "namespace": "fal.3d_to_3d",
      "node_type": "fal.3d_to_3d.MeshyV5Retexture",
      "properties": [
        {
          "name": "enable_pbr",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Pbr",
          "description": "Generate PBR Maps (metallic, roughness, normal) in addition to base color."
        },
        {
          "name": "text_style_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Style Prompt",
          "description": "Describe your desired texture style using text. Maximum 600 characters. Required if image_style_url is not provided."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "enable_original_uv",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Original Uv",
          "description": "Use the original UV mapping of the model instead of generating new UVs. If the model has no original UV, output quality may be reduced."
        },
        {
          "name": "model_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model Url",
          "description": "URL or base64 data URI of a 3D model to texture. Supports .glb, .gltf, .obj, .fbx, .stl formats. Can be a publicly accessible URL or data URI with MIME type application/octet-stream."
        },
        {
          "name": "image_style_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Style Url",
          "description": "2D image to guide the texturing process. Supports .jpg, .jpeg, and .png formats. Required if text_style_prompt is not provided. If both are provided, image_style_url takes precedence."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "enable_pbr",
        "text_style_prompt",
        "enable_safety_checker",
        "enable_original_uv",
        "model_url"
      ]
    },
    {
      "title": "Sam 33D Align",
      "description": "Sam 3\n    3d_to_3d\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.3d_to_3d",
      "node_type": "fal.3d_to_3d.Sam33DAlign",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the original image used for MoGe depth estimation"
        },
        {
          "name": "body_mesh_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Body Mesh Url",
          "description": "URL of the SAM-3D Body mesh file (.ply or .glb) to align"
        },
        {
          "name": "object_mesh_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Object Mesh Url",
          "description": "Optional URL of SAM-3D Object mesh (.glb) to create combined scene"
        },
        {
          "name": "focal_length",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Focal Length",
          "description": "Focal length from SAM-3D Body metadata. If not provided, estimated from MoGe."
        },
        {
          "name": "body_mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Body Mask Url",
          "description": "URL of the human mask image. If not provided, uses full image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image_url",
        "body_mesh_url",
        "object_mesh_url",
        "focal_length",
        "body_mask_url"
      ]
    },
    {
      "title": "Ultrashape",
      "description": "Ultrashape\n    3d_to_3d\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.3d_to_3d",
      "node_type": "fal.3d_to_3d.Ultrashape",
      "properties": [
        {
          "name": "octree_resolution",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Octree Resolution",
          "description": "Marching cubes resolution."
        },
        {
          "name": "remove_background",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Remove Background",
          "description": "Remove image background."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Diffusion steps."
        },
        {
          "name": "model_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model Url",
          "description": "URL of the coarse mesh (.glb or .obj) to refine."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Random seed."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the reference image for mesh refinement."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "octree_resolution",
        "remove_background",
        "num_inference_steps",
        "model_url",
        "seed"
      ]
    },
    {
      "title": "Ai Baby And Aging Generator Multi",
      "description": "AI Baby and Aging Generator Multi shows age progression or regression for multiple people in one image.\n    image, aging, age-progression, multi-face\n\n    Use cases:\n    - Show age progression for multiple people\n    - Generate family aging visualizations\n    - Create multi-person aging results\n    - Produce group age transformations\n    - Visualize multiple people at different ages",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.AiBabyAndAgingGeneratorMulti",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "a newborn baby, well dressed",
          "title": "Prompt",
          "description": "Text prompt to guide the image generation"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image"
        },
        {
          "name": "father_weight",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Father Weight",
          "description": "Weight of the father's influence in multi mode generation"
        },
        {
          "name": "mother_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Mother Image Urls",
          "description": "List of mother images for multi mode"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiBabyAndAgingGeneratorMulti.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image. Choose from: 'jpeg' or 'png'."
        },
        {
          "name": "age_group",
          "type": {
            "type": "enum",
            "values": [
              "baby",
              "toddler",
              "preschool",
              "gradeschooler",
              "teen",
              "adult",
              "mid",
              "senior"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiBabyAndAgingGeneratorMulti.AgeGroup"
          },
          "default": "",
          "title": "Age Group",
          "description": "Age group for the generated image. Choose from: 'baby' (0-12 months), 'toddler' (1-3 years), 'preschool' (3-5 years), 'gradeschooler' (6-12 years), 'teen' (13-19 years), 'adult' (20-40 years), 'mid' (40-60 years), 'senior' (60+ years)."
        },
        {
          "name": "gender",
          "type": {
            "type": "enum",
            "values": [
              "male",
              "female"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiBabyAndAgingGeneratorMulti.Gender"
          },
          "default": "",
          "title": "Gender",
          "description": "Gender for the generated image. Choose from: 'male' or 'female'."
        },
        {
          "name": "father_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Father Image Urls",
          "description": "List of father images for multi mode"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed will be used"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Ai Baby And Aging Generator Single",
      "description": "AI Baby and Aging Generator Single shows age progression or regression for a single person.\n    image, aging, age-progression, face-manipulation\n\n    Use cases:\n    - Show age progression of person\n    - Generate younger or older versions\n    - Create aging visualizations\n    - Produce age transformation results\n    - Visualize person at different ages",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.AiBabyAndAgingGeneratorSingle",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "a newborn baby, well dressed",
          "title": "Prompt",
          "description": "Text prompt to guide the image generation"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image"
        },
        {
          "name": "id_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Id Image Urls",
          "description": "List of ID images for single mode (or general reference images)"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiBabyAndAgingGeneratorSingle.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image. Choose from: 'jpeg' or 'png'."
        },
        {
          "name": "age_group",
          "type": {
            "type": "enum",
            "values": [
              "baby",
              "toddler",
              "preschool",
              "gradeschooler",
              "teen",
              "adult",
              "mid",
              "senior"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiBabyAndAgingGeneratorSingle.AgeGroup"
          },
          "default": "",
          "title": "Age Group",
          "description": "Age group for the generated image. Choose from: 'baby' (0-12 months), 'toddler' (1-3 years), 'preschool' (3-5 years), 'gradeschooler' (6-12 years), 'teen' (13-19 years), 'adult' (20-40 years), 'mid' (40-60 years), 'senior' (60+ years)."
        },
        {
          "name": "gender",
          "type": {
            "type": "enum",
            "values": [
              "male",
              "female"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiBabyAndAgingGeneratorSingle.Gender"
          },
          "default": "",
          "title": "Gender",
          "description": "Gender for the generated image. Choose from: 'male' or 'female'."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed will be used"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Ai Face Swap Image",
      "description": "AI Face Swap replaces faces in images with source faces while maintaining natural appearance.\n    image, face-swap, ai, face-manipulation\n\n    Use cases:\n    - Swap faces between images\n    - Replace faces in photos\n    - Create face-swapped variations\n    - Generate face replacement results\n    - Produce face-substituted images",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.AiFaceSwapImage",
      "properties": [
        {
          "name": "enable_occlusion_prevention",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Occlusion Prevention",
          "description": "Enable occlusion prevention for handling faces covered by hands/objects. Warning: Enabling this runs an occlusion-aware model which costs 2x more."
        },
        {
          "name": "source_face_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Source Face Url",
          "description": "Source face image. Allowed items: bmp, jpeg, png, tiff, webp"
        },
        {
          "name": "target_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Target Image Url",
          "description": "Target image URL. Allowed items: bmp, jpeg, png, tiff, webp"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Ai Home Edit",
      "description": "AI Home Edit modifies interior spaces with renovations, furniture changes, and design adjustments.\n    image, interior-design, editing, home, renovation\n\n    Use cases:\n    - Edit interior spaces\n    - Modify room furniture and decor\n    - Create renovation visualizations\n    - Generate design modification options\n    - Produce home editing results",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.AiHomeEdit",
      "properties": [
        {
          "name": "input_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image Url",
          "description": "URL of the image to do architectural editing"
        },
        {
          "name": "editing_type",
          "type": {
            "type": "enum",
            "values": [
              "structural editing",
              "virtual staging",
              "both"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeEdit.EditingType"
          },
          "default": "",
          "title": "Editing Type",
          "description": "Type of editing. Structural editing only edits structural elements such as windows, walls etc. Virtual staging edits your furniture. Both do full editing including structural and furniture"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "minimalistic-interior",
              "farmhouse-interior",
              "luxury-interior",
              "modern-interior",
              "zen-interior",
              "mid century-interior",
              "airbnb-interior",
              "cozy-interior",
              "rustic-interior",
              "christmas-interior",
              "bohemian-interior",
              "tropical-interior",
              "industrial-interior",
              "japanese-interior",
              "vintage-interior",
              "loft-interior",
              "halloween-interior",
              "soho-interior",
              "baroque-interior",
              "kids room-interior",
              "girls room-interior",
              "boys room-interior",
              "scandinavian-interior",
              "french country-interior",
              "mediterranean-interior",
              "cyberpunk-interior",
              "hot pink-interior",
              "biophilic-interior",
              "ancient egypt-interior",
              "pixel-interior",
              "art deco-interior",
              "modern-exterior",
              "minimalistic-exterior",
              "farmhouse-exterior",
              "cozy-exterior",
              "luxury-exterior",
              "colonial-exterior",
              "zen-exterior",
              "asian-exterior",
              "creepy-exterior",
              "airstone-exterior",
              "ancient greek-exterior",
              "art deco-exterior",
              "brutalist-exterior",
              "christmas lights-exterior",
              "contemporary-exterior",
              "cottage-exterior",
              "dutch colonial-exterior",
              "federal colonial-exterior",
              "fire-exterior",
              "french provincial-exterior",
              "full glass-exterior",
              "georgian colonial-exterior",
              "gothic-exterior",
              "greek revival-exterior",
              "ice-exterior",
              "italianate-exterior",
              "mediterranean-exterior",
              "midcentury-exterior",
              "middle eastern-exterior",
              "minecraft-exterior",
              "morocco-exterior",
              "neoclassical-exterior",
              "spanish-exterior",
              "tudor-exterior",
              "underwater-exterior",
              "winter-exterior",
              "yard lighting-exterior"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeEdit.Style"
          },
          "default": "",
          "title": "Style",
          "description": "Style for furniture and decor"
        },
        {
          "name": "additional_elements",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Additional Elements",
          "description": "Additional elements to include in the options above (e.g., plants, lighting)"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeEdit.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image. Choose from: 'jpeg' or 'png'."
        },
        {
          "name": "architecture_type",
          "type": {
            "type": "enum",
            "values": [
              "living room-interior",
              "bedroom-interior",
              "kitchen-interior",
              "dining room-interior",
              "bathroom-interior",
              "laundry room-interior",
              "home office-interior",
              "study room-interior",
              "dorm room-interior",
              "coffee shop-interior",
              "gaming room-interior",
              "restaurant-interior",
              "office-interior",
              "attic-interior",
              "toilet-interior",
              "other-interior",
              "house-exterior",
              "villa-exterior",
              "backyard-exterior",
              "courtyard-exterior",
              "ranch-exterior",
              "office-exterior",
              "retail-exterior",
              "tower-exterior",
              "apartment-exterior",
              "school-exterior",
              "museum-exterior",
              "commercial-exterior",
              "residential-exterior",
              "other-exterior"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeEdit.ArchitectureType"
          },
          "default": "",
          "title": "Architecture Type",
          "description": "Type of architecture for appropriate furniture selection"
        },
        {
          "name": "color_palette",
          "type": {
            "type": "enum",
            "values": [
              "surprise me",
              "golden beige",
              "refined blues",
              "dusky elegance",
              "emerald charm",
              "crimson luxury",
              "golden sapphire",
              "soft pastures",
              "candy sky",
              "peach meadow",
              "muted sands",
              "ocean breeze",
              "frosted pastels",
              "spring bloom",
              "gentle horizon",
              "seaside breeze",
              "azure coast",
              "golden shore",
              "mediterranean gem",
              "ocean serenity",
              "serene blush",
              "muted horizon",
              "pastel shores",
              "dusky calm",
              "woodland retreat",
              "meadow glow",
              "forest canopy",
              "riverbank calm",
              "earthy tones",
              "earthy neutrals",
              "arctic mist",
              "aqua drift",
              "blush bloom",
              "coral haze",
              "retro rust",
              "autumn glow",
              "rustic charm",
              "vintage sage",
              "faded plum",
              "electric lime",
              "violet pulse",
              "neon sorbet",
              "aqua glow",
              "fluorescent sunset",
              "lavender bloom",
              "petal fresh",
              "meadow light",
              "sunny pastures",
              "frosted mauve",
              "snowy hearth",
              "icy blues",
              "winter twilight",
              "earthy hues",
              "stone balance",
              "neutral sands",
              "slate shades"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeEdit.ColorPalette"
          },
          "default": "",
          "title": "Color Palette",
          "description": "Color palette for furniture and decor"
        },
        {
          "name": "custom_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Custom Prompt",
          "description": "Custom prompt for architectural editing, it overrides above options when used"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ai Home Style",
      "description": "AI Home Style transforms interior spaces with different design styles and aesthetics.\n    image, interior-design, style-transfer, home, decoration\n\n    Use cases:\n    - Transform interior design styles\n    - Apply different home aesthetics\n    - Create styled room variations\n    - Generate interior design options\n    - Produce home styling transformations",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.AiHomeStyle",
      "properties": [
        {
          "name": "input_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Input Image Url",
          "description": "URL of the image to do architectural styling"
        },
        {
          "name": "input_image_strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Input Image Strength",
          "description": "Strength of the input image"
        },
        {
          "name": "additional_elements",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Additional Elements",
          "description": "Additional elements to include in the options above (e.g., plants, lighting)"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeStyle.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image. Choose from: 'jpeg' or 'png'."
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "minimalistic-interior",
              "farmhouse-interior",
              "luxury-interior",
              "modern-interior",
              "zen-interior",
              "mid century-interior",
              "airbnb-interior",
              "cozy-interior",
              "rustic-interior",
              "christmas-interior",
              "bohemian-interior",
              "tropical-interior",
              "industrial-interior",
              "japanese-interior",
              "vintage-interior",
              "loft-interior",
              "halloween-interior",
              "soho-interior",
              "baroque-interior",
              "kids room-interior",
              "girls room-interior",
              "boys room-interior",
              "scandinavian-interior",
              "french country-interior",
              "mediterranean-interior",
              "cyberpunk-interior",
              "hot pink-interior",
              "biophilic-interior",
              "ancient egypt-interior",
              "pixel-interior",
              "art deco-interior",
              "modern-exterior",
              "minimalistic-exterior",
              "farmhouse-exterior",
              "cozy-exterior",
              "luxury-exterior",
              "colonial-exterior",
              "zen-exterior",
              "asian-exterior",
              "creepy-exterior",
              "airstone-exterior",
              "ancient greek-exterior",
              "art deco-exterior",
              "brutalist-exterior",
              "christmas lights-exterior",
              "contemporary-exterior",
              "cottage-exterior",
              "dutch colonial-exterior",
              "federal colonial-exterior",
              "fire-exterior",
              "french provincial-exterior",
              "full glass-exterior",
              "georgian colonial-exterior",
              "gothic-exterior",
              "greek revival-exterior",
              "ice-exterior",
              "italianate-exterior",
              "mediterranean-exterior",
              "midcentury-exterior",
              "middle eastern-exterior",
              "minecraft-exterior",
              "morocco-exterior",
              "neoclassical-exterior",
              "spanish-exterior",
              "tudor-exterior",
              "underwater-exterior",
              "winter-exterior",
              "yard lighting-exterior"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeStyle.Style"
          },
          "default": "",
          "title": "Style",
          "description": "Style for furniture and decor"
        },
        {
          "name": "architecture_type",
          "type": {
            "type": "enum",
            "values": [
              "living room-interior",
              "bedroom-interior",
              "kitchen-interior",
              "dining room-interior",
              "bathroom-interior",
              "laundry room-interior",
              "home office-interior",
              "study room-interior",
              "dorm room-interior",
              "coffee shop-interior",
              "gaming room-interior",
              "restaurant-interior",
              "office-interior",
              "attic-interior",
              "toilet-interior",
              "other-interior",
              "house-exterior",
              "villa-exterior",
              "backyard-exterior",
              "courtyard-exterior",
              "ranch-exterior",
              "office-exterior",
              "retail-exterior",
              "tower-exterior",
              "apartment-exterior",
              "school-exterior",
              "museum-exterior",
              "commercial-exterior",
              "residential-exterior",
              "other-exterior"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeStyle.ArchitectureType"
          },
          "default": "",
          "title": "Architecture Type",
          "description": "Type of architecture for appropriate furniture selection"
        },
        {
          "name": "color_palette",
          "type": {
            "type": "enum",
            "values": [
              "surprise me",
              "golden beige",
              "refined blues",
              "dusky elegance",
              "emerald charm",
              "crimson luxury",
              "golden sapphire",
              "soft pastures",
              "candy sky",
              "peach meadow",
              "muted sands",
              "ocean breeze",
              "frosted pastels",
              "spring bloom",
              "gentle horizon",
              "seaside breeze",
              "azure coast",
              "golden shore",
              "mediterranean gem",
              "ocean serenity",
              "serene blush",
              "muted horizon",
              "pastel shores",
              "dusky calm",
              "woodland retreat",
              "meadow glow",
              "forest canopy",
              "riverbank calm",
              "earthy tones",
              "earthy neutrals",
              "arctic mist",
              "aqua drift",
              "blush bloom",
              "coral haze",
              "retro rust",
              "autumn glow",
              "rustic charm",
              "vintage sage",
              "faded plum",
              "electric lime",
              "violet pulse",
              "neon sorbet",
              "aqua glow",
              "fluorescent sunset",
              "lavender bloom",
              "petal fresh",
              "meadow light",
              "sunny pastures",
              "frosted mauve",
              "snowy hearth",
              "icy blues",
              "winter twilight",
              "earthy hues",
              "stone balance",
              "neutral sands",
              "slate shades"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.AiHomeStyle.ColorPalette"
          },
          "default": "",
          "title": "Color Palette",
          "description": "Color palette for furniture and decor"
        },
        {
          "name": "style_image_url",
          "type": {
            "type": "image"
          },
          "default": "",
          "title": "Style Image Url",
          "description": "URL of the style image, optional. If given, other parameters are ignored"
        },
        {
          "name": "custom_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Custom Prompt",
          "description": "Custom prompt for architectural editing, it overrides above options when used"
        },
        {
          "name": "enhanced_rendering",
          "type": {
            "type": "str"
          },
          "default": false,
          "title": "Enhanced Rendering",
          "description": "It gives better rendering quality with more processing time, additional cost is 0.01 USD per image"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bagel Edit",
      "description": "Bagel is a 7B parameter multimodal model from Bytedance-Seed that can generate both images and text.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BagelEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image with."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation."
        },
        {
          "name": "use_thought",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Thought",
          "description": "Whether to use thought tokens for generation. If set to true, the model will \"think\" to potentially improve generation quality. Increases generation time and increases the cost by 20%."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image to edit."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ben V2 Image",
      "description": "A fast and high quality model for image background removal.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BenV2Image",
      "properties": [
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to be used for background removal"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bi Ref Net",
      "description": "BiRefNet (Bilateral Reference Network) performs high-quality background removal with precise edge detection and detail preservation.\n    image, background-removal, segmentation, birefnet, mask\n\n    Use cases:\n    - Remove backgrounds from product photos\n    - Create transparent PNGs from images\n    - Extract subjects for compositing\n    - Generate clean cutouts for design work\n    - Prepare images for background replacement",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BiRefNet",
      "properties": [
        {
          "name": "operating_resolution",
          "type": {
            "type": "enum",
            "values": [
              "1024x1024",
              "2048x2048"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BiRefNet.OperatingResolution"
          },
          "default": "1024x1024",
          "title": "Operating Resolution",
          "description": "The resolution to operate on. The higher the resolution, the more accurate the output will be for high res input images."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "webp",
              "png",
              "gif"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BiRefNet.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to remove background from"
        },
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "General Use (Light)",
              "General Use (Heavy)",
              "Portrait"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BiRefNet.Model"
          },
          "default": "General Use (Light)",
          "title": "Model",
          "description": "Model to use for background removal. The 'General Use (Light)' model is the original model used in the BiRefNet repository. The 'General Use (Heavy)' model is a slower but more accurate model. The 'Portrait' model is a model trained specifically for portrait images. The 'General Use (Light)' model is recommended for most use cases. The corresponding models are as follows: - 'General Use (Light)': BiRefNet-DIS_ep580.pth - 'General Use (Heavy)': BiRefNet-massive-epoch_240.pth - 'Portrait': BiRefNet-portrait-TR_P3M_10k-epoch_120.pth"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_mask",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Output Mask",
          "description": "Whether to output the mask used to remove the background"
        },
        {
          "name": "refine_foreground",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Refine Foreground",
          "description": "Whether to refine the foreground using the estimated mask"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Birefnet V2",
      "description": "bilateral reference framework (BiRefNet) for high-resolution dichotomous image segmentation (DIS)\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BirefnetV2",
      "properties": [
        {
          "name": "operating_resolution",
          "type": {
            "type": "enum",
            "values": [
              "1024x1024",
              "2048x2048",
              "2304x2304"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BirefnetV2.OperatingResolution"
          },
          "default": "1024x1024",
          "title": "Operating Resolution",
          "description": "The resolution to operate on. The higher the resolution, the more accurate the output will be for high res input images. The '2304x2304' option is only available for the 'General Use (Dynamic)' model."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "webp",
              "png",
              "gif"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BirefnetV2.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to remove background from"
        },
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "General Use (Light)",
              "General Use (Light 2K)",
              "General Use (Heavy)",
              "Matting",
              "Portrait",
              "General Use (Dynamic)"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BirefnetV2.Model"
          },
          "default": "General Use (Light)",
          "title": "Model",
          "description": "Model to use for background removal. The 'General Use (Light)' model is the original model used in the BiRefNet repository. The 'General Use (Light 2K)' model is the original model used in the BiRefNet repository but trained with 2K images. The 'General Use (Heavy)' model is a slower but more accurate model. The 'Matting' model is a model trained specifically for matting images. The 'Portrait' model is a model trained specifically for portrait images. The 'General Use (Dynamic)' model supports dynamic resolutions from 256x256 to 2304x2304. The 'General Use (Light)' model is recommended for most use cases. The corresponding models are as follows: - 'General Use (Light)': BiRefNet - 'General Use (Light 2K)': BiRefNet_lite-2K - 'General Use (Heavy)': BiRefNet_lite - 'Matting': BiRefNet-matting - 'Portrait': BiRefNet-portrait - 'General Use (Dynamic)': BiRefNet_dynamic"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_mask",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Output Mask",
          "description": "Whether to output the mask used to remove the background"
        },
        {
          "name": "refine_foreground",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Refine Foreground",
          "description": "Whether to refine the foreground using the estimated mask"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Background Remove",
      "description": "Bria RMBG 2.0 enables seamless removal of backgrounds from images, ideal for professional editing tasks. Trained exclusively on licensed data for safe and risk-free commercial use. Model weights for commercial use are available here: https://share-eu1.hsforms.com/2GLpEVQqJTI2Lj7AMYwgfIwf4e04\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaBackgroundRemove",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Input Image to erase from"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Background Replace",
      "description": "Bria Background Replace swaps image backgrounds with new content. Intelligently separates subjects and generates contextually appropriate backgrounds.\n    image, background, replacement, segmentation, bria\n\n    Use cases:\n    - Replace photo backgrounds with custom scenes\n    - Create product shots with various backgrounds\n    - Change image context while preserving subject\n    - Generate professional portraits with studio backgrounds\n    - Create marketing materials with branded backgrounds",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaBackgroundReplace",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Description of the new background to generate"
        },
        {
          "name": "steps_num",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Steps Num",
          "description": "Number of inference steps."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If true, returns the image directly in the response (increases latency)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 4925634,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for background replacement."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": "https://v3b.fal.media/files/b/0a8bea8c/Mztgx0NG3HPdby-4iPqwH_a_coffee_machine_standing_in_the_kitchen.png",
          "title": "Image Url",
          "description": "Reference image (file or URL)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Background Replace V2",
      "description": "Bria Background Replace allows for efficient swapping of backgrounds in images via text prompts or reference image, delivering realistic and polished results. Trained exclusively on licensed data for safe and risk-free commercial use \n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaBackgroundReplaceV2",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt you would like to use to generate images."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of Images to generate."
        },
        {
          "name": "ref_image_url",
          "type": {
            "type": "image"
          },
          "default": "",
          "title": "Ref Image Url",
          "description": "The URL of the reference image to be used for generating the new background. Use \"\" to leave empty. Either ref_image_url or bg_prompt has to be provided but not both. If both ref_image_url and ref_image_file are provided, ref_image_url will be used. Accepted formats are jpeg, jpg, png, webp."
        },
        {
          "name": "refine_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Refine Prompt",
          "description": "Whether to refine prompt"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Input Image to erase from"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "fast",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Fast",
          "description": "Whether to use the fast model"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt you would like to use to generate images."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Eraser",
      "description": "Bria Eraser removes unwanted objects from images using intelligent inpainting. Seamlessly fill removed areas with contextually appropriate content.\n    image, eraser, removal, inpainting, bria, cleanup\n\n    Use cases:\n    - Remove unwanted objects from photos\n    - Clean up image backgrounds\n    - Erase text or watermarks\n    - Delete distracting elements\n    - Create clean product shots",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaEraser",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "preserve_alpha",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preserve Alpha",
          "description": "If set to true, attempts to preserve the alpha channel of the input image."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The URL of the binary mask image that represents the area that will be cleaned."
        },
        {
          "name": "mask_type",
          "type": {
            "type": "enum",
            "values": [
              "manual",
              "automatic"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BriaEraser.MaskType"
          },
          "default": "manual",
          "title": "Mask Type",
          "description": "You can use this parameter to specify the type of the input mask from the list. 'manual' opttion should be used in cases in which the mask had been generated by a user (e.g. with a brush tool), and 'automatic' mask type should be used when mask had been generated by an algorithm like 'SAM'."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Input Image to erase from"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "mask"
      ]
    },
    {
      "title": "Bria Eraser V2",
      "description": "Bria Eraser enables precise removal of unwanted objects from images while maintaining high-quality outputs. Trained exclusively on licensed data for safe and risk-free commercial use. Access the model's source code and weights: https://bria.ai/contact-us\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaEraserV2",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "preserve_alpha",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preserve Alpha",
          "description": "If set to true, attempts to preserve the alpha channel of the input image."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The URL of the binary mask image that represents the area that will be cleaned."
        },
        {
          "name": "mask_type",
          "type": {
            "type": "enum",
            "values": [
              "manual",
              "automatic"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BriaEraserV2.MaskType"
          },
          "default": "manual",
          "title": "Mask Type",
          "description": "You can use this parameter to specify the type of the input mask from the list. 'manual' opttion should be used in cases in which the mask had been generated by a user (e.g. with a brush tool), and 'automatic' mask type should be used when mask had been generated by an algorithm like 'SAM'."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Input Image to erase from"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Expand",
      "description": "Bria Expand expands images beyond their borders in high quality. Trained exclusively on licensed data for safe and risk-free commercial use. Access the model's source code and weights: https://bria.ai/contact-us\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaExpand",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text on which you wish to base the image expansion. This parameter is optional. Bria currently supports prompts in English only, excluding special characters."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "1:1",
              "2:3",
              "3:2",
              "3:4",
              "4:3",
              "4:5",
              "5:4",
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BriaExpand.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The desired aspect ratio of the final image. Will be used over original_image_size and original_image_location if provided."
        },
        {
          "name": "original_image_location",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "int"
              }
            ]
          },
          "default": [],
          "title": "Original Image Location",
          "description": "The desired location of the original image, inside the full canvas. Provide the location of the upper left corner of the original image. The location can also be outside the canvas (the original image will be cropped). Will be ignored if aspect_ratio is provided."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the input image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "original_image_size",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "int"
              }
            ]
          },
          "default": [],
          "title": "Original Image Size",
          "description": "The desired size of the original image, inside the full canvas. Ensure that the ratio of input image foreground or main subject to the canvas area is greater than 15% to achieve optimal results. Will be ignored if aspect_ratio is provided."
        },
        {
          "name": "canvas_size",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "int"
              }
            ]
          },
          "default": [],
          "title": "Canvas Size",
          "description": "The desired size of the final image, after the expansion. should have an area of less than 5000x5000 pixels."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "You can choose whether you want your generated expension to be random or predictable. You can recreate the same result in the future by using the seed value of a result from the response. You can exclude this parameter if you are not interested in recreating your results. This parameter is optional."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt you would like to use to generate images."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit",
      "description": "Bria FIBO Edit provides general-purpose image editing with AI-powered modifications and enhancements.\n    image, editing, bria, fibo, general\n\n    Use cases:\n    - Edit images with general-purpose AI\n    - Apply various modifications to photos\n    - Create edited versions of images\n    - Transform images with flexible edits\n    - Produce AI-powered modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEdit",
      "properties": [
        {
          "name": "steps_num",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Steps Num",
          "description": "Number of inference steps."
        },
        {
          "name": "instruction",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Instruction",
          "description": "Instruction for image editing."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Reference image (file or URL)."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If true, returns the image directly in the response (increases latency)."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "str"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for text."
        },
        {
          "name": "structured_instruction",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Structured Instruction",
          "description": "The structured prompt to generate an image from."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "Mask image (file or URL). Optional"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 5555,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Add Object By Text",
      "description": "Bria FIBO Edit Add Object by Text inserts new objects into images using text descriptions.\n    image, editing, bria, fibo, object-insertion\n\n    Use cases:\n    - Add objects to images with text\n    - Insert elements using descriptions\n    - Place new items in scenes\n    - Augment images with additional objects\n    - Generate object additions",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditAddObjectByText",
      "properties": [
        {
          "name": "instruction",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Instruction",
          "description": "The full natural language command describing what to add and where."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Blend",
      "description": "Bria FIBO Edit Blend seamlessly combines multiple images or elements with natural transitions.\n    image, editing, bria, fibo, blending\n\n    Use cases:\n    - Blend multiple images together\n    - Create seamless compositions\n    - Merge elements naturally\n    - Combine images with smooth transitions\n    - Generate blended composites",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditBlend",
      "properties": [
        {
          "name": "instruction",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Instruction",
          "description": "Instruct what elements you would like to blend in your image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Colorize",
      "description": "Bria FIBO Edit Colorize adds realistic colors to grayscale or black-and-white images.\n    image, editing, bria, fibo, colorization\n\n    Use cases:\n    - Colorize black and white photos\n    - Add colors to grayscale images\n    - Restore color in old photographs\n    - Transform monochrome to color\n    - Generate colored versions of grayscale images",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditColorize",
      "properties": [
        {
          "name": "color",
          "type": {
            "type": "enum",
            "values": [
              "contemporary color",
              "vivid color",
              "black and white colors",
              "sepia vintage"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BriaFiboEditColorize.Color"
          },
          "default": "",
          "title": "Color",
          "description": "Select the color palette or aesthetic for the output image"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Bria Fibo Edit Erase By Text",
      "description": "Bria FIBO Edit Erase by Text removes objects from images using natural language descriptions.\n    image, editing, bria, fibo, object-removal\n\n    Use cases:\n    - Remove objects using text descriptions\n    - Erase unwanted elements from photos\n    - Clean up images by describing what to remove\n    - Delete specific items from scenes\n    - Remove objects with natural language",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditEraseByText",
      "properties": [
        {
          "name": "object_name",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Object Name",
          "description": "The name of the object to remove."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Relight",
      "description": "Bria FIBO Edit Relight adjusts lighting conditions in images for dramatic or natural effects.\n    image, editing, bria, fibo, relighting\n\n    Use cases:\n    - Adjust lighting in photos\n    - Change illumination conditions\n    - Create dramatic lighting effects\n    - Relight scenes for better ambiance\n    - Transform lighting in images",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditRelight",
      "properties": [
        {
          "name": "light_type",
          "type": {
            "type": "enum",
            "values": [
              "midday",
              "blue hour light",
              "low-angle sunlight",
              "sunrise light",
              "spotlight on subject",
              "overcast light",
              "soft overcast daylight lighting",
              "cloud-filtered lighting",
              "fog-diffused lighting",
              "moonlight lighting",
              "starlight nighttime",
              "soft bokeh lighting",
              "harsh studio lighting"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BriaFiboEditRelight.LightType"
          },
          "default": "",
          "title": "Light Type",
          "description": "The quality/style/time of day."
        },
        {
          "name": "light_direction",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Light Direction",
          "description": "Where the light comes from."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Replace Object By Text",
      "description": "Bria FIBO Edit Replace Object by Text replaces objects in images with new ones specified by text.\n    image, editing, bria, fibo, object-replacement\n\n    Use cases:\n    - Replace objects using text descriptions\n    - Swap elements in photos\n    - Change specific items in scenes\n    - Transform objects with text guidance\n    - Substitute objects with new ones",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditReplaceObjectByText",
      "properties": [
        {
          "name": "instruction",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Instruction",
          "description": "The full natural language command describing what to replace."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Reseason",
      "description": "Bria FIBO Edit Reseason changes the seasonal appearance of outdoor scenes in images.\n    image, editing, bria, fibo, seasonal\n\n    Use cases:\n    - Change seasons in outdoor photos\n    - Transform summer to winter scenes\n    - Modify seasonal appearance\n    - Create seasonal variations\n    - Generate different season versions",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditReseason",
      "properties": [
        {
          "name": "season",
          "type": {
            "type": "enum",
            "values": [
              "spring",
              "summer",
              "autumn",
              "winter"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BriaFiboEditReseason.Season"
          },
          "default": "",
          "title": "Season",
          "description": "The desired season."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Restore",
      "description": "Bria FIBO Edit Restore repairs and enhances damaged or degraded images with AI reconstruction.\n    image, editing, bria, fibo, restoration\n\n    Use cases:\n    - Restore damaged photographs\n    - Repair degraded images\n    - Enhance old photo quality\n    - Fix scratches and artifacts\n    - Reconstruct missing image parts",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditRestore",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Bria Fibo Edit Restyle",
      "description": "Bria FIBO Edit Restyle transforms images with artistic style transfers and visual aesthetics.\n    image, editing, bria, fibo, style-transfer\n\n    Use cases:\n    - Apply artistic styles to images\n    - Transform photos with new aesthetics\n    - Create stylized versions of images\n    - Generate artistic variations\n    - Produce style-transferred images",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditRestyle",
      "properties": [
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "3D Render",
              "Cubism",
              "Oil Painting",
              "Anime",
              "Cartoon",
              "Coloring Book",
              "Retro Ad",
              "Pop Art Halftone",
              "Vector Art",
              "Story Board",
              "Art Nouveau",
              "Cross Etching",
              "Wood Cut"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BriaFiboEditRestyle.Style"
          },
          "default": "",
          "title": "Style",
          "description": "Select the desired artistic style for the output image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Rewrite Text",
      "description": "Bria FIBO Edit Rewrite Text modifies or replaces text content within images naturally.\n    image, editing, bria, fibo, text-editing\n\n    Use cases:\n    - Change text in images\n    - Replace written content in photos\n    - Modify signs and labels\n    - Update text naturally in scenes\n    - Edit textual elements in images",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditRewriteText",
      "properties": [
        {
          "name": "new_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "New Text",
          "description": "The new text string to appear in the image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Sketch To Colored Image",
      "description": "Bria FIBO Edit Sketch to Colored Image transforms sketches and line art into full-color images.\n    image, editing, bria, fibo, sketch-to-image\n\n    Use cases:\n    - Convert sketches to colored images\n    - Transform line art to full color\n    - Generate colored versions of drawings\n    - Create realistic images from sketches\n    - Produce colored artwork from outlines",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaFiboEditSketchToColoredImage",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Genfill",
      "description": "Bria GenFill enables high-quality object addition or visual transformation. Trained exclusively on licensed data for safe and risk-free commercial use. Access the model's source code and weights: https://bria.ai/contact-us\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaGenfill",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt you would like to use to generate images."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of Images to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Input Image to erase from"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The URL of the binary mask image that represents the area that will be cleaned."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt you would like to use to generate images."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Product Shot",
      "description": "Place any product in any scenery with just a prompt or reference image while maintaining high integrity of the product. Trained exclusively on licensed data for safe and risk-free commercial use and optimized for eCommerce.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaProductShot",
      "properties": [
        {
          "name": "ref_image_url",
          "type": {
            "type": "image"
          },
          "default": "",
          "title": "Ref Image Url",
          "description": "The URL of the reference image to be used for generating the new scene or background for the product shot. Use \"\" to leave empty.Either ref_image_url or scene_description has to be provided but not both. If both ref_image_url and ref_image_file are provided, ref_image_url will be used. Accepted formats are jpeg, jpg, png, webp."
        },
        {
          "name": "manual_placement_selection",
          "type": {
            "type": "enum",
            "values": [
              "upper_left",
              "upper_right",
              "bottom_left",
              "bottom_right",
              "right_center",
              "left_center",
              "upper_center",
              "bottom_center",
              "center_vertical",
              "center_horizontal"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BriaProductShot.ManualPlacementSelection"
          },
          "default": "bottom_center",
          "title": "Manual Placement Selection",
          "description": "If you've selected placement_type=manual_placement, you should use this parameter to specify which placements/positions you would like to use from the list. You can select more than one placement in one request."
        },
        {
          "name": "num_results",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Results",
          "description": "The number of lifestyle product shots you would like to generate. You will get num_results x 10 results when placement_type=automatic and according to the number of required placements x num_results if placement_type=manual_placement."
        },
        {
          "name": "padding_values",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "int"
              }
            ]
          },
          "default": [],
          "title": "Padding Values",
          "description": "The desired padding in pixels around the product, when using placement_type=manual_padding. The order of the values is [left, right, top, bottom]. For optimal results, the total number of pixels, including padding, should be around 1,000,000. It is recommended to first use the product cutout API, get the cutout and understand the size of the result, and then define the required padding and use the cutout as an input for this API."
        },
        {
          "name": "shot_size",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "int"
              }
            ]
          },
          "default": [],
          "title": "Shot Size",
          "description": "The desired size of the final product shot. For optimal results, the total number of pixels should be around 1,000,000. This parameter is only relevant when placement_type=automatic or placement_type=manual_placement."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "placement_type",
          "type": {
            "type": "enum",
            "values": [
              "original",
              "automatic",
              "manual_placement",
              "manual_padding"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BriaProductShot.PlacementType"
          },
          "default": "manual_placement",
          "title": "Placement Type",
          "description": "This parameter allows you to control the positioning of the product in the image. Choosing 'original' will preserve the original position of the product in the image. Choosing 'automatic' will generate results with the 10 recommended positions for the product. Choosing 'manual_placement' will allow you to select predefined positions (using the parameter 'manual_placement_selection'). Selecting 'manual_padding' will allow you to control the position and size of the image by defining the desired padding in pixels around the product."
        },
        {
          "name": "original_quality",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Original Quality",
          "description": "This flag is only relevant when placement_type=original. If true, the output image retains the original input image's size; otherwise, the image is scaled to 1 megapixel (1MP) while preserving its aspect ratio."
        },
        {
          "name": "fast",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Fast",
          "description": "Whether to use the fast model"
        },
        {
          "name": "optimize_description",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Optimize Description",
          "description": "Whether to optimize the scene description"
        },
        {
          "name": "scene_description",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Scene Description",
          "description": "Text description of the new scene or background for the provided product shot. Bria currently supports prompts in English only, excluding special characters."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the product shot to be placed in a lifestyle shot. If both image_url and image_file are provided, image_url will be used. Accepted formats are jpeg, jpg, png, webp. Maximum file size 12MB."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Reimagine",
      "description": "Bria\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaReimagine",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt you would like to use to generate images."
        },
        {
          "name": "num_results",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Results",
          "description": "How many images you would like to generate. When using any Guidance Method, Value is set to 1."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "structure_ref_influence",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Structure Ref Influence",
          "description": "The influence of the structure reference on the generated image."
        },
        {
          "name": "fast",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Fast",
          "description": "Whether to use the fast model"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of iterations the model goes through to refine the generated image. This parameter is optional."
        },
        {
          "name": "structure_image_url",
          "type": {
            "type": "image"
          },
          "default": "",
          "title": "Structure Image Url",
          "description": "The URL of the structure reference image. Use \"\" to leave empty. Accepted formats are jpeg, jpg, png, webp."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bria Reimagine 32",
      "description": "Reimagine\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BriaReimagine32",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt for image generation."
        },
        {
          "name": "depth_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Depth Preprocess",
          "description": "Depth image preprocess."
        },
        {
          "name": "canny_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Canny Preprocess",
          "description": "Canny image preprocess."
        },
        {
          "name": "depth_image_url",
          "type": {
            "type": "image"
          },
          "default": "",
          "title": "Depth Image Url",
          "description": "Depth control image (file or URL)."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for text."
        },
        {
          "name": "canny_image_url",
          "type": {
            "type": "image"
          },
          "default": "",
          "title": "Canny Image Url",
          "description": "Canny edge control image (file or URL)."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Logo,Watermark,Ugly,Morbid,Extra fingers,Poorly drawn hands,Mutation,Blurry,Extra limbs,Gross proportions,Missing arms,Mutated hands,Long neck,Duplicate,Mutilated,Mutilated hands,Poorly drawn face,Deformed,Bad anatomy,Cloned face,Malformed limbs,Missing legs,Too many fingers",
          "title": "Negative Prompt",
          "description": "Negative prompt for image generation."
        },
        {
          "name": "depth_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Depth Scale",
          "description": "Depth control strength (0.0 to 1.0)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "2:3",
              "3:2",
              "3:4",
              "4:3",
              "4:5",
              "5:4",
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BriaReimagine32.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "Aspect ratio. Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If true, returns the image directly in the response (increases latency)."
        },
        {
          "name": "prompt_enhancer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Enhancer",
          "description": "Whether to improve the prompt."
        },
        {
          "name": "truncate_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Truncate Prompt",
          "description": "Whether to truncate the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 5555,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "canny_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Canny Scale",
          "description": "Canny edge control strength (0.0 to 1.0)."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bytedance Seedream V45 Edit",
      "description": "ByteDance SeeDream v4.5 Edit provides advanced image editing with cutting-edge AI technology.\n    image, editing, bytedance, seedream, v4.5\n\n    Use cases:\n    - Edit images with SeeDream v4.5\n    - Apply advanced modifications\n    - Create high-quality edits\n    - Transform images with latest tech\n    - Produce cutting-edge modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BytedanceSeedreamV45Edit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to edit the image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of separate model generations to be run with the prompt."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Width and height must be between 1920 and 4096, or total number of pixels must be between 2560*1440 and 4096*4096."
        },
        {
          "name": "max_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Max Images",
          "description": "If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`. The total number of images (image inputs + image outputs) must not exceed 15"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control the stochasticity of image generation."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of URLs of input images for editing. Presently, up to 10 image inputs are allowed. If over 10 images are sent, only the last 10 will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bytedance Seedream V4 Edit",
      "description": "Bytedance Seedream v4 Edit\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.BytedanceSeedreamV4Edit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to edit the image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of separate model generations to be run with the prompt."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. The minimum total image area is 921600 pixels. Failing this, the image size will be adjusted to by scaling it up, while maintaining the aspect ratio."
        },
        {
          "name": "max_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Max Images",
          "description": "If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`. The total number of images (image inputs + image outputs) must not exceed 15"
        },
        {
          "name": "enhance_prompt_mode",
          "type": {
            "type": "enum",
            "values": [
              "standard",
              "fast"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.BytedanceSeedreamV4Edit.EnhancePromptMode"
          },
          "default": "standard",
          "title": "Enhance Prompt Mode",
          "description": "The mode to use for enhancing prompt enhancement. Standard mode provides higher quality results but takes longer to generate. Fast mode provides average quality results but takes less time to generate."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control the stochasticity of image generation."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of URLs of input images for editing. Presently, up to 10 image inputs are allowed. If over 10 images are sent, only the last 10 will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Calligrapher",
      "description": "Calligrapher\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Calligrapher",
      "properties": [
        {
          "name": "use_context",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Context",
          "description": "Whether to prepend context reference to the input"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "How many images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "Target image size for generation"
        },
        {
          "name": "auto_mask_generation",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Mask Generation",
          "description": "Whether to automatically generate mask from detected text"
        },
        {
          "name": "reference_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Reference Image Url",
          "description": "Optional base64 reference image for style"
        },
        {
          "name": "source_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Source Image Url",
          "description": "Base64-encoded source image with drawn mask layers"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to inpaint or customize"
        },
        {
          "name": "mask_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image Url",
          "description": "Base64-encoded mask image (optional if using auto_mask_generation)"
        },
        {
          "name": "source_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Source Text",
          "description": "Source text to replace (if empty, masks all detected text)"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps (1-100)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Cfg Scale",
          "description": "Guidance or strength scale for the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Cartoonify",
      "description": "Transform images into 3D cartoon artwork using an AI model that applies cartoon stylization while preserving the original image's composition and details.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Cartoonify",
      "properties": [
        {
          "name": "use_cfg_zero",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Cfg Zero",
          "description": "Whether to use CFG zero"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to apply Pixar style to"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "Number of inference steps"
        },
        {
          "name": "scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Scale",
          "description": "Scale factor for the Pixar effect"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for image generation. Same seed with same parameters will generate same image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Cat Vton",
      "description": "Image based high quality Virtual Try-On\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.CatVton",
      "properties": [
        {
          "name": "garment_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Garment Image Url",
          "description": "Url to the garment image."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "portrait_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "human_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Human Image Url",
          "description": "Url for the human image."
        },
        {
          "name": "cloth_type",
          "type": {
            "type": "enum",
            "values": [
              "upper",
              "lower",
              "overall",
              "inner",
              "outer"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.CatVton.ClothType"
          },
          "default": "",
          "title": "Cloth Type",
          "description": "Type of the Cloth to be tried on. Options: upper: Upper body cloth lower: Lower body cloth overall: Full body cloth inner: Inner cloth, like T-shirt inside a jacket outer: Outer cloth, like a jacket over a T-shirt"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same input given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Chain Of Zoom",
      "description": "Extreme Super-Resolution via Scale Autoregression and Preference Alignment\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ChainOfZoom",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "center_y",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Center Y",
          "description": "Y coordinate of zoom center (0-1)"
        },
        {
          "name": "scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Scale",
          "description": "Zoom scale in powers of 2"
        },
        {
          "name": "center_x",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Center X",
          "description": "X coordinate of zoom center (0-1)"
        },
        {
          "name": "user_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "User Prompt",
          "description": "Additional prompt text to guide the zoom enhancement"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Input image to zoom into"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Chrono Edit",
      "description": "Chrono Edit\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ChronoEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ChronoEdit.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the output image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ChronoEdit.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image to edit."
        },
        {
          "name": "turbo_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Turbo Mode",
          "description": "Enable turbo mode to use for faster inference."
        },
        {
          "name": "num_temporal_reasoning_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Temporal Reasoning Steps",
          "description": "The number of temporal reasoning steps to perform."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "Whether to return the image in sync mode."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The guidance scale for the inference."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_temporal_reasoning",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Temporal Reasoning",
          "description": "Whether to enable temporal reasoning."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the inference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Chrono Edit Lora",
      "description": "Chrono Edit Lora\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ChronoEditLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "Optional additional LoRAs to merge for this request (max 3)."
        },
        {
          "name": "turbo_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Turbo Mode",
          "description": "Enable turbo mode to use for faster inference."
        },
        {
          "name": "enable_temporal_reasoning",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Temporal Reasoning",
          "description": "Whether to enable temporal reasoning."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The guidance scale for the inference."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ChronoEditLora.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the output image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ChronoEditLora.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "num_temporal_reasoning_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Temporal Reasoning Steps",
          "description": "The number of temporal reasoning steps to perform."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "Whether to return the image in sync mode."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image to edit."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the inference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Chrono Edit Lora Gallery Paintbrush",
      "description": "Chrono Edit Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ChronoEditLoraGalleryPaintbrush",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Describe how to transform the sketched regions."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ChronoEditLoraGalleryPaintbrush.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the output image."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA adapter."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ChronoEditLoraGalleryPaintbrush.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "Whether to return the image in sync mode."
        },
        {
          "name": "turbo_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Turbo Mode",
          "description": "Enable turbo mode to use faster inference."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "Optional additional LoRAs to merge (max 3)."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps to run."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "Optional mask image where black areas indicate regions to sketch/paint."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the inference."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Chrono Edit Lora Gallery Upscaler",
      "description": "Chrono Edit Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ChronoEditLoraGalleryUpscaler",
      "properties": [
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA adapter."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ChronoEditLoraGalleryUpscaler.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image to upscale."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "Whether to return the image in sync mode."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "Optional additional LoRAs to merge (max 3)."
        },
        {
          "name": "upscale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Upscale Factor",
          "description": "Target scale factor for the output resolution."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The guidance scale for the inference."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for the upscaling pass."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the inference."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Clarity Upscaler",
      "description": "Clarity Upscaler increases image resolution using AI-powered super-resolution. Enhance image quality, sharpness, and detail up to 4x scale.\n    image, upscaling, enhancement, super-resolution, clarity\n\n    Use cases:\n    - Increase image resolution for printing\n    - Improve clarity of low-quality images\n    - Enhance textures and fine details\n    - Prepare images for large displays\n    - Restore detail in compressed images",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ClarityUpscaler",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "masterpiece, best quality, highres",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "resemblance",
          "type": {
            "type": "float"
          },
          "default": 0.6,
          "title": "Resemblance",
          "description": "The resemblance of the upscaled image to the original image. The higher the resemblance, the more the model will try to keep the original image. Refers to the strength of the ControlNet."
        },
        {
          "name": "creativity",
          "type": {
            "type": "float"
          },
          "default": 0.35,
          "title": "Creativity",
          "description": "The creativity of the model. The higher the creativity, the more the model will deviate from the prompt. Refers to the denoise strength of the sampling."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to upscale."
        },
        {
          "name": "upscale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Upscale Factor",
          "description": "The upscale factor"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 18,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(worst quality, low quality, normal quality:2)",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to false, the safety checker will be disabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "scale"
      ]
    },
    {
      "title": "Clarityai Crystal Upscaler",
      "description": "Crystal Upscaler\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ClarityaiCrystalUpscaler",
      "properties": [
        {
          "name": "creativity",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Creativity",
          "description": "Creativity level for upscaling"
        },
        {
          "name": "scale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Scale Factor",
          "description": "Scale factor"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL to the input image"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Code Former",
      "description": "CodeFormer restores and enhances face quality in images. Advanced face restoration with fidelity control for natural-looking results.\n    image, face-restoration, enhancement, codeformer, quality\n\n    Use cases:\n    - Restore quality in degraded face photos\n    - Enhance facial details in low-quality images\n    - Improve portrait quality for professional use\n    - Fix compressed or damaged face images\n    - Enhance facial features while maintaining identity",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.CodeFormer",
      "properties": [
        {
          "name": "aligned",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Aligned",
          "description": "Should faces etc should be aligned."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to be used for relighting"
        },
        {
          "name": "upscale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Upscale Factor",
          "description": "Upscaling factor"
        },
        {
          "name": "fidelity",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Fidelity",
          "description": "Fidelity level (0-1, higher = more faithful to input)"
        },
        {
          "name": "face_upscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Face Upscale",
          "description": "Should faces be upscaled"
        },
        {
          "name": "only_center_face",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Only Center Face",
          "description": "Should only center face be restored"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "fidelity"
      ]
    },
    {
      "title": "Creative Upscaler",
      "description": "Create creative upscaled images.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.CreativeUpscaler",
      "properties": [
        {
          "name": "shape_preservation",
          "type": {
            "type": "float"
          },
          "default": 0.25,
          "title": "Shape Preservation",
          "description": "How much to preserve the shape of the original image"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results. If no prompt is provide BLIP2 will be used to generate a prompt."
        },
        {
          "name": "additional_embedding_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Additional Embedding Url",
          "description": "The URL to the additional embeddings to use for the upscaling. Default is None"
        },
        {
          "name": "enable_safety_checks",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checks",
          "description": "If set to true, the resulting image will be checked whether it includes any potentially unsafe content. If it does, it will be replaced with a black image."
        },
        {
          "name": "additional_lora_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Additional Lora Url",
          "description": "The URL to the additional LORA model to use for the upscaling. Default is None"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Scale",
          "description": "The scale of the output image. The higher the scale, the bigger the output image will be."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, low resolution, bad, ugly, low quality, pixelated, interpolated, compression artifacts, noisey, grainy",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "skip_ccsr",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Skip Ccsr",
          "description": "If set to true, the image will not be processed by the CCSR model before being processed by the creativity model."
        },
        {
          "name": "additional_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Additional Lora Scale",
          "description": "The scale of the additional LORA model to use for the upscaling. Default is 1.0"
        },
        {
          "name": "detail",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Detail",
          "description": "How much detail to add"
        },
        {
          "name": "base_model_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Base Model Url",
          "description": "The URL to the base model to use for the upscaling"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image to upscale."
        },
        {
          "name": "creativity",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Creativity",
          "description": "How much the output can deviate from the original"
        },
        {
          "name": "override_size_limits",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Override Size Limits",
          "description": "Allow for large uploads that could take a very long time."
        },
        {
          "name": "prompt_suffix",
          "type": {
            "type": "str"
          },
          "default": " high quality, highly detailed, high resolution, sharp",
          "title": "Prompt Suffix",
          "description": "The suffix to add to the prompt. This is useful to add a common ending to all prompts such as 'high quality' etc or embedding tokens."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 20,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for generating the image. The more steps the better the image will be but it will also take longer to generate."
        },
        {
          "name": "model_type",
          "type": {
            "type": "enum",
            "values": [
              "SD_1_5",
              "SDXL"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.CreativeUpscaler.ModelType"
          },
          "default": "SD_1_5",
          "title": "Model Type",
          "description": "The type of model to use for the upscaling. Default is SD_1_5"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ddcolor",
      "description": "Bring colors into old or new black and white photos with DDColor.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Ddcolor",
      "properties": [
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "seed to be used for generation"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to be used for relighting"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Docres",
      "description": "Enhance low-resolution, blur, shadowed documents with the superior quality of docres for sharper, clearer results.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Docres",
      "properties": [
        {
          "name": "task",
          "type": {
            "type": "enum",
            "values": [
              "deshadowing",
              "appearance",
              "deblurring",
              "binarization"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Docres.Task"
          },
          "default": "",
          "title": "Task",
          "description": "Task to perform"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to be used for relighting"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Docres Dewarp",
      "description": "Enhance wraped, folded documents with the superior quality of docres for sharper, clearer results.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.DocresDewarp",
      "properties": [
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to be used for relighting"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Dreamomni 2 Edit",
      "description": "DreamOmni2\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Dreamomni2Edit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of URLs of input images for editing."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Dwpose",
      "description": "Predict poses from images.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Dwpose",
      "properties": [
        {
          "name": "draw_mode",
          "type": {
            "type": "enum",
            "values": [
              "full-pose",
              "body-pose",
              "face-pose",
              "hand-pose",
              "face-hand-mask",
              "face-mask",
              "hand-mask"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Dwpose.DrawMode"
          },
          "default": "body-pose",
          "title": "Draw Mode",
          "description": "Mode of drawing the pose on the image. Options are: 'full-pose', 'body-pose', 'face-pose', 'hand-pose', 'face-hand-mask', 'face-mask', 'hand-mask'."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be processed"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Emu 35 Image Edit Image",
      "description": "Emu 3.5 Image\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Emu35ImageEditImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Emu35ImageEditImage.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the output image."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Emu35ImageEditImage.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the output image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Emu35ImageEditImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "Whether to return the image in sync mode."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the inference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Era 3 d",
      "description": "A powerful image to novel multiview model with normals.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Era3d",
      "properties": [
        {
          "name": "cfg",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Cfg",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "background_removal",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Background Removal",
          "description": "Background removal"
        },
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Steps",
          "description": "Number of steps to run the model for"
        },
        {
          "name": "crop_size",
          "type": {
            "type": "int"
          },
          "default": 400,
          "title": "Crop Size",
          "description": "Size of the image to crop to"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for random number generation"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to remove background from"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Esrgan",
      "description": "Upscale images by a given factor.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Esrgan",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "RealESRGAN_x4plus",
              "RealESRGAN_x2plus",
              "RealESRGAN_x4plus_anime_6B",
              "RealESRGAN_x4_v3",
              "RealESRGAN_x4_wdn_v3",
              "RealESRGAN_x4_anime_v3"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Esrgan.Model"
          },
          "default": "RealESRGAN_x4plus",
          "title": "Model",
          "description": "Model to use for upscaling"
        },
        {
          "name": "face",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Face",
          "description": "Upscaling a face"
        },
        {
          "name": "scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Scale",
          "description": "Rescaling factor"
        },
        {
          "name": "tile",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Tile",
          "description": "Tile size. Default is 0, that is no tile. When encountering the out-of-GPU-memory issue, please specify it, e.g., 400 or 200"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Esrgan.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output image format (png or jpeg)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Url to input image"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Evf Sam",
      "description": "EVF-SAM2 combines natural language understanding with advanced segmentation capabilities, allowing you to precisely mask image regions using intuitive positive and negative text prompts.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.EvfSam",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate segmentation from."
        },
        {
          "name": "use_grounding_dino",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Grounding Dino",
          "description": "Use GroundingDINO instead of SAM for segmentation"
        },
        {
          "name": "semantic_type",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Semantic Type",
          "description": "Enable semantic level segmentation for body parts, background or multi objects"
        },
        {
          "name": "fill_holes",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Fill Holes",
          "description": "Fill holes in the mask using morphological operations"
        },
        {
          "name": "expand_mask",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Expand Mask",
          "description": "Expand/dilate the mask by specified pixels"
        },
        {
          "name": "mask_only",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Mask Only",
          "description": "Output only the binary mask instead of masked image"
        },
        {
          "name": "revert_mask",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Revert Mask",
          "description": "Invert the mask (background becomes foreground and vice versa)"
        },
        {
          "name": "blur_mask",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Blur Mask",
          "description": "Apply Gaussian blur to the mask. Value determines kernel size (must be odd number)"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Areas to exclude from segmentation (will be subtracted from prompt results)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Face To Sticker",
      "description": "Create stickers from faces.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FaceToSticker",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to false, the safety checker will be disabled."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "ip_adapter_weight",
          "type": {
            "type": "float"
          },
          "default": 0.2,
          "title": "Ip Adapter Weight",
          "description": "The weight of the IP adapter."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the video."
        },
        {
          "name": "upscale_steps",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Upscale Steps",
          "description": "The number of steps to use for upscaling. Only used if `upscale` is `true`."
        },
        {
          "name": "instant_id_strength",
          "type": {
            "type": "float"
          },
          "default": 0.7,
          "title": "Instant Id Strength",
          "description": "The strength of the instant ID."
        },
        {
          "name": "upscale",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Upscale",
          "description": "Whether to upscale the image 2x."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 20,
          "title": "Num Inference Steps",
          "description": "Increasing the amount of steps tells Stable Diffusion that it should take more steps to generate your final result which can increase the amount of detail in your image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "ip_adapter_noise",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Ip Adapter Noise",
          "description": "The amount of noise to add to the IP adapter."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Fashn Tryon V15",
      "description": "FASHN v1.5 delivers precise virtual try-on capabilities, accurately rendering garment details like text and patterns at 576x864 resolution from both on-model and flat-lay photo references.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FashnTryonV15",
      "properties": [
        {
          "name": "model_image",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model Image",
          "description": "URL or base64 of the model image"
        },
        {
          "name": "moderation_level",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "permissive",
              "conservative"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FashnTryonV15.ModerationLevel"
          },
          "default": "permissive",
          "title": "Moderation Level",
          "description": "Content moderation level for garment images. 'none' disables moderation, 'permissive' blocks only explicit content, 'conservative' also blocks underwear and swimwear."
        },
        {
          "name": "garment_photo_type",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "model",
              "flat-lay"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FashnTryonV15.GarmentPhotoType"
          },
          "default": "auto",
          "title": "Garment Photo Type",
          "description": "Specifies the type of garment photo to optimize internal parameters for better performance. 'model' is for photos of garments on a model, 'flat-lay' is for flat-lay or ghost mannequin images, and 'auto' attempts to automatically detect the photo type."
        },
        {
          "name": "garment_image",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Garment Image",
          "description": "URL or base64 of the garment image"
        },
        {
          "name": "category",
          "type": {
            "type": "enum",
            "values": [
              "tops",
              "bottoms",
              "one-pieces",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FashnTryonV15.Category"
          },
          "default": "auto",
          "title": "Category",
          "description": "Category of the garment to try-on. 'auto' will attempt to automatically detect the category of the garment."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "segmentation_free",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Segmentation Free",
          "description": "Disables human parsing on the model image."
        },
        {
          "name": "num_samples",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Samples",
          "description": "Number of images to generate in a single run. Image generation has a random element in it, so trying multiple images at once increases the chances of getting a good result."
        },
        {
          "name": "mode",
          "type": {
            "type": "enum",
            "values": [
              "performance",
              "balanced",
              "quality"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FashnTryonV15.Mode"
          },
          "default": "balanced",
          "title": "Mode",
          "description": "Specifies the mode of operation. 'performance' mode is faster but may sacrifice quality, 'balanced' mode is a balance between speed and quality, and 'quality' mode is slower but produces higher quality results."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Sets random operations to a fixed state. Use the same seed to reproduce results with the same inputs, or different seed to force different results."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FashnTryonV15.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format of the generated images. 'png' is highest quality, while 'jpeg' is faster"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Fashn Tryon V16",
      "description": "FASHN v1.6 delivers precise virtual try-on capabilities, accurately rendering garment details like text and patterns at 864x1296 resolution from both on-model and flat-lay photo references.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FashnTryonV16",
      "properties": [
        {
          "name": "model_image",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model Image",
          "description": "URL or base64 of the model image"
        },
        {
          "name": "moderation_level",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "permissive",
              "conservative"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FashnTryonV16.ModerationLevel"
          },
          "default": "permissive",
          "title": "Moderation Level",
          "description": "Content moderation level for garment images. 'none' disables moderation, 'permissive' blocks only explicit content, 'conservative' also blocks underwear and swimwear."
        },
        {
          "name": "garment_photo_type",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "model",
              "flat-lay"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FashnTryonV16.GarmentPhotoType"
          },
          "default": "auto",
          "title": "Garment Photo Type",
          "description": "Specifies the type of garment photo to optimize internal parameters for better performance. 'model' is for photos of garments on a model, 'flat-lay' is for flat-lay or ghost mannequin images, and 'auto' attempts to automatically detect the photo type."
        },
        {
          "name": "garment_image",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Garment Image",
          "description": "URL or base64 of the garment image"
        },
        {
          "name": "category",
          "type": {
            "type": "enum",
            "values": [
              "tops",
              "bottoms",
              "one-pieces",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FashnTryonV16.Category"
          },
          "default": "auto",
          "title": "Category",
          "description": "Category of the garment to try-on. 'auto' will attempt to automatically detect the category of the garment."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "segmentation_free",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Segmentation Free",
          "description": "Disables human parsing on the model image."
        },
        {
          "name": "num_samples",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Samples",
          "description": "Number of images to generate in a single run. Image generation has a random element in it, so trying multiple images at once increases the chances of getting a good result."
        },
        {
          "name": "mode",
          "type": {
            "type": "enum",
            "values": [
              "performance",
              "balanced",
              "quality"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FashnTryonV16.Mode"
          },
          "default": "balanced",
          "title": "Mode",
          "description": "Specifies the mode of operation. 'performance' mode is faster but may sacrifice quality, 'balanced' mode is a balance between speed and quality, and 'quality' mode is slower but produces higher quality results."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Sets random operations to a fixed state. Use the same seed to reproduce results with the same inputs, or different seed to force different results."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FashnTryonV16.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format of the generated images. 'png' is highest quality, while 'jpeg' is faster"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Fast Lcm Diffusion Image To Image",
      "description": "Run SDXL at the speed of light\n    editing, transformation, image-to-image, img2img, fast\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FastLcmDiffusionImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "guidance_rescale",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Rescale",
          "description": "The rescale factor for the CFG."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "preserve_aspect_ratio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preserve Aspect Ratio",
          "description": "If set to true, the aspect ratio of the generated image will be preserved even if the image size is too large. However, if the image is not a multiple of 32 in width or height, it will be resized to the nearest multiple of 32. By default, this snapping to the nearest multiple of 32 will not preserve the aspect ratio. Set crop_output to True, to crop the output to the proper aspect ratio after generating."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "crop_output",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Crop Output",
          "description": "If set to true, the output cropped to the proper aspect ratio after generating."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FastLcmDiffusionImageToImage.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "model_name",
          "type": {
            "type": "enum",
            "values": [
              "stabilityai/stable-diffusion-xl-base-1.0",
              "runwayml/stable-diffusion-v1-5"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FastLcmDiffusionImageToImage.ModelName"
          },
          "default": "stabilityai/stable-diffusion-xl-base-1.0",
          "title": "Model Name",
          "description": "The name of the model to use."
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FastLcmDiffusionImageToImage.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "determines how much the generated image resembles the initial image"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Fast Lcm Diffusion Inpainting",
      "description": "Run SDXL at the speed of light\n    editing, transformation, image-to-image, img2img, fast\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FastLcmDiffusionInpainting",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "guidance_rescale",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Rescale",
          "description": "The rescale factor for the CFG."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FastLcmDiffusionInpainting.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "determines how much the generated image resembles the initial image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FastLcmDiffusionInpainting.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "The URL of the mask to use for inpainting."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "model_name",
          "type": {
            "type": "enum",
            "values": [
              "stabilityai/stable-diffusion-xl-base-1.0",
              "runwayml/stable-diffusion-v1-5"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FastLcmDiffusionInpainting.ModelName"
          },
          "default": "stabilityai/stable-diffusion-xl-base-1.0",
          "title": "Model Name",
          "description": "The name of the model to use."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Fast Lightning Sdxl Image To Image",
      "description": "Run SDXL at the speed of light\n    editing, transformation, image-to-image, img2img, fast\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FastLightningSdxlImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "guidance_rescale",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Rescale",
          "description": "The rescale factor for the CFG."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "preserve_aspect_ratio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preserve Aspect Ratio",
          "description": "If set to true, the aspect ratio of the generated image will be preserved even if the image size is too large. However, if the image is not a multiple of 32 in width or height, it will be resized to the nearest multiple of 32. By default, this snapping to the nearest multiple of 32 will not preserve the aspect ratio. Set crop_output to True, to crop the output to the proper aspect ratio after generating."
        },
        {
          "name": "crop_output",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Crop Output",
          "description": "If set to true, the output cropped to the proper aspect ratio after generating."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FastLightningSdxlImageToImage.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "determines how much the generated image resembles the initial image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FastLightningSdxlImageToImage.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "4",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FastLightningSdxlImageToImage.NumInferenceSteps"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Fast Lightning Sdxl Inpainting",
      "description": "Run SDXL at the speed of light\n    editing, transformation, image-to-image, img2img, fast\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FastLightningSdxlInpainting",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "guidance_rescale",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Rescale",
          "description": "The rescale factor for the CFG."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FastLightningSdxlInpainting.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "determines how much the generated image resembles the initial image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FastLightningSdxlInpainting.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "4",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FastLightningSdxlInpainting.NumInferenceSteps"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "The URL of the mask to use for inpainting."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Fast Sdxl Controlnet Canny Image To Image",
      "description": "Generate Images with ControlNet.\n    editing, transformation, image-to-image, img2img, fast\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FastSdxlControlnetCannyImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Leave it none to automatically infer from the control image."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The list of LoRA weights to use."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale of the controlnet conditioning."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "determines how much the generated image resembles the initial image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "control_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Fast Sdxl Controlnet Canny Inpainting",
      "description": "Generate Images with ControlNet.\n    editing, transformation, image-to-image, img2img, fast\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FastSdxlControlnetCannyInpainting",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Leave it none to automatically infer from the control image."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The list of LoRA weights to use."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale of the controlnet conditioning."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "determines how much the generated image resembles the initial image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "control_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "The URL of the mask to use for inpainting."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Fast Sdxl Image To Image",
      "description": "Run SDXL at the speed of light\n    editing, transformation, image-to-image, img2img, fast\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FastSdxlImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The list of LoRA weights to use."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "preserve_aspect_ratio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preserve Aspect Ratio",
          "description": "If set to true, the aspect ratio of the generated image will be preserved even if the image size is too large. However, if the image is not a multiple of 32 in width or height, it will be resized to the nearest multiple of 32. By default, this snapping to the nearest multiple of 32 will not preserve the aspect ratio. Set crop_output to True, to crop the output to the proper aspect ratio after generating."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "crop_output",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Crop Output",
          "description": "If set to true, the output cropped to the proper aspect ratio after generating."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FastSdxlImageToImage.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "determines how much the generated image resembles the initial image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FastSdxlImageToImage.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Fast Sdxl Inpainting",
      "description": "Run SDXL at the speed of light\n    editing, transformation, image-to-image, img2img, fast\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FastSdxlInpainting",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The list of LoRA weights to use."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FastSdxlInpainting.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "determines how much the generated image resembles the initial image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FastSdxlInpainting.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "The URL of the mask to use for inpainting."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ffmpeg Api Extract Frame",
      "description": "ffmpeg endpoint for first, middle and last frame extraction from videos\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FfmpegApiExtractFrame",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video file to use as the video track"
        },
        {
          "name": "frame_type",
          "type": {
            "type": "enum",
            "values": [
              "first",
              "middle",
              "last"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FfmpegApiExtractFrame.FrameType"
          },
          "default": "first",
          "title": "Frame Type",
          "description": "Type of frame to extract: first, middle, or last frame of the video"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Film",
      "description": "FILM\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Film",
      "properties": [
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Film.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the output video. Only applicable if output_type is 'video'."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Frames",
          "description": "The number of frames to generate between the input images."
        },
        {
          "name": "include_start",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Include Start",
          "description": "Whether to include the start image in the output."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Film.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the output video. Only applicable if output_type is 'video'."
        },
        {
          "name": "include_end",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Include End",
          "description": "Whether to include the end image in the output."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Fps",
          "description": "Frames per second for the output video. Only applicable if output_type is 'video'."
        },
        {
          "name": "start_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Start Image Url",
          "description": "The URL of the first image to use as the starting point for interpolation."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the second image to use as the ending point for interpolation."
        },
        {
          "name": "image_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Film.ImageFormat"
          },
          "default": "jpeg",
          "title": "Image Format",
          "description": "The format of the output images. Only applicable if output_type is 'images'."
        },
        {
          "name": "output_type",
          "type": {
            "type": "enum",
            "values": [
              "images",
              "video"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Film.OutputType"
          },
          "default": "images",
          "title": "Output Type",
          "description": "The type of output to generate; either individual images or a video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "enum",
            "values": [
              "images",
              "video"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Film.OutputType"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Finegrain Eraser",
      "description": "Finegrain Eraser removes objects\u2014along with their shadows, reflections, and lighting artifacts\u2014using only natural language, seamlessly filling the scene with contextually accurate content.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FinegrainEraser",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text description of what to erase"
        },
        {
          "name": "mode",
          "type": {
            "type": "enum",
            "values": [
              "express",
              "standard",
              "premium"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FinegrainEraser.Mode"
          },
          "default": "standard",
          "title": "Mode",
          "description": "Erase quality mode"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to edit"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Finegrain Eraser Bbox",
      "description": "Finegrain Eraser removes any object selected with a bounding box\u2014along with its shadows, reflections, and lighting artifacts\u2014seamlessly reconstructing the scene with contextually accurate content.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FinegrainEraserBbox",
      "properties": [
        {
          "name": "mode",
          "type": {
            "type": "enum",
            "values": [
              "express",
              "standard",
              "premium"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FinegrainEraserBbox.Mode"
          },
          "default": "standard",
          "title": "Mode",
          "description": "Erase quality mode"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation"
        },
        {
          "name": "box_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Box Prompts",
          "description": "List of bounding box coordinates to erase (only one box prompt is supported)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to edit"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Finegrain Eraser Mask",
      "description": "Finegrain Eraser removes any object selected with a mask\u2014along with its shadows, reflections, and lighting artifacts\u2014seamlessly reconstructing the scene with contextually accurate content.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FinegrainEraserMask",
      "properties": [
        {
          "name": "mode",
          "type": {
            "type": "enum",
            "values": [
              "express",
              "standard",
              "premium"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FinegrainEraserMask.Mode"
          },
          "default": "standard",
          "title": "Mode",
          "description": "Erase quality mode"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation"
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "URL of the mask image. Should be a binary mask where white (255) indicates areas to erase"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to edit"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Florence 2 Large Caption To Phrase Grounding",
      "description": "Florence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Florence2LargeCaptionToPhraseGrounding",
      "properties": [
        {
          "name": "text_input",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Input",
          "description": "Text input for the task"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Florence 2 Large Dense Region Caption",
      "description": "Florence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Florence2LargeDenseRegionCaption",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Florence 2 Large Object Detection",
      "description": "Florence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Florence2LargeObjectDetection",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Florence 2 Large Ocr With Region",
      "description": "Florence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Florence2LargeOcrWithRegion",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Florence 2 Large Open Vocabulary Detection",
      "description": "Florence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Florence2LargeOpenVocabularyDetection",
      "properties": [
        {
          "name": "text_input",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Input",
          "description": "Text input for the task"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Florence 2 Large Referring Expression Segmentation",
      "description": "Florence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Florence2LargeReferringExpressionSegmentation",
      "properties": [
        {
          "name": "text_input",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Input",
          "description": "Text input for the task"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Florence 2 Large Region Proposal",
      "description": "Florence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Florence2LargeRegionProposal",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Florence 2 Large Region To Segmentation",
      "description": "Florence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Florence2LargeRegionToSegmentation",
      "properties": [
        {
          "name": "region",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Region",
          "description": "The user input coordinates"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flowedit",
      "description": "The model provides you high quality image editing capabilities.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flowedit",
      "properties": [
        {
          "name": "src_guidance_scale",
          "type": {
            "type": "int"
          },
          "default": 1.5,
          "title": "Src Guidance Scale",
          "description": "Guidance scale for the source."
        },
        {
          "name": "n_min",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "N Min",
          "description": "Minimum step for improved style edits"
        },
        {
          "name": "n_max",
          "type": {
            "type": "int"
          },
          "default": 23,
          "title": "N Max",
          "description": "Control the strength of the edit"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to be used for relighting"
        },
        {
          "name": "source_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Source Prompt",
          "description": "Prompt of the image to be used."
        },
        {
          "name": "tar_guidance_scale",
          "type": {
            "type": "int"
          },
          "default": 5.5,
          "title": "Tar Guidance Scale",
          "description": "Guidance scale for target."
        },
        {
          "name": "target_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Target Prompt",
          "description": "Prompt of the image to be made."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation. If set none, a random seed will be used."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "Steps for which the model should run."
        },
        {
          "name": "n_avg",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "N Avg",
          "description": "Average step count"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 1 Dev Image To Image",
      "description": "FLUX.1 [dev] is a 12 billion parameter flow transformer that generates high-quality images from text. It is suitable for personal and commercial use. \n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux1DevImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1DevImageToImage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1DevImageToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "The strength of the initial image. Higher strength values are better for this model."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 1 Dev Redux",
      "description": "FLUX.1 [dev] Redux is a high-performance endpoint for the FLUX.1 [dev] model that enables rapid transformation of existing images, delivering high-quality style transfers and image modifications with the core FLUX capabilities.\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux1DevRedux",
      "properties": [
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1DevRedux.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1DevRedux.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 1 Krea Image To Image",
      "description": "FLUX.1 Krea [dev]\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux1KreaImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1KreaImageToImage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1KreaImageToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "The strength of the initial image. Higher strength values are better for this model."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 1 Krea Redux",
      "description": "FLUX.1 Krea [dev] Redux\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux1KreaRedux",
      "properties": [
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1KreaRedux.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1KreaRedux.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 1 Schnell Redux",
      "description": "FLUX.1 [schnell] Redux is a high-performance endpoint for the FLUX.1 [schnell] model that enables rapid transformation of existing images, delivering high-quality style transfers and image modifications with the core FLUX capabilities. \n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux1SchnellRedux",
      "properties": [
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1SchnellRedux.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1SchnellRedux.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 1 Srpo Image To Image",
      "description": "FLUX.1 SRPO [dev]\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux1SrpoImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1SrpoImageToImage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux1SrpoImageToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "The strength of the initial image. Higher strength values are better for this model."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Flash Edit",
      "description": "FLUX-2 Flash Edit provides ultra-fast image editing for rapid iteration and quick modifications.\n    image, editing, flux-2, flash, ultra-fast\n\n    Use cases:\n    - Edit images with ultra-fast processing\n    - Apply instant modifications to photos\n    - Create rapid edits for quick turnaround\n    - Transform images at maximum speed\n    - Produce instant image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2FlashEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the image to generate. The width and height must be between 512 and 2048 pixels."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2FlashEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for editing. A maximum of 4 images are allowed, if more are provided, only the first 4 will be used."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "If set to true, the prompt will be expanded for better results."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Flex Edit",
      "description": "FLUX-2 Flex Edit provides flexible image editing with customizable parameters and versatile control.\n    image, editing, flux-2, flex, versatile\n\n    Use cases:\n    - Edit images with flexible controls\n    - Apply customizable modifications\n    - Create versatile edits\n    - Transform images with adaptable settings\n    - Produce flexible image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2FlexEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the generation."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The size of the generated image. If `auto`, the size will be determined by the model."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2FlexEdit.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2FlexEdit.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to expand the prompt using the model's own knowledge."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of URLs of input images for editing"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 4B Base Edit",
      "description": "FLUX-2 Klein 4B Base Edit provides fast image editing with the 4-billion parameter model.\n    image, editing, flux-2, klein, 4b\n\n    Use cases:\n    - Edit images with FLUX-2 Klein 4B\n    - Apply fast modifications to photos\n    - Create quick edits with AI assistance\n    - Transform images efficiently\n    - Produce rapid image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2Klein4BBaseEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, uses the input image size."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein4BBaseEdit.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for image generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein4BBaseEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for editing. A maximum of 4 images are allowed."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for classifier-free guidance. Describes what to avoid in the image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 4B Base Edit Lora",
      "description": "FLUX-2 Klein 4B Base Edit with LoRA enables custom-trained models for specialized editing.\n    image, editing, flux-2, klein, 4b, lora\n\n    Use cases:\n    - Edit images with custom FLUX-2 models\n    - Apply specialized modifications using LoRA\n    - Create domain-specific edits\n    - Transform images with fine-tuned 4B model\n    - Produce customized modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2Klein4BBaseEditLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, uses the input image size."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein4BBaseEditLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for image generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein4BBaseEditLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for editing. A maximum of 4 images are allowed."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for classifier-free guidance. Describes what to avoid in the image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 4B Edit",
      "description": "FLUX-2 Klein 4B Edit provides efficient image editing with the streamlined 4-billion parameter model.\n    image, editing, flux-2, klein, 4b, efficient\n\n    Use cases:\n    - Edit images efficiently with FLUX-2\n    - Apply quick modifications to photos\n    - Create fast edits for rapid workflows\n    - Transform images with streamlined model\n    - Produce quick image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2Klein4BEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, uses the input image size."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein4BEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for editing. A maximum of 4 images are allowed."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 9B Base Edit",
      "description": "FLUX-2 Klein 9B Base Edit provides high-quality image editing with the 9-billion parameter model.\n    image, editing, flux-2, klein, 9b\n\n    Use cases:\n    - Edit images with FLUX-2 Klein 9B\n    - Apply high-quality modifications to photos\n    - Create advanced edits with powerful AI\n    - Transform images with superior quality\n    - Produce professional image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2Klein9BBaseEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, uses the input image size."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein9BBaseEdit.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for image generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein9BBaseEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for editing. A maximum of 4 images are allowed."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for classifier-free guidance. Describes what to avoid in the image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 9B Base Edit Lora",
      "description": "FLUX-2 Klein 9B Base Edit with LoRA combines powerful editing with custom-trained models.\n    image, editing, flux-2, klein, 9b, lora\n\n    Use cases:\n    - Edit images with custom 9B models\n    - Apply specialized high-quality modifications\n    - Create professional custom edits\n    - Transform images with fine-tuned powerful model\n    - Produce advanced customized results",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2Klein9BBaseEditLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, uses the input image size."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein9BBaseEditLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for image generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein9BBaseEditLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for editing. A maximum of 4 images are allowed."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for classifier-free guidance. Describes what to avoid in the image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 9B Edit",
      "description": "FLUX-2 Klein 9B Edit provides advanced image editing with the full 9-billion parameter model.\n    image, editing, flux-2, klein, 9b, advanced\n\n    Use cases:\n    - Edit images with advanced FLUX-2 model\n    - Apply sophisticated modifications\n    - Create high-quality edits\n    - Transform images with powerful AI\n    - Produce superior image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2Klein9BEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, uses the input image size."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2Klein9BEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for editing. A maximum of 4 images are allowed."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Add Background",
      "description": "FLUX-2 LoRA Gallery Add Background places subjects in new environments with realistic integration.\n    image, editing, flux-2, background, compositing\n\n    Use cases:\n    - Add backgrounds to cutout images\n    - Place subjects in new environments\n    - Create realistic background compositions\n    - Generate contextual settings\n    - Produce integrated background images",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2LoraGalleryAddBackground",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Add Background forest",
          "title": "Prompt",
          "description": "The prompt describing the background to add. Must start with 'Add Background' followed by your description."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryAddBackground.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the add background effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryAddBackground.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images. Provide an image with a white or clean background."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Apartment Staging",
      "description": "Flux 2 Lora Gallery\n    flux, editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2LoraGalleryApartmentStaging",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a furnished room. Use 'furnish this room' for best results."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryApartmentStaging.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the apartment staging effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryApartmentStaging.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the empty room image to furnish."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Face To Full Portrait",
      "description": "FLUX-2 LoRA Gallery Face to Full Portrait expands face crops into complete portrait images.\n    image, editing, flux-2, portrait, expansion\n\n    Use cases:\n    - Expand face crops to full portraits\n    - Generate complete portrait from face\n    - Create full-body images from headshots\n    - Extend facial images to portraits\n    - Produce complete portrait compositions",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2LoraGalleryFaceToFullPortrait",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Face to full portrait",
          "title": "Prompt",
          "description": "The prompt describing the full portrait to generate from the face."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryFaceToFullPortrait.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the face to full portrait effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryFaceToFullPortrait.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the cropped face image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Multiple Angles",
      "description": "FLUX-2 LoRA Gallery Multiple Angles generates images from different viewpoints for comprehensive visualization.\n    image, editing, flux-2, multi-angle, viewpoint\n\n    Use cases:\n    - Generate multiple product angles\n    - Create viewpoint variations\n    - Visualize objects from different sides\n    - Produce multi-angle image sets\n    - Generate comprehensive views",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2LoraGalleryMultipleAngles",
      "properties": [
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryMultipleAngles.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation."
        },
        {
          "name": "horizontal_angle",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Horizontal Angle",
          "description": "Horizontal rotation angle around the object in degrees. 0\u00b0=front view, 90\u00b0=right side, 180\u00b0=back view, 270\u00b0=left side, 360\u00b0=front view again."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image to adjust camera angle for."
        },
        {
          "name": "zoom",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Zoom",
          "description": "Camera zoom/distance. 0=wide shot (far away), 5=medium shot (normal), 10=close-up (very close)."
        },
        {
          "name": "vertical_angle",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Vertical Angle",
          "description": "Vertical camera angle in degrees. 0\u00b0=eye-level shot, 30\u00b0=elevated shot, 60\u00b0=high-angle shot (looking down from above)."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the multiple angles effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryMultipleAngles.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If True, the media will be returned as a data URI."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Virtual Tryon",
      "description": "FLUX-2 LoRA Gallery Virtual Try-on enables realistic clothing and accessory visualization on people.\n    image, editing, flux-2, virtual-tryon, fashion\n\n    Use cases:\n    - Visualize clothing on models\n    - Try on accessories virtually\n    - Create fashion previews\n    - Test product appearances\n    - Generate try-on images",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2LoraGalleryVirtualTryon",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a virtual try-on image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryVirtualTryon.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the virtual try-on effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2LoraGalleryVirtualTryon.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for virtual try-on. Provide person image and clothing image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Max Edit",
      "description": "FLUX-2 Max Edit provides maximum quality image editing with the most advanced FLUX-2 model.\n    image, editing, flux-2, max, premium\n\n    Use cases:\n    - Edit images with maximum quality\n    - Apply premium modifications to photos\n    - Create professional-grade edits\n    - Transform images with best quality\n    - Produce highest quality modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2MaxEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The size of the generated image. If `auto`, the size will be determined by the model."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2MaxEdit.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2MaxEdit.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of URLs of input images for editing"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Turbo Edit",
      "description": "FLUX-2 Turbo Edit provides accelerated image editing with balanced quality and speed.\n    image, editing, flux-2, turbo, fast\n\n    Use cases:\n    - Edit images with turbo speed\n    - Apply fast modifications with good quality\n    - Create quick edits efficiently\n    - Transform images rapidly\n    - Produce fast quality modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Flux2TurboEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the image to generate. The width and height must be between 512 and 2048 pixels."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Flux2TurboEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images for editing. A maximum of 4 images are allowed, if more are provided, only the first 4 will be used."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "If set to true, the prompt will be expanded for better results."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Control Lora Canny Image To Image",
      "description": "FLUX Control LoRA Canny is a high-performance endpoint that uses a control image using a Canny edge map to transfer structure to the generated image and another initial image to guide color.\n    flux, editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxControlLoraCannyImageToImage",
      "properties": [
        {
          "name": "control_lora_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Control Lora Strength",
          "description": "The strength of the control lora."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxControlLoraCannyImageToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for inpainting. or img2img"
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "control_lora_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Lora Image Url",
          "description": "The image to use for control lora. This is used to control the style of the generated image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Control Lora Depth Image To Image",
      "description": "FLUX Control LoRA Depth is a high-performance endpoint that uses a control image using a depth map to transfer structure to the generated image and another initial image to guide color.\n    flux, editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxControlLoraDepthImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "control_lora_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Control Lora Strength",
          "description": "The strength of the control lora."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxControlLoraDepthImageToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for inpainting. or img2img"
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "control_lora_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Lora Image Url",
          "description": "The image to use for control lora. This is used to control the style of the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Dev Redux",
      "description": "FLUX.1 [dev] Redux provides advanced image transformation capabilities with superior quality and more control over the style transfer process.\n    image, transformation, style-transfer, development, flux, redux\n\n    Use cases:\n    - Transform images with advanced quality controls\n    - Create customized image variations with guidance\n    - Apply precise style modifications\n    - Generate high-quality artistic transformations\n    - Produce refined image edits with better prompt adherence",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxDevRedux",
      "properties": [
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate (1-4)"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxDevRedux.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxDevRedux.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "Output format (jpeg or png)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable safety checker to filter unsafe content"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "How strictly to follow the image structure (1-20)"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform (1-50)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "image_size",
        "guidance_scale"
      ]
    },
    {
      "title": "Flux Differential Diffusion",
      "description": "FLUX.1 Differential Diffusion is a rapid endpoint that enables swift, granular control over image transformations through change maps, delivering fast and precise region-specific modifications while maintaining FLUX.1 [dev]'s high-quality output.\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxDifferentialDiffusion",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use as initial image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "The strength to use for image-to-image. 1.0 is completely remakes the image while 0.0 preserves the original."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "change_map_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Change Map Image Url",
          "description": "URL of change map."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux General Differential Diffusion",
      "description": "A specialized FLUX endpoint combining differential diffusion control with LoRA, ControlNet, and IP-Adapter support, enabling precise, region-specific image transformations through customizable change maps.\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxGeneralDifferentialDiffusion",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "nag_end",
          "type": {
            "type": "float"
          },
          "default": 0.25,
          "title": "Nag End",
          "description": "The proportion of steps to apply NAG. After the specified proportion of steps has been iterated, the remaining steps will use original attention processors in FLUX."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "control_loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Control Loras",
          "description": "The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "euler",
              "dpmpp_2m"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxGeneralDifferentialDiffusion.Scheduler"
          },
          "default": "euler",
          "title": "Scheduler",
          "description": "Scheduler for the denoising process."
        },
        {
          "name": "easycontrols",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Easycontrols",
          "description": "EasyControl Inputs to use for image generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "real_cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Real Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxGeneralDifferentialDiffusion.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "use_cfg_zero",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Cfg Zero",
          "description": "Uses CFG-zero init sampling as in https://arxiv.org/abs/2503.18886."
        },
        {
          "name": "fill_image",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Fill Image",
          "description": "Use an image input to influence the generation. Can be used to fill images in masked areas."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "sigma_schedule",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "sgm_uniform"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxGeneralDifferentialDiffusion.SigmaSchedule"
          },
          "default": null,
          "title": "Sigma Schedule",
          "description": "Sigmas schedule for the denoising process."
        },
        {
          "name": "reference_end",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Reference End",
          "description": "The percentage of the total timesteps when the reference guidance is to be ended."
        },
        {
          "name": "reference_strength",
          "type": {
            "type": "float"
          },
          "default": 0.65,
          "title": "Reference Strength",
          "description": "Strength of reference_only generation. Only used if a reference image is provided."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use as initial image."
        },
        {
          "name": "nag_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Nag Scale",
          "description": "The scale for NAG. Higher values will result in a image that is more distant to the negative prompt."
        },
        {
          "name": "reference_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Reference Image Url",
          "description": "URL of Image for Reference-Only"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "controlnet_unions",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Controlnet Unions",
          "description": "The controlnet unions to use for the image generation. Only one controlnet is supported at the moment."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to steer the image generation away from unwanted features. By default, we will be using NAG for processing the negative prompt."
        },
        {
          "name": "nag_tau",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Nag Tau",
          "description": "The tau for NAG. Controls the normalization of the hidden state. Higher values will result in a less aggressive normalization, but may also lead to unexpected changes with respect to the original image. Not recommended to change this value."
        },
        {
          "name": "change_map_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Change Map Image Url",
          "description": "URL of change map."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "use_beta_schedule",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Beta Schedule",
          "description": "Specifies whether beta sigmas ought to be used."
        },
        {
          "name": "ip_adapters",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Ip Adapters",
          "description": "IP-Adapter to use for image generation."
        },
        {
          "name": "base_shift",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Base Shift",
          "description": "Base shift for the scheduled timesteps"
        },
        {
          "name": "nag_alpha",
          "type": {
            "type": "float"
          },
          "default": 0.25,
          "title": "Nag Alpha",
          "description": "The alpha value for NAG. This value is used as a final weighting factor for steering the normalized guidance (positive and negative prompts) in the direction of the positive prompt. Higher values will result in less steering on the normalized guidance where lower values will result in considering the positive prompt guidance more."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "The strength to use for differential diffusion. 1.0 is completely remakes the image while 0.0 preserves the original."
        },
        {
          "name": "max_shift",
          "type": {
            "type": "float"
          },
          "default": 1.15,
          "title": "Max Shift",
          "description": "Max shift for the scheduled timesteps"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "controlnets",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Controlnets",
          "description": "The controlnets to use for the image generation. Only one controlnet is supported at the moment."
        },
        {
          "name": "reference_start",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Reference Start",
          "description": "The percentage of the total timesteps when the reference guidance is to bestarted."
        },
        {
          "name": "use_real_cfg",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Real Cfg",
          "description": "Uses classical CFG as in SD1.5, SDXL, etc. Increases generation times and price when set to be true. If using XLabs IP-Adapter v1, this will be turned on!."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux General Image To Image",
      "description": "FLUX General Image-to-Image is a versatile endpoint that transforms existing images with support for LoRA, ControlNet, and IP-Adapter extensions, enabling precise control over style transfer, modifications, and artistic variations through multiple guidance methods.\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxGeneralImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "nag_end",
          "type": {
            "type": "float"
          },
          "default": 0.25,
          "title": "Nag End",
          "description": "The proportion of steps to apply NAG. After the specified proportion of steps has been iterated, the remaining steps will use original attention processors in FLUX."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "control_loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Control Loras",
          "description": "The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "euler",
              "dpmpp_2m"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxGeneralImageToImage.Scheduler"
          },
          "default": "euler",
          "title": "Scheduler",
          "description": "Scheduler for the denoising process."
        },
        {
          "name": "easycontrols",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Easycontrols",
          "description": "EasyControl Inputs to use for image generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "real_cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Real Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxGeneralImageToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "use_cfg_zero",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Cfg Zero",
          "description": "Uses CFG-zero init sampling as in https://arxiv.org/abs/2503.18886."
        },
        {
          "name": "fill_image",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Fill Image",
          "description": "Use an image input to influence the generation. Can be used to fill images in masked areas."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "sigma_schedule",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "sgm_uniform"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxGeneralImageToImage.SigmaSchedule"
          },
          "default": null,
          "title": "Sigma Schedule",
          "description": "Sigmas schedule for the denoising process."
        },
        {
          "name": "reference_end",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Reference End",
          "description": "The percentage of the total timesteps when the reference guidance is to be ended."
        },
        {
          "name": "reference_strength",
          "type": {
            "type": "float"
          },
          "default": 0.65,
          "title": "Reference Strength",
          "description": "Strength of reference_only generation. Only used if a reference image is provided."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for inpainting. or img2img"
        },
        {
          "name": "nag_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Nag Scale",
          "description": "The scale for NAG. Higher values will result in a image that is more distant to the negative prompt."
        },
        {
          "name": "reference_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Reference Image Url",
          "description": "URL of Image for Reference-Only"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "controlnet_unions",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Controlnet Unions",
          "description": "The controlnet unions to use for the image generation. Only one controlnet is supported at the moment."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to steer the image generation away from unwanted features. By default, we will be using NAG for processing the negative prompt."
        },
        {
          "name": "nag_tau",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Nag Tau",
          "description": "The tau for NAG. Controls the normalization of the hidden state. Higher values will result in a less aggressive normalization, but may also lead to unexpected changes with respect to the original image. Not recommended to change this value."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "use_beta_schedule",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Beta Schedule",
          "description": "Specifies whether beta sigmas ought to be used."
        },
        {
          "name": "ip_adapters",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Ip Adapters",
          "description": "IP-Adapter to use for image generation."
        },
        {
          "name": "base_shift",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Base Shift",
          "description": "Base shift for the scheduled timesteps"
        },
        {
          "name": "nag_alpha",
          "type": {
            "type": "float"
          },
          "default": 0.25,
          "title": "Nag Alpha",
          "description": "The alpha value for NAG. This value is used as a final weighting factor for steering the normalized guidance (positive and negative prompts) in the direction of the positive prompt. Higher values will result in less steering on the normalized guidance where lower values will result in considering the positive prompt guidance more."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original."
        },
        {
          "name": "max_shift",
          "type": {
            "type": "float"
          },
          "default": 1.15,
          "title": "Max Shift",
          "description": "Max shift for the scheduled timesteps"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "controlnets",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Controlnets",
          "description": "The controlnets to use for the image generation. Only one controlnet is supported at the moment."
        },
        {
          "name": "reference_start",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Reference Start",
          "description": "The percentage of the total timesteps when the reference guidance is to bestarted."
        },
        {
          "name": "use_real_cfg",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Real Cfg",
          "description": "Uses classical CFG as in SD1.5, SDXL, etc. Increases generation times and price when set to be true. If using XLabs IP-Adapter v1, this will be turned on!."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux General Inpainting",
      "description": "FLUX General Inpainting is a versatile endpoint that enables precise image editing and completion, supporting multiple AI extensions including LoRA, ControlNet, and IP-Adapter for enhanced control over inpainting results and sophisticated image modifications.\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxGeneralInpainting",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "nag_end",
          "type": {
            "type": "float"
          },
          "default": 0.25,
          "title": "Nag End",
          "description": "The proportion of steps to apply NAG. After the specified proportion of steps has been iterated, the remaining steps will use original attention processors in FLUX."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "control_loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Control Loras",
          "description": "The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "euler",
              "dpmpp_2m"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxGeneralInpainting.Scheduler"
          },
          "default": "euler",
          "title": "Scheduler",
          "description": "Scheduler for the denoising process."
        },
        {
          "name": "easycontrols",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Easycontrols",
          "description": "EasyControl Inputs to use for image generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "real_cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Real Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxGeneralInpainting.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "use_cfg_zero",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Cfg Zero",
          "description": "Uses CFG-zero init sampling as in https://arxiv.org/abs/2503.18886."
        },
        {
          "name": "fill_image",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Fill Image",
          "description": "Use an image input to influence the generation. Can be used to fill images in masked areas."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "sigma_schedule",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "sgm_uniform"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxGeneralInpainting.SigmaSchedule"
          },
          "default": null,
          "title": "Sigma Schedule",
          "description": "Sigmas schedule for the denoising process."
        },
        {
          "name": "reference_end",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Reference End",
          "description": "The percentage of the total timesteps when the reference guidance is to be ended."
        },
        {
          "name": "reference_strength",
          "type": {
            "type": "float"
          },
          "default": 0.65,
          "title": "Reference Strength",
          "description": "Strength of reference_only generation. Only used if a reference image is provided."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "The mask to area to Inpaint in."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for inpainting. or img2img"
        },
        {
          "name": "nag_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Nag Scale",
          "description": "The scale for NAG. Higher values will result in a image that is more distant to the negative prompt."
        },
        {
          "name": "reference_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Reference Image Url",
          "description": "URL of Image for Reference-Only"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "controlnet_unions",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Controlnet Unions",
          "description": "The controlnet unions to use for the image generation. Only one controlnet is supported at the moment."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to steer the image generation away from unwanted features. By default, we will be using NAG for processing the negative prompt."
        },
        {
          "name": "nag_tau",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Nag Tau",
          "description": "The tau for NAG. Controls the normalization of the hidden state. Higher values will result in a less aggressive normalization, but may also lead to unexpected changes with respect to the original image. Not recommended to change this value."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "use_beta_schedule",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Beta Schedule",
          "description": "Specifies whether beta sigmas ought to be used."
        },
        {
          "name": "ip_adapters",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Ip Adapters",
          "description": "IP-Adapter to use for image generation."
        },
        {
          "name": "base_shift",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Base Shift",
          "description": "Base shift for the scheduled timesteps"
        },
        {
          "name": "nag_alpha",
          "type": {
            "type": "float"
          },
          "default": 0.25,
          "title": "Nag Alpha",
          "description": "The alpha value for NAG. This value is used as a final weighting factor for steering the normalized guidance (positive and negative prompts) in the direction of the positive prompt. Higher values will result in less steering on the normalized guidance where lower values will result in considering the positive prompt guidance more."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original."
        },
        {
          "name": "max_shift",
          "type": {
            "type": "float"
          },
          "default": 1.15,
          "title": "Max Shift",
          "description": "Max shift for the scheduled timesteps"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "controlnets",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Controlnets",
          "description": "The controlnets to use for the image generation. Only one controlnet is supported at the moment."
        },
        {
          "name": "reference_start",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Reference Start",
          "description": "The percentage of the total timesteps when the reference guidance is to bestarted."
        },
        {
          "name": "use_real_cfg",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Real Cfg",
          "description": "Uses classical CFG as in SD1.5, SDXL, etc. Increases generation times and price when set to be true. If using XLabs IP-Adapter v1, this will be turned on!."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux General Rf Inversion",
      "description": "A general purpose endpoint for the FLUX.1 [dev] model, implementing the RF-Inversion pipeline. This can be used to edit a reference image based on a prompt.\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxGeneralRfInversion",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image with"
        },
        {
          "name": "nag_end",
          "type": {
            "type": "float"
          },
          "default": 0.25,
          "title": "Nag End",
          "description": "The proportion of steps to apply NAG. After the specified proportion of steps has been iterated, the remaining steps will use original attention processors in FLUX."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "control_loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Control Loras",
          "description": "The LoRAs to use for the image generation which use a control image. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "controller_guidance_reverse",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Controller Guidance Reverse",
          "description": "The controller guidance (eta) used in the denoising process.Using values closer to 1 will result in an image closer to input."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "reverse_guidance_start",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Reverse Guidance Start",
          "description": "Timestep to start guidance during reverse process."
        },
        {
          "name": "easycontrols",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Easycontrols",
          "description": "EasyControl Inputs to use for image generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "euler",
              "dpmpp_2m"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxGeneralRfInversion.Scheduler"
          },
          "default": "euler",
          "title": "Scheduler",
          "description": "Scheduler for the denoising process."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxGeneralRfInversion.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "use_cfg_zero",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Cfg Zero",
          "description": "Uses CFG-zero init sampling as in https://arxiv.org/abs/2503.18886."
        },
        {
          "name": "reference_strength",
          "type": {
            "type": "float"
          },
          "default": 0.65,
          "title": "Reference Strength",
          "description": "Strength of reference_only generation. Only used if a reference image is provided."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "sigma_schedule",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "sgm_uniform"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxGeneralRfInversion.SigmaSchedule"
          },
          "default": null,
          "title": "Sigma Schedule",
          "description": "Sigmas schedule for the denoising process."
        },
        {
          "name": "reference_end",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Reference End",
          "description": "The percentage of the total timesteps when the reference guidance is to be ended."
        },
        {
          "name": "controller_guidance_forward",
          "type": {
            "type": "float"
          },
          "default": 0.6,
          "title": "Controller Guidance Forward",
          "description": "The controller guidance (gamma) used in the creation of structured noise."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to be edited"
        },
        {
          "name": "fill_image",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Fill Image",
          "description": "Use an image input to influence the generation. Can be used to fill images in masked areas."
        },
        {
          "name": "nag_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Nag Scale",
          "description": "The scale for NAG. Higher values will result in a image that is more distant to the negative prompt."
        },
        {
          "name": "reverse_guidance_schedule",
          "type": {
            "type": "enum",
            "values": [
              "constant",
              "linear_increase",
              "linear_decrease"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxGeneralRfInversion.ReverseGuidanceSchedule"
          },
          "default": "constant",
          "title": "Reverse Guidance Schedule",
          "description": "Scheduler for applying reverse guidance."
        },
        {
          "name": "reference_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Reference Image Url",
          "description": "URL of Image for Reference-Only"
        },
        {
          "name": "reverse_guidance_end",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Reverse Guidance End",
          "description": "Timestep to stop guidance during reverse process."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "controlnet_unions",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Controlnet Unions",
          "description": "The controlnet unions to use for the image generation. Only one controlnet is supported at the moment."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to steer the image generation away from unwanted features. By default, we will be using NAG for processing the negative prompt."
        },
        {
          "name": "nag_tau",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Nag Tau",
          "description": "The tau for NAG. Controls the normalization of the hidden state. Higher values will result in a less aggressive normalization, but may also lead to unexpected changes with respect to the original image. Not recommended to change this value."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "use_beta_schedule",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Beta Schedule",
          "description": "Specifies whether beta sigmas ought to be used."
        },
        {
          "name": "nag_alpha",
          "type": {
            "type": "float"
          },
          "default": 0.25,
          "title": "Nag Alpha",
          "description": "The alpha value for NAG. This value is used as a final weighting factor for steering the normalized guidance (positive and negative prompts) in the direction of the positive prompt. Higher values will result in less steering on the normalized guidance where lower values will result in considering the positive prompt guidance more."
        },
        {
          "name": "base_shift",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Base Shift",
          "description": "Base shift for the scheduled timesteps"
        },
        {
          "name": "max_shift",
          "type": {
            "type": "float"
          },
          "default": 1.15,
          "title": "Max Shift",
          "description": "Max shift for the scheduled timesteps"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "reference_start",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Reference Start",
          "description": "The percentage of the total timesteps when the reference guidance is to bestarted."
        },
        {
          "name": "controlnets",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Controlnets",
          "description": "The controlnets to use for the image generation. Only one controlnet is supported at the moment."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Kontext Dev",
      "description": "Frontier image editing model.\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxKontextDev",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "resolution_mode",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "match_input",
              "1:1",
              "16:9",
              "21:9",
              "3:2",
              "2:3",
              "4:5",
              "5:4",
              "3:4",
              "4:3",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKontextDev.ResolutionMode"
          },
          "default": "match_input",
          "title": "Resolution Mode",
          "description": "Determines how the output resolution is set for image editing. - `auto`: The model selects an optimal resolution from a predefined set that best matches the input image's aspect ratio. This is the recommended setting for most use cases as it's what the model was trained on. - `match_input`: The model will attempt to use the same resolution as the input image. The resolution will be adjusted to be compatible with the model's requirements (e.g. dimensions must be multiples of 16 and within supported limits). Apart from these, a few aspect ratios are also supported."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKontextDev.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKontextDev.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "Output format"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Kontext Lora",
      "description": "Fast endpoint for the FLUX.1 Kontext [dev] model with LoRA support, enabling rapid and high-quality image editing using pre-trained LoRA adaptations for specific styles, brand identities, and product-specific outputs.\n    flux, editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxKontextLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKontextLora.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "resolution_mode",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "match_input",
              "1:1",
              "16:9",
              "21:9",
              "3:2",
              "2:3",
              "4:5",
              "5:4",
              "3:4",
              "4:3",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKontextLora.ResolutionMode"
          },
          "default": "match_input",
          "title": "Resolution Mode",
          "description": "Determines how the output resolution is set for image editing. - `auto`: The model selects an optimal resolution from a predefined set that best matches the input image's aspect ratio. This is the recommended setting for most use cases as it's what the model was trained on. - `match_input`: The model will attempt to use the same resolution as the input image. The resolution will be adjusted to be compatible with the model's requirements (e.g. dimensions must be multiples of 16 and within supported limits). Apart from these, a few aspect ratios are also supported."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKontextLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to edit. Max width: 14142px, Max height: 14142px, Timeout: 20s"
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Kontext Lora Inpaint",
      "description": "Flux Kontext Lora\n    flux, editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxKontextLoraInpaint",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for the image to image task."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKontextLoraInpaint.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "reference_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Reference Image Url",
          "description": "The URL of the reference image for inpainting."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKontextLoraInpaint.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be inpainted."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.88,
          "title": "Strength",
          "description": "The strength of the initial image. Higher strength values are better for this model."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The URL of the mask for inpainting."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Krea Image To Image",
      "description": "FLUX.1 Krea [dev]\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxKreaImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKreaImageToImage.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKreaImageToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "The strength of the initial image. Higher strength values are better for this model."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Krea Lora Image To Image",
      "description": "FLUX.1 Krea [dev] with LoRAs\n    flux, editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxKreaLoraImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKreaLoraImageToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for inpainting. or img2img"
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Krea Lora Inpainting",
      "description": "FLUX.1 Krea [dev] Inpainting with LoRAs\n    flux, editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxKreaLoraInpainting",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKreaLoraInpainting.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for inpainting. or img2img"
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "The mask to area to Inpaint in."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Krea Redux",
      "description": "FLUX.1 Krea [dev] Redux\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxKreaRedux",
      "properties": [
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKreaRedux.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxKreaRedux.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Lora Canny",
      "description": "Utilize Flux.1 [dev] Controlnet to generate high-quality images with precise control over composition, style, and structure through advanced edge detection and guidance mechanisms.\n    flux, editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxLoraCanny",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxLoraCanny.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for canny input"
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 30,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Lora Depth",
      "description": "Generate high-quality images from depth maps using Flux.1 [dev] depth estimation model. The model produces accurate depth representations for scene understanding and 3D visualization.\n    flux, editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxLoraDepth",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxLoraDepth.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for depth input"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Lora Fill",
      "description": "FLUX.1 [dev] Fill is a high-performance endpoint for the FLUX.1 [pro] model that enables rapid transformation of existing images, delivering high-quality style transfers and image modifications with the core FLUX capabilities.\n    flux, editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxLoraFill",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "resize_to_original",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Resize To Original",
          "description": "Resizes the image back to the original size. Use when you wish to preserve the exact image size as the originally provided image."
        },
        {
          "name": "paste_back",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Paste Back",
          "description": "Specifies whether to paste-back the original image onto to the non-inpainted areas of the output"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 30,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxLoraFill.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for fill operation"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "fill_image",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Fill Image",
          "description": "Use an image fill input to fill in particular images into the masked area."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "The mask to area to Inpaint in."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Lora Image To Image",
      "description": "FLUX LoRA Image-to-Image is a high-performance endpoint that transforms existing images using FLUX models, leveraging LoRA adaptations to enable rapid and precise image style transfer, modifications, and artistic variations.\n    flux, editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxLoraImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxLoraImageToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for inpainting. or img2img"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Pro Canny",
      "description": "FLUX.1 Pro with Canny edge detection control. Generate images guided by edge maps for precise structural control while maintaining FLUX's quality.\n    image, controlnet, canny, edges, flux, professional\n\n    Use cases:\n    - Generate images following edge structures\n    - Transform images while preserving edges\n    - Create controlled variations with edge guidance\n    - Apply style transfers with structural constraints\n    - Generate content from edge maps",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxProCanny",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the desired output"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProCanny.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProCanny.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "How strictly to follow the prompt"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform"
        },
        {
          "name": "control_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Image Url",
          "description": "The control image URL to generate the Canny edge map from."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image",
        "control_strength"
      ]
    },
    {
      "title": "Flux Pro Depth",
      "description": "FLUX.1 Pro with depth map control. Generate images guided by depth information for precise 3D structure control while maintaining FLUX's quality.\n    image, controlnet, depth, 3d, flux, professional\n\n    Use cases:\n    - Generate images following depth structures\n    - Transform images while preserving 3D composition\n    - Create controlled variations with depth guidance\n    - Apply style transfers with spatial constraints\n    - Generate content from depth maps",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxProDepth",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the desired output"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProDepth.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProDepth.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "How strictly to follow the prompt"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform"
        },
        {
          "name": "control_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Image Url",
          "description": "The control image URL to generate the depth map from."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image",
        "control_strength"
      ]
    },
    {
      "title": "Flux Pro Fill",
      "description": "FLUX.1 Pro Fill provides professional inpainting and outpainting capabilities. Generate or modify image content within masked regions with precise prompt control.\n    image, inpainting, outpainting, fill, flux, professional\n\n    Use cases:\n    - Fill masked regions with new content\n    - Extend images beyond their boundaries (outpainting)\n    - Remove unwanted objects and fill gaps\n    - Generate content-aware image expansions\n    - Create seamless image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxProFill",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt describing what to generate in the masked area"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProFill.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. Needs to match the dimensions of the mask."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProFill.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "Safety tolerance level (1-6, higher is stricter)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The mask URL to inpaint the image. Needs to match the dimensions of the input image."
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image",
        "mask"
      ]
    },
    {
      "title": "Flux Pro Kontext Max",
      "description": "FLUX.1 Kontext [max] is a model with greatly improved prompt adherence and typography generation meet premium consistency for editing without compromise on speed.   \n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxProKontextMax",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProKontextMax.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProKontextMax.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image prompt for the omni model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProKontextMax.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Pro Kontext Max Multi",
      "description": "Experimental version of FLUX.1 Kontext [max] with multi image handling capabilities\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxProKontextMaxMulti",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProKontextMaxMulti.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProKontextMaxMulti.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProKontextMaxMulti.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Image prompt for the omni model."
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Pro Kontext Multi",
      "description": "Experimental version of FLUX.1 Kontext [pro] with multi image handling capabilities\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxProKontextMulti",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProKontextMulti.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProKontextMulti.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProKontextMulti.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Image prompt for the omni model."
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Pro Redux",
      "description": "FLUX.1 Pro Redux delivers professional-grade image transformations with the highest quality and safety controls for commercial use.\n    image, transformation, style-transfer, professional, flux, redux\n\n    Use cases:\n    - Create professional-quality image transformations\n    - Apply commercial-grade style transfers\n    - Generate high-fidelity image variations\n    - Produce brand-safe image modifications\n    - Transform images for production use",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxProRedux",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate (1-4)"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProRedux.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "Output format (jpeg or png)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. Needs to match the dimensions of the mask."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProRedux.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "Safety tolerance level (1-6, higher is stricter)"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "How strictly to follow the image structure (1-20)"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform (1-50)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "image_size",
        "guidance_scale"
      ]
    },
    {
      "title": "Flux Pro V11 Redux",
      "description": "FLUX1.1 [pro] Redux is a high-performance endpoint for the FLUX1.1 [pro] model that enables rapid transformation of existing images, delivering high-quality style transfers and image modifications with the core FLUX capabilities.\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxProV11Redux",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProV11Redux.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. Needs to match the dimensions of the mask."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProV11Redux.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Pro V11 Ultra Redux",
      "description": "FLUX1.1 [pro] ultra Redux is a high-performance endpoint for the FLUX1.1 [pro] model that enables rapid transformation of existing images, delivering high-quality style transfers and image modifications with the core FLUX capabilities.\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxProV11UltraRedux",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProV11UltraRedux.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. Needs to match the dimensions of the mask."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProV11UltraRedux.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "image_prompt_strength",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Image Prompt Strength",
          "description": "The strength of the image prompt, between 0 and 1."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "raw",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Raw",
          "description": "Generate less processed, more natural-looking images."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Pro V1 Fill Finetuned",
      "description": "FLUX.1 [pro] Fill Fine-tuned is a high-performance endpoint for the FLUX.1 [pro] model with a fine-tuned LoRA that enables rapid transformation of existing images, delivering high-quality style transfers and image modifications with the core FLUX capabilities.\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxProV1FillFinetuned",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to fill the masked part of the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "finetune_strength",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Finetune Strength",
          "description": "Controls finetune influence. Increase this value if your target concept isn't showing up strongly enough. The optimal setting depends on your finetune and prompt"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProV1FillFinetuned.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "finetune_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Finetune Id",
          "description": "References your specific model"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. Needs to match the dimensions of the mask."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxProV1FillFinetuned.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The mask URL to inpaint the image. Needs to match the dimensions of the input image."
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Pulid",
      "description": "An endpoint for personalized image generation using Flux as per given description.\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxPulid",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "id_weight",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Id Weight",
          "description": "The weight of the ID loss."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "start_step",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Start Step",
          "description": "The number of steps to start the CFG from."
        },
        {
          "name": "reference_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Reference Image Url",
          "description": "URL of image to use for inpainting."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "max_sequence_length",
          "type": {
            "type": "enum",
            "values": [
              "128",
              "256",
              "512"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxPulid.MaxSequenceLength"
          },
          "default": "128",
          "title": "Max Sequence Length",
          "description": "The maximum sequence length for the model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 20,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "true_cfg",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "True Cfg",
          "description": "The weight of the CFG loss."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Schnell Redux",
      "description": "FLUX.1 [schnell] Redux enables rapid transformation of existing images with high-quality style transfers and modifications using the fast FLUX.1 schnell model.\n    image, transformation, style-transfer, fast, flux, redux\n\n    Use cases:\n    - Transform images with artistic style transfers\n    - Apply quick modifications to photos\n    - Create image variations for rapid iteration\n    - Generate stylized versions of existing images\n    - Produce fast image transformations",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxSchnellRedux",
      "properties": [
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate (1-4)"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxSchnellRedux.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "Acceleration speed: 'none', 'regular', or 'high'"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxSchnellRedux.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "Output format (jpeg or png)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable safety checker to filter unsafe content"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform (1-50)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "image_size",
        "num_inference_steps"
      ]
    },
    {
      "title": "Flux Srpo Image To Image",
      "description": "FLUX.1 SRPO [dev]\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxSrpoImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxSrpoImageToImage.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.FluxSrpoImageToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "The strength of the initial image. Higher strength values are better for this model."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Flux Vision Upscaler",
      "description": "Flux Vision Upscaler\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.FluxVisionUpscaler",
      "properties": [
        {
          "name": "guidance",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance",
          "description": "CFG/guidance scale (1-4). Controls how closely the model follows the prompt."
        },
        {
          "name": "creativity",
          "type": {
            "type": "float"
          },
          "default": 0.3,
          "title": "Creativity",
          "description": "The creativity of the model. The higher the creativity, the more the model will deviate from the original. Refers to the denoise strength of the sampling."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to upscale."
        },
        {
          "name": "upscale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Upscale Factor",
          "description": "The upscale factor (1-4x)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed to use for the upscale. If not provided, a random seed will be used."
        },
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 20,
          "title": "Steps",
          "description": "Number of inference steps (4-50)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Gemini 25 Flash Image Edit",
      "description": "Gemini 2.5 Flash Image\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Gemini25FlashImageEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image editing."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "21:9",
              "16:9",
              "3:2",
              "4:3",
              "5:4",
              "1:1",
              "4:5",
              "3:4",
              "2:3",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Gemini25FlashImageEdit.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Gemini25FlashImageEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to use for image-to-image generation or image editing."
        },
        {
          "name": "limit_generations",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Limit Generations",
          "description": "Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Gemini 3 Pro Image Preview Edit",
      "description": "Gemini 3 Pro Image Preview\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Gemini3ProImagePreviewEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image editing."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "enable_web_search",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Web Search",
          "description": "Enable web search for the image generation task. This will allow the model to use the latest information from the web to generate the image."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "1K",
              "2K",
              "4K"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Gemini3ProImagePreviewEdit.Resolution"
          },
          "default": "1K",
          "title": "Resolution",
          "description": "The resolution of the image to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Gemini3ProImagePreviewEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Gemini3ProImagePreviewEdit.SafetyTolerance"
          },
          "default": "4",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for content moderation. 1 is the most strict (blocks most content), 6 is the least strict."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to use for image-to-image generation or image editing."
        },
        {
          "name": "limit_generations",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Limit Generations",
          "description": "Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Gemini Flash Edit",
      "description": "Gemini Flash Edit is a model that can edit single image using a text prompt and a reference image.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.GeminiFlashEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation or editing"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of an input image for editing. If not provided, generates a new image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Gemini Flash Edit Multi",
      "description": "Gemini Flash Edit Multi Image is a model that can edit multiple images using a text prompt and a reference image.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.GeminiFlashEditMulti",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation or editing"
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "List of URLs of input images for editing"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ghiblify",
      "description": "Reimagine and transform your ordinary photos into enchanting Studio Ghibli style artwork\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Ghiblify",
      "properties": [
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed to use for the upscale. If not provided, a random seed will be used."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to upscale."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Glm Image Image To Image",
      "description": "GLM Image image-to-image transforms and modifies images using advanced AI understanding.\n    image, transformation, glm, ai-editing\n\n    Use cases:\n    - Transform images with GLM AI\n    - Apply modifications using advanced understanding\n    - Create variations with GLM model\n    - Generate modified versions\n    - Produce AI-powered transformations",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.GlmImageImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for image generation."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "Output image size."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable NSFW safety checking on the generated images."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GlmImageImageToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "Output image format."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If True, the image will be returned as a base64 data URI instead of a URL."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values make the model follow the prompt more closely."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. The same seed with the same prompt will produce the same image."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "URL(s) of the condition image(s) for image-to-image generation. Supports up to 4 URLs for multi-image references."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "If True, the prompt will be enhanced using an LLM for more detailed and higher quality results."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of diffusion denoising steps. More steps generally produce higher quality images."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Gpt Image 15 Edit",
      "description": "GPT Image 1.5 Edit provides intelligent image editing with GPT-powered understanding and control.\n    image, editing, gpt, intelligent, ai-editing\n\n    Use cases:\n    - Edit images with GPT intelligence\n    - Apply smart modifications to photos\n    - Create intelligent edits\n    - Transform images with language understanding\n    - Produce GPT-powered modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.GptImage15Edit",
      "properties": [
        {
          "name": "background",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "transparent",
              "opaque"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage15Edit.Background"
          },
          "default": "auto",
          "title": "Background",
          "description": "Background for the generated image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "1024x1024",
              "1536x1024",
              "1024x1536"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage15Edit.ImageSize"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "Aspect ratio for the generated image"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation"
        },
        {
          "name": "quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage15Edit.Quality"
          },
          "default": "high",
          "title": "Quality",
          "description": "Quality for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage15Edit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the images"
        },
        {
          "name": "input_fidelity",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage15Edit.InputFidelity"
          },
          "default": "high",
          "title": "Input Fidelity",
          "description": "Input fidelity for the generated image"
        },
        {
          "name": "mask_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image Url",
          "description": "The URL of the mask image to use for the generation. This indicates what part of the image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to use as a reference for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Gpt Image 1 Edit Image",
      "description": "OpenAI's latest image generation and editing model: gpt-1-image.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.GptImage1EditImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "1024x1024",
              "1536x1024",
              "1024x1536"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage1EditImage.ImageSize"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "Aspect ratio for the generated image"
        },
        {
          "name": "input_fidelity",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage1EditImage.InputFidelity"
          },
          "default": "high",
          "title": "Input Fidelity",
          "description": "Input fidelity for the generated image"
        },
        {
          "name": "quality",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "low",
              "medium",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage1EditImage.Quality"
          },
          "default": "auto",
          "title": "Quality",
          "description": "Quality for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage1EditImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the images"
        },
        {
          "name": "background",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "transparent",
              "opaque"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage1EditImage.Background"
          },
          "default": "auto",
          "title": "Background",
          "description": "Background for the generated image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to use as a reference for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Gpt Image 1 Mini Edit",
      "description": "GPT Image 1 Mini\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.GptImage1MiniEdit",
      "properties": [
        {
          "name": "background",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "transparent",
              "opaque"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage1MiniEdit.Background"
          },
          "default": "auto",
          "title": "Background",
          "description": "Background for the generated image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "1024x1024",
              "1536x1024",
              "1024x1536"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage1MiniEdit.ImageSize"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "Aspect ratio for the generated image"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation"
        },
        {
          "name": "quality",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "low",
              "medium",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage1MiniEdit.Quality"
          },
          "default": "auto",
          "title": "Quality",
          "description": "Quality for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.GptImage1MiniEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the images"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to use as a reference for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Hidream E11",
      "description": "Hidream E1 1\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.HidreamE11",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Image Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your initial image when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.HidreamE11.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of an input image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "target_image_description",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Target Image Description",
          "description": "The description of the target image after your edits have been made. Leave this blank to allow the model to use its own imagination."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "low resolution, blur",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Hidream I1 Full Image To Image",
      "description": "HiDream-I1 full is a new open-source image generative foundation model with 17B parameters that achieves state-of-the-art image generation quality within seconds.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.HidreamI1FullImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Setting to None uses the input image's size."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.HidreamI1FullImageToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "A list of LoRAs to apply to the model. Each LoRA specifies its path, scale, and optional weight name."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Strength",
          "description": "Denoising strength for image-to-image generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Image V3 Instruct Edit",
      "description": "Hunyuan Image v3 Instruct Edit allows precise image editing through natural language instructions with advanced understanding.\n    image, editing, hunyuan, instruct, ai-editing\n\n    Use cases:\n    - Edit images using natural language instructions\n    - Modify specific elements in photos with text commands\n    - Apply precise adjustments through conversational editing\n    - Transform images with instruction-based control\n    - Create variations with detailed text guidance",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.HunyuanImageV3InstructEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The desired size of the generated image. If auto, image size will be determined by the model."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.HunyuanImageV3InstructEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible results. If None, a random seed is used."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to use as a reference for the generation. A maximum of 2 images are supported."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Controls how much the model adheres to the prompt. Higher values mean stricter adherence."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Hunyuan World",
      "description": "Hunyuan World\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Hunyuan_World",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for the panorama generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to convert to a panorama."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Iclight V2",
      "description": "An endpoint for re-lighting photos and changing their backgrounds per a given description\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IclightV2",
      "properties": [
        {
          "name": "initial_latent",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Left",
              "Right",
              "Top",
              "Bottom"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IclightV2.InitialLatent"
          },
          "default": "None",
          "title": "Initial Latent",
          "description": "Provide lighting conditions for the model"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "background_threshold",
          "type": {
            "type": "float"
          },
          "default": 0.67,
          "title": "Background Threshold",
          "description": "Threshold for the background removal algorithm. A high threshold will produce sharper masks. Note: This parameter is currently deprecated and has no effect on the output."
        },
        {
          "name": "mask_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image Url",
          "description": "URL of mask to be used for ic-light conditioning image"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "lowres_denoise",
          "type": {
            "type": "float"
          },
          "default": 0.98,
          "title": "Lowres Denoise",
          "description": "Strength for low-resolution pass."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative Prompt for the image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IclightV2.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "hr_downscale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Hr Downscale"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to be used for relighting"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "highres_denoise",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Highres Denoise",
          "description": "Strength for high-resolution pass. Only used if enable_hr_fix is True."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_hr_fix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Hr Fix",
          "description": "Use HR fix"
        },
        {
          "name": "cfg",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Cfg",
          "description": "The real classifier-free-guidance scale for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ideogram Character",
      "description": "Ideogram V3 Character\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramCharacter",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to fill the masked part of the image."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The resolution of the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "AUTO",
              "REALISTIC",
              "FICTION"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramCharacter.Style"
          },
          "default": "AUTO",
          "title": "Style",
          "description": "The style type to generate with. Cannot be used with style_codes."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Determine if MagicPrompt should be used in generating the request or not."
        },
        {
          "name": "rendering_speed",
          "type": {
            "type": "enum",
            "values": [
              "TURBO",
              "BALANCED",
              "QUALITY"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramCharacter.RenderingSpeed"
          },
          "default": "BALANCED",
          "title": "Rendering Speed",
          "description": "The rendering speed to use."
        },
        {
          "name": "reference_mask_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Mask Urls",
          "description": "A set of masks to apply to the character references. Currently only 1 mask is supported, rest will be ignored. (maximum total size 10MB across all character references). The masks should be in JPEG, PNG or WebP format"
        },
        {
          "name": "reference_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Image Urls",
          "description": "A set of images to use as character references. Currently only 1 image is supported, rest will be ignored. (maximum total size 10MB across all character references). The images should be in JPEG, PNG or WebP format"
        },
        {
          "name": "image_urls",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Urls",
          "description": "A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "style_codes",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Codes",
          "description": "A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style"
        },
        {
          "name": "color_palette",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Color Palette",
          "description": "A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ideogram Character Edit",
      "description": "Ideogram V3 Character Edit\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramCharacterEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to fill the masked part of the image."
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "AUTO",
              "REALISTIC",
              "FICTION"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramCharacterEdit.Style"
          },
          "default": "AUTO",
          "title": "Style",
          "description": "The style type to generate with. Cannot be used with style_codes."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Determine if MagicPrompt should be used in generating the request or not."
        },
        {
          "name": "rendering_speed",
          "type": {
            "type": "enum",
            "values": [
              "TURBO",
              "BALANCED",
              "QUALITY"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramCharacterEdit.RenderingSpeed"
          },
          "default": "BALANCED",
          "title": "Rendering Speed",
          "description": "The rendering speed to use."
        },
        {
          "name": "reference_mask_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Mask Urls",
          "description": "A set of masks to apply to the character references. Currently only 1 mask is supported, rest will be ignored. (maximum total size 10MB across all character references). The masks should be in JPEG, PNG or WebP format"
        },
        {
          "name": "reference_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Image Urls",
          "description": "A set of images to use as character references. Currently only 1 image is supported, rest will be ignored. (maximum total size 10MB across all character references). The images should be in JPEG, PNG or WebP format"
        },
        {
          "name": "image_urls",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Urls",
          "description": "A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. MUST have the exact same dimensions (width and height) as the mask image."
        },
        {
          "name": "style_codes",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Codes",
          "description": "A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style"
        },
        {
          "name": "color_palette",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Color Palette",
          "description": "A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The mask URL to inpaint the image. MUST have the exact same dimensions (width and height) as the input image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ideogram Character Remix",
      "description": "Ideogram V3 Character Remix\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramCharacterRemix",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to remix the image with"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The resolution of the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "AUTO",
              "REALISTIC",
              "FICTION"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramCharacterRemix.Style"
          },
          "default": "AUTO",
          "title": "Style",
          "description": "The style type to generate with. Cannot be used with style_codes."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Determine if MagicPrompt should be used in generating the request or not."
        },
        {
          "name": "rendering_speed",
          "type": {
            "type": "enum",
            "values": [
              "TURBO",
              "BALANCED",
              "QUALITY"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramCharacterRemix.RenderingSpeed"
          },
          "default": "BALANCED",
          "title": "Rendering Speed",
          "description": "The rendering speed to use."
        },
        {
          "name": "reference_mask_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Mask Urls",
          "description": "A set of masks to apply to the character references. Currently only 1 mask is supported, rest will be ignored. (maximum total size 10MB across all character references). The masks should be in JPEG, PNG or WebP format"
        },
        {
          "name": "reference_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Image Urls",
          "description": "A set of images to use as character references. Currently only 1 image is supported, rest will be ignored. (maximum total size 10MB across all character references). The images should be in JPEG, PNG or WebP format"
        },
        {
          "name": "image_urls",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Urls",
          "description": "A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to remix"
        },
        {
          "name": "style_codes",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Codes",
          "description": "A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style"
        },
        {
          "name": "color_palette",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Color Palette",
          "description": "A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength of the input image in the remix"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ideogram Upscale",
      "description": "Ideogram Upscale enhances the resolution of the reference image by up to 2X and might enhance the reference image too. Optionally refine outputs with a prompt for guided improvements.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramUpscale",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to upscale the image with"
        },
        {
          "name": "detail",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Detail",
          "description": "The detail of the upscaled image"
        },
        {
          "name": "resemblance",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Resemblance",
          "description": "The resemblance of the upscaled image to the original image"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt with MagicPrompt functionality."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to upscale"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ideogram V2 Edit",
      "description": "Transform existing images with Ideogram V2's editing capabilities. Modify, adjust, and refine images while maintaining high fidelity with precise prompt and mask control.\n    image, editing, inpainting, mask, ideogram, transformation\n\n    Use cases:\n    - Edit specific parts of images with precision\n    - Create targeted image modifications using masks\n    - Refine and enhance image details\n    - Generate contextual image edits\n    - Replace or modify masked regions",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramV2Edit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to fill the masked part of the image"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "general",
              "realistic",
              "design",
              "render_3D",
              "anime"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV2Edit.Style"
          },
          "default": "auto",
          "title": "Style",
          "description": "Style of generated image (auto, general, realistic, design, render_3D, anime)"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt with MagicPrompt functionality"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. Needs to match the dimensions of the mask."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The mask URL to inpaint the image. Needs to match the dimensions of the input image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image",
        "mask"
      ]
    },
    {
      "title": "Ideogram V2 Remix",
      "description": "Reimagine existing images with Ideogram V2's remix feature. Create variations and adaptations while preserving core elements through prompt guidance and strength control.\n    image, remix, variation, creativity, ideogram, adaptation\n\n    Use cases:\n    - Create artistic variations of images\n    - Generate style-transferred versions\n    - Produce creative image adaptations\n    - Transform images while preserving key elements\n    - Generate alternative interpretations",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramV2Remix",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to remix the image with"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "10:16",
              "16:10",
              "9:16",
              "16:9",
              "4:3",
              "3:4",
              "1:1",
              "1:3",
              "3:1",
              "3:2",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV2Remix.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "general",
              "realistic",
              "design",
              "render_3D",
              "anime"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV2Remix.Style"
          },
          "default": "auto",
          "title": "Style",
          "description": "Style of generated image (auto, general, realistic, design, render_3D, anime)"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt with MagicPrompt functionality"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to remix"
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength of the input image in the remix (0-1, higher = more variation)"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image",
        "strength"
      ]
    },
    {
      "title": "Ideogram V2 Turbo Edit",
      "description": "Edit images faster with Ideogram V2 Turbo. Quick modifications and adjustments while preserving the high-quality standards and realistic outputs of Ideogram.\n    editing, transformation, image-to-image, img2img, fast\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramV2TurboEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to fill the masked part of the image."
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "general",
              "realistic",
              "design",
              "render_3D",
              "anime"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV2TurboEdit.Style"
          },
          "default": "auto",
          "title": "Style",
          "description": "The style of the generated image"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt with MagicPrompt functionality."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. Needs to match the dimensions of the mask."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The mask URL to inpaint the image. Needs to match the dimensions of the input image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ideogram V2 Turbo Remix",
      "description": "Rapidly create image variations with Ideogram V2 Turbo Remix. Fast and efficient reimagining of existing images while maintaining creative control through prompt guidance.\n    editing, transformation, image-to-image, img2img, fast\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramV2TurboRemix",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to remix the image with"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "10:16",
              "16:10",
              "9:16",
              "16:9",
              "4:3",
              "3:4",
              "1:1",
              "1:3",
              "3:1",
              "3:2",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV2TurboRemix.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "general",
              "realistic",
              "design",
              "render_3D",
              "anime"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV2TurboRemix.Style"
          },
          "default": "auto",
          "title": "Style",
          "description": "The style of the generated image"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt with MagicPrompt functionality."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to remix"
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength of the input image in the remix"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ideogram V2 a Remix",
      "description": "Create variations of existing images with Ideogram V2A Remix while maintaining creative control through prompt guidance.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramV2aRemix",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to remix the image with"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "10:16",
              "16:10",
              "9:16",
              "16:9",
              "4:3",
              "3:4",
              "1:1",
              "1:3",
              "3:1",
              "3:2",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV2aRemix.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "general",
              "realistic",
              "design",
              "render_3D",
              "anime"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV2aRemix.Style"
          },
          "default": "auto",
          "title": "Style",
          "description": "The style of the generated image"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt with MagicPrompt functionality."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to remix"
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength of the input image in the remix"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ideogram V2 a Turbo Remix",
      "description": "Rapidly create image variations with Ideogram V2A Turbo Remix. Fast and efficient reimagining of existing images while maintaining creative control through prompt guidance.\n    editing, transformation, image-to-image, img2img, fast\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramV2aTurboRemix",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to remix the image with"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "10:16",
              "16:10",
              "9:16",
              "16:9",
              "4:3",
              "3:4",
              "1:1",
              "1:3",
              "3:1",
              "3:2",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV2aTurboRemix.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "general",
              "realistic",
              "design",
              "render_3D",
              "anime"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV2aTurboRemix.Style"
          },
          "default": "auto",
          "title": "Style",
          "description": "The style of the generated image"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt with MagicPrompt functionality."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to remix"
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength of the input image in the remix"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ideogram V3 Edit",
      "description": "Transform images with Ideogram V3's enhanced editing capabilities. Latest generation editing with improved quality, control, and style consistency.\n    image, editing, inpainting, mask, ideogram, v3\n\n    Use cases:\n    - Edit images with the latest Ideogram technology\n    - Apply high-fidelity masked edits\n    - Generate professional image modifications\n    - Create precise content-aware fills\n    - Refine image details with advanced controls",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramV3Edit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to fill the masked part of the image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "style_preset",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Preset",
          "description": "Style preset for generation. The chosen style preset will guide the generation."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Determine if MagicPrompt should be used in generating the request or not."
        },
        {
          "name": "rendering_speed",
          "type": {
            "type": "enum",
            "values": [
              "TURBO",
              "BALANCED",
              "QUALITY"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV3Edit.RenderingSpeed"
          },
          "default": "BALANCED",
          "title": "Rendering Speed",
          "description": "The rendering speed to use."
        },
        {
          "name": "style_codes",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Codes",
          "description": "A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style"
        },
        {
          "name": "color_palette",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Color Palette",
          "description": "A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. MUST have the exact same dimensions (width and height) as the mask image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "image_urls",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Urls",
          "description": "A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format"
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The mask URL to inpaint the image. MUST have the exact same dimensions (width and height) as the input image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image",
        "mask"
      ]
    },
    {
      "title": "Ideogram V3 Reframe",
      "description": "Extend existing images with Ideogram V3's reframe feature. Create expanded versions and adaptations while preserving main image and adding new creative directions through prompt guidance.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramV3Reframe",
      "properties": [
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The resolution for the reframed output image"
        },
        {
          "name": "style",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style",
          "description": "The style type to generate with. Cannot be used with style_codes."
        },
        {
          "name": "style_preset",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Preset",
          "description": "Style preset for generation. The chosen style preset will guide the generation."
        },
        {
          "name": "rendering_speed",
          "type": {
            "type": "enum",
            "values": [
              "TURBO",
              "BALANCED",
              "QUALITY"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV3Reframe.RenderingSpeed"
          },
          "default": "BALANCED",
          "title": "Rendering Speed",
          "description": "The rendering speed to use."
        },
        {
          "name": "style_codes",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Codes",
          "description": "A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style"
        },
        {
          "name": "color_palette",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Color Palette",
          "description": "A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to reframe"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        },
        {
          "name": "image_urls",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Urls",
          "description": "A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ideogram V3 Remix",
      "description": "Reimagine existing images with Ideogram V3's remix feature. Create variations and adaptations while preserving core elements and adding new creative directions through prompt guidance.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramV3Remix",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to remix the image with"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The resolution of the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style",
          "description": "The style type to generate with. Cannot be used with style_codes."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Determine if MagicPrompt should be used in generating the request or not."
        },
        {
          "name": "rendering_speed",
          "type": {
            "type": "enum",
            "values": [
              "TURBO",
              "BALANCED",
              "QUALITY"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV3Remix.RenderingSpeed"
          },
          "default": "BALANCED",
          "title": "Rendering Speed",
          "description": "The rendering speed to use."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Urls",
          "description": "A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to remix"
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Strength of the input image in the remix"
        },
        {
          "name": "color_palette",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Color Palette",
          "description": "A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)"
        },
        {
          "name": "style_codes",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Codes",
          "description": "A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ideogram V3 Replace Background",
      "description": "Replace backgrounds existing images with Ideogram V3's replace background feature. Create variations and adaptations while preserving core elements and adding new creative directions through prompt guidance.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IdeogramV3ReplaceBackground",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Cyber punk city with neon lights and skyscrappers"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "style",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style",
          "description": "The style type to generate with. Cannot be used with style_codes."
        },
        {
          "name": "style_preset",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Preset",
          "description": "Style preset for generation. The chosen style preset will guide the generation."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Determine if MagicPrompt should be used in generating the request or not."
        },
        {
          "name": "rendering_speed",
          "type": {
            "type": "enum",
            "values": [
              "TURBO",
              "BALANCED",
              "QUALITY"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IdeogramV3ReplaceBackground.RenderingSpeed"
          },
          "default": "BALANCED",
          "title": "Rendering Speed",
          "description": "The rendering speed to use."
        },
        {
          "name": "style_codes",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Codes",
          "description": "A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style"
        },
        {
          "name": "color_palette",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Color Palette",
          "description": "A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL whose background needs to be replaced"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        },
        {
          "name": "image_urls",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Urls",
          "description": "A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image 2 Pixel",
      "description": "Image2Pixel\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Image2Pixel",
      "properties": [
        {
          "name": "cleanup_morph",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Cleanup Morph",
          "description": "Apply morphological operations to remove noise."
        },
        {
          "name": "auto_color_detect",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Color Detect",
          "description": "Enable automatic detection of optimal number of colors."
        },
        {
          "name": "alpha_threshold",
          "type": {
            "type": "int"
          },
          "default": 128,
          "title": "Alpha Threshold",
          "description": "Alpha binarization threshold (0-255)."
        },
        {
          "name": "snap_grid",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Snap Grid",
          "description": "Align output to the pixel grid."
        },
        {
          "name": "fixed_palette",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Fixed Palette",
          "description": "Optional fixed color palette as hex strings (e.g., ['#000000', '#ffffff'])."
        },
        {
          "name": "scale",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Scale",
          "description": "Force a specific pixel scale. If None, auto-detect."
        },
        {
          "name": "cleanup_jaggy",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Cleanup Jaggy",
          "description": "Remove isolated diagonal pixels (jaggy edge cleanup)."
        },
        {
          "name": "trim_borders",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Trim Borders",
          "description": "Trim borders of the image."
        },
        {
          "name": "background_tolerance",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Background Tolerance",
          "description": "Background tolerance (0-255)."
        },
        {
          "name": "detect_method",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "runs",
              "edge"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Image2Pixel.DetectMethod"
          },
          "default": "auto",
          "title": "Detect Method",
          "description": "Scale detection method to use."
        },
        {
          "name": "transparent_background",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Transparent Background",
          "description": "Remove background of the image. This will check for contiguous color regions from the edges after correction and make them transparent."
        },
        {
          "name": "downscale_method",
          "type": {
            "type": "enum",
            "values": [
              "dominant",
              "median",
              "mode",
              "mean",
              "content-adaptive"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Image2Pixel.DownscaleMethod"
          },
          "default": "dominant",
          "title": "Downscale Method",
          "description": "Downscaling method to produce the pixel-art output."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to process into improved pixel art"
        },
        {
          "name": "background_mode",
          "type": {
            "type": "enum",
            "values": [
              "edges",
              "corners",
              "midpoints"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Image2Pixel.BackgroundMode"
          },
          "default": "corners",
          "title": "Background Mode",
          "description": "Controls where to flood-fill from when removing the background."
        },
        {
          "name": "max_colors",
          "type": {
            "type": "int"
          },
          "default": 32,
          "title": "Max Colors",
          "description": "Maximum number of colors in the output palette. Set None to disable limit."
        },
        {
          "name": "dominant_color_threshold",
          "type": {
            "type": "float"
          },
          "default": 0.05,
          "title": "Dominant Color Threshold",
          "description": "Dominant color threshold (0.0-1.0)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image 2 svg",
      "description": "Image2SVG transforms raster images into clean vector graphics, preserving visual quality while enabling scalable, customizable SVG outputs with precise control over detail levels.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Image2svg",
      "properties": [
        {
          "name": "splice_threshold",
          "type": {
            "type": "int"
          },
          "default": 45,
          "title": "Splice Threshold",
          "description": "Splice threshold for joining paths"
        },
        {
          "name": "hierarchical",
          "type": {
            "type": "enum",
            "values": [
              "stacked",
              "cutout"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Image2svg.Hierarchical"
          },
          "default": "stacked",
          "title": "Hierarchical",
          "description": "Hierarchical mode: stacked or cutout"
        },
        {
          "name": "color_precision",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Color Precision",
          "description": "Color quantization level"
        },
        {
          "name": "colormode",
          "type": {
            "type": "enum",
            "values": [
              "color",
              "binary"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Image2svg.Colormode"
          },
          "default": "color",
          "title": "Colormode",
          "description": "Choose between color or binary (black and white) output"
        },
        {
          "name": "max_iterations",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Max Iterations",
          "description": "Maximum number of iterations for optimization"
        },
        {
          "name": "length_threshold",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Length Threshold",
          "description": "Length threshold for curves/lines"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image to convert to SVG"
        },
        {
          "name": "mode",
          "type": {
            "type": "enum",
            "values": [
              "spline",
              "polygon"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Image2svg.Mode"
          },
          "default": "spline",
          "title": "Mode",
          "description": "Mode: spline (curved) or polygon (straight lines)"
        },
        {
          "name": "corner_threshold",
          "type": {
            "type": "int"
          },
          "default": 60,
          "title": "Corner Threshold",
          "description": "Corner detection threshold in degrees"
        },
        {
          "name": "path_precision",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "Path Precision",
          "description": "Decimal precision for path coordinates"
        },
        {
          "name": "filter_speckle",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Filter Speckle",
          "description": "Filter out small speckles and noise"
        },
        {
          "name": "layer_difference",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Layer Difference",
          "description": "Layer difference threshold for hierarchical mode"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Age Modify",
      "description": "Age Modify\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2AgeModify",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Portrait image URL for age modification"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output (default: 3:4 for portraits)"
        },
        {
          "name": "preserve_identity",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Identity"
        },
        {
          "name": "target_age",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Target Age"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 City Teleport",
      "description": "City Teleport\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2CityTeleport",
      "properties": [
        {
          "name": "city_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "City Image Url",
          "description": "Optional city background image URL. When provided, the person will be blended into this custom scene."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "city_name",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "City Name",
          "description": "City name (used when city_image_url is not provided)"
        },
        {
          "name": "photo_shot",
          "type": {
            "type": "enum",
            "values": [
              "extreme_close_up",
              "close_up",
              "medium_close_up",
              "medium_shot",
              "medium_long_shot",
              "long_shot",
              "extreme_long_shot",
              "full_body"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2CityTeleport.PhotoShot"
          },
          "default": "medium_shot",
          "title": "Photo Shot",
          "description": "Type of photo shot"
        },
        {
          "name": "camera_angle",
          "type": {
            "type": "enum",
            "values": [
              "eye_level",
              "low_angle",
              "high_angle",
              "dutch_angle",
              "birds_eye_view",
              "worms_eye_view",
              "overhead",
              "side_angle"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2CityTeleport.CameraAngle"
          },
          "default": "eye_level",
          "title": "Camera Angle",
          "description": "Camera angle for the shot"
        },
        {
          "name": "person_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Person Image Url",
          "description": "Person photo URL"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Expression Change",
      "description": "Expression Change\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2ExpressionChange",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output (default: 3:4 for portraits)"
        },
        {
          "name": "target_expression",
          "type": {
            "type": "enum",
            "values": [
              "smile",
              "surprise",
              "glare",
              "panic",
              "shyness",
              "laugh",
              "cry",
              "angry",
              "sad",
              "happy",
              "excited",
              "shocked",
              "confused",
              "focused",
              "dreamy",
              "serious",
              "playful",
              "mysterious",
              "confident",
              "thoughtful"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2ExpressionChange.TargetExpression"
          },
          "default": "smile",
          "title": "Target Expression"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Portrait image URL for expression change"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Hair Change",
      "description": "Hair Change\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2HairChange",
      "properties": [
        {
          "name": "target_hairstyle",
          "type": {
            "type": "enum",
            "values": [
              "short_hair",
              "medium_long_hair",
              "long_hair",
              "curly_hair",
              "wavy_hair",
              "high_ponytail",
              "bun",
              "bob_cut",
              "pixie_cut",
              "braids",
              "straight_hair",
              "afro",
              "dreadlocks",
              "buzz_cut",
              "mohawk",
              "bangs",
              "side_part",
              "middle_part"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2HairChange.TargetHairstyle"
          },
          "default": "long_hair",
          "title": "Target Hairstyle"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output (default: 3:4 for portraits)"
        },
        {
          "name": "hair_color",
          "type": {
            "type": "enum",
            "values": [
              "black",
              "dark_brown",
              "light_brown",
              "blonde",
              "platinum_blonde",
              "red",
              "auburn",
              "gray",
              "silver",
              "blue",
              "green",
              "purple",
              "pink",
              "rainbow",
              "natural",
              "highlights",
              "ombre",
              "balayage"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2HairChange.HairColor"
          },
          "default": "natural",
          "title": "Hair Color"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Portrait image URL for hair change"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Headshot Photo",
      "description": "Headshot Generator\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2HeadshotPhoto",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output (default: 3:4 for portraits)"
        },
        {
          "name": "background_style",
          "type": {
            "type": "enum",
            "values": [
              "professional",
              "corporate",
              "clean",
              "gradient"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2HeadshotPhoto.BackgroundStyle"
          },
          "default": "professional",
          "title": "Background Style"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Portrait image URL to convert to professional headshot"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Makeup Application",
      "description": "Makeup Changer\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2MakeupApplication",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output (default: 3:4 for portraits)"
        },
        {
          "name": "intensity",
          "type": {
            "type": "enum",
            "values": [
              "light",
              "medium",
              "heavy",
              "dramatic"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2MakeupApplication.Intensity"
          },
          "default": "medium",
          "title": "Intensity"
        },
        {
          "name": "makeup_style",
          "type": {
            "type": "enum",
            "values": [
              "natural",
              "glamorous",
              "smoky_eyes",
              "bold_lips",
              "no_makeup",
              "remove_makeup",
              "dramatic",
              "bridal",
              "professional",
              "korean_style",
              "artistic"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2MakeupApplication.MakeupStyle"
          },
          "default": "natural",
          "title": "Makeup Style"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Portrait image URL for makeup application"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Object Removal",
      "description": "Object Removal\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2ObjectRemoval",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "object_to_remove",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Object To Remove",
          "description": "Object to remove"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL containing object to remove"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Outpaint",
      "description": "Image Outpaint\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2Outpaint",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional prompt to guide the outpainting. If provided, it will be appended to the base outpaint instruction. Example: 'with a beautiful sunset in the background'"
        },
        {
          "name": "expand_right",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Expand Right",
          "description": "Number of pixels to add as black margin on the right side (0-700)."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "zoom_out_percentage",
          "type": {
            "type": "float"
          },
          "default": 20,
          "title": "Zoom Out Percentage",
          "description": "Percentage to zoom out the image. If set, the image will be scaled down by this percentage and black margins will be added to maintain original size. Example: 50 means the image will be 50% of original size with black margins filling the rest."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "jpg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2Outpaint.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL to outpaint"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If True, the function will wait for the image to be generated and uploaded before returning the response. If False, the function will return immediately and the image will be generated asynchronously."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "expand_left",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Expand Left",
          "description": "Number of pixels to add as black margin on the left side (0-700)."
        },
        {
          "name": "expand_bottom",
          "type": {
            "type": "int"
          },
          "default": 400,
          "title": "Expand Bottom",
          "description": "Number of pixels to add as black margin on the bottom side (0-700)."
        },
        {
          "name": "expand_top",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Expand Top",
          "description": "Number of pixels to add as black margin on the top side (0-700)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Perspective",
      "description": "Perspective Change\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2Perspective",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "target_perspective",
          "type": {
            "type": "enum",
            "values": [
              "front",
              "left_side",
              "right_side",
              "back",
              "top_down",
              "bottom_up",
              "birds_eye",
              "three_quarter_left",
              "three_quarter_right"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2Perspective.TargetPerspective"
          },
          "default": "front",
          "title": "Target Perspective"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL for perspective change"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Photo Restoration",
      "description": "Photo Restoration\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2PhotoRestoration",
      "properties": [
        {
          "name": "enhance_resolution",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enhance Resolution"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output (default: 4:3 for classic photos)"
        },
        {
          "name": "remove_scratches",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Remove Scratches"
        },
        {
          "name": "fix_colors",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Fix Colors"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Old or damaged photo URL to restore"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Photography Effects",
      "description": "Photography Effects\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2PhotographyEffects",
      "properties": [
        {
          "name": "effect_type",
          "type": {
            "type": "enum",
            "values": [
              "film",
              "vintage_film",
              "portrait_photography",
              "fashion_photography",
              "street_photography",
              "sepia_tone",
              "film_grain",
              "light_leaks",
              "vignette_effect",
              "instant_camera",
              "golden_hour",
              "dramatic_lighting",
              "soft_focus",
              "bokeh_effect",
              "high_contrast",
              "double_exposure"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2PhotographyEffects.EffectType"
          },
          "default": "film",
          "title": "Effect Type"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL for photography effects"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Portrait Enhance",
      "description": "Portrait Enhance\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2PortraitEnhance",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output (default: 3:4 for portraits)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Portrait image URL to enhance"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Product Holding",
      "description": "Product Holding\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2ProductHolding",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "product_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Product Image Url",
          "description": "Image URL of the product to be held by the person"
        },
        {
          "name": "person_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Person Image Url",
          "description": "Image URL of the person who will hold the product"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Product Photography",
      "description": "Product Photography\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2ProductPhotography",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "product_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Product Image Url",
          "description": "Image URL of the product to create professional studio photography"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Relighting",
      "description": "Relighting\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2Relighting",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "lighting_style",
          "type": {
            "type": "enum",
            "values": [
              "natural",
              "studio",
              "golden_hour",
              "blue_hour",
              "dramatic",
              "soft",
              "hard",
              "backlight",
              "side_light",
              "front_light",
              "rim_light",
              "sunset",
              "sunrise",
              "neon",
              "candlelight",
              "moonlight",
              "spotlight",
              "ambient"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2Relighting.LightingStyle"
          },
          "default": "natural",
          "title": "Lighting Style"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL for relighting"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Style Transfer",
      "description": "Style Transfer\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2StyleTransfer",
      "properties": [
        {
          "name": "target_style",
          "type": {
            "type": "enum",
            "values": [
              "anime_character",
              "cartoon_3d",
              "hand_drawn_animation",
              "cyberpunk_future",
              "anime_game_style",
              "comic_book_animation",
              "animated_series",
              "cartoon_animation",
              "lofi_aesthetic",
              "cottagecore",
              "dark_academia",
              "y2k",
              "vaporwave",
              "liminal_space",
              "weirdcore",
              "dreamcore",
              "synthwave",
              "outrun",
              "photorealistic",
              "hyperrealistic",
              "digital_art",
              "concept_art",
              "impressionist",
              "anime",
              "pixel_art",
              "claymation"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2StyleTransfer.TargetStyle"
          },
          "default": "impressionist",
          "title": "Target Style"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "style_reference_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Style Reference Image Url",
          "description": "Optional reference image URL. When provided, the style will be inferred from this image instead of the selected preset style."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL for style transfer"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Texture Transform",
      "description": "Texture Transform\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2TextureTransform",
      "properties": [
        {
          "name": "target_texture",
          "type": {
            "type": "enum",
            "values": [
              "cotton",
              "denim",
              "wool",
              "felt",
              "wood",
              "leather",
              "velvet",
              "stone",
              "marble",
              "ceramic",
              "concrete",
              "brick",
              "clay",
              "foam",
              "glass",
              "metal",
              "silk",
              "fabric",
              "crystal",
              "rubber",
              "plastic",
              "lace"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageAppsV2TextureTransform.TargetTexture"
          },
          "default": "marble",
          "title": "Target Texture"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL for texture transformation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Apps V2 Virtual Try On",
      "description": "Virtual Try-on\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageAppsV2VirtualTryOn",
      "properties": [
        {
          "name": "preserve_pose",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Pose"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for 4K output (default: 3:4 for fashion)"
        },
        {
          "name": "clothing_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Clothing Image Url",
          "description": "Clothing photo URL"
        },
        {
          "name": "person_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Person Image Url",
          "description": "Person photo URL"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Age Progression",
      "description": "See how you or others might look at different ages, from younger to older, while preserving core facial features.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingAgeProgression",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "20 years older",
          "title": "Prompt",
          "description": "The age change to apply."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingAgeProgression.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingAgeProgression.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image prompt for the omni model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingAgeProgression.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Baby Version",
      "description": "Transform any person into their baby version, while preserving the original pose and expression with childlike features.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingBabyVersion",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingBabyVersion.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingBabyVersion.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to transform into a baby version."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingBabyVersion.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Background Change",
      "description": "Replace your photo's background with any scene you desire, from beach sunsets to urban landscapes, with perfect lighting and shadows\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingBackgroundChange",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "beach sunset with palm trees",
          "title": "Prompt",
          "description": "The desired background to apply."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingBackgroundChange.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingBackgroundChange.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image prompt for the omni model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingBackgroundChange.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Broccoli Haircut",
      "description": "Transform your character's hair into broccoli style while keeping the original characters likeness\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingBroccoliHaircut",
      "properties": [
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to apply broccoli haircut style."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Cartoonify",
      "description": "Transform your photos into vibrant cool cartoons with bold outlines and rich colors.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingCartoonify",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingCartoonify.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingCartoonify.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image prompt for the omni model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingCartoonify.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Color Correction",
      "description": "Perfect your photos with professional color grading, balanced tones, and vibrant yet natural colors\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingColorCorrection",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingColorCorrection.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingColorCorrection.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image prompt for the omni model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingColorCorrection.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Expression Change",
      "description": "Change facial expressions in photos to any emotion you desire, from smiles to serious looks.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingExpressionChange",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "sad",
          "title": "Prompt",
          "description": "The desired facial expression to apply."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingExpressionChange.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingExpressionChange.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image prompt for the omni model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingExpressionChange.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Face Enhancement",
      "description": "Enhance facial features with professional retouching while maintaining a natural, realistic look\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingFaceEnhancement",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingFaceEnhancement.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingFaceEnhancement.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image prompt for the omni model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingFaceEnhancement.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Hair Change",
      "description": "Experiment with different hairstyles, from bald to any style you can imagine, while maintaining natural lighting and realistic results.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingHairChange",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "bald",
          "title": "Prompt",
          "description": "The desired hair style to apply."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingHairChange.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingHairChange.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image prompt for the omni model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingHairChange.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Object Removal",
      "description": "Remove unwanted objects or people from your photos while seamlessly blending the background.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingObjectRemoval",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "background people",
          "title": "Prompt",
          "description": "Specify which objects to remove from the image."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingObjectRemoval.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingObjectRemoval.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image prompt for the omni model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingObjectRemoval.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Photo Restoration",
      "description": "Restore and enhance old or damaged photos by removing imperfections, adding color while preserving the original character and details of the image.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingPhotoRestoration",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingPhotoRestoration.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingPhotoRestoration.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the old or damaged photo to restore."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingPhotoRestoration.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Plushie Style",
      "description": "Transform your photos into cool plushies while keeping the original characters likeness\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingPlushieStyle",
      "properties": [
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to convert to plushie style."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Professional Photo",
      "description": "Turn your casual photos into stunning professional studio portraits with perfect lighting and high-end photography style.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingProfessionalPhoto",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingProfessionalPhoto.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingProfessionalPhoto.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image prompt for the omni model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingProfessionalPhoto.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Realism",
      "description": "Image Editing\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingRealism",
      "properties": [
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.6,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to enhance with realism details."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Reframe",
      "description": "The reframe endpoint intelligently adjusts an image's aspect ratio while preserving the main subject's position, composition, pose, and perspective\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingReframe",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingReframe.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The desired aspect ratio for the reframed image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingReframe.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the old or damaged photo to restore."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingReframe.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Retouch",
      "description": "Image Editing\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingRetouch",
      "properties": [
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to retouch."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Scene Composition",
      "description": "Place your subject in any scene you imagine, from enchanted forests to urban settings, with professional composition and lighting\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingSceneComposition",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "enchanted forest",
          "title": "Prompt",
          "description": "Describe the scene where you want to place the subject."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingSceneComposition.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingSceneComposition.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image prompt for the omni model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingSceneComposition.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Style Transfer",
      "description": "Transform your photos into artistic masterpieces inspired by famous styles like Van Gogh's Starry Night or any artistic style you choose.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingStyleTransfer",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Van Gogh's Starry Night",
          "title": "Prompt",
          "description": "The artistic style to apply."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingStyleTransfer.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingStyleTransfer.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image prompt for the omni model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingStyleTransfer.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Text Removal",
      "description": "Remove all text and writing from images while preserving the background and natural appearance.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingTextRemoval",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingTextRemoval.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingTextRemoval.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image containing text to be removed."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingTextRemoval.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Time Of Day",
      "description": "Transform your photos to any time of day, from golden hour to midnight, with appropriate lighting and atmosphere.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingTimeOfDay",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "golden hour",
          "title": "Prompt",
          "description": "The time of day to transform the scene to."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingTimeOfDay.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingTimeOfDay.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image prompt for the omni model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingTimeOfDay.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Weather Effect",
      "description": "Add realistic weather effects like snowfall, rain, or fog to your photos while maintaining the scene's mood.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingWeatherEffect",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "heavy snowfall",
          "title": "Prompt",
          "description": "The weather effect to apply."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingWeatherEffect.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingWeatherEffect.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image prompt for the omni model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImageEditingWeatherEffect.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 6 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Wojak Style",
      "description": "Transform your photos into wojak style while keeping the original characters likeness\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingWojakStyle",
      "properties": [
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to convert to wojak style."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Editing Youtube Thumbnails",
      "description": "Generate YouTube thumbnails with custom text\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageEditingYoutubeThumbnails",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Generate youtube thumbnails",
          "title": "Prompt",
          "description": "The text to include in the YouTube thumbnail."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to convert to YouTube thumbnail style."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Preprocessors Depth Anything V2",
      "description": "Depth Anything v2 preprocessor.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImagePreprocessorsDepthAnythingV2",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Preprocessors Hed",
      "description": "Holistically-Nested Edge Detection (HED) preprocessor.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImagePreprocessorsHed",
      "properties": [
        {
          "name": "safe",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Safe",
          "description": "Whether to use the safe version of the HED detector"
        },
        {
          "name": "scribble",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Scribble",
          "description": "Whether to use the scribble version of the HED detector"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Preprocessors Lineart",
      "description": "Line art preprocessor.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImagePreprocessorsLineart",
      "properties": [
        {
          "name": "coarse",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Coarse",
          "description": "Whether to use the coarse model"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Preprocessors Midas",
      "description": "MiDaS depth estimation preprocessor.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImagePreprocessorsMidas",
      "properties": [
        {
          "name": "a",
          "type": {
            "type": "float"
          },
          "default": 6.283185307179586,
          "title": "A",
          "description": "A parameter for the MiDaS detector"
        },
        {
          "name": "background_threshold",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Background Threshold",
          "description": "Background threshold for the MiDaS detector"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Preprocessors Mlsd",
      "description": "M-LSD line segment detection preprocessor.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImagePreprocessorsMlsd",
      "properties": [
        {
          "name": "distance_threshold",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Distance Threshold",
          "description": "Distance threshold for the MLSD detector"
        },
        {
          "name": "score_threshold",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Score Threshold",
          "description": "Score threshold for the MLSD detector"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Preprocessors Pidi",
      "description": "PIDI (Pidinet) preprocessor.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImagePreprocessorsPidi",
      "properties": [
        {
          "name": "safe",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Safe",
          "description": "Whether to use the safe version of the Pidi detector"
        },
        {
          "name": "apply_filter",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Apply Filter",
          "description": "Whether to apply the filter to the image."
        },
        {
          "name": "scribble",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Scribble",
          "description": "Whether to use the scribble version of the Pidi detector"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Preprocessors Sam",
      "description": "Segment Anything Model (SAM) preprocessor.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImagePreprocessorsSam",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Preprocessors Scribble",
      "description": "Scribble preprocessor.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImagePreprocessorsScribble",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "HED",
              "PiDi"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ImagePreprocessorsScribble.Model"
          },
          "default": "HED",
          "title": "Model",
          "description": "The model to use for the Scribble detector"
        },
        {
          "name": "safe",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Safe",
          "description": "Whether to use the safe version of the Scribble detector"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Preprocessors Teed",
      "description": "TEED (Temporal Edge Enhancement Detection) preprocessor.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImagePreprocessorsTeed",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Image Preprocessors Zoe",
      "description": "ZoeDepth preprocessor.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImagePreprocessorsZoe",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Imageutils Depth",
      "description": "Create depth maps using Midas depth estimation.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageutilsDepth",
      "properties": [
        {
          "name": "bg_th",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Bg Th",
          "description": "bg_th"
        },
        {
          "name": "a",
          "type": {
            "type": "float"
          },
          "default": 6.283185307179586,
          "title": "A",
          "description": "a"
        },
        {
          "name": "depth_and_normal",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Depth And Normal",
          "description": "depth_and_normal"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Input image url."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Imageutils Marigold Depth",
      "description": "Create depth maps using Marigold depth estimation.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageutilsMarigoldDepth",
      "properties": [
        {
          "name": "ensemble_size",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Ensemble Size",
          "description": "Number of predictions to average over. Defaults to `10`. The higher the number, the more accurate the result, but the slower the inference."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps. Defaults to `10`. The higher the number, the more accurate the result, but the slower the inference."
        },
        {
          "name": "processing_res",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Processing Res",
          "description": "Maximum processing resolution. Defaults `0` which means it uses the size of the input image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Input image url."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Imageutils Rembg",
      "description": "Remove the background from an image.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ImageutilsRembg",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "crop_to_bbox",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Crop To Bbox",
          "description": "If set to true, the resulting image be cropped to a bounding box around the subject"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Input image url."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Inpaint",
      "description": "Inpaint images with SD and SDXL\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Inpaint",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Input image for img2img or inpaint mode"
        },
        {
          "name": "model_name",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model Name",
          "description": "URL or HuggingFace ID of the base model to generate the image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Increasing the amount of steps tells Stable Diffusion that it should take more steps to generate your final result which can increase the amount of detail in your image."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "Input mask for inpaint mode. Black areas will be preserved, white areas will be inpainted."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Instant Character",
      "description": "InstantCharacter creates high-quality, consistent characters from text prompts, supporting diverse poses, styles, and appearances with strong identity control.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.InstantCharacter",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Scale",
          "description": "The scale of the subject image. Higher values will make the subject image more prominent in the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.InstantCharacter.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. Needs to match the dimensions of the mask."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Invisible Watermark",
      "description": "Invisible Watermark is a model that can add an invisible watermark to an image.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.InvisibleWatermark",
      "properties": [
        {
          "name": "decode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Decode",
          "description": "Whether to decode a watermark from the image instead of encoding"
        },
        {
          "name": "watermark",
          "type": {
            "type": "str"
          },
          "default": "watermark",
          "title": "Watermark",
          "description": "Text to use as watermark (for encoding only)"
        },
        {
          "name": "length",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Length",
          "description": "Length of watermark bits to decode (required when decode=True)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to be watermarked or decoded"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ip Adapter Face Id",
      "description": "High quality zero-shot personalization\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.IpAdapterFaceId",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "face_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Face Image Url",
          "description": "An image of a face to match. If an image with a size of 640x640 is not provided, it will be scaled and cropped to that size."
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Width",
          "description": "The width of the generated image."
        },
        {
          "name": "face_id_det_size",
          "type": {
            "type": "int"
          },
          "default": 640,
          "title": "Face Id Det Size",
          "description": "The size of the face detection model. The higher the number the more accurate the detection will be but it will also take longer to run. The higher the number the more likely it will fail to find a face as well. Lower it if you are having trouble finding a face in the image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, low resolution, bad, ugly, low quality, pixelated, interpolated, compression artifacts, noisey, grainy",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Height",
          "description": "The height of the generated image."
        },
        {
          "name": "num_samples",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Samples",
          "description": "The number of samples for face id. The more samples the better the image will be but it will also take longer to generate. Default is 4."
        },
        {
          "name": "base_sdxl_model_repo",
          "type": {
            "type": "str"
          },
          "default": "SG161222/RealVisXL_V3.0",
          "title": "Base Sdxl Model Repo",
          "description": "The URL to the base SDXL model. Default is SG161222/RealVisXL_V3.0"
        },
        {
          "name": "base_1_5_model_repo",
          "type": {
            "type": "str"
          },
          "default": "SG161222/Realistic_Vision_V4.0_noVAE",
          "title": "Base 1 5 Model Repo",
          "description": "The URL to the base 1.5 model. Default is SG161222/Realistic_Vision_V4.0_noVAE"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for generating the image. The more steps the better the image will be but it will also take longer to generate."
        },
        {
          "name": "model_type",
          "type": {
            "type": "enum",
            "values": [
              "1_5-v1",
              "1_5-v1-plus",
              "1_5-v2-plus",
              "SDXL-v1",
              "SDXL-v2-plus",
              "1_5-auraface-v1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.IpAdapterFaceId.ModelType"
          },
          "default": "1_5-v1",
          "title": "Model Type",
          "description": "The model type to use. 1_5 is the default and is recommended for most use cases."
        },
        {
          "name": "face_images_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Face Images Data Url",
          "description": "URL to zip archive with images of faces. The images embedding will be averaged to create a more accurate face id."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Image O1",
      "description": "Kling Image O1 provides advanced image generation and transformation with optimized quality.\n    image, generation, kling, o1, optimized\n\n    Use cases:\n    - Generate images with Kling O1\n    - Transform images with optimization\n    - Create optimized quality results\n    - Produce advanced image generations\n    - Generate with balanced quality-speed",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.KlingImageO1",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for image generation. Reference images using @Image1, @Image2, etc. (or @Image if only one image). Max 2500 characters."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate (1-9)."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "1K",
              "2K"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.KlingImageO1.Resolution"
          },
          "default": "1K",
          "title": "Resolution",
          "description": "Image generation resolution. 1K: standard, 2K: high-res."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1",
              "4:3",
              "3:4",
              "3:2",
              "2:3",
              "21:9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.KlingImageO1.KlingImageO1AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of generated images. 'auto' intelligently determines based on input content."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.KlingImageO1.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "elements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Elements",
          "description": "Elements (characters/objects) to include in the image. Reference in prompt as @Element1, @Element2, etc. Maximum 10 total (elements + reference images)."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of reference images. Reference images in prompt using @Image1, @Image2, etc. (1-indexed). Max 10 images."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling V15 Kolors Virtual Try On",
      "description": "Kling Kolors Virtual TryOn v1.5 is a high quality image based Try-On endpoint which can be used for commercial try on.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.KlingV15KolorsVirtualTryOn",
      "properties": [
        {
          "name": "garment_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Garment Image Url",
          "description": "Url to the garment image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If true, the function will return the image in the response."
        },
        {
          "name": "human_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Human Image Url",
          "description": "Url for the human image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kolors Image To Image",
      "description": "Kolors transforms images using an advanced diffusion model. High-quality image-to-image generation with natural color preservation and detail retention.\n    image, transformation, kolors, diffusion, quality\n\n    Use cases:\n    - Transform images with natural color handling\n    - Create variations with preserved color harmony\n    - Apply modifications with detail retention\n    - Generate style transfers with color consistency\n    - Produce high-fidelity image transformations",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.KolorsImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the desired transformation"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.KolorsImageToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for image to image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "EulerDiscreteScheduler",
              "EulerAncestralDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DPMSolverMultistepScheduler_SDE_karras",
              "UniPCMultistepScheduler",
              "DEISMultistepScheduler"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.KolorsImageToImage.Scheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the model."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "Strength of the transformation (0-1, higher = more change)"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable safety checker."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image",
        "strength"
      ]
    },
    {
      "title": "Lcm Sd 15I 2 i",
      "description": "Produce high-quality images with minimal inference steps. Optimized for 512x512 input image size.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.LcmSd15I2i",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. The function will return a list of images with the same prompt and negative prompt but different seeds."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image to use as a base."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "The strength of the image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "enable_safety_checks",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checks",
          "description": "If set to true, the resulting image will be checked whether it includes any potentially unsafe content. If it does, it will be replaced with a black image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for generating the image. The more steps the better the image will be but it will also take longer to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Leffa Pose Transfer",
      "description": "Leffa Pose Transfer is an endpoint for changing pose of an image with a reference image.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.LeffaPoseTransfer",
      "properties": [
        {
          "name": "pose_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Pose Image Url",
          "description": "Url for the human image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LeffaPoseTransfer.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your input when generating the image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same input given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "person_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Person Image Url",
          "description": "Url to the garment image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Leffa Virtual Tryon",
      "description": "Leffa Virtual TryOn is a high quality image based Try-On endpoint which can be used for commercial try on.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.LeffaVirtualTryon",
      "properties": [
        {
          "name": "garment_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Garment Image Url",
          "description": "Url to the garment image."
        },
        {
          "name": "human_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Human Image Url",
          "description": "Url for the human image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LeffaVirtualTryon.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "garment_type",
          "type": {
            "type": "enum",
            "values": [
              "upper_body",
              "lower_body",
              "dresses"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LeffaVirtualTryon.GarmentType"
          },
          "default": "",
          "title": "Garment Type",
          "description": "The type of the garment used for virtual try-on."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your input when generating the image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same input given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Live Portrait Image",
      "description": "Transfer expression from a video to a portrait.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.LivePortraitImage",
      "properties": [
        {
          "name": "smile",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Smile",
          "description": "Amount to smile"
        },
        {
          "name": "eyebrow",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Eyebrow",
          "description": "Amount to raise or lower eyebrows"
        },
        {
          "name": "rotate_roll",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Rotate Roll",
          "description": "Amount to rotate the face in roll"
        },
        {
          "name": "wink",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Wink",
          "description": "Amount to wink"
        },
        {
          "name": "rotate_pitch",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Rotate Pitch",
          "description": "Amount to rotate the face in pitch"
        },
        {
          "name": "blink",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Blink",
          "description": "Amount to blink the eyes"
        },
        {
          "name": "scale",
          "type": {
            "type": "float"
          },
          "default": 2.3,
          "title": "Scale",
          "description": "Scaling factor for the face crop."
        },
        {
          "name": "eee",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Eee",
          "description": "Amount to shape mouth in 'eee' position"
        },
        {
          "name": "pupil_x",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Pupil X",
          "description": "Amount to move pupils horizontally"
        },
        {
          "name": "dsize",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Dsize",
          "description": "Size of the output image."
        },
        {
          "name": "flag_pasteback",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Flag Pasteback",
          "description": "Whether to paste-back/stitch the animated face cropping from the face-cropping space to the original image space."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker. If enabled, the model will check if the input image contains a face before processing it. The safety checker will process the input image"
        },
        {
          "name": "vy_ratio",
          "type": {
            "type": "float"
          },
          "default": -0.125,
          "title": "Vy Ratio",
          "description": "Vertical offset ratio for face crop. Positive values move up, negative values move down."
        },
        {
          "name": "vx_ratio",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Vx Ratio",
          "description": "Horizontal offset ratio for face crop."
        },
        {
          "name": "pupil_y",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Pupil Y",
          "description": "Amount to move pupils vertically"
        },
        {
          "name": "rotate_yaw",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Rotate Yaw",
          "description": "Amount to rotate the face in yaw"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LivePortraitImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "Output format"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be animated"
        },
        {
          "name": "woo",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Woo",
          "description": "Amount to shape mouth in 'woo' position"
        },
        {
          "name": "aaa",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Aaa",
          "description": "Amount to open mouth in 'aaa' shape"
        },
        {
          "name": "flag_do_rot",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Flag Do Rot",
          "description": "Whether to conduct the rotation when flag_do_crop is True."
        },
        {
          "name": "flag_do_crop",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Flag Do Crop",
          "description": "Whether to crop the source portrait to the face-cropping space."
        },
        {
          "name": "flag_lip_zero",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Flag Lip Zero",
          "description": "Whether to set the lip to closed state before animation. Only takes effect when flag_eye_retargeting and flag_lip_retargeting are False."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Longcat Image Edit",
      "description": "Longcat Image Edit transforms images with unique AI-powered modifications and creative control.\n    image, editing, longcat, creative\n\n    Use cases:\n    - Edit images with Longcat AI\n    - Apply creative modifications\n    - Create unique image variations\n    - Transform images creatively\n    - Produce artistic modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.LongcatImageEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image with."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LongcatImageEdit.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LongcatImageEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Lora Image To Image",
      "description": "Run Any Stable Diffusion model with customizable LoRA weights.\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.LoraImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "noise_strength",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Noise Strength",
          "description": "The amount of noise to add to noise image for image. Only used if the image_url is provided. 1.0 is complete noise and 0 is no noise."
        },
        {
          "name": "tile_height",
          "type": {
            "type": "int"
          },
          "default": 4096,
          "title": "Tile Height",
          "description": "The size of the tiles to be used for the image generation."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The embeddings to use for the image generation. Only a single embedding is supported at the moment. The embeddings will be used to map the tokens in the prompt to the embedding weights."
        },
        {
          "name": "ic_light_model_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ic Light Model Url",
          "description": "The URL of the IC Light model to use for the image generation."
        },
        {
          "name": "image_encoder_weight_name",
          "type": {
            "type": "str"
          },
          "default": "pytorch_model.bin",
          "title": "Image Encoder Weight Name",
          "description": "The weight name of the image encoder model to use for the image generation."
        },
        {
          "name": "ip_adapter",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Ip Adapter",
          "description": "The IP adapter to use for the image generation."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "DPM++ 2M",
              "DPM++ 2M Karras",
              "DPM++ 2M SDE",
              "DPM++ 2M SDE Karras",
              "Euler",
              "Euler A",
              "Euler (trailing timesteps)",
              "LCM",
              "LCM (trailing timesteps)",
              "DDIM",
              "TCD"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LoraImageToImage.Scheduler"
          },
          "default": null,
          "title": "Scheduler",
          "description": "Scheduler / sampler to use for the image denoising process."
        },
        {
          "name": "sigmas",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Sigmas",
          "description": "Optionally override the sigmas to use for the denoising process. Only works with schedulers which support the `sigmas` argument in their `set_sigmas` method. Defaults to not overriding, in which case the scheduler automatically sets the sigmas based on the `num_inference_steps` parameter. If set to a custom sigma schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `timesteps` is set."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "tile_stride_width",
          "type": {
            "type": "int"
          },
          "default": 2048,
          "title": "Tile Stride Width",
          "description": "The stride of the tiles to be used for the image generation."
        },
        {
          "name": "debug_per_pass_latents",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Debug Per Pass Latents",
          "description": "If set to true, the latents will be saved for debugging per pass."
        },
        {
          "name": "timesteps",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Timesteps",
          "description": "Optionally override the timesteps to use for the denoising process. Only works with schedulers which support the `timesteps` argument in their `set_timesteps` method. Defaults to not overriding, in which case the scheduler automatically sets the timesteps based on the `num_inference_steps` parameter. If set to a custom timestep schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `sigmas` is set."
        },
        {
          "name": "model_name",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model Name",
          "description": "URL or HuggingFace ID of the base model to generate the image."
        },
        {
          "name": "prompt_weighting",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Prompt Weighting",
          "description": "If set to true, the prompt weighting syntax will be used. Additionally, this will lift the 77 token limit by averaging embeddings."
        },
        {
          "name": "variant",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Variant",
          "description": "The variant of the model to use for huggingface models, e.g. 'fp16'."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for image to image/inpainting."
        },
        {
          "name": "controlnet_guess_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Controlnet Guess Mode",
          "description": "If set to true, the controlnet will be applied to only the conditional predictions."
        },
        {
          "name": "image_encoder_subfolder",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Encoder Subfolder",
          "description": "The subfolder of the image encoder model to use for the image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "ic_light_model_background_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ic Light Model Background Image Url",
          "description": "The URL of the IC Light model background image to use for the image generation. Make sure to use a background compatible with the model."
        },
        {
          "name": "rescale_betas_snr_zero",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Rescale Betas Snr Zero",
          "description": "Whether to set the rescale_betas_snr_zero option or not for the sampler"
        },
        {
          "name": "tile_width",
          "type": {
            "type": "int"
          },
          "default": 4096,
          "title": "Tile Width",
          "description": "The size of the tiles to be used for the image generation."
        },
        {
          "name": "prediction_type",
          "type": {
            "type": "enum",
            "values": [
              "v_prediction",
              "epsilon"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LoraImageToImage.PredictionType"
          },
          "default": "epsilon",
          "title": "Prediction Type",
          "description": "The type of prediction to use for the image generation. The `epsilon` is the default."
        },
        {
          "name": "eta",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Eta",
          "description": "The eta value to be used for the image generation."
        },
        {
          "name": "image_encoder_path",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Encoder Path",
          "description": "The path to the image encoder model to use for the image generation."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "image_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LoraImageToImage.ImageFormat"
          },
          "default": "png",
          "title": "Image Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate in one request. Note that the higher the batch size, the longer it will take to generate the images."
        },
        {
          "name": "debug_latents",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Debug Latents",
          "description": "If set to true, the latents will be saved for debugging."
        },
        {
          "name": "ic_light_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ic Light Image Url",
          "description": "The URL of the IC Light model image to use for the image generation."
        },
        {
          "name": "unet_name",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Unet Name",
          "description": "URL or HuggingFace ID of the custom U-Net model to use for the image generation."
        },
        {
          "name": "clip_skip",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Clip Skip",
          "description": "Skips part of the image generation process, leading to slightly different results. This means the image renders faster, too."
        },
        {
          "name": "tile_stride_height",
          "type": {
            "type": "int"
          },
          "default": 2048,
          "title": "Tile Stride Height",
          "description": "The stride of the tiles to be used for the image generation."
        },
        {
          "name": "controlnets",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Controlnets",
          "description": "The control nets to use for the image generation. You can use any number of control nets and they will be applied to the image at the specified timesteps."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Increasing the amount of steps tells Stable Diffusion that it should take more steps to generate your final result which can increase the amount of detail in your image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Lora Inpaint",
      "description": "Run Any Stable Diffusion model with customizable LoRA weights.\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.LoraInpaint",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "noise_strength",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Noise Strength",
          "description": "The amount of noise to add to noise image for image. Only used if the image_url is provided. 1.0 is complete noise and 0 is no noise."
        },
        {
          "name": "tile_height",
          "type": {
            "type": "int"
          },
          "default": 4096,
          "title": "Tile Height",
          "description": "The size of the tiles to be used for the image generation."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The embeddings to use for the image generation. Only a single embedding is supported at the moment. The embeddings will be used to map the tokens in the prompt to the embedding weights."
        },
        {
          "name": "ic_light_model_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ic Light Model Url",
          "description": "The URL of the IC Light model to use for the image generation."
        },
        {
          "name": "image_encoder_weight_name",
          "type": {
            "type": "str"
          },
          "default": "pytorch_model.bin",
          "title": "Image Encoder Weight Name",
          "description": "The weight name of the image encoder model to use for the image generation."
        },
        {
          "name": "ip_adapter",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Ip Adapter",
          "description": "The IP adapter to use for the image generation."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "DPM++ 2M",
              "DPM++ 2M Karras",
              "DPM++ 2M SDE",
              "DPM++ 2M SDE Karras",
              "Euler",
              "Euler A",
              "Euler (trailing timesteps)",
              "LCM",
              "LCM (trailing timesteps)",
              "DDIM",
              "TCD"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LoraInpaint.Scheduler"
          },
          "default": null,
          "title": "Scheduler",
          "description": "Scheduler / sampler to use for the image denoising process."
        },
        {
          "name": "sigmas",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Sigmas",
          "description": "Optionally override the sigmas to use for the denoising process. Only works with schedulers which support the `sigmas` argument in their `set_sigmas` method. Defaults to not overriding, in which case the scheduler automatically sets the sigmas based on the `num_inference_steps` parameter. If set to a custom sigma schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `timesteps` is set."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "tile_stride_width",
          "type": {
            "type": "int"
          },
          "default": 2048,
          "title": "Tile Stride Width",
          "description": "The stride of the tiles to be used for the image generation."
        },
        {
          "name": "debug_per_pass_latents",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Debug Per Pass Latents",
          "description": "If set to true, the latents will be saved for debugging per pass."
        },
        {
          "name": "timesteps",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Timesteps",
          "description": "Optionally override the timesteps to use for the denoising process. Only works with schedulers which support the `timesteps` argument in their `set_timesteps` method. Defaults to not overriding, in which case the scheduler automatically sets the timesteps based on the `num_inference_steps` parameter. If set to a custom timestep schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `sigmas` is set."
        },
        {
          "name": "model_name",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model Name",
          "description": "URL or HuggingFace ID of the base model to generate the image."
        },
        {
          "name": "prompt_weighting",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Prompt Weighting",
          "description": "If set to true, the prompt weighting syntax will be used. Additionally, this will lift the 77 token limit by averaging embeddings."
        },
        {
          "name": "variant",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Variant",
          "description": "The variant of the model to use for huggingface models, e.g. 'fp16'."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for image to image/inpainting."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "URL of black-and-white image to use as mask during inpainting."
        },
        {
          "name": "image_encoder_subfolder",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Encoder Subfolder",
          "description": "The subfolder of the image encoder model to use for the image generation."
        },
        {
          "name": "ic_light_model_background_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ic Light Model Background Image Url",
          "description": "The URL of the IC Light model background image to use for the image generation. Make sure to use a background compatible with the model."
        },
        {
          "name": "rescale_betas_snr_zero",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Rescale Betas Snr Zero",
          "description": "Whether to set the rescale_betas_snr_zero option or not for the sampler"
        },
        {
          "name": "tile_width",
          "type": {
            "type": "int"
          },
          "default": 4096,
          "title": "Tile Width",
          "description": "The size of the tiles to be used for the image generation."
        },
        {
          "name": "controlnet_guess_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Controlnet Guess Mode",
          "description": "If set to true, the controlnet will be applied to only the conditional predictions."
        },
        {
          "name": "prediction_type",
          "type": {
            "type": "enum",
            "values": [
              "v_prediction",
              "epsilon"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LoraInpaint.PredictionType"
          },
          "default": "epsilon",
          "title": "Prediction Type",
          "description": "The type of prediction to use for the image generation. The `epsilon` is the default."
        },
        {
          "name": "eta",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Eta",
          "description": "The eta value to be used for the image generation."
        },
        {
          "name": "image_encoder_path",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Encoder Path",
          "description": "The path to the image encoder model to use for the image generation."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "image_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LoraInpaint.ImageFormat"
          },
          "default": "png",
          "title": "Image Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate in one request. Note that the higher the batch size, the longer it will take to generate the images."
        },
        {
          "name": "debug_latents",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Debug Latents",
          "description": "If set to true, the latents will be saved for debugging."
        },
        {
          "name": "ic_light_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ic Light Image Url",
          "description": "The URL of the IC Light model image to use for the image generation."
        },
        {
          "name": "unet_name",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Unet Name",
          "description": "URL or HuggingFace ID of the custom U-Net model to use for the image generation."
        },
        {
          "name": "clip_skip",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Clip Skip",
          "description": "Skips part of the image generation process, leading to slightly different results. This means the image renders faster, too."
        },
        {
          "name": "tile_stride_height",
          "type": {
            "type": "int"
          },
          "default": 2048,
          "title": "Tile Stride Height",
          "description": "The stride of the tiles to be used for the image generation."
        },
        {
          "name": "controlnets",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Controlnets",
          "description": "The control nets to use for the image generation. You can use any number of control nets and they will be applied to the image at the specified timesteps."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Increasing the amount of steps tells Stable Diffusion that it should take more steps to generate your final result which can increase the amount of detail in your image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Lucidflux",
      "description": "Lucidflux\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Lucidflux",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "guidance",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance",
          "description": "The guidance to use for the diffusion process."
        },
        {
          "name": "target_height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Target Height",
          "description": "The height of the output image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to edit."
        },
        {
          "name": "target_width",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Target Width",
          "description": "The width of the output image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Seed used for random number generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Luma Photon Flash Modify",
      "description": "Edit images from your prompts using Luma Photon. Photon is the most creative, personalizable, and intelligent visual models for creatives, bringing a step-function change in the cost of high-quality image generation.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.LumaPhotonFlashModify",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Instruction for modifying the image"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "21:9",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LumaPhotonFlashModify.AspectRatio"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the reframed image"
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Strength",
          "description": "The strength of the initial image. Higher strength values are corresponding to more influence of the initial image on the output."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image to reframe"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Luma Photon Flash Reframe",
      "description": "This advanced tool intelligently expands your visuals, seamlessly blending new content to enhance creativity and adaptability, offering unmatched speed and quality for creators at a fraction of the cost.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.LumaPhotonFlashReframe",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional prompt for reframing"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "21:9",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LumaPhotonFlashReframe.AspectRatio"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the reframed image"
        },
        {
          "name": "y_start",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Y Start",
          "description": "Start Y coordinate for reframing"
        },
        {
          "name": "x_end",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "X End",
          "description": "End X coordinate for reframing"
        },
        {
          "name": "y_end",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Y End",
          "description": "End Y coordinate for reframing"
        },
        {
          "name": "grid_position_y",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Grid Position Y",
          "description": "Y position of the grid for reframing"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image to reframe"
        },
        {
          "name": "grid_position_x",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Grid Position X",
          "description": "X position of the grid for reframing"
        },
        {
          "name": "x_start",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "X Start",
          "description": "Start X coordinate for reframing"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Luma Photon Modify",
      "description": "Edit images from your prompts using Luma Photon. Photon is the most creative, personalizable, and intelligent visual models for creatives, bringing a step-function change in the cost of high-quality image generation.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.LumaPhotonModify",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Instruction for modifying the image"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "21:9",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LumaPhotonModify.AspectRatio"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the reframed image"
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Strength",
          "description": "The strength of the initial image. Higher strength values are corresponding to more influence of the initial image on the output."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image to reframe"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Luma Photon Reframe",
      "description": "Extend and reframe images with Luma Photon Reframe. This advanced tool intelligently expands your visuals, seamlessly blending new content to enhance creativity and adaptability, offering unmatched personalization and quality for creators at a fraction of the cost.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.LumaPhotonReframe",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional prompt for reframing"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "21:9",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.LumaPhotonReframe.AspectRatio"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the reframed image"
        },
        {
          "name": "y_start",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Y Start",
          "description": "Start Y coordinate for reframing"
        },
        {
          "name": "x_end",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "X End",
          "description": "End X coordinate for reframing"
        },
        {
          "name": "y_end",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Y End",
          "description": "End Y coordinate for reframing"
        },
        {
          "name": "grid_position_y",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Grid Position Y",
          "description": "Y position of the grid for reframing"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image to reframe"
        },
        {
          "name": "grid_position_x",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Grid Position X",
          "description": "X position of the grid for reframing"
        },
        {
          "name": "x_start",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "X Start",
          "description": "Start X coordinate for reframing"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Minimax Image 01 Subject Reference",
      "description": "Generate images from text and a reference image using MiniMax Image-01 for consistent character appearance.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.MinimaxImage01SubjectReference",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Prompt Optimizer",
          "description": "Whether to enable automatic prompt optimization"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate (1-9)"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "16:9",
              "4:3",
              "3:2",
              "2:3",
              "3:4",
              "9:16",
              "21:9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.MinimaxImage01SubjectReference.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated image"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for image generation (max 1500 characters)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the subject reference image to use for consistent character appearance"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Mix Dehaze Net",
      "description": "An advanced dehaze model to remove atmospheric haze, restoring clarity and detail in images through intelligent neural network processing.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.MixDehazeNet",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "indoor",
              "outdoor"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.MixDehazeNet.Model"
          },
          "default": "indoor",
          "title": "Model",
          "description": "Model to be used for dehazing"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "seed to be used for generation"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to be used for image enhancement"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Moondream 3 Preview Segment",
      "description": "Moondream3 Preview [Segment]\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Moondream3PreviewSegment",
      "properties": [
        {
          "name": "spatial_references",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Spatial References",
          "description": "Spatial references to guide the segmentation. By feeding in references you can help the segmentation process. Must be either list of Point object with x and y members, or list of arrays containing either 2 floats (x,y) or 4 floats (x1,y1,x2,y2). **NOTE**: You can also use the [**point endpoint**](https://fal.ai/models/fal-ai/moondream3-preview/point) to get points for the objects, and pass them in here."
        },
        {
          "name": "settings",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Settings",
          "description": "Sampling settings for the segmentation model"
        },
        {
          "name": "object",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Object",
          "description": "Object to be segmented in the image"
        },
        {
          "name": "preview",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preview",
          "description": "Whether to preview the output and return a binary mask of the image"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be processed Max width: 7000px, Max height: 7000px, Timeout: 20.0s"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Moondream Next Detection",
      "description": "MoonDreamNext Detection is a multimodal vision-language model for gaze detection, bbox detection, point detection, and more.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.MoondreamNextDetection",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL to be processed"
        },
        {
          "name": "detection_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Detection Prompt",
          "description": "Text description of what to detect"
        },
        {
          "name": "task_type",
          "type": {
            "type": "enum",
            "values": [
              "bbox_detection",
              "point_detection",
              "gaze_detection"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.MoondreamNextDetection.TaskType"
          },
          "default": "",
          "title": "Task Type",
          "description": "Type of detection to perform"
        },
        {
          "name": "show_visualization",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Show Visualization",
          "description": "Whether to show visualization for detection"
        },
        {
          "name": "combine_points",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Combine Points",
          "description": "Whether to combine points into a single point for point detection. This has no effect for bbox detection or gaze detection."
        },
        {
          "name": "use_ensemble",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Ensemble",
          "description": "Whether to use ensemble for gaze detection"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Nafnet Deblur",
      "description": "Use NAFNet to fix issues like blurriness and noise in your images. This model specializes in image restoration and can help enhance the overall quality of your photography.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.NafnetDeblur",
      "properties": [
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "seed to be used for generation"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to be used for relighting"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Nafnet Denoise",
      "description": "Use NAFNet to fix issues like blurriness and noise in your images. This model specializes in image restoration and can help enhance the overall quality of your photography.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.NafnetDenoise",
      "properties": [
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "seed to be used for generation"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to be used for relighting"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Nano Banana Edit",
      "description": "Nano Banana\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.NanoBananaEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image editing."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "21:9",
              "16:9",
              "3:2",
              "4:3",
              "5:4",
              "1:1",
              "4:5",
              "3:4",
              "2:3",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.NanoBananaEdit.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.NanoBananaEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to use for image-to-image generation or image editing."
        },
        {
          "name": "limit_generations",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Limit Generations",
          "description": "Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Nano Banana Pro Edit",
      "description": "Nano Banana Pro\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.NanoBananaProEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image editing."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "enable_web_search",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Web Search",
          "description": "Enable web search for the image generation task. This will allow the model to use the latest information from the web to generate the image."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "1K",
              "2K",
              "4K"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.NanoBananaProEdit.Resolution"
          },
          "default": "1K",
          "title": "Resolution",
          "description": "The resolution of the image to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.NanoBananaProEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.NanoBananaProEdit.SafetyTolerance"
          },
          "default": "4",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for content moderation. 1 is the most strict (blocks most content), 6 is the least strict."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "limit_generations",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Limit Generations",
          "description": "Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to use for image-to-image generation or image editing."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Nextstep 1",
      "description": "Nextstep 1\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Nextstep1",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to edit."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Object Removal",
      "description": "Removes objects and their visual effects using natural language, replacing them with contextually appropriate content\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ObjectRemoval",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text description of the object to remove."
        },
        {
          "name": "mask_expansion",
          "type": {
            "type": "int"
          },
          "default": 15,
          "title": "Mask Expansion",
          "description": "Amount of pixels to expand the mask by. Range: 0-50"
        },
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "low_quality",
              "medium_quality",
              "high_quality",
              "best_quality"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ObjectRemoval.Model"
          },
          "default": "best_quality",
          "title": "Model"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to remove objects from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Object Removal Bbox",
      "description": "Removes box-selected objects and their visual effects, seamlessly reconstructing the scene with contextually appropriate content.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ObjectRemovalBbox",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "low_quality",
              "medium_quality",
              "high_quality",
              "best_quality"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ObjectRemovalBbox.Model"
          },
          "default": "best_quality",
          "title": "Model"
        },
        {
          "name": "mask_expansion",
          "type": {
            "type": "int"
          },
          "default": 15,
          "title": "Mask Expansion",
          "description": "Amount of pixels to expand the mask by. Range: 0-50"
        },
        {
          "name": "box_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Box Prompts",
          "description": "List of bounding box coordinates to erase (only one box prompt is supported)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to remove objects from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Object Removal Mask",
      "description": "Removes mask-selected objects and their visual effects, seamlessly reconstructing the scene with contextually appropriate content.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ObjectRemovalMask",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "low_quality",
              "medium_quality",
              "high_quality",
              "best_quality"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ObjectRemovalMask.Model"
          },
          "default": "best_quality",
          "title": "Model"
        },
        {
          "name": "mask_expansion",
          "type": {
            "type": "int"
          },
          "default": 15,
          "title": "Mask Expansion",
          "description": "Amount of pixels to expand the mask by. Range: 0-50"
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The URL of the mask image. White pixels (255) indicate areas to remove."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to remove objects from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Omni Zero",
      "description": "Any pose, any style, any identity\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.OmniZero",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to guide the image generation."
        },
        {
          "name": "identity_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Identity Image Url",
          "description": "Identity image url."
        },
        {
          "name": "identity_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Identity Strength",
          "description": "Identity strength."
        },
        {
          "name": "number_of_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Number Of Images",
          "description": "Number of images."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Image Strength",
          "description": "Image strength."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to guide the image generation."
        },
        {
          "name": "composition_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Composition Image Url",
          "description": "Composition image url."
        },
        {
          "name": "depth_strength",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Depth Strength",
          "description": "Depth strength."
        },
        {
          "name": "composition_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Composition Strength",
          "description": "Composition strength."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Input image url."
        },
        {
          "name": "style_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Style Image Url",
          "description": "Style image url."
        },
        {
          "name": "face_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Face Strength",
          "description": "Face strength."
        },
        {
          "name": "style_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Style Strength",
          "description": "Style strength."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Seed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pasd",
      "description": "Pixel-Aware Diffusion Model for Realistic Image Super-Resolution and Personalized Stylization\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Pasd",
      "properties": [
        {
          "name": "conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Conditioning Scale",
          "description": "ControlNet conditioning scale (0.1-1.0)"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Additional prompt to guide super-resolution"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Input image to super-resolve"
        },
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Steps",
          "description": "Number of inference steps (10-50)"
        },
        {
          "name": "scale",
          "type": {
            "type": "int"
          },
          "default": 2,
          "title": "Scale",
          "description": "Upscaling factor (1-4x)"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7,
          "title": "Guidance Scale",
          "description": "Guidance scale for diffusion (1.0-20.0)"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, dirty, messy, frames, deformed, dotted, noise, raster lines, unclear, lowres, over-smoothed, painting, ai generated",
          "title": "Negative Prompt",
          "description": "Negative prompt to avoid unwanted artifacts"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Photomaker",
      "description": "Customizing Realistic Human Photos via Stacked ID Embedding\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Photomaker",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate in one request. Note that the higher the batch size, the longer it will take to generate the images."
        },
        {
          "name": "style_strength",
          "type": {
            "type": "int"
          },
          "default": 20,
          "title": "Style Strength"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "(No style)",
              "Cinematic",
              "Disney Character",
              "Digital Art",
              "Photographic",
              "Fantasy art",
              "Neonpunk",
              "Enhance",
              "Comic book",
              "Lowpoly",
              "Line art"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Photomaker.Style"
          },
          "default": "Photographic",
          "title": "Style"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "image_archive_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Archive Url",
          "description": "The URL of the image archive containing the images you want to use."
        },
        {
          "name": "initial_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Initial Image Url",
          "description": "Optional initial image for img2img"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Increasing the amount of steps tells Stable Diffusion that it should take more steps to generate your final result which can increase the amount of detail in your image."
        },
        {
          "name": "initial_image_strength",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Initial Image Strength",
          "description": "How much noise to add to the latent image. O for no noise, 1 for maximum noise."
        },
        {
          "name": "base_pipeline",
          "type": {
            "type": "enum",
            "values": [
              "photomaker",
              "photomaker-style"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Photomaker.BasePipeline"
          },
          "default": "photomaker",
          "title": "Base Pipeline",
          "description": "The base pipeline to use for generating the image."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Playground V25 Image To Image",
      "description": "State-of-the-art open-source model in aesthetic quality\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PlaygroundV25ImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "guidance_rescale",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Rescale",
          "description": "The rescale factor for the CFG."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "preserve_aspect_ratio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preserve Aspect Ratio",
          "description": "If set to true, the aspect ratio of the generated image will be preserved even if the image size is too large. However, if the image is not a multiple of 32 in width or height, it will be resized to the nearest multiple of 32. By default, this snapping to the nearest multiple of 32 will not preserve the aspect ratio. Set crop_output to True, to crop the output to the proper aspect ratio after generating."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "crop_output",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Crop Output",
          "description": "If set to true, the output cropped to the proper aspect ratio after generating."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PlaygroundV25ImageToImage.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "determines how much the generated image resembles the initial image"
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PlaygroundV25ImageToImage.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Playground V25 Inpainting",
      "description": "State-of-the-art open-source model in aesthetic quality\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PlaygroundV25Inpainting",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "guidance_rescale",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Rescale",
          "description": "The rescale factor for the CFG."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PlaygroundV25Inpainting.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "determines how much the generated image resembles the initial image"
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PlaygroundV25Inpainting.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "The URL of the mask to use for inpainting."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Plushify",
      "description": "Turn any image into a cute plushie!\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Plushify",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt for the generation. Default is empty which is usually best, but sometimes it can help to add a description of the subject."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "use_cfg_zero",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Cfg Zero",
          "description": "Whether to use CFG zero"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to apply cartoon style to"
        },
        {
          "name": "scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Scale",
          "description": "Scale factor for the Cartoon effect"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "Number of inference steps"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for the generation"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for image generation. Same seed with same parameters will generate same image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing",
      "description": "Post Processing is an endpoint that can enhance images using a variety of techniques including grain, blur, sharpen, and more.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessing",
      "properties": [
        {
          "name": "blue_shift",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Blue Shift",
          "description": "Blue channel shift amount"
        },
        {
          "name": "vertex_y",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Vertex Y",
          "description": "Vertex Y position"
        },
        {
          "name": "green_direction",
          "type": {
            "type": "enum",
            "values": [
              "horizontal",
              "vertical"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessing.GreenDirection"
          },
          "default": "horizontal",
          "title": "Green Direction",
          "description": "Green channel shift direction"
        },
        {
          "name": "enable_glow",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Glow",
          "description": "Enable glow effect"
        },
        {
          "name": "dodge_burn_mode",
          "type": {
            "type": "enum",
            "values": [
              "dodge",
              "burn",
              "dodge_and_burn",
              "burn_and_dodge",
              "color_dodge",
              "color_burn",
              "linear_dodge",
              "linear_burn"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessing.DodgeBurnMode"
          },
          "default": "dodge",
          "title": "Dodge Burn Mode",
          "description": "Dodge and burn mode"
        },
        {
          "name": "glow_intensity",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Glow Intensity",
          "description": "Glow intensity"
        },
        {
          "name": "blur_sigma",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Blur Sigma",
          "description": "Sigma for Gaussian blur"
        },
        {
          "name": "desaturate_method",
          "type": {
            "type": "enum",
            "values": [
              "luminance (Rec.709)",
              "luminance (Rec.601)",
              "average",
              "lightness"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessing.DesaturateMethod"
          },
          "default": "luminance (Rec.709)",
          "title": "Desaturate Method",
          "description": "Desaturation method"
        },
        {
          "name": "enable_blur",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Blur",
          "description": "Enable blur effect"
        },
        {
          "name": "blur_radius",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "Blur Radius",
          "description": "Blur radius"
        },
        {
          "name": "grain_style",
          "type": {
            "type": "enum",
            "values": [
              "modern",
              "analog",
              "kodak",
              "fuji",
              "cinematic",
              "newspaper"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessing.GrainStyle"
          },
          "default": "modern",
          "title": "Grain Style",
          "description": "Style of film grain to apply"
        },
        {
          "name": "cas_amount",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Cas Amount",
          "description": "CAS sharpening amount"
        },
        {
          "name": "gamma",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Gamma",
          "description": "Gamma adjustment"
        },
        {
          "name": "tint_mode",
          "type": {
            "type": "enum",
            "values": [
              "sepia",
              "red",
              "green",
              "blue",
              "cyan",
              "magenta",
              "yellow",
              "purple",
              "orange",
              "warm",
              "cool",
              "lime",
              "navy",
              "vintage",
              "rose",
              "teal",
              "maroon",
              "peach",
              "lavender",
              "olive"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessing.TintMode"
          },
          "default": "sepia",
          "title": "Tint Mode",
          "description": "Tint color mode"
        },
        {
          "name": "blur_type",
          "type": {
            "type": "enum",
            "values": [
              "gaussian",
              "kuwahara"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessing.BlurType"
          },
          "default": "gaussian",
          "title": "Blur Type",
          "description": "Type of blur to apply"
        },
        {
          "name": "enable_vignette",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Vignette",
          "description": "Enable vignette effect"
        },
        {
          "name": "dissolve_image_url",
          "type": {
            "type": "image"
          },
          "default": "",
          "title": "Dissolve Image Url",
          "description": "URL of second image for dissolve"
        },
        {
          "name": "red_shift",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Red Shift",
          "description": "Red channel shift amount"
        },
        {
          "name": "enable_desaturate",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Desaturate",
          "description": "Enable desaturation effect"
        },
        {
          "name": "grain_intensity",
          "type": {
            "type": "float"
          },
          "default": 0.4,
          "title": "Grain Intensity",
          "description": "Film grain intensity (when enabled)"
        },
        {
          "name": "dodge_burn_intensity",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Dodge Burn Intensity",
          "description": "Dodge and burn intensity"
        },
        {
          "name": "smart_sharpen_strength",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Smart Sharpen Strength",
          "description": "Smart sharpen strength"
        },
        {
          "name": "red_direction",
          "type": {
            "type": "enum",
            "values": [
              "horizontal",
              "vertical"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessing.RedDirection"
          },
          "default": "horizontal",
          "title": "Red Direction",
          "description": "Red channel shift direction"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        },
        {
          "name": "vertex_x",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Vertex X",
          "description": "Vertex X position"
        },
        {
          "name": "tint_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Tint Strength",
          "description": "Tint strength"
        },
        {
          "name": "enable_dissolve",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Dissolve",
          "description": "Enable dissolve effect"
        },
        {
          "name": "enable_parabolize",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Parabolize",
          "description": "Enable parabolize effect"
        },
        {
          "name": "enable_grain",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Grain",
          "description": "Enable film grain effect"
        },
        {
          "name": "solarize_threshold",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Solarize Threshold",
          "description": "Solarize threshold"
        },
        {
          "name": "enable_sharpen",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Sharpen",
          "description": "Enable sharpen effect"
        },
        {
          "name": "enable_dodge_burn",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Dodge Burn",
          "description": "Enable dodge and burn effect"
        },
        {
          "name": "glow_radius",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Glow Radius",
          "description": "Glow blur radius"
        },
        {
          "name": "sharpen_alpha",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Sharpen Alpha",
          "description": "Sharpen strength (for basic mode)"
        },
        {
          "name": "enable_color_correction",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Color Correction",
          "description": "Enable color correction"
        },
        {
          "name": "contrast",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Contrast",
          "description": "Contrast adjustment"
        },
        {
          "name": "enable_solarize",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Solarize",
          "description": "Enable solarize effect"
        },
        {
          "name": "noise_radius",
          "type": {
            "type": "int"
          },
          "default": 7,
          "title": "Noise Radius",
          "description": "Noise radius for smart sharpen"
        },
        {
          "name": "grain_scale",
          "type": {
            "type": "float"
          },
          "default": 10,
          "title": "Grain Scale",
          "description": "Film grain scale (when enabled)"
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Temperature",
          "description": "Color temperature adjustment"
        },
        {
          "name": "brightness",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Brightness",
          "description": "Brightness adjustment"
        },
        {
          "name": "blue_direction",
          "type": {
            "type": "enum",
            "values": [
              "horizontal",
              "vertical"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessing.BlueDirection"
          },
          "default": "horizontal",
          "title": "Blue Direction",
          "description": "Blue channel shift direction"
        },
        {
          "name": "dissolve_factor",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Dissolve Factor",
          "description": "Dissolve blend factor"
        },
        {
          "name": "sharpen_mode",
          "type": {
            "type": "enum",
            "values": [
              "basic",
              "smart",
              "cas"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessing.SharpenMode"
          },
          "default": "basic",
          "title": "Sharpen Mode",
          "description": "Type of sharpening to apply"
        },
        {
          "name": "vignette_strength",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Vignette Strength",
          "description": "Vignette strength (when enabled)"
        },
        {
          "name": "sharpen_radius",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Sharpen Radius",
          "description": "Sharpen radius (for basic mode)"
        },
        {
          "name": "parabolize_coeff",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Parabolize Coeff",
          "description": "Parabolize coefficient"
        },
        {
          "name": "saturation",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Saturation",
          "description": "Saturation adjustment"
        },
        {
          "name": "enable_tint",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Tint",
          "description": "Enable color tint effect"
        },
        {
          "name": "green_shift",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Green Shift",
          "description": "Green channel shift amount"
        },
        {
          "name": "preserve_edges",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Preserve Edges",
          "description": "Edge preservation factor"
        },
        {
          "name": "desaturate_factor",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Desaturate Factor",
          "description": "Desaturation factor"
        },
        {
          "name": "smart_sharpen_ratio",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Smart Sharpen Ratio",
          "description": "Smart sharpen blend ratio"
        },
        {
          "name": "enable_chromatic",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Chromatic",
          "description": "Enable chromatic aberration"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Blur",
      "description": "Apply Gaussian or Kuwahara blur effects with adjustable radius and sigma parameters\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingBlur",
      "properties": [
        {
          "name": "blur_sigma",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Blur Sigma",
          "description": "Sigma for Gaussian blur"
        },
        {
          "name": "blur_radius",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "Blur Radius",
          "description": "Blur radius"
        },
        {
          "name": "blur_type",
          "type": {
            "type": "enum",
            "values": [
              "gaussian",
              "kuwahara"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessingBlur.BlurType"
          },
          "default": "gaussian",
          "title": "Blur Type",
          "description": "Type of blur to apply"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Chromatic Aberration",
      "description": "Create chromatic aberration by shifting red, green, and blue channels horizontally or vertically with customizable shift amounts.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingChromaticAberration",
      "properties": [
        {
          "name": "blue_shift",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Blue Shift",
          "description": "Blue channel shift amount"
        },
        {
          "name": "red_shift",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Red Shift",
          "description": "Red channel shift amount"
        },
        {
          "name": "green_direction",
          "type": {
            "type": "enum",
            "values": [
              "horizontal",
              "vertical"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessingChromaticAberration.GreenDirection"
          },
          "default": "horizontal",
          "title": "Green Direction",
          "description": "Green channel shift direction"
        },
        {
          "name": "blue_direction",
          "type": {
            "type": "enum",
            "values": [
              "horizontal",
              "vertical"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessingChromaticAberration.BlueDirection"
          },
          "default": "horizontal",
          "title": "Blue Direction",
          "description": "Blue channel shift direction"
        },
        {
          "name": "red_direction",
          "type": {
            "type": "enum",
            "values": [
              "horizontal",
              "vertical"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessingChromaticAberration.RedDirection"
          },
          "default": "horizontal",
          "title": "Red Direction",
          "description": "Red channel shift direction"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        },
        {
          "name": "green_shift",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Green Shift",
          "description": "Green channel shift amount"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Color Correction",
      "description": "Adjust color temperature, brightness, contrast, saturation, and gamma values for color correction.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingColorCorrection",
      "properties": [
        {
          "name": "gamma",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Gamma",
          "description": "Gamma adjustment"
        },
        {
          "name": "saturation",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Saturation",
          "description": "Saturation adjustment"
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Temperature",
          "description": "Color temperature adjustment"
        },
        {
          "name": "brightness",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Brightness",
          "description": "Brightness adjustment"
        },
        {
          "name": "contrast",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Contrast",
          "description": "Contrast adjustment"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Color Tint",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingColorTint",
      "properties": [
        {
          "name": "tint_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Tint Strength",
          "description": "Tint strength"
        },
        {
          "name": "tint_mode",
          "type": {
            "type": "enum",
            "values": [
              "sepia",
              "red",
              "green",
              "blue",
              "cyan",
              "magenta",
              "yellow",
              "purple",
              "orange",
              "warm",
              "cool",
              "lime",
              "navy",
              "vintage",
              "rose",
              "teal",
              "maroon",
              "peach",
              "lavender",
              "olive"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessingColorTint.TintMode"
          },
          "default": "sepia",
          "title": "Tint Mode",
          "description": "Tint color mode"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Desaturate",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingDesaturate",
      "properties": [
        {
          "name": "desaturate_method",
          "type": {
            "type": "enum",
            "values": [
              "luminance (Rec.709)",
              "luminance (Rec.601)",
              "average",
              "lightness"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessingDesaturate.DesaturateMethod"
          },
          "default": "luminance (Rec.709)",
          "title": "Desaturate Method",
          "description": "Desaturation method"
        },
        {
          "name": "desaturate_factor",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Desaturate Factor",
          "description": "Desaturation factor"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Dissolve",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingDissolve",
      "properties": [
        {
          "name": "dissolve_factor",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Dissolve Factor",
          "description": "Dissolve blend factor"
        },
        {
          "name": "dissolve_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Dissolve Image Url",
          "description": "URL of second image for dissolve"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Dodge Burn",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingDodgeBurn",
      "properties": [
        {
          "name": "dodge_burn_mode",
          "type": {
            "type": "enum",
            "values": [
              "dodge",
              "burn",
              "dodge_and_burn",
              "burn_and_dodge",
              "color_dodge",
              "color_burn",
              "linear_dodge",
              "linear_burn"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessingDodgeBurn.DodgeBurnMode"
          },
          "default": "dodge",
          "title": "Dodge Burn Mode",
          "description": "Dodge and burn mode"
        },
        {
          "name": "dodge_burn_intensity",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Dodge Burn Intensity",
          "description": "Dodge and burn intensity"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Grain",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingGrain",
      "properties": [
        {
          "name": "grain_style",
          "type": {
            "type": "enum",
            "values": [
              "modern",
              "analog",
              "kodak",
              "fuji",
              "cinematic",
              "newspaper"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessingGrain.GrainStyle"
          },
          "default": "modern",
          "title": "Grain Style",
          "description": "Style of film grain to apply"
        },
        {
          "name": "grain_intensity",
          "type": {
            "type": "float"
          },
          "default": 0.4,
          "title": "Grain Intensity",
          "description": "Film grain intensity"
        },
        {
          "name": "grain_scale",
          "type": {
            "type": "float"
          },
          "default": 10,
          "title": "Grain Scale",
          "description": "Film grain scale"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Parabolize",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingParabolize",
      "properties": [
        {
          "name": "parabolize_coeff",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Parabolize Coeff",
          "description": "Parabolize coefficient"
        },
        {
          "name": "vertex_y",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Vertex Y",
          "description": "Vertex Y position"
        },
        {
          "name": "vertex_x",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Vertex X",
          "description": "Vertex X position"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Sharpen",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingSharpen",
      "properties": [
        {
          "name": "sharpen_mode",
          "type": {
            "type": "enum",
            "values": [
              "basic",
              "smart",
              "cas"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.PostProcessingSharpen.SharpenMode"
          },
          "default": "basic",
          "title": "Sharpen Mode",
          "description": "Type of sharpening to apply"
        },
        {
          "name": "sharpen_alpha",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Sharpen Alpha",
          "description": "Sharpen strength (for basic mode)"
        },
        {
          "name": "noise_radius",
          "type": {
            "type": "int"
          },
          "default": 7,
          "title": "Noise Radius",
          "description": "Noise radius for smart sharpen"
        },
        {
          "name": "sharpen_radius",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Sharpen Radius",
          "description": "Sharpen radius (for basic mode)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        },
        {
          "name": "smart_sharpen_strength",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Smart Sharpen Strength",
          "description": "Smart sharpen strength"
        },
        {
          "name": "cas_amount",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Cas Amount",
          "description": "CAS sharpening amount"
        },
        {
          "name": "preserve_edges",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Preserve Edges",
          "description": "Edge preservation factor"
        },
        {
          "name": "smart_sharpen_ratio",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Smart Sharpen Ratio",
          "description": "Smart sharpen blend ratio"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Solarize",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingSolarize",
      "properties": [
        {
          "name": "solarize_threshold",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Solarize Threshold",
          "description": "Solarize threshold"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Post Processing Vignette",
      "description": "Post Processing\n    editing, transformation, image-to-image, img2img, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.PostProcessingVignette",
      "properties": [
        {
          "name": "vignette_strength",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Vignette Strength",
          "description": "Vignette strength"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pulid",
      "description": "Tuning-free ID customization.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Pulid",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to generate the face from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "Size of the generated image"
        },
        {
          "name": "id_scale",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Id Scale",
          "description": "ID scale"
        },
        {
          "name": "mode",
          "type": {
            "type": "enum",
            "values": [
              "fidelity",
              "extreme style"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Pulid.Mode"
          },
          "default": "fidelity",
          "title": "Mode",
          "description": "Mode of generation"
        },
        {
          "name": "id_mix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Id Mix",
          "description": "if you want to mix two ID image, please turn this on, otherwise, turn this off"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.2,
          "title": "Guidance Scale",
          "description": "Guidance scale"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "Number of steps to take"
        },
        {
          "name": "reference_images",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Images",
          "description": "List of reference faces, ideally 4 images."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "flaws in the eyes, flaws in the face, flaws, lowres, non-HDRi, low quality, worst quality,artifacts noise, text, watermark, glitch, deformed, mutated, ugly, disfigured, hands, low resolution, partially rendered objects,  deformed or partially rendered eyes, deformed, deformed eyeballs, cross-eyed,blurry",
          "title": "Negative Prompt",
          "description": "Negative prompt to generate the face from"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit",
      "description": "Qwen Image Edit\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509",
      "description": "Qwen Image Edit 2509 provides powerful image editing with advanced AI capabilities and high-quality output.\n    image, editing, qwen, 2509, ai-editing\n\n    Use cases:\n    - Edit images with Qwen 2509 technology\n    - Apply sophisticated modifications to photos\n    - Create quality edits with AI assistance\n    - Transform images with advanced models\n    - Produce professional image changes",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to edit."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora",
      "description": "Qwen Image Edit 2509 with LoRA enables fine-tuned models for specialized image editing applications.\n    image, editing, qwen, lora, fine-tuned\n\n    Use cases:\n    - Edit images with fine-tuned models\n    - Apply custom modifications using LoRA\n    - Create specialized edits for specific domains\n    - Transform images with trained models\n    - Produce tailored image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509Lora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used to calculate the size of the output image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509Lora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509Lora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to edit."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Add Background",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryAddBackground",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Remove white background and add a realistic scene behind the object",
          "title": "Prompt",
          "description": "Describe the background/scene you want to add behind the object. The model will remove the white background and add the specified environment."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryAddBackground.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryAddBackground.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to edit. Provide an image with a white or clean background."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Face To Full Portrait",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryFaceToFullPortrait",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Photography. A portrait of the person in professional attire with natural lighting",
          "title": "Prompt",
          "description": "Describe the full portrait you want to generate from the face. Include clothing, setting, pose, and style details."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryFaceToFullPortrait.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryFaceToFullPortrait.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the cropped face image. Provide a close-up face photo."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Group Photo",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryGroupPhoto",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Two people standing next to each other outside with a landscape background",
          "title": "Prompt",
          "description": "Describe the group photo scene, setting, and style. The model will maintain character consistency and add vintage effects like grain, blur, and retro filters."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryGroupPhoto.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryGroupPhoto.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to combine into a group photo. Provide 2 or more individual portrait images."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Integrate Product",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryIntegrateProduct",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Blend and integrate the product into the background",
          "title": "Prompt",
          "description": "Describe how to blend and integrate the product/element into the background. The model will automatically correct perspective, lighting and shadows for natural integration."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryIntegrateProduct.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryIntegrateProduct.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image with product to integrate into background."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Lighting Restoration",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryLightingRestoration",
      "properties": [
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryLightingRestoration.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryLightingRestoration.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image to restore lighting for."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Multiple Angles",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryMultipleAngles",
      "properties": [
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "wide_angle_lens",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Wide Angle Lens",
          "description": "Enable wide-angle lens effect"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryMultipleAngles.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image to adjust camera angle for."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "vertical_angle",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Vertical Angle",
          "description": "Adjust vertical camera angle (-1=bird's-eye view/looking down, 0=neutral, 1=worm's-eye view/looking up)"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "move_forward",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Move Forward",
          "description": "Move camera forward (0=no movement, 10=close-up)"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryMultipleAngles.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "rotate_right_left",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Rotate Right Left",
          "description": "Rotate camera left (positive) or right (negative) in degrees. Positive values rotate left, negative values rotate right."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1.25,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the camera control effect."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Next Scene",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryNextScene",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Next Scene: The camera moves forward revealing more of the scene",
          "title": "Prompt",
          "description": "Describe the camera movement, framing change, or scene transition. Start with 'Next Scene:' for best results. Examples: camera movements (dolly, push-in, pull-back), framing changes (wide to close-up), new elements entering frame."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryNextScene.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryNextScene.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image to create the next scene from."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Remove Element",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryRemoveElement",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Remove the specified element from the scene",
          "title": "Prompt",
          "description": "Specify what element(s) to remove from the image (objects, people, text, etc.). The model will cleanly remove the element while maintaining consistency of the rest of the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryRemoveElement.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryRemoveElement.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image containing elements to remove."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Remove Lighting",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryRemoveLighting",
      "properties": [
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryRemoveLighting.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryRemoveLighting.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image with lighting/shadows to remove."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Lora Gallery Shirt Design",
      "description": "Qwen Image Edit 2509 Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2509LoraGalleryShirtDesign",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Put this design on their shirt",
          "title": "Prompt",
          "description": "Describe what design to put on the shirt. The model will apply the design from your input image onto the person's shirt."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryShirtDesign.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2509LoraGalleryShirtDesign.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images: first image is the person wearing a shirt, second image is the design/logo to put on the shirt."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2511",
      "description": "Qwen Image Edit 2511 provides state-of-the-art image editing with latest AI advancements and improved quality.\n    image, editing, qwen, 2511, latest\n\n    Use cases:\n    - Edit images with latest Qwen technology\n    - Apply advanced modifications to photos\n    - Create high-quality edits with AI assistance\n    - Transform images with cutting-edge models\n    - Produce professional image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2511",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image with."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2511.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If None, uses the input image dimensions."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2511.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to edit."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate an image from."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2511 Lora",
      "description": "Qwen Image Edit 2511 with LoRA support enables custom-trained models for specialized editing tasks.\n    image, editing, qwen, lora, custom\n\n    Use cases:\n    - Edit images with custom-trained models\n    - Apply specialized modifications using LoRA\n    - Create domain-specific edits\n    - Transform images with fine-tuned models\n    - Produce customized image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2511Lora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to edit the image with."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2511Lora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If None, uses the input image dimensions."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2511Lora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to edit."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate an image from."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit 2511 Multiple Angles",
      "description": "Qwen Image Edit 2511 Multiple Angles generates images from different viewpoints based on a single input image.\n    image, editing, qwen, multi-angle, viewpoint\n\n    Use cases:\n    - Generate multiple viewpoints from single image\n    - Create product views from different angles\n    - Visualize objects from various perspectives\n    - Produce multi-angle image sets\n    - Transform images to show different sides",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEdit2511MultipleAngles",
      "properties": [
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2511MultipleAngles.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the input image will be used."
        },
        {
          "name": "horizontal_angle",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Horizontal Angle",
          "description": "Horizontal rotation angle around the object in degrees. 0\u00b0=front view, 90\u00b0=right side, 180\u00b0=back view, 270\u00b0=left side, 360\u00b0=front view again."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image to adjust camera angle for."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "zoom",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Zoom",
          "description": "Camera zoom/distance. 0=wide shot (far away), 5=medium shot (normal), 10=close-up (very close)."
        },
        {
          "name": "vertical_angle",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Vertical Angle",
          "description": "Vertical camera angle in degrees. -30\u00b0=low-angle shot (looking up), 0\u00b0=eye-level, 30\u00b0=elevated, 60\u00b0=high-angle, 90\u00b0=bird's-eye view (looking down)."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the camera control effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEdit2511MultipleAngles.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI."
        },
        {
          "name": "additional_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Additional Prompt",
          "description": "Additional text to append to the automatically generated prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Image To Image",
      "description": "Qwen Image Edit\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditImageToImage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditImageToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.94,
          "title": "Strength",
          "description": "Strength of the image-to-image transformation. Lower values preserve more of the original image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Inpaint",
      "description": "Qwen Image Edit\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditInpaint",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditInpaint.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditInpaint.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.93,
          "title": "Strength",
          "description": "Strength of noising process for inpainting"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "The URL of the mask for inpainting"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Lora",
      "description": "Qwen Image Edit Lora\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditLora.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to edit."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus",
      "description": "Qwen Image Edit Plus\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlus",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlus.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlus.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to edit."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora",
      "description": "Qwen Image Edit Plus Lora\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used to calculate the size of the output image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular'. Higher acceleration increases speed. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to edit."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Add Background",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryAddBackground",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Remove white background and add a realistic scene behind the object",
          "title": "Prompt",
          "description": "Describe the background/scene you want to add behind the object. The model will remove the white background and add the specified environment."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryAddBackground.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryAddBackground.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to edit. Provide an image with a white or clean background."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Face To Full Portrait",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryFaceToFullPortrait",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Photography. A portrait of the person in professional attire with natural lighting",
          "title": "Prompt",
          "description": "Describe the full portrait you want to generate from the face. Include clothing, setting, pose, and style details."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryFaceToFullPortrait.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryFaceToFullPortrait.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the cropped face image. Provide a close-up face photo."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Group Photo",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryGroupPhoto",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Two people standing next to each other outside with a landscape background",
          "title": "Prompt",
          "description": "Describe the group photo scene, setting, and style. The model will maintain character consistency and add vintage effects like grain, blur, and retro filters."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryGroupPhoto.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryGroupPhoto.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images to combine into a group photo. Provide 2 or more individual portrait images."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Integrate Product",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora, professional\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryIntegrateProduct",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Blend and integrate the product into the background",
          "title": "Prompt",
          "description": "Describe how to blend and integrate the product/element into the background. The model will automatically correct perspective, lighting and shadows for natural integration."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryIntegrateProduct.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryIntegrateProduct.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image with product to integrate into background."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Lighting Restoration",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryLightingRestoration",
      "properties": [
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryLightingRestoration.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryLightingRestoration.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image to restore lighting for."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Multiple Angles",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryMultipleAngles",
      "properties": [
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "wide_angle_lens",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Wide Angle Lens",
          "description": "Enable wide-angle lens effect"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryMultipleAngles.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image to adjust camera angle for."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "vertical_angle",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Vertical Angle",
          "description": "Adjust vertical camera angle (-1=bird's-eye view/looking down, 0=neutral, 1=worm's-eye view/looking up)"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "move_forward",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Move Forward",
          "description": "Move camera forward (0=no movement, 10=close-up)"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryMultipleAngles.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "rotate_right_left",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Rotate Right Left",
          "description": "Rotate camera left (positive) or right (negative) in degrees. Positive values rotate left, negative values rotate right."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1.25,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the camera control effect."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Next Scene",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryNextScene",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Next Scene: The camera moves forward revealing more of the scene",
          "title": "Prompt",
          "description": "Describe the camera movement, framing change, or scene transition. Start with 'Next Scene:' for best results. Examples: camera movements (dolly, push-in, pull-back), framing changes (wide to close-up), new elements entering frame."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryNextScene.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryNextScene.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image to create the next scene from."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Remove Element",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryRemoveElement",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Remove the specified element from the scene",
          "title": "Prompt",
          "description": "Specify what element(s) to remove from the image (objects, people, text, etc.). The model will cleanly remove the element while maintaining consistency of the rest of the image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryRemoveElement.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryRemoveElement.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image containing elements to remove."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Remove Lighting",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryRemoveLighting",
      "properties": [
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryRemoveLighting.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryRemoveLighting.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URL of the image with lighting/shadows to remove."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Lora Gallery Shirt Design",
      "description": "Qwen Image Edit Plus Lora Gallery\n    editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageEditPlusLoraGalleryShirtDesign",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Put this design on their shirt",
          "title": "Prompt",
          "description": "Describe what design to put on the shirt. The model will apply the design from your input image onto the person's shirt."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryShirtDesign.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale factor for the LoRA model. Controls the strength of the LoRA effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageEditPlusLoraGalleryShirtDesign.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "The URLs of the images: first image is the person wearing a shirt, second image is the design/logo to put on the shirt."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Image To Image",
      "description": "Qwen Image\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageImageToImage.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular', 'high'. Higher acceleration increases speed. 'regular' balances speed and quality. 'high' is recommended for images without text."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. By default, we will use the provided image for determining the image_size."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "use_turbo",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Turbo",
          "description": "Enable turbo mode for faster generation with high quality. When enabled, uses optimized settings (10 steps, CFG=1.2)."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageImageToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The reference image to guide the generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.6,
          "title": "Strength",
          "description": "Denoising strength. 1.0 = fully remake; 0.0 = preserve original."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Layered",
      "description": "Qwen Image Layered provides layer-based image editing for complex compositions and precise control.\n    image, editing, qwen, layered, composition\n\n    Use cases:\n    - Edit images with layer-based control\n    - Create complex compositions\n    - Apply modifications to specific layers\n    - Build multi-layer image edits\n    - Produce sophisticated image compositions",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageLayered",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "A caption for the input image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageLayered.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "num_layers",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Layers",
          "description": "The number of layers to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageLayered.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the input image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate an image from."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Layered Lora",
      "description": "Qwen Image Layered with LoRA combines layer-based editing with custom-trained models for specialized tasks.\n    image, editing, qwen, layered, lora\n\n    Use cases:\n    - Edit layered images with custom models\n    - Create specialized layer compositions\n    - Apply fine-tuned modifications\n    - Build complex edits with trained models\n    - Produce custom layer-based results",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageLayeredLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "A caption for the input image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageLayeredLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "num_layers",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Layers",
          "description": "The number of layers to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageLayeredLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the input image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate an image from."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Max Edit",
      "description": "Qwen Image Max Edit provides powerful image editing capabilities with advanced AI understanding and high-quality results.\n    image, editing, qwen, max, ai-editing\n\n    Use cases:\n    - Edit images with advanced AI understanding\n    - Apply complex modifications to photos\n    - Transform images with high-quality results\n    - Create professional edits with natural prompts\n    - Modify images with precise control",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.QwenImageMaxEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the desired image. Supports Chinese and English. Max 800 characters."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. If not provided, the size of the final input image will be used."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.QwenImageMaxEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Enable LLM prompt optimization for better results."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility (0-2147483647)."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Reference images for editing (1-3 images required). Order matters: reference as 'image 1', 'image 2', 'image 3' in prompt. Resolution: 384-5000px each dimension. Max size: 10MB each. Formats: JPEG, JPG, PNG (no alpha), WEBP."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Content to avoid in the generated image. Max 500 characters."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable content moderation for input and output."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Recraft Upscale Creative",
      "description": "Enhances a given raster image using the 'creative upscale' tool, increasing image resolution, making the image sharper and cleaner.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.RecraftUpscaleCreative",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be upscaled. Must be in PNG format."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Recraft Upscale Crisp",
      "description": "Enhances a given raster image using 'crisp upscale' tool, boosting resolution with a focus on refining small details and faces.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.RecraftUpscaleCrisp",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be upscaled. Must be in PNG format."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Recraft V3 Image To Image",
      "description": "Recraft V3 transforms images with advanced style control and quality preservation. Professional-grade image-to-image generation with fine-tuned artistic control.\n    image, transformation, recraft, style, professional\n\n    Use cases:\n    - Transform images with precise style control\n    - Create high-quality image variations\n    - Apply artistic modifications with consistency\n    - Generate professional design alternatives\n    - Produce style-coherent image transformations",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.RecraftV3ImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the desired transformation"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "any",
              "realistic_image",
              "digital_illustration",
              "vector_illustration",
              "realistic_image/b_and_w",
              "realistic_image/hard_flash",
              "realistic_image/hdr",
              "realistic_image/natural_light",
              "realistic_image/studio_portrait",
              "realistic_image/enterprise",
              "realistic_image/motion_blur",
              "realistic_image/evening_light",
              "realistic_image/faded_nostalgia",
              "realistic_image/forest_life",
              "realistic_image/mystic_naturalism",
              "realistic_image/natural_tones",
              "realistic_image/organic_calm",
              "realistic_image/real_life_glow",
              "realistic_image/retro_realism",
              "realistic_image/retro_snapshot",
              "realistic_image/urban_drama",
              "realistic_image/village_realism",
              "realistic_image/warm_folk",
              "digital_illustration/pixel_art",
              "digital_illustration/hand_drawn",
              "digital_illustration/grain",
              "digital_illustration/infantile_sketch",
              "digital_illustration/2d_art_poster",
              "digital_illustration/handmade_3d",
              "digital_illustration/hand_drawn_outline",
              "digital_illustration/engraving_color",
              "digital_illustration/2d_art_poster_2",
              "digital_illustration/antiquarian",
              "digital_illustration/bold_fantasy",
              "digital_illustration/child_book",
              "digital_illustration/child_books",
              "digital_illustration/cover",
              "digital_illustration/crosshatch",
              "digital_illustration/digital_engraving",
              "digital_illustration/expressionism",
              "digital_illustration/freehand_details",
              "digital_illustration/grain_20",
              "digital_illustration/graphic_intensity",
              "digital_illustration/hard_comics",
              "digital_illustration/long_shadow",
              "digital_illustration/modern_folk",
              "digital_illustration/multicolor",
              "digital_illustration/neon_calm",
              "digital_illustration/noir",
              "digital_illustration/nostalgic_pastel",
              "digital_illustration/outline_details",
              "digital_illustration/pastel_gradient",
              "digital_illustration/pastel_sketch",
              "digital_illustration/pop_art",
              "digital_illustration/pop_renaissance",
              "digital_illustration/street_art",
              "digital_illustration/tablet_sketch",
              "digital_illustration/urban_glow",
              "digital_illustration/urban_sketching",
              "digital_illustration/vanilla_dreams",
              "digital_illustration/young_adult_book",
              "digital_illustration/young_adult_book_2",
              "vector_illustration/bold_stroke",
              "vector_illustration/chemistry",
              "vector_illustration/colored_stencil",
              "vector_illustration/contour_pop_art",
              "vector_illustration/cosmics",
              "vector_illustration/cutout",
              "vector_illustration/depressive",
              "vector_illustration/editorial",
              "vector_illustration/emotional_flat",
              "vector_illustration/infographical",
              "vector_illustration/marker_outline",
              "vector_illustration/mosaic",
              "vector_illustration/naivector",
              "vector_illustration/roundish_flat",
              "vector_illustration/segmented_colors",
              "vector_illustration/sharp_contrast",
              "vector_illustration/thin",
              "vector_illustration/vector_photo",
              "vector_illustration/vivid_shapes",
              "vector_illustration/engraving",
              "vector_illustration/line_art",
              "vector_illustration/line_circuit",
              "vector_illustration/linocut"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.RecraftV3ImageToImage.RecraftV3ImageToImageStyle"
          },
          "default": "realistic_image",
          "title": "Style",
          "description": "The artistic style to apply"
        },
        {
          "name": "style_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Id",
          "description": "The ID of the custom style reference (optional)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to modify. Must be less than 5 MB in size, have resolution less than 16 MP and max dimension less than 4096 pixels."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Strength",
          "description": "Defines the difference with the original image, should lie in [0, 1], where 0 means almost identical, and 1 means miserable similarity"
        },
        {
          "name": "colors",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Colors",
          "description": "An array of preferable colors"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A text description of undesired elements on an image"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image",
        "style"
      ]
    },
    {
      "title": "Recraft Vectorize",
      "description": "Converts a given raster image to SVG format using Recraft model.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.RecraftVectorize",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be vectorized. Must be in PNG, JPG or WEBP format, less than 5 MB in size, have resolution less than 16 MP and max dimension less than 4096 pixels, min dimension more than 256 pixels."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Retoucher",
      "description": "Automatically retouches faces to smooth skin and remove blemishes.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Retoucher",
      "properties": [
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducibility. Different seeds will make slightly different results."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be retouched."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Reve Edit",
      "description": "Reve\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ReveEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text description of how to edit the provided image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ReveEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the reference image to edit. Must be publicly accessible or base64 data URI. Supports PNG, JPEG, WebP, AVIF, and HEIF formats."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Reve Fast Edit",
      "description": "Reve\n    editing, transformation, image-to-image, img2img, fast\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ReveFastEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text description of how to edit the provided image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ReveFastEdit.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the reference image to edit. Must be publicly accessible or base64 data URI. Supports PNG, JPEG, WebP, AVIF, and HEIF formats."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Reve Fast Remix",
      "description": "Reve\n    editing, transformation, image-to-image, img2img, fast\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ReveFastRemix",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text description of the desired image. May include XML img tags like <img>0</img> to refer to specific images by their index in the image_urls list."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "16:9",
              "9:16",
              "3:2",
              "2:3",
              "4:3",
              "3:4",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ReveFastRemix.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The desired aspect ratio of the generated image. If not provided, will be smartly chosen by the model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ReveFastRemix.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the generated image."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of URLs of reference images. Must provide between 1 and 6 images (inclusive). Each image must be less than 10 MB. Supports PNG, JPEG, WebP, AVIF, and HEIF formats."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Reve Remix",
      "description": "Reve\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ReveRemix",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text description of the desired image. May include XML img tags like <img>0</img> to refer to specific images by their index in the image_urls list."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "16:9",
              "9:16",
              "3:2",
              "2:3",
              "4:3",
              "3:4",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ReveRemix.AspectRatio"
          },
          "default": null,
          "title": "Aspect Ratio",
          "description": "The desired aspect ratio of the generated image. If not provided, will be smartly chosen by the model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ReveRemix.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the generated image."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of URLs of reference images. Must provide between 1 and 6 images (inclusive). Each image must be less than 10 MB. Supports PNG, JPEG, WebP, AVIF, and HEIF formats."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Rife",
      "description": "RIFE\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Rife",
      "properties": [
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Rife.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the output images. Only applicable if output_type is 'images'."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Fps",
          "description": "Frames per second for the output video. Only applicable if output_type is 'video'."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "include_end",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Include End",
          "description": "Whether to include the end image in the output."
        },
        {
          "name": "include_start",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Include Start",
          "description": "Whether to include the start image in the output."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Frames",
          "description": "The number of frames to generate between the input images."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the second image to use as the ending point for interpolation."
        },
        {
          "name": "output_type",
          "type": {
            "type": "enum",
            "values": [
              "images",
              "video"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Rife.OutputType"
          },
          "default": "images",
          "title": "Output Type",
          "description": "The type of output to generate; either individual images or a video."
        },
        {
          "name": "start_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Start Image Url",
          "description": "The URL of the first image to use as the starting point for interpolation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "enum",
            "values": [
              "images",
              "video"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Rife.OutputType"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Rundiffusion Fal Juggernaut Flux Base Image To Image",
      "description": "Juggernaut Base Flux by RunDiffusion is a drop-in replacement for Flux [Dev] that delivers sharper details, richer colors, and enhanced realism, while instantly boosting LoRAs and LyCORIS with full compatibility.\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.RundiffusionFalJuggernautFluxBaseImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.RundiffusionFalJuggernautFluxBaseImageToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "The strength of the initial image. Higher strength values are better for this model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Rundiffusion Fal Juggernaut Flux Lora Inpainting",
      "description": "Juggernaut Base Flux LoRA Inpainting by RunDiffusion is a drop-in replacement for Flux [Dev] inpainting that delivers sharper details, richer colors, and enhanced realism to all your LoRAs and LyCORIS with full compatibility.\n    flux, editing, transformation, image-to-image, img2img, lora\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.RundiffusionFalJuggernautFluxLoraInpainting",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.RundiffusionFalJuggernautFluxLoraInpainting.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for inpainting. or img2img"
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "The mask to area to Inpaint in."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Rundiffusion Fal Juggernaut Flux Pro Image To Image",
      "description": "Juggernaut Pro Flux by RunDiffusion is the flagship Juggernaut model rivaling some of the most advanced image models available, often surpassing them in realism. It combines Juggernaut Base with RunDiffusion Photo and features enhancements like reduced background blurriness.\n    flux, editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.RundiffusionFalJuggernautFluxProImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.RundiffusionFalJuggernautFluxProImageToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate an image from."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "The strength of the initial image. Higher strength values are better for this model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Sam 2 Auto Segment",
      "description": "SAM 2 is a model for segmenting images automatically. It can return individual masks or a single mask for the entire image.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Sam2AutoSegment",
      "properties": [
        {
          "name": "points_per_side",
          "type": {
            "type": "int"
          },
          "default": 32,
          "title": "Points Per Side",
          "description": "Number of points to sample along each side of the image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Sam2AutoSegment.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "min_mask_region_area",
          "type": {
            "type": "int"
          },
          "default": 100,
          "title": "Min Mask Region Area",
          "description": "Minimum area of a mask region."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be automatically segmented"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "pred_iou_thresh",
          "type": {
            "type": "float"
          },
          "default": 0.88,
          "title": "Pred Iou Thresh",
          "description": "Threshold for predicted IOU score."
        },
        {
          "name": "stability_score_thresh",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Stability Score Thresh",
          "description": "Threshold for stability score."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Sam 2 Image",
      "description": "SAM 2 is a model for segmenting images and videos in real-time.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Sam2Image",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Sam2Image.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Prompts",
          "description": "List of prompts to segment the image"
        },
        {
          "name": "box_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Box Prompts",
          "description": "Coordinates for boxes"
        },
        {
          "name": "apply_mask",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Apply Mask",
          "description": "Apply the mask on the image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be segmented"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Sam 3 Image",
      "description": "Segment Anything Model 3\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Sam3Image",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "wheel",
          "title": "Prompt",
          "description": "Text prompt for segmentation"
        },
        {
          "name": "include_boxes",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Include Boxes",
          "description": "Whether to include bounding boxes for each mask (when available)."
        },
        {
          "name": "return_multiple_masks",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Multiple Masks",
          "description": "If True, upload and return multiple generated masks as defined by `max_masks`."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Sam3Image.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be segmented"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If True, the media will be returned as a data URI."
        },
        {
          "name": "point_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Point Prompts",
          "description": "List of point prompts"
        },
        {
          "name": "include_scores",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Include Scores",
          "description": "Whether to include mask confidence scores."
        },
        {
          "name": "max_masks",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "Max Masks",
          "description": "Maximum number of masks to return when `return_multiple_masks` is enabled."
        },
        {
          "name": "box_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Box Prompts",
          "description": "Box prompt coordinates (x_min, y_min, x_max, y_max). Multiple boxes supported - use object_id to group boxes for the same object or leave empty for separate objects."
        },
        {
          "name": "apply_mask",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Apply Mask",
          "description": "Apply the mask on the image."
        },
        {
          "name": "text_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Prompt",
          "description": "[DEPRECATED] Use 'prompt' instead. Kept for backward compatibility."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Sam 3 Image Rle",
      "description": "Sam 3\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Sam3ImageRle",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "wheel",
          "title": "Prompt",
          "description": "Text prompt for segmentation"
        },
        {
          "name": "include_boxes",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Include Boxes",
          "description": "Whether to include bounding boxes for each mask (when available)."
        },
        {
          "name": "return_multiple_masks",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Multiple Masks",
          "description": "If True, upload and return multiple generated masks as defined by `max_masks`."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Sam3ImageRle.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be segmented"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If True, the media will be returned as a data URI."
        },
        {
          "name": "point_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Point Prompts",
          "description": "List of point prompts"
        },
        {
          "name": "include_scores",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Include Scores",
          "description": "Whether to include mask confidence scores."
        },
        {
          "name": "max_masks",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "Max Masks",
          "description": "Maximum number of masks to return when `return_multiple_masks` is enabled."
        },
        {
          "name": "box_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Box Prompts",
          "description": "Box prompt coordinates (x_min, y_min, x_max, y_max). Multiple boxes supported - use object_id to group boxes for the same object or leave empty for separate objects."
        },
        {
          "name": "apply_mask",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Apply Mask",
          "description": "Apply the mask on the image."
        },
        {
          "name": "text_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Prompt",
          "description": "[DEPRECATED] Use 'prompt' instead. Kept for backward compatibility."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Sd 15 Depth Controlnet",
      "description": "SD 1.5 ControlNet\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Sd15DepthControlnet",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Leave it none to automatically infer from the control image."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The list of LoRA weights to use."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale of the controlnet conditioning."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "control_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 35,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "enable_deep_cache",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Deep Cache",
          "description": "If set to true, DeepCache will be enabled. TBD"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Sdxl Controlnet Union Image To Image",
      "description": "An efficent SDXL multi-controlnet image-to-image model.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.SdxlControlnetUnionImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "depth_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Depth Preprocess",
          "description": "Whether to preprocess the depth image."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Leave it none to automatically infer from the control image."
        },
        {
          "name": "normal_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Normal Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "teed_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Teed Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The list of LoRA weights to use."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "canny_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Canny Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "segmentation_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Segmentation Preprocess",
          "description": "Whether to preprocess the segmentation image."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.SdxlControlnetUnionImageToImage.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "segmentation_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Segmentation Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "openpose_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Openpose Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "canny_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Canny Preprocess",
          "description": "Whether to preprocess the canny image."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "depth_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Depth Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "normal_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Normal Preprocess",
          "description": "Whether to preprocess the normal image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "preserve_aspect_ratio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preserve Aspect Ratio",
          "description": "If set to true, the aspect ratio of the generated image will be preserved even if the image size is too large. However, if the image is not a multiple of 32 in width or height, it will be resized to the nearest multiple of 32. By default, this snapping to the nearest multiple of 32 will not preserve the aspect ratio. Set crop_output to True, to crop the output to the proper aspect ratio after generating."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "crop_output",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Crop Output",
          "description": "If set to true, the output cropped to the proper aspect ratio after generating."
        },
        {
          "name": "teed_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Teed Preprocess",
          "description": "Whether to preprocess the teed image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale of the controlnet conditioning."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "determines how much the generated image resembles the initial image"
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.SdxlControlnetUnionImageToImage.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "openpose_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Openpose Preprocess",
          "description": "Whether to preprocess the openpose image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 35,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Sdxl Controlnet Union Inpainting",
      "description": "An efficent SDXL multi-controlnet inpainting model.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.SdxlControlnetUnionInpainting",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "depth_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Depth Preprocess",
          "description": "Whether to preprocess the depth image."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Leave it none to automatically infer from the control image."
        },
        {
          "name": "normal_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Normal Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "teed_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Teed Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The list of LoRA weights to use."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "canny_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Canny Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "segmentation_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Segmentation Preprocess",
          "description": "Whether to preprocess the segmentation image."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.SdxlControlnetUnionInpainting.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "The URL of the mask to use for inpainting."
        },
        {
          "name": "segmentation_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Segmentation Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "openpose_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Openpose Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "canny_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Canny Preprocess",
          "description": "Whether to preprocess the canny image."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "depth_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Depth Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "normal_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Normal Preprocess",
          "description": "Whether to preprocess the normal image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "teed_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Teed Preprocess",
          "description": "Whether to preprocess the teed image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale of the controlnet conditioning."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "determines how much the generated image resembles the initial image"
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.SdxlControlnetUnionInpainting.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "openpose_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Openpose Preprocess",
          "description": "Whether to preprocess the openpose image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 35,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Seedvr Upscale Image",
      "description": "SeedVR2\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.SeedvrUpscaleImage",
      "properties": [
        {
          "name": "upscale_mode",
          "type": {
            "type": "enum",
            "values": [
              "target",
              "factor"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.SeedvrUpscaleImage.UpscaleMode"
          },
          "default": "factor",
          "title": "Upscale Mode",
          "description": "The mode to use for the upscale. If 'target', the upscale factor will be calculated based on the target resolution. If 'factor', the upscale factor will be used directly."
        },
        {
          "name": "noise_scale",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Noise Scale",
          "description": "The noise scale to use for the generation process."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.SeedvrUpscaleImage.OutputFormat"
          },
          "default": "jpg",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "target_resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "1440p",
              "2160p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.SeedvrUpscaleImage.TargetResolution"
          },
          "default": "1080p",
          "title": "Target Resolution",
          "description": "The target resolution to upscale to when `upscale_mode` is `target`."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The input image to be processed"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "upscale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Upscale Factor",
          "description": "Upscaling factor to be used. Will multiply the dimensions with this factor when `upscale_mode` is `factor`."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The random seed used for the generation process."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Smoretalk Ai Rembg Enhance",
      "description": "Rembg-enhance is optimized for 2D vector images, 3D graphics, and photos by leveraging matting technology.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.SmoretalkAiRembgEnhance",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Stable Diffusion V3 Medium Image To Image",
      "description": "Stable Diffusion 3 Medium (Image to Image) is a Multimodal Diffusion Transformer (MMDiT) model that improves image quality, typography, prompt understanding, and efficiency.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.StableDiffusionV3MediumImageToImage",
      "properties": [
        {
          "name": "prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Prompt Expansion",
          "description": "If set to true, prompt will be upsampled with more details."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Defaults to the conditioning image's size."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Strength",
          "description": "The strength of the image-to-image transformation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate an image from."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Star Vector",
      "description": "AI vectorization model that transforms raster images into scalable SVG graphics, preserving visual details while enabling infinite scaling and easy editing capabilities.  \n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.StarVector",
      "properties": [
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "seed to be used for generation"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to be used for relighting"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Step 1 x Edit",
      "description": "Step1X-Edit transforms your photos with simple instructions into stunning, professional-quality edits\u2014rivaling top proprietary tools.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Step1xEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Step1xEdit.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. Needs to match the dimensions of the mask."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Stepx Edit 2",
      "description": "StepX Edit 2 provides multi-step image editing with progressive refinement and control.\n    image, editing, stepx, progressive, refinement\n\n    Use cases:\n    - Edit images with progressive steps\n    - Apply multi-stage modifications\n    - Create refined edits gradually\n    - Transform images with step control\n    - Produce progressively refined results",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.StepxEdit2",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_reflection_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Reflection Mode",
          "description": "Enable reflection mode. Reviews outputs, corrects unintended changes, and determines when editing is complete."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.StepxEdit2.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from. Needs to match the dimensions of the mask."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 6,
          "title": "Guidance Scale",
          "description": "The true CFG scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform. Recommended: 50."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "enable_thinking_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Thinking Mode",
          "description": "Enable thinking mode. Uses multimodal language model knowledge to interpret abstract editing instructions."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Swin 2 sr",
      "description": "Enhance low-resolution images with the superior quality of Swin2SR for sharper, clearer results.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Swin2sr",
      "properties": [
        {
          "name": "task",
          "type": {
            "type": "enum",
            "values": [
              "classical_sr",
              "compressed_sr",
              "real_sr"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Swin2sr.Task"
          },
          "default": "classical_sr",
          "title": "Task",
          "description": "Task to perform"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "seed to be used for generation"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to be used for image enhancement"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Thera",
      "description": "Fix low resolution images with fast speed and quality of thera.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Thera",
      "properties": [
        {
          "name": "upscale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Upscale Factor",
          "description": "The upscaling factor for the image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation."
        },
        {
          "name": "backbone",
          "type": {
            "type": "enum",
            "values": [
              "edsr",
              "rdn"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Thera.Backbone"
          },
          "default": "",
          "title": "Backbone",
          "description": "Backbone to use for upscaling"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to be used for upscaling"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Topaz Upscale Image",
      "description": "Use the powerful and accurate topaz image enhancer to enhance your images.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.TopazUpscaleImage",
      "properties": [
        {
          "name": "face_enhancement_creativity",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Face Enhancement Creativity",
          "description": "Creativity level for face enhancement. 0.0 means no creativity, 1.0 means maximum creativity. Ignored if face ehnancement is disabled."
        },
        {
          "name": "face_enhancement_strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Face Enhancement Strength",
          "description": "Strength of the face enhancement. 0.0 means no enhancement, 1.0 means maximum enhancement. Ignored if face ehnancement is disabled."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.TopazUpscaleImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "Output format of the upscaled image."
        },
        {
          "name": "face_enhancement",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Face Enhancement",
          "description": "Whether to apply face enhancement to the image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Url of the image to be upscaled"
        },
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "Low Resolution V2",
              "Standard V2",
              "CGI",
              "High Fidelity V2",
              "Text Refine",
              "Recovery",
              "Redefine",
              "Recovery V2"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.TopazUpscaleImage.Model"
          },
          "default": "Standard V2",
          "title": "Model",
          "description": "Model to use for image enhancement."
        },
        {
          "name": "subject_detection",
          "type": {
            "type": "enum",
            "values": [
              "All",
              "Foreground",
              "Background"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.TopazUpscaleImage.SubjectDetection"
          },
          "default": "All",
          "title": "Subject Detection",
          "description": "Subject detection mode for the image enhancement."
        },
        {
          "name": "crop_to_fill",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Crop To Fill"
        },
        {
          "name": "upscale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Upscale Factor",
          "description": "Factor to upscale the video by (e.g. 2.0 doubles width and height)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Uno",
      "description": "An AI model that transforms input images into new ones based on text prompts, blending reference visuals with your creative directions.\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Uno",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image. You can choose between some presets or custom height and width that **must be multiples of 8**."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Uno.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "URL of images to use while generating the image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation. If set none, a random seed will be used."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Uso",
      "description": "Uso\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Uso",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for generation. Can be empty for pure style transfer."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate in parallel."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.Uso.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output image format. PNG preserves transparency, JPEG is smaller."
        },
        {
          "name": "keep_size",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Keep Size",
          "description": "Preserve the layout and dimensions of the input content image. Useful for style transfer."
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "List of image URLs in order: [content_image, style_image, extra_style_image]."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If true, wait for generation and upload before returning. Increases latency but provides immediate access to images."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "How closely to follow the prompt. Higher values stick closer to the prompt."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps. More steps can improve quality but increase generation time."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation. Use same seed for consistent results."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "What you don't want in the image. Use it to exclude unwanted elements, styles, or artifacts."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable NSFW content detection and filtering."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Vidu Q2 Reference To Image",
      "description": "Vidu Q2 Reference-to-Image generates images based on reference images with style and content transfer.\n    image, generation, vidu, reference, style-transfer\n\n    Use cases:\n    - Generate images from references\n    - Transfer style and content\n    - Create reference-based variations\n    - Transform using reference images\n    - Produce style-transferred results",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ViduQ2ReferenceToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation, max 1500 characters"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ViduQ2ReferenceToImage.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the output video"
        },
        {
          "name": "reference_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Image Urls",
          "description": "URLs of the reference images to use for consistent subject appearance"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Vidu Reference To Image",
      "description": "Vidu\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ViduReferenceToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation, max 1500 characters"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ViduReferenceToImage.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the output video"
        },
        {
          "name": "reference_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Image Urls",
          "description": "URLs of the reference images to use for consistent subject appearance"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan 25 Preview Image To Image",
      "description": "Wan 2.5 Image to Image\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.Wan25PreviewImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing how to edit the image. Max 2000 characters."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate. Values from 1 to 4."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square",
          "title": "Image Size",
          "description": "The size of the generated image. Width and height must be between 384 and 1440 pixels."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "URLs of images to edit. For single-image editing, provide 1 URL. For multi-reference generation, provide up to 2 URLs. If more than 2 URLs are provided, only the first 2 will be used."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to describe content to avoid. Max 500 characters."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan V22 A14 BImage To Image",
      "description": "Wan\n    editing, transformation, image-to-image, img2img\n\n    Use cases:\n    - Professional photo editing and enhancement\n    - Creative image transformations\n    - Batch image processing workflows\n    - Product photography refinement\n    - Automated image optimization",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.WanV22A14BImageToImage",
      "properties": [
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Shift"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide image generation."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.WanV22A14BImageToImage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "image_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.WanV22A14BImageToImage.ImageFormat"
          },
          "default": "jpeg",
          "title": "Image Format",
          "description": "The format of the output image."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.WanV22A14BImageToImage.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated image. If 'auto', the aspect ratio will be determined automatically based on the input image."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Strength",
          "description": "Denoising strength. 1.0 = fully remake; 0.0 = preserve original."
        },
        {
          "name": "guidance_scale_2",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale 2",
          "description": "Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan V26 Image To Image",
      "description": "Wan v2.6 image-to-image provides high-quality image transformations with advanced AI capabilities.\n    image, transformation, wan, v2.6, quality\n\n    Use cases:\n    - Transform images with Wan v2.6\n    - Apply quality modifications to photos\n    - Create high-quality variations\n    - Generate advanced transformations\n    - Produce quality image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.WanV26ImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the desired image. Supports Chinese and English. Max 2000 characters. Example: 'Generate an image using the style of image 1 and background of image 2'."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate (1-4). Directly affects billing cost."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "Output image size. Use presets like 'square_hd', 'landscape_16_9', 'portrait_9_16', or specify exact dimensions with ImageSize(width=1280, height=720). Total pixels must be between 768*768 and 1280*1280."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable content moderation for input and output."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility (0-2147483647). Same seed produces more consistent results."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Reference images for editing (1-3 images required). Order matters: reference as 'image 1', 'image 2', 'image 3' in prompt. Resolution: 384-5000px each dimension. Max size: 10MB each. Formats: JPEG, JPG, PNG (no alpha), BMP, WEBP."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Content to avoid in the generated image. Max 500 characters."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Enable LLM prompt optimization. Significantly improves results for simple prompts but adds 3-4 seconds processing time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "ZImage Turbo Controlnet",
      "description": "Z-Image Turbo ControlNet provides fast controlled image generation with structural guidance.\n    image, controlnet, z-image, turbo, controlled\n\n    Use cases:\n    - Generate images with fast structural control\n    - Apply quick controlled modifications\n    - Create rapid guided generations\n    - Transform images with fast ControlNet\n    - Produce speedy controlled outputs",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ZImageTurboControlnet",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboControlnet.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "control_end",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Control End",
          "description": "The end of the controlnet conditioning."
        },
        {
          "name": "control_start",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Control Start",
          "description": "The start of the controlnet conditioning."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboControlnet.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of Image for ControlNet generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "control_scale",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Control Scale",
          "description": "The scale of the controlnet conditioning."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "preprocess",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "canny",
              "depth",
              "pose"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboControlnet.Preprocess"
          },
          "default": "none",
          "title": "Preprocess",
          "description": "What kind of preprocessing to apply to the image, if any."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "ZImage Turbo Controlnet Lora",
      "description": "Z-Image Turbo ControlNet with LoRA combines fast controlled generation with custom models.\n    image, controlnet, z-image, turbo, lora\n\n    Use cases:\n    - Generate with fast custom ControlNet\n    - Apply quick specialized controlled generation\n    - Create rapid custom guided outputs\n    - Transform images with fast custom control\n    - Produce speedy fine-tuned controlled results",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ZImageTurboControlnetLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboControlnetLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "control_end",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Control End",
          "description": "The end of the controlnet conditioning."
        },
        {
          "name": "control_start",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Control Start",
          "description": "The start of the controlnet conditioning."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboControlnetLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of Image for ControlNet generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "control_scale",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Control Scale",
          "description": "The scale of the controlnet conditioning."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "preprocess",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "canny",
              "depth",
              "pose"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboControlnetLora.Preprocess"
          },
          "default": "none",
          "title": "Preprocess",
          "description": "What kind of preprocessing to apply to the image, if any."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "ZImage Turbo Image To Image",
      "description": "Z-Image Turbo image-to-image provides fast image transformations with quality output.\n    image, transformation, z-image, turbo, fast\n\n    Use cases:\n    - Transform images quickly with Z-Image\n    - Apply fast modifications to photos\n    - Create rapid image variations\n    - Generate speedy transformations\n    - Produce quick image modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ZImageTurboImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboImageToImage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboImageToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of Image for Image-to-Image generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.6,
          "title": "Strength",
          "description": "The strength of the image-to-image conditioning."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "ZImage Turbo Image To Image Lora",
      "description": "Z-Image Turbo image-to-image with LoRA enables fast custom-trained model transformations.\n    image, transformation, z-image, turbo, lora\n\n    Use cases:\n    - Transform images with custom Z-Image models\n    - Apply fast specialized modifications\n    - Create rapid custom edits\n    - Generate quick customized transformations\n    - Produce fast fine-tuned modifications",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ZImageTurboImageToImageLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboImageToImageLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboImageToImageLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of Image for Image-to-Image generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.6,
          "title": "Strength",
          "description": "The strength of the image-to-image conditioning."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "ZImage Turbo Inpaint",
      "description": "Z-Image Turbo Inpaint fills masked regions in images quickly with contextually appropriate content.\n    image, inpainting, z-image, turbo, fast\n\n    Use cases:\n    - Fill masked regions in images quickly\n    - Remove unwanted objects fast\n    - Repair image areas with turbo speed\n    - Generate quick inpainting results\n    - Produce rapid contextual fills",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ZImageTurboInpaint",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboInpaint.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "mask_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image Url",
          "description": "URL of Mask for Inpaint generation."
        },
        {
          "name": "control_end",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Control End",
          "description": "The end of the controlnet conditioning."
        },
        {
          "name": "control_start",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Control Start",
          "description": "The start of the controlnet conditioning."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboInpaint.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of Image for Inpaint generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Strength",
          "description": "The strength of the inpaint conditioning."
        },
        {
          "name": "control_scale",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Control Scale",
          "description": "The scale of the controlnet conditioning."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "ZImage Turbo Inpaint Lora",
      "description": "Z-Image Turbo Inpaint with LoRA provides fast custom-trained inpainting for specialized tasks.\n    image, inpainting, z-image, turbo, lora\n\n    Use cases:\n    - Inpaint with custom fast models\n    - Fill regions using specialized training\n    - Repair images with custom inpainting\n    - Generate quick custom fills\n    - Produce rapid specialized inpainting",
      "namespace": "fal.image_to_image",
      "node_type": "fal.image_to_image.ZImageTurboInpaintLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboInpaintLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "mask_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image Url",
          "description": "URL of Mask for Inpaint generation."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "control_end",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Control End",
          "description": "The end of the controlnet conditioning."
        },
        {
          "name": "control_start",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Control Start",
          "description": "The start of the controlnet conditioning."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.image_to_image.ZImageTurboInpaintLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of Image for Inpaint generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Strength",
          "description": "The strength of the inpaint conditioning."
        },
        {
          "name": "control_scale",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Control Scale",
          "description": "The scale of the controlnet conditioning."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Hunyuan 3 d V3 Text To 3 d",
      "description": "Turn simple sketches into detailed, fully-textured 3D models. Instantly convert your concept designs into formats ready for Unity, Unreal, and Blender.\n    3d, generation, text-to-3d, modeling\n\n    Use cases:\n    - 3D model generation from text\n    - Concept visualization\n    - Game asset creation\n    - Architectural prototyping\n    - Product design visualization",
      "namespace": "fal.text_to_3d",
      "node_type": "fal.text_to_3d.Hunyuan3dV3TextTo3d",
      "properties": [
        {
          "name": "enable_pbr",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Pbr",
          "description": "Whether to enable PBR material generation"
        },
        {
          "name": "polygon_type",
          "type": {
            "type": "enum",
            "values": [
              "triangle",
              "quadrilateral"
            ],
            "type_name": "nodetool.nodes.fal.text_to_3d.Hunyuan3dV3TextTo3d.PolygonType"
          },
          "default": "triangle",
          "title": "Polygon Type",
          "description": "Polygon type. Only takes effect when GenerateType is LowPoly."
        },
        {
          "name": "face_count",
          "type": {
            "type": "int"
          },
          "default": 500000,
          "title": "Face Count",
          "description": "Target face count. Range: 40000-1500000"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text description of the 3D content to generate. Supports up to 1024 UTF-8 characters."
        },
        {
          "name": "generate_type",
          "type": {
            "type": "enum",
            "values": [
              "Normal",
              "LowPoly",
              "Geometry"
            ],
            "type_name": "nodetool.nodes.fal.text_to_3d.Hunyuan3dV3TextTo3d.GenerateType"
          },
          "default": "Normal",
          "title": "Generate Type",
          "description": "Generation type. Normal: textured model. LowPoly: polygon reduction. Geometry: white model without texture."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "enable_pbr",
        "polygon_type",
        "face_count",
        "prompt",
        "generate_type"
      ]
    },
    {
      "title": "Hunyuan Motion",
      "description": "Generate 3D human motions via text-to-generation interface of Hunyuan Motion!\n    3d, generation, text-to-3d, modeling\n\n    Use cases:\n    - 3D model generation from text\n    - Concept visualization\n    - Game asset creation\n    - Architectural prototyping\n    - Product design visualization",
      "namespace": "fal.text_to_3d",
      "node_type": "fal.text_to_3d.HunyuanMotion",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the motion to generate."
        },
        {
          "name": "duration",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Duration",
          "description": "Motion duration in seconds (0.5-12.0)."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher = more faithful to prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "fbx",
              "dict"
            ],
            "type_name": "nodetool.nodes.fal.text_to_3d.HunyuanMotion.OutputFormat"
          },
          "default": "fbx",
          "title": "Output Format",
          "description": "Output format: 'fbx' for animation files, 'dict' for raw JSON."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "duration",
        "guidance_scale",
        "seed",
        "output_format"
      ]
    },
    {
      "title": "Hunyuan Motion Fast",
      "description": "Generate 3D human motions via text-to-generation interface of Hunyuan Motion!\n    3d, generation, text-to-3d, modeling, fast\n\n    Use cases:\n    - 3D model generation from text\n    - Concept visualization\n    - Game asset creation\n    - Architectural prototyping\n    - Product design visualization",
      "namespace": "fal.text_to_3d",
      "node_type": "fal.text_to_3d.HunyuanMotionFast",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the motion to generate."
        },
        {
          "name": "duration",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Duration",
          "description": "Motion duration in seconds (0.5-12.0)."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher = more faithful to prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "fbx",
              "dict"
            ],
            "type_name": "nodetool.nodes.fal.text_to_3d.HunyuanMotionFast.OutputFormat"
          },
          "default": "fbx",
          "title": "Output Format",
          "description": "Output format: 'fbx' for animation files, 'dict' for raw JSON."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "duration",
        "guidance_scale",
        "seed",
        "output_format"
      ]
    },
    {
      "title": "Meshy V6 Preview Text To 3 d",
      "description": "Meshy-6-Preview is the latest model from Meshy. It generates realistic and production ready 3D models.\n    3d, generation, text-to-3d, modeling\n\n    Use cases:\n    - 3D model generation from text\n    - Concept visualization\n    - Game asset creation\n    - Architectural prototyping\n    - Product design visualization",
      "namespace": "fal.text_to_3d",
      "node_type": "fal.text_to_3d.MeshyV6PreviewTextTo3d",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Describe what kind of object the 3D model is. Maximum 600 characters."
        },
        {
          "name": "enable_pbr",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Pbr",
          "description": "Generate PBR Maps (metallic, roughness, normal) in addition to base color. Should be false for sculpture style."
        },
        {
          "name": "target_polycount",
          "type": {
            "type": "int"
          },
          "default": 30000,
          "title": "Target Polycount",
          "description": "Target number of polygons in the generated model"
        },
        {
          "name": "art_style",
          "type": {
            "type": "enum",
            "values": [
              "realistic",
              "sculpture"
            ],
            "type_name": "nodetool.nodes.fal.text_to_3d.MeshyV6PreviewTextTo3d.ArtStyle"
          },
          "default": "realistic",
          "title": "Art Style",
          "description": "Desired art style of the object. Note: enable_pbr should be false for sculpture style."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "mode",
          "type": {
            "type": "enum",
            "values": [
              "preview",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.text_to_3d.MeshyV6PreviewTextTo3d.Mode"
          },
          "default": "full",
          "title": "Mode",
          "description": "Generation mode. 'preview' returns untextured geometry only, 'full' returns textured model (preview + refine)."
        },
        {
          "name": "symmetry_mode",
          "type": {
            "type": "enum",
            "values": [
              "off",
              "auto",
              "on"
            ],
            "type_name": "nodetool.nodes.fal.text_to_3d.MeshyV6PreviewTextTo3d.SymmetryMode"
          },
          "default": "auto",
          "title": "Symmetry Mode",
          "description": "Controls symmetry behavior during model generation."
        },
        {
          "name": "should_remesh",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Should Remesh",
          "description": "Whether to enable the remesh phase. When false, returns unprocessed triangular mesh."
        },
        {
          "name": "texture_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Texture Image Url",
          "description": "2D image to guide the texturing process (only used in 'full' mode)"
        },
        {
          "name": "topology",
          "type": {
            "type": "enum",
            "values": [
              "quad",
              "triangle"
            ],
            "type_name": "nodetool.nodes.fal.text_to_3d.MeshyV6PreviewTextTo3d.Topology"
          },
          "default": "triangle",
          "title": "Topology",
          "description": "Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Same prompt and seed usually generate the same result."
        },
        {
          "name": "is_a_t_pose",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Is A T Pose",
          "description": "Whether to generate the model in an A/T pose"
        },
        {
          "name": "texture_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Texture Prompt",
          "description": "Additional text prompt to guide the texturing process (only used in 'full' mode)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "enable_pbr",
        "target_polycount",
        "art_style",
        "enable_safety_checker"
      ]
    },
    {
      "title": "Openrouter Router Audio",
      "description": "Run any ALM (Audio Language Model) with fal, powered by OpenRouter.\n    utility, processing, general\n\n    Use cases:\n    - General media processing\n    - Utility operations\n    - Content manipulation\n    - Automated workflows\n    - Data processing",
      "namespace": "fal.unknown",
      "node_type": "fal.unknown.OpenrouterRouterAudio",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to be used for the audio processing"
        },
        {
          "name": "reasoning",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reasoning",
          "description": "Should reasoning be the part of the final answer."
        },
        {
          "name": "system_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "System Prompt",
          "description": "System prompt to provide context or instructions to the model"
        },
        {
          "name": "model",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model",
          "description": "Name of the model to use. Charged based on actual token usage."
        },
        {
          "name": "max_tokens",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Max Tokens",
          "description": "This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Temperature",
          "description": "This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL or data URI of the audio file to process. Supported formats: wav, mp3, aiff, aac, ogg, flac, m4a."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "reasoning",
        "system_prompt",
        "model",
        "max_tokens"
      ]
    },
    {
      "title": "Qwen 3 Tts Clone Voice 06 b",
      "description": "Clone your voices using Qwen3-TTS Clone-Voice model with zero shot cloning capabilities and use it on text-to-speech models to create speeches of yours!\n    utility, processing, general\n\n    Use cases:\n    - General media processing\n    - Utility operations\n    - Content manipulation\n    - Automated workflows\n    - Data processing",
      "namespace": "fal.unknown",
      "node_type": "fal.unknown.Qwen3TtsCloneVoice06b",
      "properties": [
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL to the reference audio file used for voice cloning."
        },
        {
          "name": "reference_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Reference Text",
          "description": "Optional reference text that was used when creating the speaker embedding. Providing this can improve synthesis quality when using a cloned voice."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio_url",
        "reference_text"
      ]
    },
    {
      "title": "Qwen 3 Tts Clone Voice 17 b",
      "description": "Clone your voices using Qwen3-TTS Clone-Voice model with zero shot cloning capabilities and use it on text-to-speech models to create speeches of yours!\n    utility, processing, general\n\n    Use cases:\n    - General media processing\n    - Utility operations\n    - Content manipulation\n    - Automated workflows\n    - Data processing",
      "namespace": "fal.unknown",
      "node_type": "fal.unknown.Qwen3TtsCloneVoice17b",
      "properties": [
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL to the reference audio file used for voice cloning."
        },
        {
          "name": "reference_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Reference Text",
          "description": "Optional reference text that was used when creating the speaker embedding. Providing this can improve synthesis quality when using a cloned voice."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio_url",
        "reference_text"
      ]
    },
    {
      "title": "Workflow Utilities Interleave Video",
      "description": "ffmpeg utility to interleave videos\n    utility, processing, general\n\n    Use cases:\n    - General media processing\n    - Utility operations\n    - Content manipulation\n    - Automated workflows\n    - Data processing",
      "namespace": "fal.unknown",
      "node_type": "fal.unknown.WorkflowUtilitiesInterleaveVideo",
      "properties": [
        {
          "name": "video_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Video Urls",
          "description": "List of video URLs to interleave in order"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video_urls"
      ]
    },
    {
      "title": "Eleven Labs Scribe V2",
      "description": "ElevenLabs Scribe V2 provides blazingly fast speech-to-text transcription.\n    audio, transcription, stt, fast, elevenlabs, speech-to-text\n\n    Use cases:\n    - Fast audio transcription\n    - Real-time speech recognition\n    - Quick transcript generation\n    - High-speed audio processing\n    - Rapid speech-to-text conversion",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.ElevenLabsScribeV2",
      "properties": [
        {
          "name": "language_code",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Language Code",
          "description": "Language code of the audio"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to transcribe"
        },
        {
          "name": "diarize",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Diarize",
          "description": "Whether to annotate who is speaking"
        },
        {
          "name": "keyterms",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Keyterms",
          "description": "Words or sentences to bias the model towards transcribing. Up to 100 keyterms, max 50 characters each. Adds 30% premium over base transcription price."
        },
        {
          "name": "tag_audio_events",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Tag Audio Events",
          "description": "Tag audio events like laughter, applause, etc."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Eleven Labs Speech To Text",
      "description": "ElevenLabs Speech to Text transcribes audio to text with high accuracy.\n    audio, transcription, stt, elevenlabs, speech-to-text\n\n    Use cases:\n    - Transcribe audio files\n    - Convert speech to text\n    - Generate transcripts from audio\n    - Extract text from recordings\n    - Create captions from audio",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.ElevenLabsSpeechToText",
      "properties": [
        {
          "name": "language_code",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Language Code",
          "description": "Language code of the audio"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to transcribe"
        },
        {
          "name": "diarize",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Diarize",
          "description": "Whether to annotate who is speaking"
        },
        {
          "name": "tag_audio_events",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Tag Audio Events",
          "description": "Tag audio events like laughter, applause, etc."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Smart Turn",
      "description": "Pipecat's Smart Turn model provides native audio turn detection for conversations.\n    audio, turn-detection, conversation, pipecat, speech-analysis\n\n    Use cases:\n    - Detect conversation turns\n    - Identify speaker changes\n    - Analyze dialogue timing\n    - Detect speech boundaries\n    - Process conversational audio",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.SmartTurn",
      "properties": [
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Speech To Text",
      "description": "General-purpose speech-to-text model for accurate audio transcription.\n    audio, transcription, stt, speech-to-text\n\n    Use cases:\n    - General audio transcription\n    - Convert speech recordings to text\n    - Generate audio transcripts\n    - Process voice recordings\n    - Extract text from speech",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.SpeechToText",
      "properties": [
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "Local filesystem path (or remote URL) to a long audio file"
        },
        {
          "name": "use_pnc",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Pnc",
          "description": "Whether to use Canary's built-in punctuation & capitalization"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Speech To Text Stream",
      "description": "Streaming speech-to-text for real-time audio transcription.\n    audio, transcription, stt, streaming, real-time, speech-to-text\n\n    Use cases:\n    - Real-time transcription\n    - Live audio captioning\n    - Stream audio processing\n    - Continuous speech recognition\n    - Live speech-to-text conversion",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.SpeechToTextStream",
      "properties": [
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "Local filesystem path (or remote URL) to a long audio file"
        },
        {
          "name": "use_pnc",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Pnc",
          "description": "Whether to use Canary's built-in punctuation & capitalization"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio_stream"
      ]
    },
    {
      "title": "Speech To Text Turbo",
      "description": "High-speed speech-to-text model optimized for fast transcription.\n    audio, transcription, stt, turbo, fast, speech-to-text\n\n    Use cases:\n    - Fast audio transcription\n    - Quick speech recognition\n    - Rapid transcript generation\n    - High-speed processing\n    - Efficient speech-to-text",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.SpeechToTextTurbo",
      "properties": [
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "Local filesystem path (or remote URL) to a long audio file"
        },
        {
          "name": "use_pnc",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Pnc",
          "description": "Whether to use Canary's built-in punctuation & capitalization"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Speech To Text Turbo Stream",
      "description": "High-speed streaming speech-to-text for real-time fast transcription.\n    audio, transcription, stt, turbo, streaming, fast, speech-to-text\n\n    Use cases:\n    - Real-time fast transcription\n    - Live fast captioning\n    - High-speed streaming STT\n    - Rapid live transcription\n    - Efficient real-time processing",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.SpeechToTextTurboStream",
      "properties": [
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "Local filesystem path (or remote URL) to a long audio file"
        },
        {
          "name": "use_pnc",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Pnc",
          "description": "Whether to use Canary's built-in punctuation & capitalization"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio_stream"
      ]
    },
    {
      "title": "Whisper",
      "description": "OpenAI's Whisper model for robust multilingual speech recognition.\n    audio, transcription, stt, whisper, multilingual, speech-to-text\n\n    Use cases:\n    - Multilingual transcription\n    - Robust speech recognition\n    - Transcribe multiple languages\n    - Handle noisy audio\n    - International audio processing",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.Whisper",
      "properties": [
        {
          "name": "version",
          "type": {
            "type": "enum",
            "values": [
              "3"
            ],
            "type_name": "nodetool.nodes.fal.speech_to_text.Whisper.Version"
          },
          "default": "3",
          "title": "Version",
          "description": "Version of the model to use. All of the models are the Whisper large variant."
        },
        {
          "name": "batch_size",
          "type": {
            "type": "int"
          },
          "default": 64,
          "title": "Batch Size"
        },
        {
          "name": "language",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "af",
              "am",
              "ar",
              "as",
              "az",
              "ba",
              "be",
              "bg",
              "bn",
              "bo",
              "br",
              "bs",
              "ca",
              "cs",
              "cy",
              "da",
              "de",
              "el",
              "en",
              "es",
              "et",
              "eu",
              "fa",
              "fi",
              "fo",
              "fr",
              "gl",
              "gu",
              "ha",
              "haw",
              "he",
              "hi",
              "hr",
              "ht",
              "hu",
              "hy",
              "id",
              "is",
              "it",
              "ja",
              "jw",
              "ka",
              "kk",
              "km",
              "kn",
              "ko",
              "la",
              "lb",
              "ln",
              "lo",
              "lt",
              "lv",
              "mg",
              "mi",
              "mk",
              "ml",
              "mn",
              "mr",
              "ms",
              "mt",
              "my",
              "ne",
              "nl",
              "nn",
              "no",
              "oc",
              "pa",
              "pl",
              "ps",
              "pt",
              "ro",
              "ru",
              "sa",
              "sd",
              "si",
              "sk",
              "sl",
              "sn",
              "so",
              "sq",
              "sr",
              "su",
              "sv",
              "sw",
              "ta",
              "te",
              "tg",
              "th",
              "tk",
              "tl",
              "tr",
              "tt",
              "uk",
              "ur",
              "uz",
              "vi",
              "yi",
              "yo",
              "zh"
            ],
            "type_name": "nodetool.nodes.fal.speech_to_text.Whisper.Language"
          },
          "default": null,
          "title": "Language",
          "description": "Language of the audio file. If set to null, the language will be automatically detected. Defaults to null. If translate is selected as the task, the audio will be translated to English, regardless of the language selected."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to use for generation. Defaults to an empty string."
        },
        {
          "name": "num_speakers",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Speakers",
          "description": "Number of speakers in the audio file. Defaults to null. If not provided, the number of speakers will be automatically detected."
        },
        {
          "name": "task",
          "type": {
            "type": "enum",
            "values": [
              "transcribe",
              "translate"
            ],
            "type_name": "nodetool.nodes.fal.speech_to_text.Whisper.Task"
          },
          "default": "transcribe",
          "title": "Task",
          "description": "Task to perform on the audio file. Either transcribe or translate."
        },
        {
          "name": "chunk_level",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "segment",
              "word"
            ],
            "type_name": "nodetool.nodes.fal.speech_to_text.Whisper.ChunkLevel"
          },
          "default": "segment",
          "title": "Chunk Level",
          "description": "Level of the chunks to return. Either none, segment or word. `none` would imply that all of the audio will be transcribed without the timestamp tokens, we suggest to switch to `none` if you are not satisfied with the transcription quality, since it will usually improve the quality of the results. Switching to `none` will also provide minor speed ups in the transcription due to less amount of generated tokens. Notice that setting to none will produce **a single chunk with the whole transcription**."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm."
        },
        {
          "name": "diarize",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Diarize",
          "description": "Whether to diarize the audio file. Defaults to false. Setting to true will add costs proportional to diarization inference time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Wizper",
      "description": "Wizper provides fast and accurate speech-to-text transcription.\n    audio, transcription, stt, wizper, fast, speech-to-text\n\n    Use cases:\n    - Fast accurate transcription\n    - Quick speech recognition\n    - Efficient audio processing\n    - Rapid text extraction\n    - Speedy speech-to-text",
      "namespace": "fal.speech_to_text",
      "node_type": "fal.speech_to_text.Wizper",
      "properties": [
        {
          "name": "language",
          "type": {
            "type": "str"
          },
          "default": "en",
          "title": "Language",
          "description": "Language of the audio file. If translate is selected as the task, the audio will be translated to English, regardless of the language selected. If `None` is passed, the language will be automatically detected. This will also increase the inference time."
        },
        {
          "name": "version",
          "type": {
            "type": "str"
          },
          "default": "3",
          "title": "Version",
          "description": "Version of the model to use. All of the models are the Whisper large variant."
        },
        {
          "name": "max_segment_len",
          "type": {
            "type": "int"
          },
          "default": 29,
          "title": "Max Segment Len",
          "description": "Maximum speech segment duration in seconds before splitting."
        },
        {
          "name": "task",
          "type": {
            "type": "enum",
            "values": [
              "transcribe",
              "translate"
            ],
            "type_name": "nodetool.nodes.fal.speech_to_text.Wizper.Task"
          },
          "default": "transcribe",
          "title": "Task",
          "description": "Task to perform on the audio file. Either transcribe or translate."
        },
        {
          "name": "chunk_level",
          "type": {
            "type": "str"
          },
          "default": "segment",
          "title": "Chunk Level",
          "description": "Level of the chunks to return."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to transcribe. Supported formats: mp3, mp4, mpeg, mpga, m4a, wav or webm."
        },
        {
          "name": "merge_chunks",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Merge Chunks",
          "description": "Whether to merge consecutive chunks. When enabled, chunks are merged if their combined duration does not exceed max_segment_len."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "DWPose",
      "description": "DWPose detects human poses and keypoints in images.\n    pose, detection, keypoints, human\n\n    Use cases:\n    - Detect human poses\n    - Extract body keypoints\n    - Enable pose-guided generation\n    - Analyze body positions\n    - Create pose references",
      "namespace": "fal.segmentation",
      "node_type": "fal.segmentation.DWPose",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to analyze"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "image"
        },
        {
          "type": {
            "type": "list"
          },
          "name": "poses"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Image Preprocessor Depth Anything V2",
      "description": "Depth Anything V2 generates high-quality depth maps from images.\n    depth, preprocessor, depth-map, estimation\n\n    Use cases:\n    - Generate accurate depth maps\n    - Enable depth-aware effects\n    - Create 3D visualizations\n    - Prepare ControlNet inputs\n    - Analyze image depth",
      "namespace": "fal.segmentation",
      "node_type": "fal.segmentation.ImagePreprocessorDepthAnythingV2",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Marigold Depth",
      "description": "Marigold Depth generates high-quality monocular depth maps.\n    depth, marigold, depth-map, estimation\n\n    Use cases:\n    - Generate precise depth maps\n    - Create depth visualizations\n    - Enable depth-based effects\n    - Prepare 3D conversions\n    - Analyze scene depth",
      "namespace": "fal.segmentation",
      "node_type": "fal.segmentation.MarigoldDepth",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to process"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "SAM2 Image",
      "description": "SAM 2 Image segments objects in images with high accuracy.\n    segmentation, sam, image, masks\n\n    Use cases:\n    - Segment objects in images\n    - Create object masks\n    - Enable object selection\n    - Generate cutouts\n    - Create selection masks",
      "namespace": "fal.segmentation",
      "node_type": "fal.segmentation.SAM2Image",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to segment"
        },
        {
          "name": "point_coords",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "list",
                "type_args": [
                  {
                    "type": "float"
                  }
                ]
              }
            ]
          },
          "default": [],
          "title": "Point Coords",
          "description": "Point coordinates for prompts [[x, y], ...]"
        },
        {
          "name": "point_labels",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "int"
              }
            ]
          },
          "default": [],
          "title": "Point Labels",
          "description": "Labels for points (1=foreground, 0=background)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list"
          },
          "name": "masks"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "SAM2 Video",
      "description": "SAM 2 Video segments and tracks objects across video frames.\n    segmentation, sam, video, tracking\n\n    Use cases:\n    - Track objects in videos\n    - Create video masks\n    - Segment moving objects\n    - Generate video cutouts\n    - Enable video object selection",
      "namespace": "fal.segmentation",
      "node_type": "fal.segmentation.SAM2Video",
      "properties": [
        {
          "name": "video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video",
          "description": "The video to segment"
        },
        {
          "name": "point_coords",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "list",
                "type_args": [
                  {
                    "type": "float"
                  }
                ]
              }
            ]
          },
          "default": [],
          "title": "Point Coords",
          "description": "Point coordinates for prompts [[x, y], ...]"
        },
        {
          "name": "point_labels",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "int"
              }
            ]
          },
          "default": [],
          "title": "Point Labels",
          "description": "Labels for points (1=foreground, 0=background)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "str"
          },
          "name": "masks_video"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "SAM3 Image",
      "description": "SAM 3 Image provides advanced segmentation with improved accuracy.\n    segmentation, sam3, image, masks, advanced\n\n    Use cases:\n    - High-accuracy object segmentation\n    - Complex scene segmentation\n    - Precise mask generation\n    - Advanced object selection\n    - Detailed cutout creation",
      "namespace": "fal.segmentation",
      "node_type": "fal.segmentation.SAM3Image",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to segment"
        },
        {
          "name": "point_coords",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "list",
                "type_args": [
                  {
                    "type": "float"
                  }
                ]
              }
            ]
          },
          "default": [],
          "title": "Point Coords",
          "description": "Point coordinates for prompts [[x, y], ...]"
        },
        {
          "name": "point_labels",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "int"
              }
            ]
          },
          "default": [],
          "title": "Point Labels",
          "description": "Labels for points (1=foreground, 0=background)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list"
          },
          "name": "masks"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Fal AI",
      "description": "Dynamic FAL node for running any fal.ai endpoint.\n    fal, schema, dynamic, openapi, inference, runtime, model\n\n    Use cases:\n    - Call new fal.ai endpoints without adding new Python nodes\n    - Prototype workflows with experimental FAL models\n    - Run custom endpoints by sharing model info (llms.txt)\n    - Build flexible pipelines that depend on runtime model selection",
      "namespace": "fal.dynamic_schema",
      "node_type": "fal.dynamic_schema.FalAI",
      "properties": [
        {
          "name": "model_info",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model Info",
          "description": "Paste the full llms.txt from the fal.ai model page (e.g. fal.ai/models/... \u2192 copy all)."
        }
      ],
      "basic_fields": [
        "model_info"
      ],
      "is_dynamic": true,
      "supports_dynamic_outputs": true
    },
    {
      "title": "Era 3D",
      "description": "Era3D creates multi-view consistent 3D models from images.\n    3d, generation, image-to-3d, era3d, multi-view\n\n    Use cases:\n    - Generate multi-view 3D models\n    - Create consistent 3D assets\n    - Produce 3D content with multiple views\n    - Generate detailed 3D models\n    - Create multi-view 3D for games",
      "namespace": "fal.model3d",
      "node_type": "fal.model3d.Era3D",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to convert to 3D"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "Number of inference steps",
          "min": 10.0,
          "max": 100.0
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list"
          },
          "name": "mv_images"
        },
        {
          "type": {
            "type": "model_3d"
          },
          "name": "model"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Hunyuan 3DV 2",
      "description": "Hunyuan3D V2 generates high-quality 3D models from images.\n    3d, generation, image-to-3d, hunyuan\n\n    Use cases:\n    - Generate detailed 3D models\n    - Create 3D assets from photos\n    - Produce high-quality 3D content\n    - Create 3D visualizations\n    - Generate 3D for productions",
      "namespace": "fal.model3d",
      "node_type": "fal.model3d.Hunyuan3DV2",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to convert to 3D"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps",
          "min": 1.0,
          "max": 100.0
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.0,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation",
          "min": 1.0,
          "max": 10.0
        },
        {
          "name": "octree_resolution",
          "type": {
            "type": "int"
          },
          "default": 256,
          "title": "Octree Resolution",
          "description": "Octree resolution for 3D structure"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "model_3d"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Trellis",
      "description": "Trellis generates 3D models from single images.\n    3d, generation, image-to-3d, trellis\n\n    Use cases:\n    - Generate 3D models from images\n    - Create 3D assets from photos\n    - Produce 3D content for games\n    - Create 3D visualizations\n    - Generate 3D for AR/VR",
      "namespace": "fal.model3d",
      "node_type": "fal.model3d.Trellis",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to convert to 3D"
        },
        {
          "name": "ss_guidance_strength",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Ss Guidance Strength",
          "description": "Guidance strength for sparse structure",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "ss_sampling_steps",
          "type": {
            "type": "int"
          },
          "default": 12,
          "title": "Ss Sampling Steps",
          "description": "Sampling steps for sparse structure",
          "min": 1.0,
          "max": 50.0
        },
        {
          "name": "slat_guidance_strength",
          "type": {
            "type": "float"
          },
          "default": 3.0,
          "title": "Slat Guidance Strength",
          "description": "Guidance strength for structured latent",
          "min": 0.0,
          "max": 20.0
        },
        {
          "name": "slat_sampling_steps",
          "type": {
            "type": "int"
          },
          "default": 12,
          "title": "Slat Sampling Steps",
          "description": "Sampling steps for structured latent",
          "min": 1.0,
          "max": 50.0
        },
        {
          "name": "mesh_simplify",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Mesh Simplify",
          "description": "Mesh simplification ratio",
          "min": 0.9,
          "max": 0.98
        },
        {
          "name": "texture_size",
          "type": {
            "type": "enum",
            "values": [
              512,
              1024,
              2048
            ],
            "type_name": "nodetool.nodes.fal.model3d.TextureSizeEnum"
          },
          "default": 1024,
          "title": "Texture Size",
          "description": "Texture resolution"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "model_3d"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "texture_size"
      ]
    },
    {
      "title": "Tripo SR",
      "description": "TripoSR generates 3D models from images with fast processing.\n    3d, generation, image-to-3d, triposr, fast\n\n    Use cases:\n    - Quick 3D model generation\n    - Rapid prototyping\n    - Create 3D assets from photos\n    - Generate 3D content quickly\n    - Fast 3D for AR/VR",
      "namespace": "fal.model3d",
      "node_type": "fal.model3d.TripoSR",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The input image to convert to 3D"
        },
        {
          "name": "foreground_ratio",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Foreground Ratio",
          "description": "Foreground ratio for cropping",
          "min": 0.5,
          "max": 1.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "model_3d"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "ACEStep",
      "description": "ACE-Step generates music with lyrics from text using advanced audio synthesis.\n    audio, generation, music, lyrics, ace-step, text-to-audio\n\n    Use cases:\n    - Generate songs with lyrics\n    - Create music with vocal tracks\n    - Produce complete songs from text\n    - Generate lyrical content\n    - Create vocal music compositions",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.ACEStep",
      "properties": [
        {
          "name": "number_of_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Number Of Steps",
          "description": "Number of steps to generate the audio."
        },
        {
          "name": "duration",
          "type": {
            "type": "float"
          },
          "default": 60,
          "title": "Duration",
          "description": "The duration of the generated audio in seconds."
        },
        {
          "name": "tags",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Tags",
          "description": "Comma-separated list of genre tags to control the style of the generated audio."
        },
        {
          "name": "minimum_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Minimum Guidance Scale",
          "description": "Minimum guidance scale for the generation after the decay."
        },
        {
          "name": "lyrics",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Lyrics",
          "description": "Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song."
        },
        {
          "name": "tag_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Tag Guidance Scale",
          "description": "Tag guidance scale for the generation."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "euler",
              "heun"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.ACEStep.Scheduler"
          },
          "default": "euler",
          "title": "Scheduler",
          "description": "Scheduler to use for the generation process."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Guidance Scale",
          "description": "Guidance scale for the generation."
        },
        {
          "name": "guidance_type",
          "type": {
            "type": "enum",
            "values": [
              "cfg",
              "apg",
              "cfg_star"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.ACEStep.GuidanceType"
          },
          "default": "apg",
          "title": "Guidance Type",
          "description": "Type of CFG to use for the generation process."
        },
        {
          "name": "lyric_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Lyric Guidance Scale",
          "description": "Lyric guidance scale for the generation."
        },
        {
          "name": "guidance_interval",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Guidance Interval",
          "description": "Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)"
        },
        {
          "name": "guidance_interval_decay",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Interval Decay",
          "description": "Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If not provided, a random seed will be used."
        },
        {
          "name": "granularity_scale",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Granularity Scale",
          "description": "Granularity scale for the generation process. Higher values can reduce artifacts."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "ACEStep Prompt To Audio",
      "description": "ACE-Step generates music from text prompts with high-quality audio synthesis.\n    audio, generation, music, ace-step, text-to-audio\n\n    Use cases:\n    - Generate music from text descriptions\n    - Create background music for videos\n    - Produce royalty-free music\n    - Generate audio soundtracks\n    - Create custom music compositions",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.ACEStepPromptToAudio",
      "properties": [
        {
          "name": "number_of_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Number Of Steps",
          "description": "Number of steps to generate the audio."
        },
        {
          "name": "duration",
          "type": {
            "type": "float"
          },
          "default": 60,
          "title": "Duration",
          "description": "The duration of the generated audio in seconds."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to control the style of the generated audio. This will be used to generate tags and lyrics."
        },
        {
          "name": "minimum_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Minimum Guidance Scale",
          "description": "Minimum guidance scale for the generation after the decay."
        },
        {
          "name": "tag_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Tag Guidance Scale",
          "description": "Tag guidance scale for the generation."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "euler",
              "heun"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.ACEStepPromptToAudio.Scheduler"
          },
          "default": "euler",
          "title": "Scheduler",
          "description": "Scheduler to use for the generation process."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Guidance Scale",
          "description": "Guidance scale for the generation."
        },
        {
          "name": "guidance_type",
          "type": {
            "type": "enum",
            "values": [
              "cfg",
              "apg",
              "cfg_star"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.ACEStepPromptToAudio.GuidanceType"
          },
          "default": "apg",
          "title": "Guidance Type",
          "description": "Type of CFG to use for the generation process."
        },
        {
          "name": "instrumental",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Instrumental",
          "description": "Whether to generate an instrumental version of the audio."
        },
        {
          "name": "lyric_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Lyric Guidance Scale",
          "description": "Lyric guidance scale for the generation."
        },
        {
          "name": "guidance_interval",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Guidance Interval",
          "description": "Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)"
        },
        {
          "name": "guidance_interval_decay",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Interval Decay",
          "description": "Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If not provided, a random seed will be used."
        },
        {
          "name": "granularity_scale",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Granularity Scale",
          "description": "Granularity scale for the generation process. Higher values can reduce artifacts."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Beatoven Music Generation",
      "description": "Music Generation\n    audio, generation, text-to-audio, tts\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.BeatovenMusicGeneration",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Describe the music you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "float"
          },
          "default": 90,
          "title": "Duration",
          "description": "Length of the generated music in seconds"
        },
        {
          "name": "refinement",
          "type": {
            "type": "int"
          },
          "default": 100,
          "title": "Refinement",
          "description": "Refinement level - higher values may improve quality but take longer"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducible results - leave empty for random generation"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Describe what you want to avoid in the music (instruments, styles, moods). Leave blank for none."
        },
        {
          "name": "creativity",
          "type": {
            "type": "float"
          },
          "default": 16,
          "title": "Creativity",
          "description": "Creativity level - higher values allow more creative interpretation of the prompt"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Beatoven Sound Effect Generation",
      "description": "Sound Effect Generation\n    audio, generation, text-to-audio, tts\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.BeatovenSoundEffectGeneration",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Describe the sound effect you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Duration",
          "description": "Length of the generated sound effect in seconds"
        },
        {
          "name": "refinement",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Refinement",
          "description": "Refinement level - Higher values may improve quality but take longer"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducible results - leave empty for random generation"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Describe the types of sounds you don't want to generate in the output, avoid double-negatives, compare with positive prompts"
        },
        {
          "name": "creativity",
          "type": {
            "type": "float"
          },
          "default": 16,
          "title": "Creativity",
          "description": "Creativity level - higher values allow more creative interpretation of the prompt"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "CSM1 B",
      "description": "CSM (Conversational Speech Model) generates natural conversational speech from text.\n    audio, speech, tts, conversational, text-to-speech\n\n    Use cases:\n    - Generate natural conversation audio\n    - Create dialogue for characters\n    - Produce conversational voice content\n    - Generate realistic speech\n    - Create interactive voice responses",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.CSM1B",
      "properties": [
        {
          "name": "scene",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Scene",
          "description": "The text to generate an audio from."
        },
        {
          "name": "context",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Context",
          "description": "The context to generate an audio from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Cassetteai Music Generator",
      "description": "CassetteAI\u2019s model generates a 30-second sample in under 2 seconds and a full 3-minute track in under 10 seconds. At 44.1 kHz stereo audio, expect a level of professional consistency with no breaks, no squeaks, and no random interruptions in your creations.  \n    audio, generation, text-to-audio, sound\n\n    Use cases:\n    - Sound effect generation\n    - Music composition\n    - Audio content creation\n    - Background music generation\n    - Podcast audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.CassetteaiMusicGenerator",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate music from."
        },
        {
          "name": "duration",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Duration",
          "description": "The duration of the generated music in seconds."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Cassetteai Sound Effects Generator",
      "description": "Create stunningly realistic sound effects in seconds - CassetteAI's Sound Effects Model generates high-quality SFX up to 30 seconds long in just 1 second of processing time\n    audio, generation, text-to-audio, sound\n\n    Use cases:\n    - Sound effect generation\n    - Music composition\n    - Audio content creation\n    - Background music generation\n    - Podcast audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.CassetteaiSoundEffectsGenerator",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate SFX."
        },
        {
          "name": "duration",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Duration",
          "description": "The duration of the generated SFX in seconds."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Diff Rhythm",
      "description": "DiffRhythm generates rhythmic music and beats using diffusion models.\n    audio, generation, rhythm, beats, music, text-to-audio\n\n    Use cases:\n    - Generate rhythmic music\n    - Create drum beats\n    - Produce percussion tracks\n    - Generate rhythm patterns\n    - Create beat sequences",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.DiffRhythm",
      "properties": [
        {
          "name": "lyrics",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Lyrics",
          "description": "The prompt to generate the song from. Must have two sections. Sections start with either [chorus] or a [verse]."
        },
        {
          "name": "cfg_strength",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Cfg Strength",
          "description": "The CFG strength to use for the music generation."
        },
        {
          "name": "reference_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Reference Audio Url",
          "description": "The URL of the reference audio to use for the music generation."
        },
        {
          "name": "music_duration",
          "type": {
            "type": "enum",
            "values": [
              "95s",
              "285s"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.DiffRhythm.MusicDuration"
          },
          "default": "95s",
          "title": "Music Duration",
          "description": "The duration of the music to generate."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "euler",
              "midpoint",
              "rk4",
              "implicit_adams"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.DiffRhythm.Scheduler"
          },
          "default": "euler",
          "title": "Scheduler",
          "description": "The scheduler to use for the music generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 32,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for the music generation."
        },
        {
          "name": "style_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Prompt",
          "description": "The style prompt to use for the music generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Eleven Labs Music",
      "description": "ElevenLabs Music generates custom music compositions from text descriptions.\n    audio, music, generation, elevenlabs, text-to-audio\n\n    Use cases:\n    - Generate custom music\n    - Create background scores\n    - Produce original compositions\n    - Generate mood music\n    - Create cinematic soundtracks",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.ElevenLabsMusic",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the music to generate"
        },
        {
          "name": "composition_plan",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Composition Plan",
          "description": "The composition plan for the music"
        },
        {
          "name": "music_length_ms",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Music Length Ms",
          "description": "The length of the song to generate in milliseconds. Used only in conjunction with prompt. Must be between 3000ms and 600000ms. Optional - if not provided, the model will choose a length based on the prompt."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "mp3_22050_32",
              "mp3_44100_32",
              "mp3_44100_64",
              "mp3_44100_96",
              "mp3_44100_128",
              "mp3_44100_192",
              "pcm_8000",
              "pcm_16000",
              "pcm_22050",
              "pcm_24000",
              "pcm_44100",
              "pcm_48000",
              "ulaw_8000",
              "alaw_8000",
              "opus_48000_32",
              "opus_48000_64",
              "opus_48000_96",
              "opus_48000_128",
              "opus_48000_192"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.ElevenLabsMusic.OutputFormat"
          },
          "default": "mp3_44100_128",
          "title": "Output Format",
          "description": "Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the \u03bc-law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs."
        },
        {
          "name": "respect_sections_durations",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Respect Sections Durations",
          "description": "Controls how strictly section durations in the composition_plan are enforced. It will only have an effect if it is used with composition_plan. When set to true, the model will precisely respect each section's duration_ms from the plan. When set to false, the model may adjust individual section durations which will generally lead to better generation quality and improved latency, while always preserving the total song duration from the plan."
        },
        {
          "name": "force_instrumental",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Force Instrumental",
          "description": "If true, guarantees that the generated song will be instrumental. If false, the song may or may not be instrumental depending on the prompt. Can only be used with prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Eleven Labs Sound Effects V2",
      "description": "ElevenLabs Sound Effects v2 generates custom sound effects from text descriptions.\n    audio, sound-effects, sfx, elevenlabs, text-to-audio\n\n    Use cases:\n    - Generate custom sound effects\n    - Create audio effects for videos\n    - Produce game sound effects\n    - Generate environmental sounds\n    - Create audio atmosphere",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.ElevenLabsSoundEffectsV2",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text describing the sound effect to generate"
        },
        {
          "name": "loop",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Loop",
          "description": "Whether to create a sound effect that loops smoothly."
        },
        {
          "name": "prompt_influence",
          "type": {
            "type": "float"
          },
          "default": 0.3,
          "title": "Prompt Influence",
          "description": "How closely to follow the prompt (0-1). Higher values mean less variation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "mp3_22050_32",
              "mp3_44100_32",
              "mp3_44100_64",
              "mp3_44100_96",
              "mp3_44100_128",
              "mp3_44100_192",
              "pcm_8000",
              "pcm_16000",
              "pcm_22050",
              "pcm_24000",
              "pcm_44100",
              "pcm_48000",
              "ulaw_8000",
              "alaw_8000",
              "opus_48000_32",
              "opus_48000_64",
              "opus_48000_96",
              "opus_48000_128",
              "opus_48000_192"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.ElevenLabsSoundEffectsV2.OutputFormat"
          },
          "default": "mp3_44100_128",
          "title": "Output Format",
          "description": "Output format of the generated audio. Formatted as codec_sample_rate_bitrate."
        },
        {
          "name": "duration_seconds",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Duration Seconds",
          "description": "Duration in seconds (0.5-22). If None, optimal duration will be determined from prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Eleven Labs TTSMultilingual V2",
      "description": "ElevenLabs Multilingual TTS v2 generates natural speech in multiple languages.\n    audio, tts, speech, multilingual, elevenlabs, text-to-speech\n\n    Use cases:\n    - Generate multilingual speech\n    - Create voiceovers in multiple languages\n    - Produce localized audio content\n    - Generate international voice content\n    - Create translated audio",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.ElevenLabsTTSMultilingualV2",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to convert to speech"
        },
        {
          "name": "next_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Next Text",
          "description": "The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation."
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Speed",
          "description": "Speech speed (0.7-1.2). Values below 1.0 slow down the speech, above 1.0 speed it up. Extreme values may affect quality."
        },
        {
          "name": "style",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Style",
          "description": "Style exaggeration (0-1)"
        },
        {
          "name": "stability",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Stability",
          "description": "Voice stability (0-1)"
        },
        {
          "name": "timestamps",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Timestamps",
          "description": "Whether to return timestamps for each word in the generated speech"
        },
        {
          "name": "similarity_boost",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Similarity Boost",
          "description": "Similarity boost (0-1)"
        },
        {
          "name": "voice",
          "type": {
            "type": "str"
          },
          "default": "Rachel",
          "title": "Voice",
          "description": "The voice to use for speech generation"
        },
        {
          "name": "language_code",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Language Code",
          "description": "Language code (ISO 639-1) used to enforce a language for the model. An error will be returned if language code is not supported by the model."
        },
        {
          "name": "apply_text_normalization",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "on",
              "off"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.ElevenLabsTTSMultilingualV2.ApplyTextNormalization"
          },
          "default": "auto",
          "title": "Apply Text Normalization",
          "description": "This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped."
        },
        {
          "name": "previous_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Previous Text",
          "description": "The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text",
        "language"
      ]
    },
    {
      "title": "Eleven Labs TTSV3",
      "description": "ElevenLabs TTS v3 generates high-quality natural speech with advanced voice control.\n    audio, tts, speech, elevenlabs, text-to-speech\n\n    Use cases:\n    - Generate high-quality voiceovers\n    - Create natural speech audio\n    - Produce professional narration\n    - Generate expressive speech\n    - Create audiobook content",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.ElevenLabsTTSV3",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to convert to speech"
        },
        {
          "name": "stability",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Stability",
          "description": "Voice stability (0-1)"
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Speed",
          "description": "Speech speed (0.7-1.2). Values below 1.0 slow down the speech, above 1.0 speed it up. Extreme values may affect quality."
        },
        {
          "name": "style",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Style",
          "description": "Style exaggeration (0-1)"
        },
        {
          "name": "timestamps",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Timestamps",
          "description": "Whether to return timestamps for each word in the generated speech"
        },
        {
          "name": "similarity_boost",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Similarity Boost",
          "description": "Similarity boost (0-1)"
        },
        {
          "name": "voice",
          "type": {
            "type": "str"
          },
          "default": "Rachel",
          "title": "Voice",
          "description": "The voice to use for speech generation"
        },
        {
          "name": "language_code",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Language Code",
          "description": "Language code (ISO 639-1) used to enforce a language for the model."
        },
        {
          "name": "apply_text_normalization",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "on",
              "off"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.ElevenLabsTTSV3.ApplyTextNormalization"
          },
          "default": "auto",
          "title": "Apply Text Normalization",
          "description": "This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Eleven Labs Text To Dialogue V3",
      "description": "ElevenLabs Text to Dialogue v3 generates conversational dialogue with multiple speakers.\n    audio, dialogue, conversation, elevenlabs, text-to-speech\n\n    Use cases:\n    - Generate multi-speaker dialogue\n    - Create conversational audio\n    - Produce podcast-style content\n    - Generate character conversations\n    - Create interactive dialogues",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.ElevenLabsTextToDialogueV3",
      "properties": [
        {
          "name": "stability",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Stability",
          "description": "Determines how stable the voice is and the randomness between each generation. Lower values introduce broader emotional range for the voice. Higher values can result in a monotonous voice with limited emotion. Must be one of 0.0, 0.5, 1.0, else it will be rounded to the nearest value."
        },
        {
          "name": "inputs",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Inputs",
          "description": "A list of dialogue inputs, each containing text and a voice ID which will be converted into speech."
        },
        {
          "name": "language_code",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Language Code",
          "description": "Language code (ISO 639-1) used to enforce a language for the model. An error will be returned if language code is not supported by the model."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "use_speaker_boost",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Use Speaker Boost",
          "description": "This setting boosts the similarity to the original speaker. Using this setting requires a slightly higher computational load, which in turn increases latency."
        },
        {
          "name": "pronunciation_dictionary_locators",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Pronunciation Dictionary Locators",
          "description": "A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "F5 TTS",
      "description": "F5 TTS generates natural speech with fast inference and high quality.\n    audio, tts, speech, fast, text-to-speech\n\n    Use cases:\n    - Fast speech generation\n    - Real-time TTS applications\n    - Quick voiceover creation\n    - Efficient speech synthesis\n    - Rapid audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.F5TTS",
      "properties": [
        {
          "name": "ref_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Ref Text",
          "description": "The reference text to be used for TTS. If not provided, an ASR (Automatic Speech Recognition) model will be used to generate the reference text."
        },
        {
          "name": "remove_silence",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Remove Silence",
          "description": "Whether to remove the silence from the audio file."
        },
        {
          "name": "gen_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Gen Text",
          "description": "The text to be converted to speech."
        },
        {
          "name": "model_type",
          "type": {
            "type": "enum",
            "values": [
              "F5-TTS",
              "E2-TTS"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.F5TTS.ModelType"
          },
          "default": "",
          "title": "Model Type",
          "description": "The name of the model to be used for TTS."
        },
        {
          "name": "ref_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ref Audio Url",
          "description": "The URL of the reference audio file."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Kokoro",
      "description": "Kokoro generates expressive and emotional speech with advanced prosody control.\n    audio, tts, speech, expressive, emotional, text-to-speech\n\n    Use cases:\n    - Generate expressive speech\n    - Create emotional voiceovers\n    - Produce dramatic narration\n    - Generate character voices\n    - Create emotive audio content",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.Kokoro",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "af_heart",
              "af_alloy",
              "af_aoede",
              "af_bella",
              "af_jessica",
              "af_kore",
              "af_nicole",
              "af_nova",
              "af_river",
              "af_sarah",
              "af_sky",
              "am_adam",
              "am_echo",
              "am_eric",
              "am_fenrir",
              "am_liam",
              "am_michael",
              "am_onyx",
              "am_puck",
              "am_santa"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.Kokoro.Voice"
          },
          "default": "af_heart",
          "title": "Voice",
          "description": "Voice ID for the desired voice."
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Speed",
          "description": "Speed of the generated audio. Default is 1.0."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Kokoro American English",
      "description": "Kokoro is a lightweight text-to-speech model that delivers comparable quality to larger models while being significantly faster and more cost-efficient.\n    audio, generation, text-to-audio, sound\n\n    Use cases:\n    - Sound effect generation\n    - Music composition\n    - Audio content creation\n    - Background music generation\n    - Podcast audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.KokoroAmericanEnglish",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "af_heart",
              "af_alloy",
              "af_aoede",
              "af_bella",
              "af_jessica",
              "af_kore",
              "af_nicole",
              "af_nova",
              "af_river",
              "af_sarah",
              "af_sky",
              "am_adam",
              "am_echo",
              "am_eric",
              "am_fenrir",
              "am_liam",
              "am_michael",
              "am_onyx",
              "am_puck",
              "am_santa"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.KokoroAmericanEnglish.Voice"
          },
          "default": "af_heart",
          "title": "Voice",
          "description": "Voice ID for the desired voice."
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Speed",
          "description": "Speed of the generated audio. Default is 1.0."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Kokoro Brazilian Portuguese",
      "description": "A natural and expressive Brazilian Portuguese text-to-speech model optimized for clarity and fluency.\n    audio, generation, text-to-audio, sound\n\n    Use cases:\n    - Sound effect generation\n    - Music composition\n    - Audio content creation\n    - Background music generation\n    - Podcast audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.KokoroBrazilianPortuguese",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "pf_dora",
              "pm_alex",
              "pm_santa"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.KokoroBrazilianPortuguese.Voice"
          },
          "default": "",
          "title": "Voice",
          "description": "Voice ID for the desired voice."
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Speed",
          "description": "Speed of the generated audio. Default is 1.0."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Kokoro British English",
      "description": "A high-quality British English text-to-speech model offering natural and expressive voice synthesis.\n    audio, generation, text-to-audio, sound\n\n    Use cases:\n    - Sound effect generation\n    - Music composition\n    - Audio content creation\n    - Background music generation\n    - Podcast audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.KokoroBritishEnglish",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "bf_alice",
              "bf_emma",
              "bf_isabella",
              "bf_lily",
              "bm_daniel",
              "bm_fable",
              "bm_george",
              "bm_lewis"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.KokoroBritishEnglish.Voice"
          },
          "default": "",
          "title": "Voice",
          "description": "Voice ID for the desired voice."
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Speed",
          "description": "Speed of the generated audio. Default is 1.0."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Kokoro French",
      "description": "An expressive and natural French text-to-speech model for both European and Canadian French.\n    audio, generation, text-to-audio, sound\n\n    Use cases:\n    - Sound effect generation\n    - Music composition\n    - Audio content creation\n    - Background music generation\n    - Podcast audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.KokoroFrench",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "ff_siwis"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.KokoroFrench.Voice"
          },
          "default": "",
          "title": "Voice",
          "description": "Voice ID for the desired voice."
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Speed",
          "description": "Speed of the generated audio. Default is 1.0."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Kokoro Hindi",
      "description": "A fast and expressive Hindi text-to-speech model with clear pronunciation and accurate intonation.\n    audio, generation, text-to-audio, sound\n\n    Use cases:\n    - Sound effect generation\n    - Music composition\n    - Audio content creation\n    - Background music generation\n    - Podcast audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.KokoroHindi",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "hf_alpha",
              "hf_beta",
              "hm_omega",
              "hm_psi"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.KokoroHindi.Voice"
          },
          "default": "",
          "title": "Voice",
          "description": "Voice ID for the desired voice."
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Speed",
          "description": "Speed of the generated audio. Default is 1.0."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Kokoro Italian",
      "description": "A high-quality Italian text-to-speech model delivering smooth and expressive speech synthesis.\n    audio, generation, text-to-audio, sound\n\n    Use cases:\n    - Sound effect generation\n    - Music composition\n    - Audio content creation\n    - Background music generation\n    - Podcast audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.KokoroItalian",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "if_sara",
              "im_nicola"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.KokoroItalian.Voice"
          },
          "default": "",
          "title": "Voice",
          "description": "Voice ID for the desired voice."
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Speed",
          "description": "Speed of the generated audio. Default is 1.0."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Kokoro Japanese",
      "description": "A fast and natural-sounding Japanese text-to-speech model optimized for smooth pronunciation.\n    audio, generation, text-to-audio, sound\n\n    Use cases:\n    - Sound effect generation\n    - Music composition\n    - Audio content creation\n    - Background music generation\n    - Podcast audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.KokoroJapanese",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "jf_alpha",
              "jf_gongitsune",
              "jf_nezumi",
              "jf_tebukuro",
              "jm_kumo"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.KokoroJapanese.Voice"
          },
          "default": "",
          "title": "Voice",
          "description": "Voice ID for the desired voice."
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Speed",
          "description": "Speed of the generated audio. Default is 1.0."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Kokoro Mandarin Chinese",
      "description": "A highly efficient Mandarin Chinese text-to-speech model that captures natural tones and prosody.\n    audio, generation, text-to-audio, sound\n\n    Use cases:\n    - Sound effect generation\n    - Music composition\n    - Audio content creation\n    - Background music generation\n    - Podcast audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.KokoroMandarinChinese",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "zf_xiaobei",
              "zf_xiaoni",
              "zf_xiaoxiao",
              "zf_xiaoyi",
              "zm_yunjian",
              "zm_yunxi",
              "zm_yunxia",
              "zm_yunyang"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.KokoroMandarinChinese.Voice"
          },
          "default": "",
          "title": "Voice",
          "description": "Voice ID for the desired voice."
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Speed",
          "description": "Speed of the generated audio. Default is 1.0."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Kokoro Spanish",
      "description": "A natural-sounding Spanish text-to-speech model optimized for Latin American and European Spanish.\n    audio, generation, text-to-audio, sound\n\n    Use cases:\n    - Sound effect generation\n    - Music composition\n    - Audio content creation\n    - Background music generation\n    - Podcast audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.KokoroSpanish",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "ef_dora",
              "em_alex",
              "em_santa"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.KokoroSpanish.Voice"
          },
          "default": "",
          "title": "Voice",
          "description": "Voice ID for the desired voice."
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Speed",
          "description": "Speed of the generated audio. Default is 1.0."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Lyria 2",
      "description": "Lyria 2 is Google's latest music generation model, you can generate any type of music with this model.\n    audio, generation, text-to-audio, sound\n\n    Use cases:\n    - Sound effect generation\n    - Music composition\n    - Audio content creation\n    - Background music generation\n    - Podcast audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.Lyria2",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the music you want to generate"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "A seed for deterministic generation. If provided, the model will attempt to produce the same audio given the same prompt and other parameters."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "low quality",
          "title": "Negative Prompt",
          "description": "A description of what to exclude from the generated audio"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Minimax Music",
      "description": "Generate music from text prompts using the MiniMax model, which leverages advanced AI techniques to create high-quality, diverse musical compositions.\n    audio, generation, text-to-audio, sound\n\n    Use cases:\n    - Sound effect generation\n    - Music composition\n    - Audio content creation\n    - Background music generation\n    - Podcast audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.MinimaxMusic",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Lyrics with optional formatting. You can use a newline to separate each line of lyrics. You can use two newlines to add a pause between lines. You can use double hash marks (##) at the beginning and end of the lyrics to add accompaniment. Maximum 600 characters."
        },
        {
          "name": "reference_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Reference Audio Url",
          "description": "Reference song, should contain music and vocals. Must be a .wav or .mp3 file longer than 15 seconds."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Minimax Music V15",
      "description": "MiniMax (Hailuo AI) Music v1.5\n    audio, generation, text-to-audio, tts, professional\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.MinimaxMusicV15",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Lyrics, supports [intro][verse][chorus][bridge][outro] sections. 10-600 characters."
        },
        {
          "name": "lyrics_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Lyrics Prompt",
          "description": "Control music generation. 10-3000 characters."
        },
        {
          "name": "audio_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Audio Setting",
          "description": "Audio configuration settings"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Minimax Music V2",
      "description": "Minimax Music\n    audio, generation, text-to-audio, tts, professional\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.MinimaxMusicV2",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "A description of the music, specifying style, mood, and scenario. 10-300 characters."
        },
        {
          "name": "lyrics_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Lyrics Prompt",
          "description": "Lyrics of the song. Use n to separate lines. You may add structure tags like [Intro], [Verse], [Chorus], [Bridge], [Outro] to enhance the arrangement. 10-3000 characters."
        },
        {
          "name": "audio_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Audio Setting",
          "description": "Audio configuration settings"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Mmaudio V2 Text To Audio",
      "description": "MMAudio generates synchronized audio given text inputs. It can generate sounds described by a prompt.\n    audio, generation, text-to-audio, sound\n\n    Use cases:\n    - Sound effect generation\n    - Music composition\n    - Audio content creation\n    - Background music generation\n    - Podcast audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.MmaudioV2TextToAudio",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the audio for."
        },
        {
          "name": "num_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Steps",
          "description": "The number of steps to generate the audio for."
        },
        {
          "name": "duration",
          "type": {
            "type": "float"
          },
          "default": 8,
          "title": "Duration",
          "description": "The duration of the audio to generate."
        },
        {
          "name": "cfg_strength",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Cfg Strength",
          "description": "The strength of Classifier Free Guidance."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator"
        },
        {
          "name": "mask_away_clip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Mask Away Clip",
          "description": "Whether to mask away the clip."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the audio for."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Sonauto V2 Inpaint",
      "description": "Sonauto V2\n    audio, generation, text-to-audio, tts\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.SonautoV2Inpaint",
      "properties": [
        {
          "name": "lyrics_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Lyrics Prompt",
          "description": "The lyrics sung in the generated song. An empty string will generate an instrumental track."
        },
        {
          "name": "tags",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Tags",
          "description": "Tags/styles of the music to generate. You can view a list of all available tags at https://sonauto.ai/tag-explorer."
        },
        {
          "name": "prompt_strength",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Prompt Strength",
          "description": "Controls how strongly your prompt influences the output. Greater values adhere more to the prompt but sound less natural. (This is CFG.)"
        },
        {
          "name": "output_bit_rate",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Output Bit Rate",
          "description": "The bit rate to use for mp3 and m4a formats. Not available for other formats."
        },
        {
          "name": "num_songs",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Songs",
          "description": "Generating 2 songs costs 1.5x the price of generating 1 song. Also, note that using the same seed may not result in identical songs if the number of songs generated is changed."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "flac",
              "mp3",
              "wav",
              "ogg",
              "m4a"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.SonautoV2Inpaint.OutputFormat"
          },
          "default": "wav",
          "title": "Output Format"
        },
        {
          "name": "selection_crop",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Selection Crop",
          "description": "Crop to the selected region"
        },
        {
          "name": "sections",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Sections",
          "description": "List of sections to inpaint. Currently, only one section is supported so the list length must be 1."
        },
        {
          "name": "balance_strength",
          "type": {
            "type": "float"
          },
          "default": 0.7,
          "title": "Balance Strength",
          "description": "Greater means more natural vocals. Lower means sharper instrumentals. We recommend 0.7."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file to alter. Must be a valid publicly accessible URL."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed to use for generation. Will pick a random seed if not provided. Repeating a request with identical parameters (must use lyrics and tags, not prompt) and the same seed will generate the same song."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Sonauto V2 Text To Music",
      "description": "Create full songs in any style\n    audio, generation, text-to-audio, sound\n\n    Use cases:\n    - Sound effect generation\n    - Music composition\n    - Audio content creation\n    - Background music generation\n    - Podcast audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.SonautoV2TextToMusic",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "A description of the track you want to generate. This prompt will be used to automatically generate the tags and lyrics unless you manually set them. For example, if you set prompt and tags, then the prompt will be used to generate only the lyrics."
        },
        {
          "name": "lyrics_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Lyrics Prompt",
          "description": "The lyrics sung in the generated song. An empty string will generate an instrumental track."
        },
        {
          "name": "tags",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Tags",
          "description": "Tags/styles of the music to generate. You can view a list of all available tags at https://sonauto.ai/tag-explorer."
        },
        {
          "name": "prompt_strength",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Prompt Strength",
          "description": "Controls how strongly your prompt influences the output. Greater values adhere more to the prompt but sound less natural. (This is CFG.)"
        },
        {
          "name": "output_bit_rate",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Output Bit Rate",
          "description": "The bit rate to use for mp3 and m4a formats. Not available for other formats."
        },
        {
          "name": "num_songs",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Songs",
          "description": "Generating 2 songs costs 1.5x the price of generating 1 song. Also, note that using the same seed may not result in identical songs if the number of songs generated is changed."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "flac",
              "mp3",
              "wav",
              "ogg",
              "m4a"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.SonautoV2TextToMusic.OutputFormat"
          },
          "default": "wav",
          "title": "Output Format"
        },
        {
          "name": "bpm",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Bpm",
          "description": "The beats per minute of the song. This can be set to an integer or the literal string \"auto\" to pick a suitable bpm based on the tags. Set bpm to null to not condition the model on bpm information."
        },
        {
          "name": "balance_strength",
          "type": {
            "type": "float"
          },
          "default": 0.7,
          "title": "Balance Strength",
          "description": "Greater means more natural vocals. Lower means sharper instrumentals. We recommend 0.7."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed to use for generation. Will pick a random seed if not provided. Repeating a request with identical parameters (must use lyrics and tags, not prompt) and the same seed will generate the same song."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Stable Audio",
      "description": "Stable Audio generates high-quality audio from text with consistent results.\n    audio, generation, stable, music, text-to-audio\n\n    Use cases:\n    - Generate consistent audio\n    - Create reliable soundtracks\n    - Produce predictable audio\n    - Generate stable music\n    - Create dependable audio content",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.StableAudio",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate audio from"
        },
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 100,
          "title": "Steps",
          "description": "The number of steps to denoise the audio for"
        },
        {
          "name": "seconds_total",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Seconds Total",
          "description": "The duration of the audio clip to generate"
        },
        {
          "name": "seconds_start",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seconds Start",
          "description": "The start point of the audio clip to generate"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Stable Audio 25 Text To Audio",
      "description": "Stable Audio 2.5\n    audio, generation, text-to-audio, tts\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.StableAudio25TextToAudio",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate audio from"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seconds_total",
          "type": {
            "type": "int"
          },
          "default": 190,
          "title": "Seconds Total",
          "description": "The duration of the audio clip to generate"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of steps to denoise the audio for"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "XTTS",
      "description": "XTTS generates expressive speech with voice cloning capabilities.\n    audio, tts, speech, voice-cloning, expressive, text-to-speech\n\n    Use cases:\n    - Clone and generate voices\n    - Create personalized speech\n    - Produce voice-matched content\n    - Generate custom voice audio\n    - Create voice replications",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.XTTS",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt you would like to convert to speech."
        },
        {
          "name": "repetition_penalty",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Repetition Penalty",
          "description": "The repetition penalty to use for generation. Defaults to 5.0."
        },
        {
          "name": "language",
          "type": {
            "type": "enum",
            "values": [
              "English",
              "Spanish",
              "French",
              "German",
              "Italian",
              "Portuguese",
              "Polish",
              "Turkish",
              "Russian",
              "Dutch",
              "Czech",
              "Arabic",
              "Chinese",
              "Japanese",
              "Hungarian",
              "Korean",
              "Hindi"
            ],
            "type_name": "nodetool.nodes.fal.text_to_audio.XTTS.Language"
          },
          "default": "English",
          "title": "Language",
          "description": "The language to use for generation. Defaults to English."
        },
        {
          "name": "gpt_cond_len",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Gpt Cond Len",
          "description": "The length of the GPT conditioning. Defaults to 30."
        },
        {
          "name": "gpt_cond_chunk_len",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Gpt Cond Chunk Len",
          "description": "The length of the GPT conditioning chunks. Defaults to 4."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the voice file to match"
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Temperature",
          "description": "The temperature to use for generation. Higher is more creative. Defaults to 0.75."
        },
        {
          "name": "sample_rate",
          "type": {
            "type": "int"
          },
          "default": 24000,
          "title": "Sample Rate",
          "description": "The sample rate of the audio. Defaults to 24000."
        },
        {
          "name": "max_ref_length",
          "type": {
            "type": "int"
          },
          "default": 60,
          "title": "Max Ref Length",
          "description": "The maximum length of the reference. Defaults to 60."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Yue",
      "description": "YuE is a groundbreaking series of open-source foundation models designed for music generation, specifically for transforming lyrics into full songs.\n    audio, generation, text-to-audio, sound\n\n    Use cases:\n    - Sound effect generation\n    - Music composition\n    - Audio content creation\n    - Background music generation\n    - Podcast audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.Yue",
      "properties": [
        {
          "name": "lyrics",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Lyrics",
          "description": "The prompt to generate an image from. Must have two sections. Sections start with either [chorus] or a [verse]."
        },
        {
          "name": "genres",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Genres",
          "description": "The genres (separated by a space ' ') to guide the music generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Zonos",
      "description": "Clone voice of any person and speak anything in their voice using zonos' voice cloning.\n    audio, generation, text-to-audio, sound\n\n    Use cases:\n    - Sound effect generation\n    - Music composition\n    - Audio content creation\n    - Background music generation\n    - Podcast audio production",
      "namespace": "fal.text_to_audio",
      "node_type": "fal.text_to_audio.Zonos",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The content generated using cloned voice."
        },
        {
          "name": "reference_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Reference Audio Url",
          "description": "The reference audio."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Animate Diff Sparse Ctrl LCM",
      "description": "AnimateDiff SparseCtrl LCM animates drawings with latent consistency models for fast generation.\n    video, generation, animatediff, sparsectrl, lcm, animation, text-to-video\n\n    Use cases:\n    - Animate hand-drawn sketches\n    - Bring drawings to life\n    - Create animated illustrations\n    - Generate animations from concept art\n    - Produce animation from sparse frames",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.AnimateDiffSparseCtrlLCM",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "controlnet_type",
          "type": {
            "type": "enum",
            "values": [
              "scribble",
              "rgb"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.AnimateDiffSparseCtrlLCM.ControlnetType"
          },
          "default": "scribble",
          "title": "Controlnet Type",
          "description": "The type of controlnet to use for generating the video. The controlnet determines how the video will be animated."
        },
        {
          "name": "keyframe_2_index",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Keyframe 2 Index",
          "description": "The frame index of the third keyframe to use for the generation."
        },
        {
          "name": "keyframe_0_index",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Keyframe 0 Index",
          "description": "The frame index of the first keyframe to use for the generation."
        },
        {
          "name": "keyframe_1_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Keyframe 1 Image Url",
          "description": "The URL of the second keyframe to use for the generation."
        },
        {
          "name": "keyframe_1_index",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Keyframe 1 Index",
          "description": "The frame index of the second keyframe to use for the generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "Increasing the amount of steps tells Stable Diffusion that it should take more steps to generate your final result which can increase the amount of detail in your image."
        },
        {
          "name": "keyframe_2_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Keyframe 2 Image Url",
          "description": "The URL of the third keyframe to use for the generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to specify what you don't want."
        },
        {
          "name": "keyframe_0_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Keyframe 0 Image Url",
          "description": "The URL of the first keyframe to use for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Animate Diff Text To Video",
      "description": "AnimateDiff generates smooth animations from text prompts using diffusion models.\n    video, generation, animatediff, animation, text-to-video, txt2vid\n\n    Use cases:\n    - Animate ideas from text descriptions\n    - Create animated content quickly\n    - Generate motion graphics from prompts\n    - Produce animated concept art\n    - Create video loops and sequences",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.AnimateDiffTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the video. Be as descriptive as possible for best results."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Fps",
          "description": "Number of frames per second to extract from the video."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "square",
          "title": "Video Size",
          "description": "The size of the video to generate."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Num Frames",
          "description": "The number of frames to generate for the video."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(bad quality, worst quality:1.2), ugly faces, bad anime",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "motions",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Motions",
          "description": "The motions to apply to the video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Animate Diff Turbo Text To Video",
      "description": "AnimateDiff Turbo generates animations at lightning speed with reduced steps.\n    video, generation, animatediff, turbo, fast, text-to-video, txt2vid\n\n    Use cases:\n    - Rapidly prototype video animations\n    - Create quick video previews\n    - Generate animations with minimal latency\n    - Iterate on video concepts quickly\n    - Produce real-time animation effects",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.AnimateDiffTurboTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the video. Be as descriptive as possible for best results."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Fps",
          "description": "Number of frames per second to extract from the video."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "square",
          "title": "Video Size",
          "description": "The size of the video to generate."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Num Frames",
          "description": "The number of frames to generate for the video."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform. 4-12 is recommended for turbo mode."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(bad quality, worst quality:1.2), ugly faces, bad anime",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "motions",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Motions",
          "description": "The motions to apply to the video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Argil Avatars Text To Video",
      "description": "Argil Avatars creates realistic talking avatar videos from text descriptions.\n    video, generation, avatar, talking-head, argil, text-to-video\n\n    Use cases:\n    - Generate avatar spokesperson videos\n    - Create virtual presenter content\n    - Produce automated video announcements\n    - Generate character-based narratives\n    - Create social media avatar videos",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.ArgilAvatarsTextToVideo",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text"
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "Rachel",
              "Clyde",
              "Roger",
              "Sarah",
              "Laura",
              "Thomas",
              "Charlie",
              "George",
              "Callum",
              "River",
              "Harry",
              "Liam",
              "Alice",
              "Matilda",
              "Will",
              "Jessica",
              "Lilly",
              "Bill",
              "Oxley",
              "Luna"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo.Voice"
          },
          "default": "",
          "title": "Voice"
        },
        {
          "name": "remove_background",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Remove Background",
          "description": "Enabling the remove background feature will result in a 50% increase in the price."
        },
        {
          "name": "avatar",
          "type": {
            "type": "enum",
            "values": [
              "Mia outdoor (UGC)",
              "Lara (Masterclass)",
              "Ines (UGC)",
              "Maria (Masterclass)",
              "Emma (UGC)",
              "Sienna (Masterclass)",
              "Elena (UGC)",
              "Jasmine (Masterclass)",
              "Amara (Masterclass)",
              "Ryan podcast (UGC)",
              "Tyler (Masterclass)",
              "Jayse (Masterclass)",
              "Paul (Masterclass)",
              "Matteo (UGC)",
              "Daniel car (UGC)",
              "Dario (Masterclass)",
              "Viva (Masterclass)",
              "Chen (Masterclass)",
              "Alex (Masterclass)",
              "Vanessa (UGC)",
              "Laurent (UGC)",
              "Noemie car (UGC)",
              "Brandon (UGC)",
              "Byron (Masterclass)",
              "Calista (Masterclass)",
              "Milo (Masterclass)",
              "Fabien (Masterclass)",
              "Rose (UGC)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo.Avatar"
          },
          "default": "",
          "title": "Avatar"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bytedance Seedance V1 Lite Text To Video",
      "description": "Seedance 1.0 Lite\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.BytedanceSeedanceV1LiteTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the video"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1LiteTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "2",
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1LiteTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the video in seconds"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1LiteTextToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Video resolution - 480p for faster generation, 720p for higher quality"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "camera_fixed",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Camera Fixed",
          "description": "Whether to fix the camera position"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control video generation. Use -1 for random."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bytedance Seedance V1 Pro Text To Video",
      "description": "Seedance 1.0 Pro, a high quality video generation model developed by Bytedance.\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.BytedanceSeedanceV1ProTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the video"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1ProTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "2",
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1ProTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the video in seconds"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1ProTextToVideo.Resolution"
          },
          "default": "1080p",
          "title": "Resolution",
          "description": "Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "camera_fixed",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Camera Fixed",
          "description": "Whether to fix the camera position"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control video generation. Use -1 for random."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Cog Video X5 B",
      "description": "CogVideoX-5B is a powerful open-source text-to-video generation model with 5 billion parameters.\n    video, generation, cogvideo, text-to-video, txt2vid\n\n    Use cases:\n    - Generate detailed videos from text prompts\n    - Create animated storytelling content\n    - Produce concept videos for pitches\n    - Generate video storyboards\n    - Create educational demonstrations",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.CogVideoX5B",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "use_rife",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Rife",
          "description": "Use RIFE for video interpolation"
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. We currently support one lora."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related video to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "export_fps",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Export Fps",
          "description": "The target FPS of the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate video from"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Fast Svd Lcm Text To Video",
      "description": "Generate short video clips from your images using SVD v1.1 at Lightning Speed\n    video, generation, text-to-video, txt2vid, fast\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.FastSvdLcmTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use as a starting point for the generation."
        },
        {
          "name": "cond_aug",
          "type": {
            "type": "float"
          },
          "default": 0.02,
          "title": "Cond Aug",
          "description": "The conditoning augmentation determines the amount of noise that will be added to the conditioning frame. The higher the number, the more noise there will be, and the less the video will look like the initial image. Increase it for more motion."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Fps",
          "description": "The FPS of the generated video. The higher the number, the faster the video will play. Total video length is 25 frames."
        },
        {
          "name": "motion_bucket_id",
          "type": {
            "type": "int"
          },
          "default": 127,
          "title": "Motion Bucket Id",
          "description": "The motion bucket id determines the motion of the generated video. The higher the number, the more motion there will be."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_16_9",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Steps",
          "description": "The number of steps to run the model for. The higher the number the better the quality and longer it will take to generate."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Fast Svd Text To Video",
      "description": "Generate short video clips from your prompts using SVD v1.1\n    video, generation, text-to-video, txt2vid, fast\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.FastSvdTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use as a starting point for the generation."
        },
        {
          "name": "cond_aug",
          "type": {
            "type": "float"
          },
          "default": 0.02,
          "title": "Cond Aug",
          "description": "The conditoning augmentation determines the amount of noise that will be added to the conditioning frame. The higher the number, the more noise there will be, and the less the video will look like the initial image. Increase it for more motion."
        },
        {
          "name": "deep_cache",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "minimum",
              "medium",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.FastSvdTextToVideo.DeepCache"
          },
          "default": "none",
          "title": "Deep Cache",
          "description": "Enabling [DeepCache](https://github.com/horseee/DeepCache) will make the execution faster, but might sometimes degrade overall quality. The higher the setting, the faster the execution will be, but the more quality might be lost."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Fps",
          "description": "The FPS of the generated video. The higher the number, the faster the video will play. Total video length is 25 frames."
        },
        {
          "name": "motion_bucket_id",
          "type": {
            "type": "int"
          },
          "default": 127,
          "title": "Motion Bucket Id",
          "description": "The motion bucket id determines the motion of the generated video. The higher the number, the more motion there will be."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_16_9",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 20,
          "title": "Steps",
          "description": "The number of steps to run the model for. The higher the number the better the quality and longer it will take to generate."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "unrealistic, saturated, high contrast, big nose, painting, drawing, sketch, cartoon, anime, manga, render, CG, 3d, watermark, signature, label",
          "title": "Negative Prompt",
          "description": "The negative prompt to use as a starting point for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Video",
      "description": "Hunyuan Video is Tencent's advanced text-to-video model for high-quality video generation.\n    video, generation, hunyuan, text-to-video, txt2vid\n\n    Use cases:\n    - Generate cinematic videos from text descriptions\n    - Create marketing videos from product descriptions\n    - Produce educational video content\n    - Generate creative video concepts\n    - Create animated scenes from stories",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.HunyuanVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.HunyuanVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video to generate."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.HunyuanVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the video to generate."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to run. Lower gets faster results, higher gets better results."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for generating the video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "enum",
            "values": [
              "129",
              "85"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.HunyuanVideo.NumFrames"
          },
          "default": 129,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "pro_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Pro Mode",
          "description": "By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Video Lora",
      "description": "Hunyuan Video is an Open video generation model with high visual quality, motion diversity, text-video alignment, and generation stability\n    video, generation, text-to-video, txt2vid, lora\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.HunyuanVideoLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.HunyuanVideoLora.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video to generate."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.HunyuanVideoLora.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the video to generate."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for generating the video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "enum",
            "values": [
              "129",
              "85"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.HunyuanVideoLora.NumFrames"
          },
          "default": 129,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "pro_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Pro Mode",
          "description": "By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Video V15 Text To Video",
      "description": "Hunyuan Video V1.5\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.HunyuanVideoV15TextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.HunyuanVideoV15TextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.HunyuanVideoV15TextToVideo.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Enable prompt expansion to enhance the input prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what not to generate."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Infinitalk Single Text",
      "description": "Infinitalk\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.InfinitalkSingleText",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the video to generate. Must be either 480p or 720p."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for generation."
        },
        {
          "name": "text_input",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Input",
          "description": "The text input to guide video generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "Aria",
              "Roger",
              "Sarah",
              "Laura",
              "Charlie",
              "George",
              "Callum",
              "River",
              "Liam",
              "Charlotte",
              "Alice",
              "Matilda",
              "Will",
              "Jessica",
              "Eric",
              "Chris",
              "Brian",
              "Daniel",
              "Lily",
              "Bill"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Voice"
          },
          "default": "",
          "title": "Voice",
          "description": "The voice to use for speech generation"
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 145,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 41 to 721."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Infinity Star Text To Video",
      "description": "Infinity Star\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.InfinityStarTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for generating the video"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.InfinityStarTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated output"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enhance Prompt",
          "description": "Whether to use an LLM to enhance the prompt."
        },
        {
          "name": "use_apg",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Apg",
          "description": "Whether to use APG"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. Leave empty for random generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to guide what to avoid in generation"
        },
        {
          "name": "tau_video",
          "type": {
            "type": "float"
          },
          "default": 0.4,
          "title": "Tau Video",
          "description": "Tau value for video scale"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kandinsky 5 Pro Text To Video",
      "description": "Kandinsky5 Pro\n    video, generation, text-to-video, txt2vid, professional\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Kandinsky5ProTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "512P",
              "1024P"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Resolution"
          },
          "default": "512P",
          "title": "Resolution",
          "description": "Video resolution: 512p or 1024p."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for faster generation."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "3:2",
              "1:1",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.AspectRatio"
          },
          "default": "3:2",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video. One of (3:2, 1:1, 2:3)."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5s"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Duration"
          },
          "default": "5s",
          "title": "Duration",
          "description": "The length of the video to generate (5s or 10s)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kandinsky 5 Text To Video",
      "description": "Kandinsky5\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Kandinsky5TextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "768x512"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.Resolution"
          },
          "default": "768x512",
          "title": "Resolution",
          "description": "Resolution of the generated video in W:H format. Will be calculated based on the aspect ratio(768x512, 512x512, 512x768)."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5s",
              "10s"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.Duration"
          },
          "default": "5s",
          "title": "Duration",
          "description": "The length of the video to generate (5s or 10s)"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "3:2",
              "1:1",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.AspectRatio"
          },
          "default": "3:2",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video. One of (3:2, 1:1, 2:3)."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kandinsky 5 Text To Video Distill",
      "description": "Kandinsky5\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Kandinsky5TextToVideoDistill",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5s",
              "10s"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.Duration"
          },
          "default": "5s",
          "title": "Duration",
          "description": "The length of the video to generate (5s or 10s)"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "3:2",
              "1:1",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.AspectRatio"
          },
          "default": "3:2",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video. One of (3:2, 1:1, 2:3)."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "768x512"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.Resolution"
          },
          "default": "768x512",
          "title": "Resolution",
          "description": "Resolution of the generated video in W:H format. Will be calculated based on the aspect ratio(768x512, 512x512, 512x768)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kling Video Lipsync Audio To Video",
      "description": "Kling LipSync is an audio-to-video model that generates realistic lip movements from audio input.\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KlingVideoLipsyncAudioToVideo",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to generate the lip sync for. Supports .mp4/.mov, \u2264100MB, 2\u201310s, 720p/1080p only, width/height 720\u20131920px."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio to generate the lip sync for. Minimum duration is 2s and maximum duration is 60s. Maximum file size is 5MB."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kling Video Lipsync Text To Video",
      "description": "Kling LipSync is a text-to-video model that generates realistic lip movements from text input.\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KlingVideoLipsyncTextToVideo",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "Text content for lip-sync video generation. Max 120 characters."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to generate the lip sync for. Supports .mp4/.mov, \u2264100MB, 2-60s, 720p/1080p only, width/height 720\u20131920px. If validation fails, an error is returned."
        },
        {
          "name": "voice_id",
          "type": {
            "type": "enum",
            "values": [
              "genshin_vindi2",
              "zhinen_xuesheng",
              "AOT",
              "ai_shatang",
              "genshin_klee2",
              "genshin_kirara",
              "ai_kaiya",
              "oversea_male1",
              "ai_chenjiahao_712",
              "girlfriend_4_speech02",
              "chat1_female_new-3",
              "chat_0407_5-1",
              "cartoon-boy-07",
              "uk_boy1",
              "cartoon-girl-01",
              "PeppaPig_platform",
              "ai_huangzhong_712",
              "ai_huangyaoshi_712",
              "ai_laoguowang_712",
              "chengshu_jiejie",
              "you_pingjing",
              "calm_story1",
              "uk_man2",
              "laopopo_speech02",
              "heainainai_speech02",
              "reader_en_m-v1",
              "commercial_lady_en_f-v1",
              "tiyuxi_xuedi",
              "tiexin_nanyou",
              "girlfriend_1_speech02",
              "girlfriend_2_speech02",
              "zhuxi_speech02",
              "uk_oldman3",
              "dongbeilaotie_speech02",
              "chongqingxiaohuo_speech02",
              "chuanmeizi_speech02",
              "chaoshandashu_speech02",
              "ai_taiwan_man2_speech02",
              "xianzhanggui_speech02",
              "tianjinjiejie_speech02",
              "diyinnansang_DB_CN_M_04-v2",
              "yizhipiannan-v1",
              "guanxiaofang-v2",
              "tianmeixuemei-v1",
              "daopianyansang-v1",
              "mengwa-v1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoLipsyncTextToVideo.VoiceId"
          },
          "default": "",
          "title": "Voice Id",
          "description": "Voice ID to use for speech synthesis"
        },
        {
          "name": "voice_language",
          "type": {
            "type": "enum",
            "values": [
              "zh",
              "en"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoLipsyncTextToVideo.VoiceLanguage"
          },
          "default": "en",
          "title": "Voice Language",
          "description": "The voice language corresponding to the Voice ID"
        },
        {
          "name": "voice_speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Voice Speed",
          "description": "Speech rate for Text to Video generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kling Video V15 Pro Effects",
      "description": "Generate video clips from your prompts using Kling 1.5 (pro)\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KlingVideoV15ProEffects",
      "properties": [
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV15ProEffects.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "URL of images to be used for hug, kiss or heart_gesture video."
        },
        {
          "name": "effect_scene",
          "type": {
            "type": "enum",
            "values": [
              "hug",
              "kiss",
              "heart_gesture",
              "squish",
              "expansion",
              "fuzzyfuzzy",
              "bloombloom",
              "dizzydizzy",
              "jelly_press",
              "jelly_slice",
              "jelly_squish",
              "jelly_jiggle",
              "pixelpixel",
              "yearbook",
              "instant_film",
              "anime_figure",
              "rocketrocket",
              "fly_fly",
              "disappear",
              "lightning_power",
              "bullet_time",
              "bullet_time_360",
              "media_interview",
              "day_to_night",
              "let's_ride",
              "jumpdrop",
              "swish_swish",
              "running_man",
              "jazz_jazz",
              "swing_swing",
              "skateskate",
              "building_sweater",
              "pure_white_wings",
              "black_wings",
              "golden_wing",
              "pink_pink_wings",
              "rampage_ape",
              "a_list_look",
              "countdown_teleport",
              "firework_2026",
              "instant_christmas",
              "birthday_star",
              "firework",
              "celebration",
              "tiger_hug_pro",
              "pet_lion_pro",
              "guardian_spirit",
              "squeeze_scream",
              "inner_voice",
              "memory_alive",
              "guess_what",
              "eagle_snatch",
              "hug_from_past",
              "instant_kid",
              "dollar_rain",
              "cry_cry",
              "building_collapse",
              "mushroom",
              "jesus_hug",
              "shark_alert",
              "lie_flat",
              "polar_bear_hug",
              "brown_bear_hug",
              "office_escape_plow",
              "watermelon_bomb",
              "boss_coming",
              "wig_out",
              "car_explosion",
              "tiger_hug",
              "siblings",
              "construction_worker",
              "snatched",
              "felt_felt",
              "plushcut",
              "drunk_dance",
              "drunk_dance_pet",
              "daoma_dance",
              "bouncy_dance",
              "smooth_sailing_dance",
              "new_year_greeting",
              "lion_dance",
              "prosperity",
              "great_success",
              "golden_horse_fortune",
              "red_packet_box",
              "lucky_horse_year",
              "lucky_red_packet",
              "lucky_money_come",
              "lion_dance_pet",
              "dumpling_making_pet",
              "fish_making_pet",
              "pet_red_packet",
              "lantern_glow",
              "expression_challenge",
              "overdrive",
              "heart_gesture_dance",
              "poping",
              "martial_arts",
              "running",
              "nezha",
              "motorcycle_dance",
              "subject_3_dance",
              "ghost_step_dance",
              "phantom_jewel",
              "zoom_out",
              "cheers_2026",
              "kiss_pro",
              "fight_pro",
              "hug_pro",
              "heart_gesture_pro",
              "dollar_rain_pro",
              "pet_bee_pro",
              "santa_random_surprise",
              "magic_match_tree",
              "happy_birthday",
              "thumbs_up_pro",
              "surprise_bouquet",
              "bouquet_drop",
              "3d_cartoon_1_pro",
              "glamour_photo_shoot",
              "box_of_joy",
              "first_toast_of_the_year",
              "my_santa_pic",
              "santa_gift",
              "steampunk_christmas",
              "snowglobe",
              "christmas_photo_shoot",
              "ornament_crash",
              "santa_express",
              "particle_santa_surround",
              "coronation_of_frost",
              "spark_in_the_snow",
              "scarlet_and_snow",
              "cozy_toon_wrap",
              "bullet_time_lite",
              "magic_cloak",
              "balloon_parade",
              "jumping_ginger_joy",
              "c4d_cartoon_pro",
              "venomous_spider",
              "throne_of_king",
              "luminous_elf",
              "woodland_elf",
              "japanese_anime_1",
              "american_comics",
              "snowboarding",
              "witch_transform",
              "vampire_transform",
              "pumpkin_head_transform",
              "demon_transform",
              "mummy_transform",
              "zombie_transform",
              "cute_pumpkin_transform",
              "cute_ghost_transform",
              "knock_knock_halloween",
              "halloween_escape",
              "baseball",
              "trampoline",
              "trampoline_night",
              "pucker_up",
              "feed_mooncake",
              "flyer",
              "dishwasher",
              "pet_chinese_opera",
              "magic_fireball",
              "gallery_ring",
              "pet_moto_rider",
              "muscle_pet",
              "pet_delivery",
              "mythic_style",
              "steampunk",
              "3d_cartoon_2",
              "pet_chef",
              "santa_gifts",
              "santa_hug",
              "girlfriend",
              "boyfriend",
              "heart_gesture_1",
              "pet_wizard",
              "smoke_smoke",
              "gun_shot",
              "double_gun",
              "pet_warrior",
              "long_hair",
              "pet_dance",
              "wool_curly",
              "pet_bee",
              "marry_me",
              "piggy_morph",
              "ski_ski",
              "magic_broom",
              "splashsplash",
              "surfsurf",
              "fairy_wing",
              "angel_wing",
              "dark_wing",
              "emoji"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV15ProEffects.EffectScene"
          },
          "default": "",
          "title": "Effect Scene",
          "description": "The effect scene to use for the video generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kling Video V15 Pro Text To Video",
      "description": "Generate video clips from your prompts using Kling 1.5 (pro)\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KlingVideoV15ProTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV15ProTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV15ProTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kling Video V16 Pro Effects",
      "description": "Generate video clips from your prompts using Kling 1.6 (pro)\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KlingVideoV16ProEffects",
      "properties": [
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV16ProEffects.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "URL of images to be used for hug, kiss or heart_gesture video."
        },
        {
          "name": "effect_scene",
          "type": {
            "type": "enum",
            "values": [
              "hug",
              "kiss",
              "heart_gesture",
              "squish",
              "expansion",
              "fuzzyfuzzy",
              "bloombloom",
              "dizzydizzy",
              "jelly_press",
              "jelly_slice",
              "jelly_squish",
              "jelly_jiggle",
              "pixelpixel",
              "yearbook",
              "instant_film",
              "anime_figure",
              "rocketrocket",
              "fly_fly",
              "disappear",
              "lightning_power",
              "bullet_time",
              "bullet_time_360",
              "media_interview",
              "day_to_night",
              "let's_ride",
              "jumpdrop",
              "swish_swish",
              "running_man",
              "jazz_jazz",
              "swing_swing",
              "skateskate",
              "building_sweater",
              "pure_white_wings",
              "black_wings",
              "golden_wing",
              "pink_pink_wings",
              "rampage_ape",
              "a_list_look",
              "countdown_teleport",
              "firework_2026",
              "instant_christmas",
              "birthday_star",
              "firework",
              "celebration",
              "tiger_hug_pro",
              "pet_lion_pro",
              "guardian_spirit",
              "squeeze_scream",
              "inner_voice",
              "memory_alive",
              "guess_what",
              "eagle_snatch",
              "hug_from_past",
              "instant_kid",
              "dollar_rain",
              "cry_cry",
              "building_collapse",
              "mushroom",
              "jesus_hug",
              "shark_alert",
              "lie_flat",
              "polar_bear_hug",
              "brown_bear_hug",
              "office_escape_plow",
              "watermelon_bomb",
              "boss_coming",
              "wig_out",
              "car_explosion",
              "tiger_hug",
              "siblings",
              "construction_worker",
              "snatched",
              "felt_felt",
              "plushcut",
              "drunk_dance",
              "drunk_dance_pet",
              "daoma_dance",
              "bouncy_dance",
              "smooth_sailing_dance",
              "new_year_greeting",
              "lion_dance",
              "prosperity",
              "great_success",
              "golden_horse_fortune",
              "red_packet_box",
              "lucky_horse_year",
              "lucky_red_packet",
              "lucky_money_come",
              "lion_dance_pet",
              "dumpling_making_pet",
              "fish_making_pet",
              "pet_red_packet",
              "lantern_glow",
              "expression_challenge",
              "overdrive",
              "heart_gesture_dance",
              "poping",
              "martial_arts",
              "running",
              "nezha",
              "motorcycle_dance",
              "subject_3_dance",
              "ghost_step_dance",
              "phantom_jewel",
              "zoom_out",
              "cheers_2026",
              "kiss_pro",
              "fight_pro",
              "hug_pro",
              "heart_gesture_pro",
              "dollar_rain_pro",
              "pet_bee_pro",
              "santa_random_surprise",
              "magic_match_tree",
              "happy_birthday",
              "thumbs_up_pro",
              "surprise_bouquet",
              "bouquet_drop",
              "3d_cartoon_1_pro",
              "glamour_photo_shoot",
              "box_of_joy",
              "first_toast_of_the_year",
              "my_santa_pic",
              "santa_gift",
              "steampunk_christmas",
              "snowglobe",
              "christmas_photo_shoot",
              "ornament_crash",
              "santa_express",
              "particle_santa_surround",
              "coronation_of_frost",
              "spark_in_the_snow",
              "scarlet_and_snow",
              "cozy_toon_wrap",
              "bullet_time_lite",
              "magic_cloak",
              "balloon_parade",
              "jumping_ginger_joy",
              "c4d_cartoon_pro",
              "venomous_spider",
              "throne_of_king",
              "luminous_elf",
              "woodland_elf",
              "japanese_anime_1",
              "american_comics",
              "snowboarding",
              "witch_transform",
              "vampire_transform",
              "pumpkin_head_transform",
              "demon_transform",
              "mummy_transform",
              "zombie_transform",
              "cute_pumpkin_transform",
              "cute_ghost_transform",
              "knock_knock_halloween",
              "halloween_escape",
              "baseball",
              "trampoline",
              "trampoline_night",
              "pucker_up",
              "feed_mooncake",
              "flyer",
              "dishwasher",
              "pet_chinese_opera",
              "magic_fireball",
              "gallery_ring",
              "pet_moto_rider",
              "muscle_pet",
              "pet_delivery",
              "mythic_style",
              "steampunk",
              "3d_cartoon_2",
              "pet_chef",
              "santa_gifts",
              "santa_hug",
              "girlfriend",
              "boyfriend",
              "heart_gesture_1",
              "pet_wizard",
              "smoke_smoke",
              "gun_shot",
              "double_gun",
              "pet_warrior",
              "long_hair",
              "pet_dance",
              "wool_curly",
              "pet_bee",
              "marry_me",
              "piggy_morph",
              "ski_ski",
              "magic_broom",
              "splashsplash",
              "surfsurf",
              "fairy_wing",
              "angel_wing",
              "dark_wing",
              "emoji"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV16ProEffects.EffectScene"
          },
          "default": "",
          "title": "Effect Scene",
          "description": "The effect scene to use for the video generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kling Video V16 Pro Text To Video",
      "description": "Generate video clips from your prompts using Kling 1.6 (pro)\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KlingVideoV16ProTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV16ProTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV16ProTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kling Video V16 Standard Effects",
      "description": "Generate video clips from your prompts using Kling 1.6 (std)\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KlingVideoV16StandardEffects",
      "properties": [
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV16StandardEffects.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "URL of images to be used for hug, kiss or heart_gesture video."
        },
        {
          "name": "effect_scene",
          "type": {
            "type": "enum",
            "values": [
              "hug",
              "kiss",
              "heart_gesture",
              "squish",
              "expansion",
              "fuzzyfuzzy",
              "bloombloom",
              "dizzydizzy",
              "jelly_press",
              "jelly_slice",
              "jelly_squish",
              "jelly_jiggle",
              "pixelpixel",
              "yearbook",
              "instant_film",
              "anime_figure",
              "rocketrocket",
              "fly_fly",
              "disappear",
              "lightning_power",
              "bullet_time",
              "bullet_time_360",
              "media_interview",
              "day_to_night",
              "let's_ride",
              "jumpdrop",
              "swish_swish",
              "running_man",
              "jazz_jazz",
              "swing_swing",
              "skateskate",
              "building_sweater",
              "pure_white_wings",
              "black_wings",
              "golden_wing",
              "pink_pink_wings",
              "rampage_ape",
              "a_list_look",
              "countdown_teleport",
              "firework_2026",
              "instant_christmas",
              "birthday_star",
              "firework",
              "celebration",
              "tiger_hug_pro",
              "pet_lion_pro",
              "guardian_spirit",
              "squeeze_scream",
              "inner_voice",
              "memory_alive",
              "guess_what",
              "eagle_snatch",
              "hug_from_past",
              "instant_kid",
              "dollar_rain",
              "cry_cry",
              "building_collapse",
              "mushroom",
              "jesus_hug",
              "shark_alert",
              "lie_flat",
              "polar_bear_hug",
              "brown_bear_hug",
              "office_escape_plow",
              "watermelon_bomb",
              "boss_coming",
              "wig_out",
              "car_explosion",
              "tiger_hug",
              "siblings",
              "construction_worker",
              "snatched",
              "felt_felt",
              "plushcut",
              "drunk_dance",
              "drunk_dance_pet",
              "daoma_dance",
              "bouncy_dance",
              "smooth_sailing_dance",
              "new_year_greeting",
              "lion_dance",
              "prosperity",
              "great_success",
              "golden_horse_fortune",
              "red_packet_box",
              "lucky_horse_year",
              "lucky_red_packet",
              "lucky_money_come",
              "lion_dance_pet",
              "dumpling_making_pet",
              "fish_making_pet",
              "pet_red_packet",
              "lantern_glow",
              "expression_challenge",
              "overdrive",
              "heart_gesture_dance",
              "poping",
              "martial_arts",
              "running",
              "nezha",
              "motorcycle_dance",
              "subject_3_dance",
              "ghost_step_dance",
              "phantom_jewel",
              "zoom_out",
              "cheers_2026",
              "kiss_pro",
              "fight_pro",
              "hug_pro",
              "heart_gesture_pro",
              "dollar_rain_pro",
              "pet_bee_pro",
              "santa_random_surprise",
              "magic_match_tree",
              "happy_birthday",
              "thumbs_up_pro",
              "surprise_bouquet",
              "bouquet_drop",
              "3d_cartoon_1_pro",
              "glamour_photo_shoot",
              "box_of_joy",
              "first_toast_of_the_year",
              "my_santa_pic",
              "santa_gift",
              "steampunk_christmas",
              "snowglobe",
              "christmas_photo_shoot",
              "ornament_crash",
              "santa_express",
              "particle_santa_surround",
              "coronation_of_frost",
              "spark_in_the_snow",
              "scarlet_and_snow",
              "cozy_toon_wrap",
              "bullet_time_lite",
              "magic_cloak",
              "balloon_parade",
              "jumping_ginger_joy",
              "c4d_cartoon_pro",
              "venomous_spider",
              "throne_of_king",
              "luminous_elf",
              "woodland_elf",
              "japanese_anime_1",
              "american_comics",
              "snowboarding",
              "witch_transform",
              "vampire_transform",
              "pumpkin_head_transform",
              "demon_transform",
              "mummy_transform",
              "zombie_transform",
              "cute_pumpkin_transform",
              "cute_ghost_transform",
              "knock_knock_halloween",
              "halloween_escape",
              "baseball",
              "trampoline",
              "trampoline_night",
              "pucker_up",
              "feed_mooncake",
              "flyer",
              "dishwasher",
              "pet_chinese_opera",
              "magic_fireball",
              "gallery_ring",
              "pet_moto_rider",
              "muscle_pet",
              "pet_delivery",
              "mythic_style",
              "steampunk",
              "3d_cartoon_2",
              "pet_chef",
              "santa_gifts",
              "santa_hug",
              "girlfriend",
              "boyfriend",
              "heart_gesture_1",
              "pet_wizard",
              "smoke_smoke",
              "gun_shot",
              "double_gun",
              "pet_warrior",
              "long_hair",
              "pet_dance",
              "wool_curly",
              "pet_bee",
              "marry_me",
              "piggy_morph",
              "ski_ski",
              "magic_broom",
              "splashsplash",
              "surfsurf",
              "fairy_wing",
              "angel_wing",
              "dark_wing",
              "emoji"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV16StandardEffects.EffectScene"
          },
          "default": "",
          "title": "Effect Scene",
          "description": "The effect scene to use for the video generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kling Video V16 Standard Text To Video",
      "description": "Generate video clips from your prompts using Kling 1.6 (std)\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KlingVideoV16StandardTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV16StandardTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV16StandardTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kling Video V1 Standard Effects",
      "description": "Generate video clips from your prompts using Kling 1.0\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KlingVideoV1StandardEffects",
      "properties": [
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV1StandardEffects.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "URL of images to be used for hug, kiss or heart_gesture video."
        },
        {
          "name": "effect_scene",
          "type": {
            "type": "enum",
            "values": [
              "hug",
              "kiss",
              "heart_gesture",
              "squish",
              "expansion",
              "fuzzyfuzzy",
              "bloombloom",
              "dizzydizzy",
              "jelly_press",
              "jelly_slice",
              "jelly_squish",
              "jelly_jiggle",
              "pixelpixel",
              "yearbook",
              "instant_film",
              "anime_figure",
              "rocketrocket",
              "fly_fly",
              "disappear",
              "lightning_power",
              "bullet_time",
              "bullet_time_360",
              "media_interview",
              "day_to_night",
              "let's_ride",
              "jumpdrop",
              "swish_swish",
              "running_man",
              "jazz_jazz",
              "swing_swing",
              "skateskate",
              "building_sweater",
              "pure_white_wings",
              "black_wings",
              "golden_wing",
              "pink_pink_wings",
              "rampage_ape",
              "a_list_look",
              "countdown_teleport",
              "firework_2026",
              "instant_christmas",
              "birthday_star",
              "firework",
              "celebration",
              "tiger_hug_pro",
              "pet_lion_pro",
              "guardian_spirit",
              "squeeze_scream",
              "inner_voice",
              "memory_alive",
              "guess_what",
              "eagle_snatch",
              "hug_from_past",
              "instant_kid",
              "dollar_rain",
              "cry_cry",
              "building_collapse",
              "mushroom",
              "jesus_hug",
              "shark_alert",
              "lie_flat",
              "polar_bear_hug",
              "brown_bear_hug",
              "office_escape_plow",
              "watermelon_bomb",
              "boss_coming",
              "wig_out",
              "car_explosion",
              "tiger_hug",
              "siblings",
              "construction_worker",
              "snatched",
              "felt_felt",
              "plushcut",
              "drunk_dance",
              "drunk_dance_pet",
              "daoma_dance",
              "bouncy_dance",
              "smooth_sailing_dance",
              "new_year_greeting",
              "lion_dance",
              "prosperity",
              "great_success",
              "golden_horse_fortune",
              "red_packet_box",
              "lucky_horse_year",
              "lucky_red_packet",
              "lucky_money_come",
              "lion_dance_pet",
              "dumpling_making_pet",
              "fish_making_pet",
              "pet_red_packet",
              "lantern_glow",
              "expression_challenge",
              "overdrive",
              "heart_gesture_dance",
              "poping",
              "martial_arts",
              "running",
              "nezha",
              "motorcycle_dance",
              "subject_3_dance",
              "ghost_step_dance",
              "phantom_jewel",
              "zoom_out",
              "cheers_2026",
              "kiss_pro",
              "fight_pro",
              "hug_pro",
              "heart_gesture_pro",
              "dollar_rain_pro",
              "pet_bee_pro",
              "santa_random_surprise",
              "magic_match_tree",
              "happy_birthday",
              "thumbs_up_pro",
              "surprise_bouquet",
              "bouquet_drop",
              "3d_cartoon_1_pro",
              "glamour_photo_shoot",
              "box_of_joy",
              "first_toast_of_the_year",
              "my_santa_pic",
              "santa_gift",
              "steampunk_christmas",
              "snowglobe",
              "christmas_photo_shoot",
              "ornament_crash",
              "santa_express",
              "particle_santa_surround",
              "coronation_of_frost",
              "spark_in_the_snow",
              "scarlet_and_snow",
              "cozy_toon_wrap",
              "bullet_time_lite",
              "magic_cloak",
              "balloon_parade",
              "jumping_ginger_joy",
              "c4d_cartoon_pro",
              "venomous_spider",
              "throne_of_king",
              "luminous_elf",
              "woodland_elf",
              "japanese_anime_1",
              "american_comics",
              "snowboarding",
              "witch_transform",
              "vampire_transform",
              "pumpkin_head_transform",
              "demon_transform",
              "mummy_transform",
              "zombie_transform",
              "cute_pumpkin_transform",
              "cute_ghost_transform",
              "knock_knock_halloween",
              "halloween_escape",
              "baseball",
              "trampoline",
              "trampoline_night",
              "pucker_up",
              "feed_mooncake",
              "flyer",
              "dishwasher",
              "pet_chinese_opera",
              "magic_fireball",
              "gallery_ring",
              "pet_moto_rider",
              "muscle_pet",
              "pet_delivery",
              "mythic_style",
              "steampunk",
              "3d_cartoon_2",
              "pet_chef",
              "santa_gifts",
              "santa_hug",
              "girlfriend",
              "boyfriend",
              "heart_gesture_1",
              "pet_wizard",
              "smoke_smoke",
              "gun_shot",
              "double_gun",
              "pet_warrior",
              "long_hair",
              "pet_dance",
              "wool_curly",
              "pet_bee",
              "marry_me",
              "piggy_morph",
              "ski_ski",
              "magic_broom",
              "splashsplash",
              "surfsurf",
              "fairy_wing",
              "angel_wing",
              "dark_wing",
              "emoji"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV1StandardEffects.EffectScene"
          },
          "default": "",
          "title": "Effect Scene",
          "description": "The effect scene to use for the video generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kling Video V1 Standard Text To Video",
      "description": "Kling Video v1 Standard generates videos from text with balanced quality and speed.\n    video, generation, kling, text-to-video, txt2vid\n\n    Use cases:\n    - Generate standard quality videos\n    - Create video content efficiently\n    - Produce videos for web use\n    - Generate video previews\n    - Create video concepts",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KlingVideoV1StandardTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame"
        },
        {
          "name": "advanced_camera_control",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Advanced Camera Control",
          "description": "Advanced Camera control parameters"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "camera_control",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "down_back",
              "forward_up",
              "right_turn_forward",
              "left_turn_forward"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.CameraControl"
          },
          "default": null,
          "title": "Camera Control",
          "description": "Camera control parameters"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kling Video V21 Master Text To Video",
      "description": "Kling 2.1 Master: The premium endpoint for Kling 2.1, designed for top-tier text-to-video generation with unparalleled motion fluidity, cinematic visuals, and exceptional prompt precision.\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KlingVideoV21MasterTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV21MasterTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV21MasterTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kling Video V26 Pro Text To Video",
      "description": "Kling Video v2.6 Text to Video\n    video, generation, text-to-video, txt2vid, professional\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KlingVideoV26ProTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV26ProTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV26ProTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kling Video V3 Pro Text To Video",
      "description": "Kling Video V3 Pro generates professional quality videos from text prompts with enhanced visual fidelity using the latest V3 model.\n    video, generation, kling, v3, pro, text-to-video, txt2vid\n\n    Use cases:\n    - Create professional-grade videos from detailed prompts\n    - Generate cinematic video content with precise motion\n    - Produce high-fidelity advertising videos\n    - Create premium animated content from scripts\n    - Generate top-tier video for film and media",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KlingVideoV3ProTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV3ProTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12",
              "13",
              "14",
              "15"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV3ProTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate native audio for the video."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "duration",
        "aspect_ratio"
      ]
    },
    {
      "title": "Kling Video V3 Standard Text To Video",
      "description": "Kling Video V3 Standard generates videos from text prompts with balanced quality and speed using the latest V3 model.\n    video, generation, kling, v3, standard, text-to-video, txt2vid\n\n    Use cases:\n    - Generate cinematic videos from text descriptions\n    - Create marketing videos from product descriptions\n    - Produce educational video content from scripts\n    - Generate social media video content\n    - Create animated scenes from text prompts",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KlingVideoV3StandardTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV3StandardTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12",
              "13",
              "14",
              "15"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.KlingVideoV3StandardTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate native audio for the video."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "duration",
        "aspect_ratio"
      ]
    },
    {
      "title": "Krea Wan 14B Text To Video",
      "description": "Krea Wan 14b- Text to Video\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.KreaWan14BTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt for the video-to-video generation."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 78,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be a multiple of 12 plus 6, for example 6, 18, 30, 42, etc."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the video-to-video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "LTXVideo",
      "description": "LTX Video generates high-quality videos from text prompts with advanced temporal consistency.\n    video, generation, ltx, text-to-video, txt2vid\n\n    Use cases:\n    - Generate temporally consistent videos\n    - Create smooth video sequences\n    - Produce high-quality video content\n    - Generate professional video clips\n    - Create cinematic video scenes",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LTXVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for random number generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to take."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Longcat Video Distilled Text To Video 480P",
      "description": "LongCat Video Distilled\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LongcatVideoDistilledTextToVideo480P",
      "properties": [
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 15,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 162,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 12,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Longcat Video Distilled Text To Video 720P",
      "description": "LongCat Video Distilled\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LongcatVideoDistilledTextToVideo720P",
      "properties": [
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "num_refine_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 12,
          "title": "Num Refine Inference Steps",
          "description": "The number of inference steps to use for refinement."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 162,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 12,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Longcat Video Text To Video 480P",
      "description": "LongCat Video\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LongcatVideoTextToVideo480P",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for the video generation."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 15,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the video generation."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 162,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "The negative prompt to use for the video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Longcat Video Text To Video 720P",
      "description": "LongCat Video\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LongcatVideoTextToVideo720P",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for the video generation."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "num_refine_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Refine Inference Steps",
          "description": "The number of inference steps to use for refinement."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the video generation."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 162,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "The negative prompt to use for the video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for the video generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ltx 219B Distilled Text To Video",
      "description": "LTX-2 19B Distilled\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Ltx219BDistilledTextToVideo",
      "properties": [
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ltx 219B Distilled Text To Video Lora",
      "description": "LTX-2 19B Distilled\n    video, generation, text-to-video, txt2vid, lora\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Ltx219BDistilledTextToVideoLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ltx 219B Text To Video",
      "description": "LTX-2 19B\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Ltx219BTextToVideo",
      "properties": [
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ltx 219B Text To Video Lora",
      "description": "LTX-2 19B\n    video, generation, text-to-video, txt2vid, lora\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Ltx219BTextToVideoLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ltx Video 13 b Dev",
      "description": "Generate videos from prompts using LTX Video-0.9.7 13B and custom LoRA\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LtxVideo13bDev",
      "properties": [
        {
          "name": "second_pass_skip_initial_steps",
          "type": {
            "type": "int"
          },
          "default": 17,
          "title": "Second Pass Skip Initial Steps",
          "description": "The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes."
        },
        {
          "name": "first_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "First Pass Num Inference Steps",
          "description": "Number of inference steps during the first pass."
        },
        {
          "name": "frame_rate",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Frame Rate",
          "description": "The frame rate of the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide generation"
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "Whether to reverse the video."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt using a language model."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to use for generation"
        },
        {
          "name": "second_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Second Pass Num Inference Steps",
          "description": "Number of inference steps during the second pass."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames in the video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "worst quality, inconsistent motion, blurry, jittery, distorted",
          "title": "Negative Prompt",
          "description": "Negative prompt for generation"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LtxVideo13bDev.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p or 720p)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "1:1",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LtxVideo13bDev.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video (16:9, 1:1 or 9:16)."
        },
        {
          "name": "first_pass_skip_final_steps",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "First Pass Skip Final Steps",
          "description": "Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ltx Video 13 b Distilled",
      "description": "Generate videos from prompts using LTX Video-0.9.7 13B Distilled and custom LoRA\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LtxVideo13bDistilled",
      "properties": [
        {
          "name": "second_pass_skip_initial_steps",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Second Pass Skip Initial Steps",
          "description": "The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes."
        },
        {
          "name": "first_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "First Pass Num Inference Steps",
          "description": "Number of inference steps during the first pass."
        },
        {
          "name": "frame_rate",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Frame Rate",
          "description": "The frame rate of the video."
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "Whether to reverse the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide generation"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt using a language model."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to use for generation"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames in the video."
        },
        {
          "name": "second_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Second Pass Num Inference Steps",
          "description": "Number of inference steps during the second pass."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "worst quality, inconsistent motion, blurry, jittery, distorted",
          "title": "Negative Prompt",
          "description": "Negative prompt for generation"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LtxVideo13bDistilled.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p or 720p)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "1:1",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LtxVideo13bDistilled.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video (16:9, 1:1 or 9:16)."
        },
        {
          "name": "first_pass_skip_final_steps",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "First Pass Skip Final Steps",
          "description": "Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ltx Video V095",
      "description": "Generate videos from prompts using LTX Video-0.9.5\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LtxVideoV095",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide generation"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LtxVideoV095.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p or 720p)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LtxVideoV095.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video (16:9 or 9:16)."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt using the model's own capabilities."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "Number of inference steps"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "worst quality, inconsistent motion, blurry, jittery, distorted",
          "title": "Negative Prompt",
          "description": "Negative prompt for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ltxv 13 b 098 Distilled",
      "description": "Generate long videos from prompts using LTX Video-0.9.8 13B Distilled and custom LoRA\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Ltxv13b098Distilled",
      "properties": [
        {
          "name": "second_pass_skip_initial_steps",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Second Pass Skip Initial Steps",
          "description": "The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes."
        },
        {
          "name": "first_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "First Pass Num Inference Steps",
          "description": "Number of inference steps during the first pass."
        },
        {
          "name": "frame_rate",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Frame Rate",
          "description": "The frame rate of the video."
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "Whether to reverse the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide generation"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt using a language model."
        },
        {
          "name": "temporal_adain_factor",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Temporal Adain Factor",
          "description": "The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to use for generation"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames in the video."
        },
        {
          "name": "second_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Second Pass Num Inference Steps",
          "description": "Number of inference steps during the second pass."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "worst quality, inconsistent motion, blurry, jittery, distorted",
          "title": "Negative Prompt",
          "description": "Negative prompt for generation"
        },
        {
          "name": "enable_detail_pass",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Detail Pass",
          "description": "Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltxv13b098Distilled.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "1:1",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ltxv13b098Distilled.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "tone_map_compression_ratio",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Tone Map Compression Ratio",
          "description": "The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Luma Dream Machine Ray 2",
      "description": "Ray2 is a large-scale video generative model capable of creating realistic visuals with natural, coherent motion.\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LumaDreamMachineRay2",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "21:9",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2.Resolution"
          },
          "default": "540p",
          "title": "Resolution",
          "description": "The resolution of the generated video (720p costs 2x more, 1080p costs 4x more)"
        },
        {
          "name": "loop",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Loop",
          "description": "Whether the video should loop (end of video is blended with the beginning)"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5s",
              "9s"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2.Duration"
          },
          "default": "5s",
          "title": "Duration",
          "description": "The duration of the generated video (9s costs 2x more)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Luma Dream Machine Ray 2 Flash",
      "description": "Ray2 Flash is a fast video generative model capable of creating realistic visuals with natural, coherent motion.\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LumaDreamMachineRay2Flash",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "21:9",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2Flash.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2Flash.Resolution"
          },
          "default": "540p",
          "title": "Resolution",
          "description": "The resolution of the generated video (720p costs 2x more, 1080p costs 4x more)"
        },
        {
          "name": "loop",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Loop",
          "description": "Whether the video should loop (end of video is blended with the beginning)"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5s",
              "9s"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2Flash.Duration"
          },
          "default": "5s",
          "title": "Duration",
          "description": "The duration of the generated video (9s costs 2x more)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Luma Dream Machine Text To Video",
      "description": "Luma Dream Machine generates creative videos from text with dreamlike aesthetics.\n    video, generation, luma, dream-machine, text-to-video, txt2vid\n\n    Use cases:\n    - Generate dreamlike video content\n    - Create surreal video sequences\n    - Produce artistic video interpretations\n    - Generate creative video concepts\n    - Create imaginative video art",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LumaDreamMachineTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "21:9",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LumaDreamMachineTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "loop",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Loop",
          "description": "Whether the video should loop (end of video is blended with the beginning)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Luma Photon",
      "description": "Luma Photon generates photorealistic videos from text with high visual fidelity.\n    video, generation, luma, photon, photorealistic, text-to-video\n\n    Use cases:\n    - Generate photorealistic video content\n    - Create realistic video simulations\n    - Produce lifelike video scenes\n    - Generate high-fidelity video outputs\n    - Create realistic visual content",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.LumaPhoton",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1",
              "4:3",
              "3:4",
              "21:9",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.LumaPhoton.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Minimax Hailuo 02 Pro Text To Video",
      "description": "MiniMax Hailuo-02 Text To Video API (Pro, 1080p): Advanced video generation model with 1080p resolution\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.MinimaxHailuo02ProTextToVideo",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Minimax Hailuo 23 Pro Text To Video",
      "description": "MiniMax Hailuo 2.3 [Pro] (Text to Video)\n    video, generation, text-to-video, txt2vid, professional\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.MinimaxHailuo23ProTextToVideo",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Minimax Hailuo 23 Standard Text To Video",
      "description": "MiniMax Hailuo 2.3 [Standard] (Text to Video)\n    video, generation, text-to-video, txt2vid, professional\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.MinimaxHailuo23StandardTextToVideo",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "6",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.MinimaxHailuo23StandardTextToVideo.Duration"
          },
          "default": "6",
          "title": "Duration",
          "description": "The duration of the video in seconds."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Minimax Video 01",
      "description": "Generate video clips from your prompts using MiniMax model\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.MinimaxVideo01",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Minimax Video 01 Director",
      "description": "Generate video clips more accurately with respect to natural language descriptions and using camera movement instructions for shot control.\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.MinimaxVideo01Director",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation. Camera movement instructions can be added using square brackets (e.g. [Pan left] or [Zoom in]). You can use up to 3 combined movements per prompt. Supported movements: Truck left/right, Pan left/right, Push in/Pull out, Pedestal up/down, Tilt up/down, Zoom in/out, Shake, Tracking shot, Static shot. For example: [Truck left, Pan right, Zoom in]. For a more detailed guide, refer https://sixth-switch-2ac.notion.site/T2V-01-Director-Model-Tutorial-with-camera-movement-1886c20a98eb80f395b8e05291ad8645"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Minimax Video 01 Live",
      "description": "Generate video clips from your prompts using MiniMax model\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.MinimaxVideo01Live",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Mochi V1",
      "description": "Mochi v1 generates creative videos from text with unique artistic style.\n    video, generation, mochi, artistic, text-to-video, txt2vid\n\n    Use cases:\n    - Generate artistic video content\n    - Create stylized animations\n    - Produce creative video art\n    - Generate experimental videos\n    - Create unique visual content",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.MochiV1",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a video from."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for generating the video."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt for the video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Moonvalley Marey T2 V",
      "description": "Marey Realism V1.5\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.MoonvalleyMareyT2V",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a video from"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5s",
              "10s"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V.Duration"
          },
          "default": "5s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "dimensions",
          "type": {
            "type": "enum",
            "values": [
              "1920x1080",
              "1152x1152",
              "1536x1152",
              "1152x1536"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V.Dimensions"
          },
          "default": "1920x1080",
          "title": "Dimensions",
          "description": "The dimensions of the generated video in width x height format."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Guidance Scale",
          "description": "Controls how strongly the generation is guided by the prompt (0-20). Higher values follow the prompt more closely."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for random number generation. Use -1 for random seed each run."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "<synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts",
          "title": "Negative Prompt",
          "description": "Negative prompt used to guide the model away from undesirable features."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ovi",
      "description": "Ovi Text to Video\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Ovi",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "512x992",
              "992x512",
              "960x512",
              "512x960",
              "720x720",
              "448x1120",
              "1120x448"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Ovi.Resolution"
          },
          "default": "992x512",
          "title": "Resolution",
          "description": "Resolution of the generated video in W:H format. One of (512x992, 992x512, 960x512, 512x960, 720x720, or 448x1120)."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps."
        },
        {
          "name": "audio_negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "robotic, muffled, echo, distorted",
          "title": "Audio Negative Prompt",
          "description": "Negative prompt for audio generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "jitter, bad hands, blur, distortion",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Pika V21 Text To Video",
      "description": "Start with a simple text input to create dynamic generations that defy expectations. Anything you dream can come to life with sharp details, impressive character control and cinematic camera moves.\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.PikaV21TextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PikaV21TextToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1",
              "4:5",
              "5:4",
              "3:2",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PikaV21TextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Pika V2 Turbo Text To Video",
      "description": "Pika v2 Turbo creates videos from a text prompt with high quality output.\n    video, generation, text-to-video, txt2vid, fast\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.PikaV2TurboTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PikaV2TurboTextToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1",
              "4:5",
              "5:4",
              "3:2",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PikaV2TurboTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Pixverse V35 Text To Video",
      "description": "Generate high quality video clips from text prompts using PixVerse v3.5\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.PixverseV35TextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV35TextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV35TextToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV35TextToVideo.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV35TextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Pixverse V35 Text To Video Fast",
      "description": "Generate high quality video clips quickly from text prompts using PixVerse v3.5 Fast\n    video, generation, text-to-video, txt2vid, fast\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.PixverseV35TextToVideoFast",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV35TextToVideoFast.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV35TextToVideoFast.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV35TextToVideoFast.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Pixverse V45 Text To Video",
      "description": "Generate high quality video clips from text and image prompts using PixVerse v4.5\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.PixverseV45TextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV45TextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV45TextToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV45TextToVideo.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV45TextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Pixverse V45 Text To Video Fast",
      "description": "Generate high quality and fast video clips from text and image prompts using PixVerse v4.5 fast\n    video, generation, text-to-video, txt2vid, fast\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.PixverseV45TextToVideoFast",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV45TextToVideoFast.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV45TextToVideoFast.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV45TextToVideoFast.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Pixverse V4 Text To Video",
      "description": "Generate high quality video clips from text and image prompts using PixVerse v4\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.PixverseV4TextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV4TextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV4TextToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV4TextToVideo.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV4TextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Pixverse V4 Text To Video Fast",
      "description": "Generate high quality and fast video clips from text and image prompts using PixVerse v4 fast\n    video, generation, text-to-video, txt2vid, fast\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.PixverseV4TextToVideoFast",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV4TextToVideoFast.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV4TextToVideoFast.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV4TextToVideoFast.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Pixverse V55 Text To Video",
      "description": "Pixverse\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.PixverseV55TextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "thinking_type",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "enabled",
              "disabled",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.ThinkingType"
          },
          "default": null,
          "title": "Thinking Type",
          "description": "Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision"
        },
        {
          "name": "generate_multi_clip_switch",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Generate Multi Clip Switch",
          "description": "Enable multi-clip generation with dynamic camera changes"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds"
        },
        {
          "name": "generate_audio_switch",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Generate Audio Switch",
          "description": "Enable audio generation (BGM, SFX, dialogue)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Pixverse V56 Text To Video",
      "description": "Pixverse\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.PixverseV56TextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "thinking_type",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "enabled",
              "disabled",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.ThinkingType"
          },
          "default": null,
          "title": "Thinking Type",
          "description": "Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. 1080p videos are limited to 5 or 8 seconds"
        },
        {
          "name": "generate_audio_switch",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Generate Audio Switch",
          "description": "Enable audio generation (BGM, SFX, dialogue)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Pixverse V5 Text To Video",
      "description": "Pixverse\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.PixverseV5TextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Sana Video",
      "description": "Sana Video\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.SanaVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the video to generate"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.SanaVideo.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the output video"
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Fps",
          "description": "Frames per second for the output video"
        },
        {
          "name": "motion_score",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Motion Score",
          "description": "Motion intensity score (higher = more motion)"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 6,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation (higher = more prompt adherence)"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation. If not provided, a random seed will be used."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "A chaotic sequence with misshapen, deformed limbs in heavy motion blur, sudden disappearance, jump cuts, jerky movements, rapid shot changes, frames out of sync, inconsistent character shapes, temporal artifacts, jitter, and ghosting effects, creating a disorienting visual experience.",
          "title": "Negative Prompt",
          "description": "The negative prompt describing what to avoid in the generation"
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "See Dance V15 Pro Text To Video",
      "description": "SeeDance v1.5 Pro from ByteDance generates high-quality dance videos from text prompts.\n    video, generation, dance, seedance, bytedance, text-to-video\n\n    Use cases:\n    - Generate dance choreography videos\n    - Create dance performance visualizations\n    - Produce music video concepts\n    - Generate dance training content\n    - Create dance animation prototypes",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.SeeDanceV15ProTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the video in seconds"
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "camera_fixed",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Camera Fixed",
          "description": "Whether to fix the camera position"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control video generation. Use -1 for random."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "See Dance V1 Pro Fast Text To Video",
      "description": "SeeDance v1 Pro Fast generates dance videos quickly from text with reduced generation time.\n    video, generation, dance, seedance, fast, bytedance, text-to-video\n\n    Use cases:\n    - Rapidly prototype dance videos\n    - Create quick dance previews\n    - Generate dance concepts efficiently\n    - Iterate on choreography ideas\n    - Produce dance storyboards",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.SeeDanceV1ProFastTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the video"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "2",
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the video in seconds"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.Resolution"
          },
          "default": "1080p",
          "title": "Resolution",
          "description": "Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "camera_fixed",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Camera Fixed",
          "description": "Whether to fix the camera position"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control video generation. Use -1 for random."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Stable Video",
      "description": "Stable Video generates consistent and stable video sequences from text prompts.\n    video, generation, stable, text-to-video, txt2vid\n\n    Use cases:\n    - Generate stable video sequences\n    - Create consistent video content\n    - Produce reliable video outputs\n    - Generate predictable video scenes\n    - Create controlled video generation",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.StableVideo",
      "properties": [
        {
          "name": "motion_bucket_id",
          "type": {
            "type": "int"
          },
          "default": 127,
          "title": "Motion Bucket Id",
          "description": "The motion bucket id determines the motion of the generated video. The higher the number, the more motion there will be."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "cond_aug",
          "type": {
            "type": "float"
          },
          "default": 0.02,
          "title": "Cond Aug",
          "description": "The conditoning augmentation determines the amount of noise that will be added to the conditioning frame. The higher the number, the more noise there will be, and the less the video will look like the initial image. Increase it for more motion."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "T2 VTurbo",
      "description": "T2V Turbo generates videos from text at high speed with optimized performance.\n    video, generation, turbo, fast, text-to-video, txt2vid\n\n    Use cases:\n    - Generate videos with minimal latency\n    - Create rapid video prototypes\n    - Produce quick video previews\n    - Generate real-time video content\n    - Create efficient video workflows",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.T2VTurbo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate images from"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The guidance scale"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed to use for the random number generator"
        },
        {
          "name": "export_fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Export Fps",
          "description": "The FPS of the exported video"
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Num Frames",
          "description": "The number of frames to generate"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of steps to sample"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Transpixar",
      "description": "Transform text into stunning videos with TransPixar - an AI model that generates both RGB footage and alpha channels, enabling seamless compositing and creative video effects.\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Transpixar",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related video to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "export_fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Export Fps",
          "description": "The target FPS of the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate video from"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Veed Avatars Text To Video",
      "description": "VEED Avatars generates talking avatar videos from text using realistic AI-powered characters.\n    video, generation, avatar, talking-head, veed, text-to-video\n\n    Use cases:\n    - Create talking avatar presentations\n    - Generate spokesperson videos\n    - Produce educational talking head videos\n    - Create personalized video messages\n    - Generate multilingual avatar content",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.VeedAvatarsTextToVideo",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text"
        },
        {
          "name": "avatar_id",
          "type": {
            "type": "enum",
            "values": [
              "emily_vertical_primary",
              "emily_vertical_secondary",
              "marcus_vertical_primary",
              "marcus_vertical_secondary",
              "mira_vertical_primary",
              "mira_vertical_secondary",
              "jasmine_vertical_primary",
              "jasmine_vertical_secondary",
              "jasmine_vertical_walking",
              "aisha_vertical_walking",
              "elena_vertical_primary",
              "elena_vertical_secondary",
              "any_male_vertical_primary",
              "any_female_vertical_primary",
              "any_male_vertical_secondary",
              "any_female_vertical_secondary",
              "any_female_vertical_walking",
              "emily_primary",
              "emily_side",
              "marcus_primary",
              "marcus_side",
              "aisha_walking",
              "elena_primary",
              "elena_side",
              "any_male_primary",
              "any_female_primary",
              "any_male_side",
              "any_female_side"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.VeedAvatarsTextToVideo.AvatarId"
          },
          "default": "",
          "title": "Avatar Id",
          "description": "The avatar to use for the video"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Veed Fabric 10 Text",
      "description": "VEED Fabric 1.0 generates video content from text using advanced video synthesis.\n    video, generation, fabric, veed, text-to-video, txt2vid\n\n    Use cases:\n    - Generate marketing videos from text\n    - Create explainer video content\n    - Produce video ads from copy\n    - Generate social media videos\n    - Create branded video content",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.VeedFabric10Text",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "480p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.VeedFabric10Text.Resolution"
          },
          "default": "",
          "title": "Resolution",
          "description": "Resolution"
        },
        {
          "name": "voice_description",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Voice Description",
          "description": "Optional additional voice description. The primary voice description is auto-generated from the image. You can use simple descriptors like 'British accent' or 'Confident' or provide a detailed description like 'Confident male voice, mid-20s, with notes of...'"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Veo 2",
      "description": "Veo 2 creates videos with realistic motion and high quality output. Explore different styles and find your own with extensive camera controls.\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Veo2",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the video you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5s",
              "6s",
              "7s",
              "8s"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Veo2.Duration"
          },
          "default": "5s",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Veo2.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "A seed to use for the video generation"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the video generation"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the video generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Veo 31",
      "description": "Veo 3.1\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Veo31",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the video you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4s",
              "6s",
              "8s"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Veo31.Duration"
          },
          "default": "8s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Veo31.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video"
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "4k"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Veo31.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Veo 31 Fast",
      "description": "Veo 3.1 Fast\n    video, generation, text-to-video, txt2vid, fast\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Veo31Fast",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the video you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4s",
              "6s",
              "8s"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Veo31Fast.Duration"
          },
          "default": "8s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Veo31Fast.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video"
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "4k"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Veo31Fast.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Vidu Q1 Text To Video",
      "description": "Vidu Q1 Text to Video generates high-quality 1080p videos with exceptional visual quality and motion diversity\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.ViduQ1TextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation, max 1500 characters"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.ViduQ1TextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the output video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "general",
              "anime"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.ViduQ1TextToVideo.Style"
          },
          "default": "general",
          "title": "Style",
          "description": "The style of output video"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator"
        },
        {
          "name": "movement_amplitude",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "small",
              "medium",
              "large"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.ViduQ1TextToVideo.MovementAmplitude"
          },
          "default": "auto",
          "title": "Movement Amplitude",
          "description": "The movement amplitude of objects in the frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan 25 Preview Text To Video",
      "description": "Wan 2.5 Text to Video\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.Wan25PreviewTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt for video generation. Supports Chinese and English, max 800 characters."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the generated video in seconds. Choose between 5 or 10 seconds."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.Resolution"
          },
          "default": "1080p",
          "title": "Resolution",
          "description": "Video resolution tier"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "URL of the audio to use as the background music. Must be publicly accessible. Limit handling: If the audio duration exceeds the duration value (5 or 10 seconds), the audio is truncated to the first 5 or 10 seconds, and the rest is discarded. If the audio is shorter than the video, the remaining part of the video will be silent. For example, if the audio is 3 seconds long and the video duration is 5 seconds, the first 3 seconds of the output video will have sound, and the last 2 seconds will be silent. - Format: WAV, MP3. - Duration: 3 to 30 s. - File size: Up to 15 MB."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to describe content to avoid. Max 500 characters."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan Alpha",
      "description": "Wan Alpha\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.WanAlpha",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 10.5,
          "title": "Shift",
          "description": "The shift of the generated video."
        },
        {
          "name": "mask_clamp_upper",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Mask Clamp Upper",
          "description": "The upper bound of the mask clamping."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "mask_clamp_lower",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Mask Clamp Lower",
          "description": "The lower bound of the mask clamping."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "mask_binarization_threshold",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Mask Binarization Threshold",
          "description": "The threshold for mask binarization. When binarize_mask is True, this threshold will be used to binarize the mask. This will also be used for transparency when the output type is `.webm`."
        },
        {
          "name": "sampler",
          "type": {
            "type": "enum",
            "values": [
              "unipc",
              "dpm++",
              "euler"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanAlpha.Sampler"
          },
          "default": "euler",
          "title": "Sampler",
          "description": "The sampler to use."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanAlpha.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanAlpha.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanAlpha.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanAlpha.VideoOutputType"
          },
          "default": "VP9 (.webm)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "binarize_mask",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Binarize Mask",
          "description": "Whether to binarize the mask."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanAlpha.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan Pro Text To Video",
      "description": "Wan-2.1 Pro is a premium text-to-video model that generates high-quality 1080p videos at 30fps with up to 6 seconds duration, delivering exceptional visual quality and motion diversity from text prompts\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.WanProTextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan T2 v",
      "description": "Wan-2.1 is a text-to-video model that generates high-quality videos with high visual quality and motion diversity from text prompts\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.WanT2v",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanT2v.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video (16:9 or 9:16)."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanT2v.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p, 580p, or 720p)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "turbo_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Turbo Mode",
          "description": "If true, the video will be generated faster with no noticeable degradation in the visual quality."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 24."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 100 (inclusive)."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan T2 v Lora",
      "description": "Add custom LoRAs to Wan-2.1 is a text-to-video model that generates high-quality videos with high visual quality and motion diversity from images\n    video, generation, text-to-video, txt2vid, lora\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.WanT2vLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanT2vLora.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p,580p, or 720p)."
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "If true, the video will be reversed."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanT2vLora.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video (16:9 or 9:16)."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to be used in the inference."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 24."
        },
        {
          "name": "turbo_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Turbo Mode",
          "description": "If true, the video will be generated faster with no noticeable degradation in the visual quality."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 100 (inclusive)."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan V225 b Text To Video",
      "description": "Wan 2.2's 5B model produces up to 5 seconds of video 720p at 24FPS with fluid motion and powerful prompt understanding\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.WanV225bTextToVideo",
      "properties": [
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift value for the video. Must be between 1.0 and 10.0."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 17 to 161 (inclusive)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video (16:9 or 9:16)."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (580p or 720p)."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the output video. Higher quality means better visual quality but larger file size."
        },
        {
          "name": "adjust_fps_for_interpolation",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Adjust Fps For Interpolation",
          "description": "If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "film",
              "rife"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. If None, no interpolation is applied."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan V225 b Text To Video Distill",
      "description": "Wan 2.2's 5B distill model produces up to 5 seconds of video 720p at 24FPS with fluid motion and powerful prompt understanding\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.WanV225bTextToVideoDistill",
      "properties": [
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift value for the video. Must be between 1.0 and 10.0."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 17 to 161 (inclusive)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video (16:9 or 9:16)."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (580p or 720p)."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the output video. Higher quality means better visual quality but larger file size."
        },
        {
          "name": "adjust_fps_for_interpolation",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Adjust Fps For Interpolation",
          "description": "If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "film",
              "rife"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. If None, no interpolation is applied."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan V225 b Text To Video Fast Wan",
      "description": "Wan 2.2's 5B FastVideo model produces up to 5 seconds of video 720p at 24FPS with fluid motion and powerful prompt understanding\n    video, generation, text-to-video, txt2vid, fast\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.WanV225bTextToVideoFastWan",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 17 to 161 (inclusive)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video (16:9 or 9:16)."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (580p or 720p)."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the output video. Higher quality means better visual quality but larger file size."
        },
        {
          "name": "adjust_fps_for_interpolation",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Adjust Fps For Interpolation",
          "description": "If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "film",
              "rife"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. If None, no interpolation is applied."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan V22 A14 b Text To Video",
      "description": "Wan-2.2 text-to-video is a video model that generates high-quality videos with high visual quality and motion diversity from text prompts. \n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.WanV22A14bTextToVideo",
      "properties": [
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift value for the video. Must be between 1.0 and 10.0."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 17 to 161 (inclusive)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video (16:9 or 9:16)."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p, 580p, or 720p)."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "guidance_scale_2",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale 2",
          "description": "Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the output video. Higher quality means better visual quality but larger file size."
        },
        {
          "name": "adjust_fps_for_interpolation",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Adjust Fps For Interpolation",
          "description": "If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "film",
              "rife"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. If None, no interpolation is applied."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan V22 A14 b Text To Video Lora",
      "description": "Wan-2.2 text-to-video is a video model that generates high-quality videos with high visual quality and motion diversity from text prompts. This endpoint supports LoRAs made for Wan 2.2.\n    video, generation, text-to-video, txt2vid, lora\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.WanV22A14bTextToVideoLora",
      "properties": [
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift value for the video. Must be between 1.0 and 10.0."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'."
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "If true, the video will be reversed."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to be used in the inference."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 17 to 161 (inclusive)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video (16:9 or 9:16)."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p, 580p, or 720p)."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "guidance_scale_2",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale 2",
          "description": "Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the output video. Higher quality means better visual quality but larger file size."
        },
        {
          "name": "adjust_fps_for_interpolation",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Adjust Fps For Interpolation",
          "description": "If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "film",
              "rife"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. If None, no interpolation is applied."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan V22 A14 b Text To Video Turbo",
      "description": "Wan-2.2 turbo text-to-video is a video model that generates high-quality videos with high visual quality and motion diversity from text prompts. \n    video, generation, text-to-video, txt2vid, fast\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.WanV22A14bTextToVideoTurbo",
      "properties": [
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p, 580p, or 720p)."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video (16:9 or 9:16)."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the output video. Higher quality means better visual quality but larger file size."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan V26 Text To Video",
      "description": "Wan v2.6 Text to Video\n    video, generation, text-to-video, txt2vid\n\n    Use cases:\n    - AI-generated video content\n    - Marketing and advertising videos\n    - Educational content creation\n    - Social media video posts\n    - Automated video production",
      "namespace": "fal.text_to_video",
      "node_type": "fal.text_to_video.WanV26TextToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt for video generation. Supports Chinese and English, max 800 characters. For multi-shot videos, use format: 'Overall description. First shot [0-3s] content. Second shot [3-5s] content.'"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10",
              "15"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV26TextToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the generated video in seconds. Choose between 5, 10, or 15 seconds."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV26TextToVideo.Resolution"
          },
          "default": "1080p",
          "title": "Resolution",
          "description": "Video resolution tier. Wan 2.6 T2V only supports 720p and 1080p (no 480p)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1",
              "4:3",
              "3:4"
            ],
            "type_name": "nodetool.nodes.fal.text_to_video.WanV26TextToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video. Wan 2.6 supports additional ratios."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "URL of the audio to use as the background music. Must be publicly accessible. Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds), the audio is truncated to the first N seconds, and the rest is discarded. If the audio is shorter than the video, the remaining part of the video will be silent. For example, if the audio is 3 seconds long and the video duration is 5 seconds, the first 3 seconds of the output video will have sound, and the last 2 seconds will be silent. - Format: WAV, MP3. - Duration: 3 to 30 s. - File size: Up to 15 MB."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "multi_shots",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Multi Shots",
          "description": "When true, enables intelligent multi-shot segmentation for coherent narrative videos. Only active when enable_prompt_expansion is True. Set to false for single-shot generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to describe content to avoid. Max 500 characters."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Chatterbox Speech To Speech",
      "description": "Whether you're working on memes, videos, games, or AI agents, Chatterbox brings your content to life. Use the first tts from resemble ai.\n    speech, voice, transformation, cloning\n\n    Use cases:\n    - Voice cloning and transformation\n    - Real-time voice conversion\n    - Voice style transfer\n    - Speech enhancement\n    - Accent conversion",
      "namespace": "fal.speech_to_speech",
      "node_type": "fal.speech_to_speech.ChatterboxSpeechToSpeech",
      "properties": [
        {
          "name": "source_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Source Audio Url"
        },
        {
          "name": "target_voice_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Target Voice Audio Url",
          "description": "Optional URL to an audio file to use as a reference for the generated speech. If provided, the model will try to match the style and tone of the reference audio."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "source_audio_url",
        "target_voice_audio_url"
      ]
    },
    {
      "title": "Resemble Ai Chatterboxhd Speech To Speech",
      "description": "Transform voices using Resemble AI's Chatterbox. Convert audio to new voices or your own samples, with expressive results and built-in perceptual watermarking.\n    speech, voice, transformation, cloning\n\n    Use cases:\n    - Voice cloning and transformation\n    - Real-time voice conversion\n    - Voice style transfer\n    - Speech enhancement\n    - Accent conversion",
      "namespace": "fal.speech_to_speech",
      "node_type": "fal.speech_to_speech.ResembleAiChatterboxhdSpeechToSpeech",
      "properties": [
        {
          "name": "high_quality_audio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "High Quality Audio",
          "description": "If True, the generated audio will be upscaled to 48kHz. The generation of the audio will take longer, but the quality will be higher. If False, the generated audio will be 24kHz."
        },
        {
          "name": "target_voice_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Target Voice Audio Url",
          "description": "URL to the audio file which represents the voice of the output audio. If provided, this will override the target_voice setting. If neither target_voice nor target_voice_audio_url are provided, the default target voice will be used."
        },
        {
          "name": "source_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Source Audio Url",
          "description": "URL to the source audio file to be voice-converted."
        },
        {
          "name": "target_voice",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "Aurora",
              "Blade",
              "Britney",
              "Carl",
              "Cliff",
              "Richard",
              "Rico",
              "Siobhan",
              "Vicky"
            ],
            "type_name": "nodetool.nodes.fal.speech_to_speech.ResembleAiChatterboxhdSpeechToSpeech.TargetVoice"
          },
          "default": null,
          "title": "Target Voice",
          "description": "The voice to use for the speech-to-speech request. If neither target_voice nor target_voice_audio_url are provided, a random target voice will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "high_quality_audio",
        "target_voice_audio_url",
        "source_audio_url",
        "target_voice"
      ]
    },
    {
      "title": "Flux 2 Klein 4B Base Trainer",
      "description": "Flux 2 Klein 4B Base Trainer\n    flux, training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.Flux2Klein4BBaseTrainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Total number of training steps."
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better. The zip can also contain a text file for each image. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 5e-05,
          "title": "Learning Rate",
          "description": "Learning rate applied to trainable parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        },
        {
          "name": "output_lora_format",
          "type": {
            "type": "enum",
            "values": [
              "fal",
              "comfy"
            ],
            "type_name": "nodetool.nodes.fal.training.Flux2Klein4BBaseTrainer.OutputLoraFormat"
          },
          "default": "fal",
          "title": "Output Lora Format",
          "description": "Dictates the naming scheme for the output weights"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption",
        "output_lora_format"
      ]
    },
    {
      "title": "Flux 2 Klein 4B Base Trainer Edit",
      "description": "Flux 2 Klein 4B Base Trainer\n    flux, training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.Flux2Klein4BBaseTrainerEdit",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Total number of training steps."
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain up to four reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 5e-05,
          "title": "Learning Rate",
          "description": "Learning rate applied to trainable parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        },
        {
          "name": "output_lora_format",
          "type": {
            "type": "enum",
            "values": [
              "fal",
              "comfy"
            ],
            "type_name": "nodetool.nodes.fal.training.Flux2Klein4BBaseTrainerEdit.OutputLoraFormat"
          },
          "default": "fal",
          "title": "Output Lora Format",
          "description": "Dictates the naming scheme for the output weights"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption",
        "output_lora_format"
      ]
    },
    {
      "title": "Flux 2 Klein 9B Base Trainer",
      "description": "Flux 2 Klein 9B Base Trainer\n    flux, training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.Flux2Klein9BBaseTrainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Total number of training steps."
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better. The zip can also contain a text file for each image. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 5e-05,
          "title": "Learning Rate",
          "description": "Learning rate applied to trainable parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        },
        {
          "name": "output_lora_format",
          "type": {
            "type": "enum",
            "values": [
              "fal",
              "comfy"
            ],
            "type_name": "nodetool.nodes.fal.training.Flux2Klein9BBaseTrainer.OutputLoraFormat"
          },
          "default": "fal",
          "title": "Output Lora Format",
          "description": "Dictates the naming scheme for the output weights"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption",
        "output_lora_format"
      ]
    },
    {
      "title": "Flux 2 Klein 9B Base Trainer Edit",
      "description": "Flux 2 Klein 9B Base Trainer\n    flux, training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.Flux2Klein9BBaseTrainerEdit",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Total number of training steps."
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain up to four reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 5e-05,
          "title": "Learning Rate",
          "description": "Learning rate applied to trainable parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        },
        {
          "name": "output_lora_format",
          "type": {
            "type": "enum",
            "values": [
              "fal",
              "comfy"
            ],
            "type_name": "nodetool.nodes.fal.training.Flux2Klein9BBaseTrainerEdit.OutputLoraFormat"
          },
          "default": "fal",
          "title": "Output Lora Format",
          "description": "Dictates the naming scheme for the output weights"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption",
        "output_lora_format"
      ]
    },
    {
      "title": "Flux 2 Trainer",
      "description": "Fine-tune FLUX.2 [dev] from Black Forest Labs with custom datasets. Create specialized LoRA adaptations for specific styles and domains.\n    flux, training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Custom model fine-tuning\n    - LoRA training for personalization\n    - Style-specific model training\n    - Brand-specific image generation\n    - Specialized domain adaptation",
      "namespace": "fal.training",
      "node_type": "fal.training.Flux2Trainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Total number of training steps."
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better. The zip can also contain a text file for each image. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 5e-05,
          "title": "Learning Rate",
          "description": "Learning rate applied to trainable parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        },
        {
          "name": "output_lora_format",
          "type": {
            "type": "enum",
            "values": [
              "fal",
              "comfy"
            ],
            "type_name": "nodetool.nodes.fal.training.Flux2Trainer.OutputLoraFormat"
          },
          "default": "fal",
          "title": "Output Lora Format",
          "description": "Dictates the naming scheme for the output weights"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption",
        "output_lora_format"
      ]
    },
    {
      "title": "Flux 2 Trainer Edit",
      "description": "Fine-tune FLUX.2 [dev] from Black Forest Labs with custom datasets. Create specialized LoRA adaptations for specific editing tasks.\n    flux, training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Custom model fine-tuning\n    - LoRA training for personalization\n    - Style-specific model training\n    - Brand-specific image generation\n    - Specialized domain adaptation",
      "namespace": "fal.training",
      "node_type": "fal.training.Flux2TrainerEdit",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Total number of training steps."
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain up to four reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 5e-05,
          "title": "Learning Rate",
          "description": "Learning rate applied to trainable parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        },
        {
          "name": "output_lora_format",
          "type": {
            "type": "enum",
            "values": [
              "fal",
              "comfy"
            ],
            "type_name": "nodetool.nodes.fal.training.Flux2TrainerEdit.OutputLoraFormat"
          },
          "default": "fal",
          "title": "Output Lora Format",
          "description": "Dictates the naming scheme for the output weights"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption",
        "output_lora_format"
      ]
    },
    {
      "title": "Flux 2 Trainer V2",
      "description": "Flux 2 Trainer V2\n    flux, training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.Flux2TrainerV2",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Total number of training steps."
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better. The zip can also contain a text file for each image. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 5e-05,
          "title": "Learning Rate",
          "description": "Learning rate applied to trainable parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        },
        {
          "name": "output_lora_format",
          "type": {
            "type": "enum",
            "values": [
              "fal",
              "comfy"
            ],
            "type_name": "nodetool.nodes.fal.training.Flux2TrainerV2.OutputLoraFormat"
          },
          "default": "fal",
          "title": "Output Lora Format",
          "description": "Dictates the naming scheme for the output weights"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption",
        "output_lora_format"
      ]
    },
    {
      "title": "Flux 2 Trainer V2 Edit",
      "description": "Flux 2 Trainer V2\n    flux, training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.Flux2TrainerV2Edit",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Total number of training steps."
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain up to four reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 5e-05,
          "title": "Learning Rate",
          "description": "Learning rate applied to trainable parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        },
        {
          "name": "output_lora_format",
          "type": {
            "type": "enum",
            "values": [
              "fal",
              "comfy"
            ],
            "type_name": "nodetool.nodes.fal.training.Flux2TrainerV2Edit.OutputLoraFormat"
          },
          "default": "fal",
          "title": "Output Lora Format",
          "description": "Dictates the naming scheme for the output weights"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption",
        "output_lora_format"
      ]
    },
    {
      "title": "Hunyuan Video Lora Training",
      "description": "Train Hunyuan Video lora on people, objects, characters and more!\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Custom model fine-tuning\n    - LoRA training for personalization\n    - Style-specific model training\n    - Brand-specific image generation\n    - Specialized domain adaptation",
      "namespace": "fal.training",
      "node_type": "fal.training.HunyuanVideoLoraTraining",
      "properties": [
        {
          "name": "trigger_word",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Trigger Word",
          "description": "The trigger word to use."
        },
        {
          "name": "images_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Images Data Url",
          "description": "URL to zip archive with images. Try to use at least 4 images in general the more the better. In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to."
        },
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Steps",
          "description": "Number of steps to train the LoRA on."
        },
        {
          "name": "data_archive_format",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Data Archive Format",
          "description": "The format of the archive. If not specified, the format will be inferred from the URL."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0001,
          "title": "Learning Rate",
          "description": "Learning rate to use for training."
        },
        {
          "name": "do_caption",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Do Caption",
          "description": "Whether to generate captions for the images."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "trigger_word",
        "images_data_url",
        "steps",
        "data_archive_format",
        "learning_rate"
      ]
    },
    {
      "title": "Qwen Image 2512 Trainer",
      "description": "Qwen Image 2512 Trainer\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.QwenImage2512Trainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Number of steps to train for"
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive for text-to-image training. The zip should contain images with their corresponding text captions: image.EXT and image.txt For example: photo.jpg and photo.txt The text file contains the caption/prompt describing the target image. If no text file is provided for an image, the default_caption will be used. If no default_caption is provided and a text file is missing, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0005,
          "title": "Learning Rate",
          "description": "Learning rate for LoRA parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption"
      ]
    },
    {
      "title": "Qwen Image 2512 Trainer V2",
      "description": "Qwen Image 2512 Trainer V2\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.QwenImage2512TrainerV2",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 2000,
          "title": "Steps",
          "description": "Number of steps to train for"
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images and corresponding captions. The images should be named: ROOT.EXT. For example: 001.jpg The corresponding captions should be named: ROOT.txt. For example: 001.txt If no text file is provided for an image, the default_caption will be used."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0005,
          "title": "Learning Rate",
          "description": "Learning rate."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption"
      ]
    },
    {
      "title": "Qwen Image Edit 2509 Trainer",
      "description": "Qwen Image Edit 2509 Trainer\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.QwenImageEdit2509Trainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Number of steps to train for"
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain more than one reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The Reference Image Count field should be set to the number of reference images. The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0001,
          "title": "Learning Rate",
          "description": "Learning rate for LoRA parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption"
      ]
    },
    {
      "title": "Qwen Image Edit 2511 Trainer",
      "description": "Qwen Image Edit 2511 Trainer\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.QwenImageEdit2511Trainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Number of steps to train for"
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain more than one reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The Reference Image Count field should be set to the number of reference images. The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0001,
          "title": "Learning Rate",
          "description": "Learning rate for LoRA parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption"
      ]
    },
    {
      "title": "Qwen Image Edit Plus Trainer",
      "description": "LoRA trainer for Qwen Image Edit Plus\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Custom model fine-tuning\n    - LoRA training for personalization\n    - Style-specific model training\n    - Brand-specific image generation\n    - Specialized domain adaptation",
      "namespace": "fal.training",
      "node_type": "fal.training.QwenImageEditPlusTrainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Number of steps to train for"
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain more than one reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The Reference Image Count field should be set to the number of reference images. The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0001,
          "title": "Learning Rate",
          "description": "Learning rate for LoRA parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption"
      ]
    },
    {
      "title": "Qwen Image Edit Trainer",
      "description": "LoRA trainer for Qwen Image Edit\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Custom model fine-tuning\n    - LoRA training for personalization\n    - Style-specific model training\n    - Brand-specific image generation\n    - Specialized domain adaptation",
      "namespace": "fal.training",
      "node_type": "fal.training.QwenImageEditTrainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Number of steps to train for"
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0001,
          "title": "Learning Rate",
          "description": "Learning rate for LoRA parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption"
      ]
    },
    {
      "title": "Qwen Image Layered Trainer",
      "description": "Qwen Image Layered Trainer\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.QwenImageLayeredTrainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Number of steps to train for"
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain groups of images. The images should be named: ROOT_start.EXT, ROOT_end.EXT, ROOT_end2.EXT, ..., ROOT_endN.EXT For example: photo_start.png, photo_end.png, photo_end2.png, ..., photo_endN.png The start image is the base image that will be decomposed into layers. The end images are the layers that will be added to the base image. ROOT_end.EXT is the first layer, ROOT_end2.EXT is the second layer, and so on. You can have up to 8 layers. All image groups must have the same number of output layers. The end images can contain transparent regions. Only PNG and WebP images are supported since these are the only formats that support transparency. The zip can also contain a text file for each image group. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify a description of the base image. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0001,
          "title": "Learning Rate",
          "description": "Learning rate for LoRA parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption"
      ]
    },
    {
      "title": "Qwen Image Trainer",
      "description": "Qwen Image LoRA training\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Custom model fine-tuning\n    - LoRA training for personalization\n    - Style-specific model training\n    - Brand-specific image generation\n    - Specialized domain adaptation",
      "namespace": "fal.training",
      "node_type": "fal.training.QwenImageTrainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Total number of training steps to perform. Default is 4000."
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to zip archive with images for training. The archive should contain images and corresponding text files with captions. Each text file should have the same name as the image file it corresponds to (e.g., image1.jpg and image1.txt). If text files are missing for some images, you can provide a trigger_phrase to automatically create them. Supported image formats: PNG, JPG, JPEG, WEBP. Try to use at least 10 images, although more is better."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0005,
          "title": "Learning Rate",
          "description": "Learning rate for training. Default is 5e-4"
        },
        {
          "name": "trigger_phrase",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Trigger Phrase",
          "description": "Default caption to use for images that don't have corresponding text files. If provided, missing .txt files will be created automatically."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "trigger_phrase"
      ]
    },
    {
      "title": "Recraft V3 Create Style",
      "description": "Recraft V3 Create Style is capable of creating unique styles for Recraft V3 based on your images.\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Custom model fine-tuning\n    - LoRA training for personalization\n    - Style-specific model training\n    - Brand-specific image generation\n    - Specialized domain adaptation",
      "namespace": "fal.training",
      "node_type": "fal.training.RecraftV3CreateStyle",
      "properties": [
        {
          "name": "images_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Images Data Url",
          "description": "URL to zip archive with images, use PNG format. Maximum 5 images are allowed."
        },
        {
          "name": "base_style",
          "type": {
            "type": "enum",
            "values": [
              "any",
              "realistic_image",
              "digital_illustration",
              "vector_illustration",
              "realistic_image/b_and_w",
              "realistic_image/hard_flash",
              "realistic_image/hdr",
              "realistic_image/natural_light",
              "realistic_image/studio_portrait",
              "realistic_image/enterprise",
              "realistic_image/motion_blur",
              "realistic_image/evening_light",
              "realistic_image/faded_nostalgia",
              "realistic_image/forest_life",
              "realistic_image/mystic_naturalism",
              "realistic_image/natural_tones",
              "realistic_image/organic_calm",
              "realistic_image/real_life_glow",
              "realistic_image/retro_realism",
              "realistic_image/retro_snapshot",
              "realistic_image/urban_drama",
              "realistic_image/village_realism",
              "realistic_image/warm_folk",
              "digital_illustration/pixel_art",
              "digital_illustration/hand_drawn",
              "digital_illustration/grain",
              "digital_illustration/infantile_sketch",
              "digital_illustration/2d_art_poster",
              "digital_illustration/handmade_3d",
              "digital_illustration/hand_drawn_outline",
              "digital_illustration/engraving_color",
              "digital_illustration/2d_art_poster_2",
              "digital_illustration/antiquarian",
              "digital_illustration/bold_fantasy",
              "digital_illustration/child_book",
              "digital_illustration/child_books",
              "digital_illustration/cover",
              "digital_illustration/crosshatch",
              "digital_illustration/digital_engraving",
              "digital_illustration/expressionism",
              "digital_illustration/freehand_details",
              "digital_illustration/grain_20",
              "digital_illustration/graphic_intensity",
              "digital_illustration/hard_comics",
              "digital_illustration/long_shadow",
              "digital_illustration/modern_folk",
              "digital_illustration/multicolor",
              "digital_illustration/neon_calm",
              "digital_illustration/noir",
              "digital_illustration/nostalgic_pastel",
              "digital_illustration/outline_details",
              "digital_illustration/pastel_gradient",
              "digital_illustration/pastel_sketch",
              "digital_illustration/pop_art",
              "digital_illustration/pop_renaissance",
              "digital_illustration/street_art",
              "digital_illustration/tablet_sketch",
              "digital_illustration/urban_glow",
              "digital_illustration/urban_sketching",
              "digital_illustration/vanilla_dreams",
              "digital_illustration/young_adult_book",
              "digital_illustration/young_adult_book_2",
              "vector_illustration/bold_stroke",
              "vector_illustration/chemistry",
              "vector_illustration/colored_stencil",
              "vector_illustration/contour_pop_art",
              "vector_illustration/cosmics",
              "vector_illustration/cutout",
              "vector_illustration/depressive",
              "vector_illustration/editorial",
              "vector_illustration/emotional_flat",
              "vector_illustration/infographical",
              "vector_illustration/marker_outline",
              "vector_illustration/mosaic",
              "vector_illustration/naivector",
              "vector_illustration/roundish_flat",
              "vector_illustration/segmented_colors",
              "vector_illustration/sharp_contrast",
              "vector_illustration/thin",
              "vector_illustration/vector_photo",
              "vector_illustration/vivid_shapes",
              "vector_illustration/engraving",
              "vector_illustration/line_art",
              "vector_illustration/line_circuit",
              "vector_illustration/linocut"
            ],
            "type_name": "nodetool.nodes.fal.training.RecraftV3CreateStyle.BaseStyle"
          },
          "default": "digital_illustration",
          "title": "Base Style",
          "description": "The base style of the generated images, this topic is covered above."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "images_data_url",
        "base_style"
      ]
    },
    {
      "title": "Turbo Flux Trainer",
      "description": "A blazing fast FLUX dev LoRA trainer for subjects and styles.\n    flux, training, fine-tuning, lora, model-training, fast\n\n    Use cases:\n    - Custom model fine-tuning\n    - LoRA training for personalization\n    - Style-specific model training\n    - Brand-specific image generation\n    - Specialized domain adaptation",
      "namespace": "fal.training",
      "node_type": "fal.training.TurboFluxTrainer",
      "properties": [
        {
          "name": "images_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Images Data Url",
          "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better."
        },
        {
          "name": "trigger_phrase",
          "type": {
            "type": "str"
          },
          "default": "ohwx",
          "title": "Trigger Phrase",
          "description": "Trigger phrase to be used in the captions. If None, a trigger word will not be used. If no captions are provide the trigger_work will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions."
        },
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Number of steps to train the LoRA on."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.00115,
          "title": "Learning Rate",
          "description": "Learning rate for the training."
        },
        {
          "name": "training_style",
          "type": {
            "type": "enum",
            "values": [
              "subject",
              "style"
            ],
            "type_name": "nodetool.nodes.fal.training.TurboFluxTrainer.TrainingStyle"
          },
          "default": "subject",
          "title": "Training Style",
          "description": "Training style to use."
        },
        {
          "name": "face_crop",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Face Crop",
          "description": "Whether to try to detect the face and crop the images to the face."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "images_data_url",
        "trigger_phrase",
        "steps",
        "learning_rate",
        "training_style"
      ]
    },
    {
      "title": "Wan 22 Image Trainer",
      "description": "Wan 2.2 text to image LoRA trainer. Fine-tune Wan 2.2 for subjects and styles with unprecedented detail.\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Custom model fine-tuning\n    - LoRA training for personalization\n    - Style-specific model training\n    - Brand-specific image generation\n    - Specialized domain adaptation",
      "namespace": "fal.training",
      "node_type": "fal.training.Wan22ImageTrainer",
      "properties": [
        {
          "name": "trigger_phrase",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Trigger Phrase",
          "description": "Trigger phrase for the model."
        },
        {
          "name": "use_masks",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Masks",
          "description": "Whether to use masks for the training data."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0007,
          "title": "Learning Rate",
          "description": "Learning rate for training."
        },
        {
          "name": "use_face_cropping",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Face Cropping",
          "description": "Whether to use face cropping for the training data. When enabled, images will be cropped to the face before resizing."
        },
        {
          "name": "training_data_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Training Data Url",
          "description": "URL to the training data."
        },
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Number of training steps."
        },
        {
          "name": "include_synthetic_captions",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Include Synthetic Captions",
          "description": "Whether to include synthetic captions."
        },
        {
          "name": "is_style",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Is Style",
          "description": "Whether the training data is style data. If true, face specific options like masking and face detection will be disabled."
        },
        {
          "name": "use_face_detection",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Face Detection",
          "description": "Whether to use face detection for the training data. When enabled, images will use the center of the face as the center of the image when resizing."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "trigger_phrase",
        "use_masks",
        "learning_rate",
        "use_face_cropping",
        "training_data_url"
      ]
    },
    {
      "title": "Wan Trainer",
      "description": "Train custom LoRAs for Wan-2.1 I2V 480P\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Custom model fine-tuning\n    - LoRA training for personalization\n    - Style-specific model training\n    - Brand-specific image generation\n    - Specialized domain adaptation",
      "namespace": "fal.training",
      "node_type": "fal.training.WanTrainer",
      "properties": [
        {
          "name": "number_of_steps",
          "type": {
            "type": "int"
          },
          "default": 400,
          "title": "Number Of Steps",
          "description": "The number of steps to train for."
        },
        {
          "name": "training_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Training Data Url",
          "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better. In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to."
        },
        {
          "name": "trigger_phrase",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Trigger Phrase",
          "description": "The phrase that will trigger the model to generate an image."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0002,
          "title": "Learning Rate",
          "description": "The rate at which the model learns. Higher values can lead to faster training, but over-fitting."
        },
        {
          "name": "auto_scale_input",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Scale Input",
          "description": "If true, the input will be automatically scale the video to 81 frames at 16fps."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "number_of_steps",
        "training_data_url",
        "trigger_phrase",
        "learning_rate",
        "auto_scale_input"
      ]
    },
    {
      "title": "Wan Trainer Flf 2 v 720 p",
      "description": "Train custom LoRAs for Wan-2.1 FLF2V 720P\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Custom model fine-tuning\n    - LoRA training for personalization\n    - Style-specific model training\n    - Brand-specific image generation\n    - Specialized domain adaptation",
      "namespace": "fal.training",
      "node_type": "fal.training.WanTrainerFlf2v720p",
      "properties": [
        {
          "name": "number_of_steps",
          "type": {
            "type": "int"
          },
          "default": 400,
          "title": "Number Of Steps",
          "description": "The number of steps to train for."
        },
        {
          "name": "training_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Training Data Url",
          "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better. In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to."
        },
        {
          "name": "trigger_phrase",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Trigger Phrase",
          "description": "The phrase that will trigger the model to generate an image."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0002,
          "title": "Learning Rate",
          "description": "The rate at which the model learns. Higher values can lead to faster training, but over-fitting."
        },
        {
          "name": "auto_scale_input",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Scale Input",
          "description": "If true, the input will be automatically scale the video to 81 frames at 16fps."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "number_of_steps",
        "training_data_url",
        "trigger_phrase",
        "learning_rate",
        "auto_scale_input"
      ]
    },
    {
      "title": "Wan Trainer I2 v 720 p",
      "description": "Train custom LoRAs for Wan-2.1 I2V 720P\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Custom model fine-tuning\n    - LoRA training for personalization\n    - Style-specific model training\n    - Brand-specific image generation\n    - Specialized domain adaptation",
      "namespace": "fal.training",
      "node_type": "fal.training.WanTrainerI2v720p",
      "properties": [
        {
          "name": "number_of_steps",
          "type": {
            "type": "int"
          },
          "default": 400,
          "title": "Number Of Steps",
          "description": "The number of steps to train for."
        },
        {
          "name": "training_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Training Data Url",
          "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better. In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to."
        },
        {
          "name": "trigger_phrase",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Trigger Phrase",
          "description": "The phrase that will trigger the model to generate an image."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0002,
          "title": "Learning Rate",
          "description": "The rate at which the model learns. Higher values can lead to faster training, but over-fitting."
        },
        {
          "name": "auto_scale_input",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Scale Input",
          "description": "If true, the input will be automatically scale the video to 81 frames at 16fps."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "number_of_steps",
        "training_data_url",
        "trigger_phrase",
        "learning_rate",
        "auto_scale_input"
      ]
    },
    {
      "title": "Wan Trainer T2 v",
      "description": "Train custom LoRAs for Wan-2.1 T2V 1.3B\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Custom model fine-tuning\n    - LoRA training for personalization\n    - Style-specific model training\n    - Brand-specific image generation\n    - Specialized domain adaptation",
      "namespace": "fal.training",
      "node_type": "fal.training.WanTrainerT2v",
      "properties": [
        {
          "name": "number_of_steps",
          "type": {
            "type": "int"
          },
          "default": 400,
          "title": "Number Of Steps",
          "description": "The number of steps to train for."
        },
        {
          "name": "training_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Training Data Url",
          "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better. In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to."
        },
        {
          "name": "trigger_phrase",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Trigger Phrase",
          "description": "The phrase that will trigger the model to generate an image."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0002,
          "title": "Learning Rate",
          "description": "The rate at which the model learns. Higher values can lead to faster training, but over-fitting."
        },
        {
          "name": "auto_scale_input",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Scale Input",
          "description": "If true, the input will be automatically scale the video to 81 frames at 16fps."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "number_of_steps",
        "training_data_url",
        "trigger_phrase",
        "learning_rate",
        "auto_scale_input"
      ]
    },
    {
      "title": "Wan Trainer T2 v 14 b",
      "description": "Train custom LoRAs for Wan-2.1 T2V 14B\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Custom model fine-tuning\n    - LoRA training for personalization\n    - Style-specific model training\n    - Brand-specific image generation\n    - Specialized domain adaptation",
      "namespace": "fal.training",
      "node_type": "fal.training.WanTrainerT2v14b",
      "properties": [
        {
          "name": "number_of_steps",
          "type": {
            "type": "int"
          },
          "default": 400,
          "title": "Number Of Steps",
          "description": "The number of steps to train for."
        },
        {
          "name": "training_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Training Data Url",
          "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better. In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to."
        },
        {
          "name": "trigger_phrase",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Trigger Phrase",
          "description": "The phrase that will trigger the model to generate an image."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0002,
          "title": "Learning Rate",
          "description": "The rate at which the model learns. Higher values can lead to faster training, but over-fitting."
        },
        {
          "name": "auto_scale_input",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Scale Input",
          "description": "If true, the input will be automatically scale the video to 81 frames at 16fps."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "number_of_steps",
        "training_data_url",
        "trigger_phrase",
        "learning_rate",
        "auto_scale_input"
      ]
    },
    {
      "title": "ZImage Base Trainer",
      "description": "Z-Image Trainer\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.ZImageBaseTrainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 2000,
          "title": "Steps",
          "description": "Number of steps to train for"
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images and corresponding captions. The images should be named: ROOT.EXT. For example: 001.jpg The corresponding captions should be named: ROOT.txt. For example: 001.txt If no text file is provided for an image, the default_caption will be used."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0005,
          "title": "Learning Rate",
          "description": "Learning rate."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption"
      ]
    },
    {
      "title": "ZImage Trainer",
      "description": "Train LoRAs on Z-Image Turbo, a super fast text-to-image model of 6B parameters developed by Tongyi-MAI.\n    training, fine-tuning, lora, model-training\n\n    Use cases:\n    - Custom model fine-tuning\n    - LoRA training for personalization\n    - Style-specific model training\n    - Brand-specific image generation\n    - Specialized domain adaptation",
      "namespace": "fal.training",
      "node_type": "fal.training.ZImageTrainer",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 1000,
          "title": "Steps",
          "description": "Total number of training steps."
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better. The zip can also contain a text file for each image. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail."
        },
        {
          "name": "training_type",
          "type": {
            "type": "enum",
            "values": [
              "content",
              "style",
              "balanced"
            ],
            "type_name": "nodetool.nodes.fal.training.ZImageTrainer.TrainingType"
          },
          "default": "balanced",
          "title": "Training Type",
          "description": "Type of training to perform. Use 'content' to focus on the content of the images, 'style' to focus on the style of the images, and 'balanced' to focus on a combination of both."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0001,
          "title": "Learning Rate",
          "description": "Learning rate applied to trainable parameters."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "training_type",
        "learning_rate",
        "default_caption"
      ]
    },
    {
      "title": "ZImage Turbo Trainer V2",
      "description": "Z Image Turbo Trainer V2\n    training, fine-tuning, lora, model-training, fast\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.training",
      "node_type": "fal.training.ZImageTurboTrainerV2",
      "properties": [
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 2000,
          "title": "Steps",
          "description": "Number of steps to train for"
        },
        {
          "name": "image_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Data Url",
          "description": "URL to the input data zip archive. The zip should contain pairs of images and corresponding captions. The images should be named: ROOT.EXT. For example: 001.jpg The corresponding captions should be named: ROOT.txt. For example: 001.txt If no text file is provided for an image, the default_caption will be used."
        },
        {
          "name": "learning_rate",
          "type": {
            "type": "float"
          },
          "default": 0.0005,
          "title": "Learning Rate",
          "description": "Learning rate."
        },
        {
          "name": "default_caption",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Default Caption",
          "description": "Default caption to use when caption files are missing. If None, missing captions will cause an error."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "steps",
        "image_data_url",
        "learning_rate",
        "default_caption"
      ]
    },
    {
      "title": "Ffmpeg Api Loudnorm",
      "description": "Get EBU R128 loudness normalization from audio files using FFmpeg API.\n    json, processing, data, utility\n\n    Use cases:\n    - JSON data processing\n    - Data transformation\n    - Metadata extraction\n    - Audio analysis\n    - Media processing utilities",
      "namespace": "fal.json_processing",
      "node_type": "fal.json_processing.FfmpegApiLoudnorm",
      "properties": [
        {
          "name": "measured_tp",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Measured Tp",
          "description": "Measured true peak of input file in dBTP. Required for linear mode."
        },
        {
          "name": "print_summary",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Print Summary",
          "description": "Return loudness measurement summary with the normalized audio"
        },
        {
          "name": "linear",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Linear",
          "description": "Use linear normalization mode (single-pass). If false, uses dynamic mode (two-pass for better quality)."
        },
        {
          "name": "measured_i",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Measured I",
          "description": "Measured integrated loudness of input file in LUFS. Required for linear mode."
        },
        {
          "name": "offset",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Offset",
          "description": "Offset gain in dB applied before the true-peak limiter"
        },
        {
          "name": "measured_lra",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Measured Lra",
          "description": "Measured loudness range of input file in LU. Required for linear mode."
        },
        {
          "name": "measured_thresh",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Measured Thresh",
          "description": "Measured threshold of input file in LUFS. Required for linear mode."
        },
        {
          "name": "dual_mono",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Dual Mono",
          "description": "Treat mono input files as dual-mono for correct EBU R128 measurement on stereo systems"
        },
        {
          "name": "true_peak",
          "type": {
            "type": "float"
          },
          "default": -0.1,
          "title": "True Peak",
          "description": "Maximum true peak in dBTP."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to normalize"
        },
        {
          "name": "integrated_loudness",
          "type": {
            "type": "float"
          },
          "default": -18,
          "title": "Integrated Loudness",
          "description": "Integrated loudness target in LUFS."
        },
        {
          "name": "loudness_range",
          "type": {
            "type": "float"
          },
          "default": 7,
          "title": "Loudness Range",
          "description": "Loudness range target in LU"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "measured_tp",
        "print_summary",
        "linear",
        "measured_i",
        "offset"
      ]
    },
    {
      "title": "Ffmpeg Api Metadata",
      "description": "Get encoding metadata from video and audio files using FFmpeg API.\n    json, processing, data, utility\n\n    Use cases:\n    - JSON data processing\n    - Data transformation\n    - Metadata extraction\n    - Audio analysis\n    - Media processing utilities",
      "namespace": "fal.json_processing",
      "node_type": "fal.json_processing.FfmpegApiMetadata",
      "properties": [
        {
          "name": "extract_frames",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Extract Frames",
          "description": "Whether to extract the start and end frames for videos. Note that when true the request will be slower."
        },
        {
          "name": "media_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Media Url",
          "description": "URL of the media file (video or audio) to analyze"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "extract_frames",
        "media_url"
      ]
    },
    {
      "title": "Ffmpeg Api Waveform",
      "description": "Get waveform data from audio files using FFmpeg API.\n    json, processing, data, utility\n\n    Use cases:\n    - JSON data processing\n    - Data transformation\n    - Metadata extraction\n    - Audio analysis\n    - Media processing utilities",
      "namespace": "fal.json_processing",
      "node_type": "fal.json_processing.FfmpegApiWaveform",
      "properties": [
        {
          "name": "precision",
          "type": {
            "type": "int"
          },
          "default": 2,
          "title": "Precision",
          "description": "Number of decimal places for the waveform values. Higher values provide more precision but increase payload size."
        },
        {
          "name": "smoothing_window",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "Smoothing Window",
          "description": "Size of the smoothing window. Higher values create a smoother waveform. Must be an odd number."
        },
        {
          "name": "media_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Media Url",
          "description": "URL of the audio file to analyze"
        },
        {
          "name": "points_per_second",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Points Per Second",
          "description": "Controls how many points are sampled per second of audio. Lower values (e.g. 1-2) create a coarser waveform, higher values (e.g. 4-10) create a more detailed one."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "precision",
        "smoothing_window",
        "media_url",
        "points_per_second"
      ]
    },
    {
      "title": "Bagel Understand",
      "description": "Bagel is a 7B parameter multimodal model from Bytedance-Seed that can generate both text and images.\n    vision, analysis, json, image-understanding\n\n    Use cases:\n    - Image analysis to structured data\n    - Visual content understanding\n    - Automated image metadata extraction\n    - Content classification\n    - Image-based data extraction",
      "namespace": "fal.image_to_json",
      "node_type": "fal.image_to_json.BagelUnderstand",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to query the image with."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image for the query."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "seed",
        "image_url"
      ]
    },
    {
      "title": "Chatterbox Text To Speech",
      "description": "Whether you're working on memes, videos, games, or AI agents, Chatterbox brings your content to life. Use the first tts from resemble ai.\n    speech, synthesis, text-to-speech, tts\n\n    Use cases:\n    - Voice synthesis for applications\n    - Audiobook narration\n    - Virtual assistant voices\n    - Accessibility solutions\n    - Content localization",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.ChatterboxTextToSpeech",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to be converted to speech. You can additionally add the following emotive tags: <laugh>, <chuckle>, <sigh>, <cough>, <sniffle>, <groan>, <yawn>, <gasp>"
        },
        {
          "name": "exaggeration",
          "type": {
            "type": "float"
          },
          "default": 0.25,
          "title": "Exaggeration",
          "description": "Exaggeration factor for the generated speech (0.0 = no exaggeration, 1.0 = maximum exaggeration)."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": "https://storage.googleapis.com/chatterbox-demo-samples/prompts/male_rickmorty.mp3",
          "title": "Audio Url",
          "description": "Optional URL to an audio file to use as a reference for the generated speech. If provided, the model will try to match the style and tone of the reference audio."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.7,
          "title": "Temperature",
          "description": "Temperature for generation (higher = more creative)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file. Set to 0 for random seed.."
        },
        {
          "name": "cfg",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Chatterbox Text To Speech Multilingual",
      "description": "Whether you're working on memes, videos, games, or AI agents, Chatterbox brings your content to life. Use the first tts from resemble ai.\n    speech, synthesis, text-to-speech, tts\n\n    Use cases:\n    - Voice synthesis for applications\n    - Audiobook narration\n    - Virtual assistant voices\n    - Accessibility solutions\n    - Content localization",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.ChatterboxTextToSpeechMultilingual",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to be converted to speech (maximum 300 characters). Supports 23 languages including English, French, German, Spanish, Italian, Portuguese, Hindi, Arabic, Chinese, Japanese, Korean, and more."
        },
        {
          "name": "custom_audio_language",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "english",
              "arabic",
              "danish",
              "german",
              "greek",
              "spanish",
              "finnish",
              "french",
              "hebrew",
              "hindi",
              "italian",
              "japanese",
              "korean",
              "malay",
              "dutch",
              "norwegian",
              "polish",
              "portuguese",
              "russian",
              "swedish",
              "swahili",
              "turkish",
              "chinese"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.ChatterboxTextToSpeechMultilingual.CustomAudioLanguage"
          },
          "default": null,
          "title": "Custom Audio Language",
          "description": "If using a custom audio URL, specify the language of the audio here. Ignored if voice is not a custom url."
        },
        {
          "name": "exaggeration",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Exaggeration",
          "description": "Controls speech expressiveness and emotional intensity (0.25-2.0). 0.5 is neutral, higher values increase expressiveness. Extreme values may be unstable."
        },
        {
          "name": "voice",
          "type": {
            "type": "str"
          },
          "default": "english",
          "title": "Voice",
          "description": "Language code for synthesis. In case using custom please provide audio url and select custom_audio_language."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Temperature",
          "description": "Controls randomness and variation in generation (0.05-5.0). Higher values create more varied speech patterns."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible results. Set to 0 for random generation, or provide a specific number for consistent outputs."
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "Configuration/pace weight controlling generation guidance (0.0-1.0). Use 0.0 for language transfer to mitigate accent inheritance."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Dia Tts",
      "description": "Dia directly generates realistic dialogue from transcripts. Audio conditioning enables emotion control. Produces natural nonverbals like laughter and throat clearing.\n    speech, synthesis, text-to-speech, tts\n\n    Use cases:\n    - Voice synthesis for applications\n    - Audiobook narration\n    - Virtual assistant voices\n    - Accessibility solutions\n    - Content localization",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.DiaTts",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to be converted to speech."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Elevenlabs Tts Turbo V25",
      "description": "Generate high-speed text-to-speech audio using ElevenLabs TTS Turbo v2.5.\n    speech, synthesis, text-to-speech, tts, fast\n\n    Use cases:\n    - Voice synthesis for applications\n    - Audiobook narration\n    - Virtual assistant voices\n    - Accessibility solutions\n    - Content localization",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.ElevenlabsTtsTurboV25",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to convert to speech"
        },
        {
          "name": "next_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Next Text",
          "description": "The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation."
        },
        {
          "name": "speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Speed",
          "description": "Speech speed (0.7-1.2). Values below 1.0 slow down the speech, above 1.0 speed it up. Extreme values may affect quality."
        },
        {
          "name": "style",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Style",
          "description": "Style exaggeration (0-1)"
        },
        {
          "name": "stability",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Stability",
          "description": "Voice stability (0-1)"
        },
        {
          "name": "timestamps",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Timestamps",
          "description": "Whether to return timestamps for each word in the generated speech"
        },
        {
          "name": "similarity_boost",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Similarity Boost",
          "description": "Similarity boost (0-1)"
        },
        {
          "name": "voice",
          "type": {
            "type": "str"
          },
          "default": "Rachel",
          "title": "Voice",
          "description": "The voice to use for speech generation"
        },
        {
          "name": "language_code",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Language Code",
          "description": "Language code (ISO 639-1) used to enforce a language for the model. An error will be returned if language code is not supported by the model."
        },
        {
          "name": "apply_text_normalization",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "on",
              "off"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.ElevenlabsTtsTurboV25.ApplyTextNormalization"
          },
          "default": "auto",
          "title": "Apply Text Normalization",
          "description": "This parameter controls text normalization with three modes: 'auto', 'on', and 'off'. When set to 'auto', the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With 'on', text normalization will always be applied, while with 'off', it will be skipped."
        },
        {
          "name": "previous_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Previous Text",
          "description": "The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Index Tts 2 Text To Speech",
      "description": "Index TTS 2 generates natural-sounding speech from text with advanced neural synthesis.\n    speech, synthesis, text-to-speech, tts, neural\n\n    Use cases:\n    - Generate natural speech from text\n    - Create voice narration\n    - Produce audio books\n    - Generate voice-overs\n    - Create speech content",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.IndexTts2TextToSpeech",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The speech prompt to generate"
        },
        {
          "name": "emotional_strengths",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Emotional Strengths",
          "description": "The strengths of individual emotions for fine-grained control."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Strength",
          "description": "The strength of the emotional style transfer. Higher values result in stronger emotional influence."
        },
        {
          "name": "emotional_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Emotional Audio Url",
          "description": "The emotional reference audio file to extract the style from."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The audio file to generate the speech from."
        },
        {
          "name": "emotion_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Emotion Prompt",
          "description": "The emotional prompt to influence the emotional style. Must be used together with should_use_prompt_for_emotion."
        },
        {
          "name": "should_use_prompt_for_emotion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Should Use Prompt For Emotion",
          "description": "Whether to use the `prompt` to calculate emotional strengths, if enabled it will overwrite the `emotional_strengths` values. If `emotion_prompt` is provided, it will be used to instead of `prompt` to extract the emotional style."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Kling Video V1 Tts",
      "description": "Generate speech from text prompts and different voices using the Kling TTS model, which leverages advanced AI techniques to create high-quality text-to-speech.\n    speech, synthesis, text-to-speech, tts\n\n    Use cases:\n    - Voice synthesis for applications\n    - Audiobook narration\n    - Virtual assistant voices\n    - Accessibility solutions\n    - Content localization",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.KlingVideoV1Tts",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to be converted to speech"
        },
        {
          "name": "voice_id",
          "type": {
            "type": "enum",
            "values": [
              "genshin_vindi2",
              "zhinen_xuesheng",
              "AOT",
              "ai_shatang",
              "genshin_klee2",
              "genshin_kirara",
              "ai_kaiya",
              "oversea_male1",
              "ai_chenjiahao_712",
              "girlfriend_4_speech02",
              "chat1_female_new-3",
              "chat_0407_5-1",
              "cartoon-boy-07",
              "uk_boy1",
              "cartoon-girl-01",
              "PeppaPig_platform",
              "ai_huangzhong_712",
              "ai_huangyaoshi_712",
              "ai_laoguowang_712",
              "chengshu_jiejie",
              "you_pingjing",
              "calm_story1",
              "uk_man2",
              "laopopo_speech02",
              "heainainai_speech02",
              "reader_en_m-v1",
              "commercial_lady_en_f-v1",
              "tiyuxi_xuedi",
              "tiexin_nanyou",
              "girlfriend_1_speech02",
              "girlfriend_2_speech02",
              "zhuxi_speech02",
              "uk_oldman3",
              "dongbeilaotie_speech02",
              "chongqingxiaohuo_speech02",
              "chuanmeizi_speech02",
              "chaoshandashu_speech02",
              "ai_taiwan_man2_speech02",
              "xianzhanggui_speech02",
              "tianjinjiejie_speech02",
              "diyinnansang_DB_CN_M_04-v2",
              "yizhipiannan-v1",
              "guanxiaofang-v2",
              "tianmeixuemei-v1",
              "daopianyansang-v1",
              "mengwa-v1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.KlingVideoV1Tts.VoiceId"
          },
          "default": "genshin_vindi2",
          "title": "Voice Id",
          "description": "The voice ID to use for speech synthesis"
        },
        {
          "name": "voice_speed",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Voice Speed",
          "description": "Rate of speech"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Maya",
      "description": "Maya generates high-quality natural speech from text with advanced voice synthesis capabilities.\n    audio, tts, maya, high-quality, text-to-speech\n\n    Use cases:\n    - Generate high-quality speech from text\n    - Create professional voice-overs\n    - Produce premium audio narration\n    - Generate natural-sounding speech\n    - Create professional audio content",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.Maya",
      "properties": [
        {
          "name": "repetition_penalty",
          "type": {
            "type": "float"
          },
          "default": 1.1,
          "title": "Repetition Penalty",
          "description": "Penalty for repeating tokens. Higher values reduce repetition artifacts."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Description of the voice/character. Includes attributes like age, accent, pitch, timbre, pacing, tone, and intensity. See examples for format."
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Top P",
          "description": "Nucleus sampling parameter. Controls diversity of token selection."
        },
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to synthesize into speech. You can embed emotion tags anywhere in the text using the format <emotion_name>. Available emotions: laugh, laugh_harder, sigh, chuckle, gasp, angry, excited, whisper, cry, scream, sing, snort, exhale, gulp, giggle, sarcastic, curious. Example: 'Hello world! <excited> This is amazing!' or 'I can't believe this <sigh> happened again.'"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "wav",
              "mp3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.Maya.OutputFormat"
          },
          "default": "wav",
          "title": "Output Format",
          "description": "Output audio format for the generated speech"
        },
        {
          "name": "max_tokens",
          "type": {
            "type": "int"
          },
          "default": 2000,
          "title": "Max Tokens",
          "description": "Maximum number of SNAC tokens to generate (7 tokens per frame). Controls maximum audio length."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.4,
          "title": "Temperature",
          "description": "Sampling temperature. Lower values (0.2-0.5) produce more stable/consistent audio. Higher values add variation."
        },
        {
          "name": "sample_rate",
          "type": {
            "type": "enum",
            "values": [
              "48 kHz",
              "24 kHz"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.Maya.SampleRate"
          },
          "default": "48 kHz",
          "title": "Sample Rate",
          "description": "Output audio sample rate. 48 kHz provides higher quality audio, 24 kHz is faster."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Maya Batch",
      "description": "Maya Batch TTS generates high-quality speech in batch mode for efficient processing.\n    speech, synthesis, text-to-speech, tts, batch, maya\n\n    Use cases:\n    - Generate speech for multiple texts\n    - Batch process narration\n    - Create bulk voice-overs\n    - Efficient audio content creation\n    - Generate multiple speech files",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.MayaBatch",
      "properties": [
        {
          "name": "repetition_penalty",
          "type": {
            "type": "float"
          },
          "default": 1.1,
          "title": "Repetition Penalty",
          "description": "Repetition penalty for all generations."
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Top P",
          "description": "Nucleus sampling parameter for all generations."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "wav",
              "mp3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MayaBatch.OutputFormat"
          },
          "default": "wav",
          "title": "Output Format",
          "description": "Output audio format for all generated speech files"
        },
        {
          "name": "texts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Texts",
          "description": "List of texts to synthesize into speech. You can embed emotion tags in each text using the format <emotion_name>."
        },
        {
          "name": "prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Prompts",
          "description": "List of voice descriptions for each text. Must match the length of texts list. Each describes the voice/character attributes."
        },
        {
          "name": "max_tokens",
          "type": {
            "type": "int"
          },
          "default": 2000,
          "title": "Max Tokens",
          "description": "Maximum SNAC tokens per generation."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.4,
          "title": "Temperature",
          "description": "Sampling temperature for all generations."
        },
        {
          "name": "sample_rate",
          "type": {
            "type": "enum",
            "values": [
              "48 kHz",
              "24 kHz"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MayaBatch.SampleRate"
          },
          "default": "48 kHz",
          "title": "Sample Rate",
          "description": "Output audio sample rate for all generations. 48 kHz provides higher quality, 24 kHz is faster."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Maya Stream",
      "description": "Maya Stream TTS generates high-quality speech in streaming mode for real-time applications.\n    speech, synthesis, text-to-speech, tts, streaming, maya\n\n    Use cases:\n    - Generate speech in real-time\n    - Stream narration dynamically\n    - Create live voice-overs\n    - Real-time audio synthesis\n    - Generate streaming speech",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.MayaStream",
      "properties": [
        {
          "name": "repetition_penalty",
          "type": {
            "type": "float"
          },
          "default": 1.1,
          "title": "Repetition Penalty",
          "description": "Penalty for repeating tokens. Higher values reduce repetition artifacts."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Description of the voice/character. Includes attributes like age, accent, pitch, timbre, pacing, tone, and intensity. See examples for format."
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Top P",
          "description": "Nucleus sampling parameter. Controls diversity of token selection."
        },
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to synthesize into speech. You can embed emotion tags anywhere in the text using the format <emotion_name>. Available emotions: laugh, laugh_harder, sigh, chuckle, gasp, angry, excited, whisper, cry, scream, sing, snort, exhale, gulp, giggle, sarcastic, curious. Example: 'Hello world! <excited> This is amazing!' or 'I can't believe this <sigh> happened again.'"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "mp3",
              "wav",
              "pcm"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MayaStream.OutputFormat"
          },
          "default": "mp3",
          "title": "Output Format",
          "description": "Output audio format. 'mp3' for browser-playable audio, 'wav' for uncompressed audio, 'pcm' for raw PCM (lowest latency, requires client-side decoding)."
        },
        {
          "name": "max_tokens",
          "type": {
            "type": "int"
          },
          "default": 2000,
          "title": "Max Tokens",
          "description": "Maximum number of SNAC tokens to generate (7 tokens per frame). Controls maximum audio length."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.4,
          "title": "Temperature",
          "description": "Sampling temperature. Lower values (0.2-0.5) produce more stable/consistent audio. Higher values add variation."
        },
        {
          "name": "sample_rate",
          "type": {
            "type": "enum",
            "values": [
              "48 kHz",
              "24 kHz"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MayaStream.SampleRate"
          },
          "default": "24 kHz",
          "title": "Sample Rate",
          "description": "Output audio sample rate. 48 kHz uses upsampling for higher quality audio, 24 kHz is native SNAC output (faster, lower latency)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Minimax Preview Speech 25 Hd",
      "description": "Generate speech from text prompts and different voices using the MiniMax Speech-02 HD model, which leverages advanced AI techniques to create high-quality text-to-speech.\n    speech, synthesis, text-to-speech, tts\n\n    Use cases:\n    - Voice synthesis for applications\n    - Audiobook narration\n    - Virtual assistant voices\n    - Accessibility solutions\n    - Content localization",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.MinimaxPreviewSpeech25Hd",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "Text to convert to speech (max 5000 characters, minimum 1 non-whitespace character)"
        },
        {
          "name": "language_boost",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "Persian",
              "Filipino",
              "Tamil",
              "Chinese",
              "Chinese,Yue",
              "English",
              "Arabic",
              "Russian",
              "Spanish",
              "French",
              "Portuguese",
              "German",
              "Turkish",
              "Dutch",
              "Ukrainian",
              "Vietnamese",
              "Indonesian",
              "Japanese",
              "Italian",
              "Korean",
              "Thai",
              "Polish",
              "Romanian",
              "Greek",
              "Czech",
              "Finnish",
              "Hindi",
              "Bulgarian",
              "Danish",
              "Hebrew",
              "Malay",
              "Slovak",
              "Swedish",
              "Croatian",
              "Hungarian",
              "Norwegian",
              "Slovenian",
              "Catalan",
              "Nynorsk",
              "Afrikaans",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MinimaxPreviewSpeech25Hd.LanguageBoost"
          },
          "default": null,
          "title": "Language Boost",
          "description": "Enhance recognition of specified languages and dialects"
        },
        {
          "name": "voice_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Voice Setting",
          "description": "Voice configuration settings"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "url",
              "hex"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MinimaxPreviewSpeech25Hd.OutputFormat"
          },
          "default": "hex",
          "title": "Output Format",
          "description": "Format of the output content (non-streaming only)"
        },
        {
          "name": "pronunciation_dict",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Pronunciation Dict",
          "description": "Custom pronunciation dictionary for text replacement"
        },
        {
          "name": "audio_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Audio Setting",
          "description": "Audio configuration settings"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Minimax Preview Speech 25 Turbo",
      "description": "Generate fast speech from text prompts and different voices using the MiniMax Speech-02 Turbo model, which leverages advanced AI techniques to create high-quality text-to-speech.\n    speech, synthesis, text-to-speech, tts, fast\n\n    Use cases:\n    - Voice synthesis for applications\n    - Audiobook narration\n    - Virtual assistant voices\n    - Accessibility solutions\n    - Content localization",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.MinimaxPreviewSpeech25Turbo",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "Text to convert to speech (max 5000 characters, minimum 1 non-whitespace character)"
        },
        {
          "name": "language_boost",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "Persian",
              "Filipino",
              "Tamil",
              "Chinese",
              "Chinese,Yue",
              "English",
              "Arabic",
              "Russian",
              "Spanish",
              "French",
              "Portuguese",
              "German",
              "Turkish",
              "Dutch",
              "Ukrainian",
              "Vietnamese",
              "Indonesian",
              "Japanese",
              "Italian",
              "Korean",
              "Thai",
              "Polish",
              "Romanian",
              "Greek",
              "Czech",
              "Finnish",
              "Hindi",
              "Bulgarian",
              "Danish",
              "Hebrew",
              "Malay",
              "Slovak",
              "Swedish",
              "Croatian",
              "Hungarian",
              "Norwegian",
              "Slovenian",
              "Catalan",
              "Nynorsk",
              "Afrikaans",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MinimaxPreviewSpeech25Turbo.LanguageBoost"
          },
          "default": null,
          "title": "Language Boost",
          "description": "Enhance recognition of specified languages and dialects"
        },
        {
          "name": "voice_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Voice Setting",
          "description": "Voice configuration settings"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "url",
              "hex"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MinimaxPreviewSpeech25Turbo.OutputFormat"
          },
          "default": "hex",
          "title": "Output Format",
          "description": "Format of the output content (non-streaming only)"
        },
        {
          "name": "pronunciation_dict",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Pronunciation Dict",
          "description": "Custom pronunciation dictionary for text replacement"
        },
        {
          "name": "audio_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Audio Setting",
          "description": "Audio configuration settings"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Minimax Speech 02 Hd",
      "description": "Generate speech from text prompts and different voices using the MiniMax Speech-02 HD model, which leverages advanced AI techniques to create high-quality text-to-speech.\n    speech, synthesis, text-to-speech, tts\n\n    Use cases:\n    - Voice synthesis for applications\n    - Audiobook narration\n    - Virtual assistant voices\n    - Accessibility solutions\n    - Content localization",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.MinimaxSpeech02Hd",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "Text to convert to speech (max 5000 characters, minimum 1 non-whitespace character)"
        },
        {
          "name": "language_boost",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "Chinese",
              "Chinese,Yue",
              "English",
              "Arabic",
              "Russian",
              "Spanish",
              "French",
              "Portuguese",
              "German",
              "Turkish",
              "Dutch",
              "Ukrainian",
              "Vietnamese",
              "Indonesian",
              "Japanese",
              "Italian",
              "Korean",
              "Thai",
              "Polish",
              "Romanian",
              "Greek",
              "Czech",
              "Finnish",
              "Hindi",
              "Bulgarian",
              "Danish",
              "Hebrew",
              "Malay",
              "Slovak",
              "Swedish",
              "Croatian",
              "Hungarian",
              "Norwegian",
              "Slovenian",
              "Catalan",
              "Nynorsk",
              "Afrikaans",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MinimaxSpeech02Hd.LanguageBoost"
          },
          "default": null,
          "title": "Language Boost",
          "description": "Enhance recognition of specified languages and dialects"
        },
        {
          "name": "voice_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Voice Setting",
          "description": "Voice configuration settings"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "url",
              "hex"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MinimaxSpeech02Hd.OutputFormat"
          },
          "default": "hex",
          "title": "Output Format",
          "description": "Format of the output content (non-streaming only)"
        },
        {
          "name": "pronunciation_dict",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Pronunciation Dict",
          "description": "Custom pronunciation dictionary for text replacement"
        },
        {
          "name": "audio_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Audio Setting",
          "description": "Audio configuration settings"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Minimax Speech 02 Turbo",
      "description": "Generate fast speech from text prompts and different voices using the MiniMax Speech-02 Turbo model, which leverages advanced AI techniques to create high-quality text-to-speech.\n    speech, synthesis, text-to-speech, tts, fast\n\n    Use cases:\n    - Voice synthesis for applications\n    - Audiobook narration\n    - Virtual assistant voices\n    - Accessibility solutions\n    - Content localization",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.MinimaxSpeech02Turbo",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "Text to convert to speech (max 5000 characters, minimum 1 non-whitespace character)"
        },
        {
          "name": "language_boost",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "Chinese",
              "Chinese,Yue",
              "English",
              "Arabic",
              "Russian",
              "Spanish",
              "French",
              "Portuguese",
              "German",
              "Turkish",
              "Dutch",
              "Ukrainian",
              "Vietnamese",
              "Indonesian",
              "Japanese",
              "Italian",
              "Korean",
              "Thai",
              "Polish",
              "Romanian",
              "Greek",
              "Czech",
              "Finnish",
              "Hindi",
              "Bulgarian",
              "Danish",
              "Hebrew",
              "Malay",
              "Slovak",
              "Swedish",
              "Croatian",
              "Hungarian",
              "Norwegian",
              "Slovenian",
              "Catalan",
              "Nynorsk",
              "Afrikaans",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MinimaxSpeech02Turbo.LanguageBoost"
          },
          "default": null,
          "title": "Language Boost",
          "description": "Enhance recognition of specified languages and dialects"
        },
        {
          "name": "voice_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Voice Setting",
          "description": "Voice configuration settings"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "url",
              "hex"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MinimaxSpeech02Turbo.OutputFormat"
          },
          "default": "hex",
          "title": "Output Format",
          "description": "Format of the output content (non-streaming only)"
        },
        {
          "name": "pronunciation_dict",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Pronunciation Dict",
          "description": "Custom pronunciation dictionary for text replacement"
        },
        {
          "name": "audio_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Audio Setting",
          "description": "Audio configuration settings"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Minimax Speech 26 Hd",
      "description": "Minimax Speech 2.6 HD generates high-definition speech from text with superior audio quality.\n    audio, tts, minimax, 2.6, hd, high-quality\n\n    Use cases:\n    - Generate HD quality speech from text\n    - Create premium voice-overs\n    - Produce high-fidelity audio narration\n    - Generate superior audio quality speech\n    - Create broadcast-quality audio",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.MinimaxSpeech26Hd",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text to convert to speech. Paragraph breaks should be marked with newline characters. **NOTE**: You can customize speech pauses by adding markers in the form `<#x#>`, where `x` is the pause duration in seconds. Valid range: `[0.01, 99.99]`, up to two decimal places. Pause markers must be placed between speakable text segments and cannot be used consecutively."
        },
        {
          "name": "language_boost",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "Chinese",
              "Chinese,Yue",
              "English",
              "Arabic",
              "Russian",
              "Spanish",
              "French",
              "Portuguese",
              "German",
              "Turkish",
              "Dutch",
              "Ukrainian",
              "Vietnamese",
              "Indonesian",
              "Japanese",
              "Italian",
              "Korean",
              "Thai",
              "Polish",
              "Romanian",
              "Greek",
              "Czech",
              "Finnish",
              "Hindi",
              "Bulgarian",
              "Danish",
              "Hebrew",
              "Malay",
              "Slovak",
              "Swedish",
              "Croatian",
              "Hungarian",
              "Norwegian",
              "Slovenian",
              "Catalan",
              "Nynorsk",
              "Afrikaans",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MinimaxSpeech26Hd.LanguageBoost"
          },
          "default": null,
          "title": "Language Boost",
          "description": "Enhance recognition of specified languages and dialects"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "url",
              "hex"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MinimaxSpeech26Hd.MinimaxSpeech26HdOutputFormat"
          },
          "default": "hex",
          "title": "Output Format",
          "description": "Format of the output content (non-streaming only)"
        },
        {
          "name": "pronunciation_dict",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Pronunciation Dict",
          "description": "Custom pronunciation dictionary for text replacement"
        },
        {
          "name": "voice_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Voice Setting",
          "description": "Voice configuration settings"
        },
        {
          "name": "normalization_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Normalization Setting",
          "description": "Loudness normalization settings for the audio"
        },
        {
          "name": "audio_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Audio Setting",
          "description": "Audio configuration settings"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Minimax Speech 26 Turbo",
      "description": "Minimax Speech 2.6 Turbo generates speech from text with optimized speed and good quality.\n    audio, tts, minimax, 2.6, turbo, fast\n\n    Use cases:\n    - Generate speech quickly from text\n    - Create fast voice-overs\n    - Produce rapid audio narration\n    - Generate speech with turbo speed\n    - Create efficient audio content",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.MinimaxSpeech26Turbo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text to convert to speech. Paragraph breaks should be marked with newline characters. **NOTE**: You can customize speech pauses by adding markers in the form `<#x#>`, where `x` is the pause duration in seconds. Valid range: `[0.01, 99.99]`, up to two decimal places. Pause markers must be placed between speakable text segments and cannot be used consecutively."
        },
        {
          "name": "language_boost",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "Chinese",
              "Chinese,Yue",
              "English",
              "Arabic",
              "Russian",
              "Spanish",
              "French",
              "Portuguese",
              "German",
              "Turkish",
              "Dutch",
              "Ukrainian",
              "Vietnamese",
              "Indonesian",
              "Japanese",
              "Italian",
              "Korean",
              "Thai",
              "Polish",
              "Romanian",
              "Greek",
              "Czech",
              "Finnish",
              "Hindi",
              "Bulgarian",
              "Danish",
              "Hebrew",
              "Malay",
              "Slovak",
              "Swedish",
              "Croatian",
              "Hungarian",
              "Norwegian",
              "Slovenian",
              "Catalan",
              "Nynorsk",
              "Afrikaans",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MinimaxSpeech26Turbo.LanguageBoost"
          },
          "default": null,
          "title": "Language Boost",
          "description": "Enhance recognition of specified languages and dialects"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "url",
              "hex"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MinimaxSpeech26Turbo.MinimaxSpeech26TurboOutputFormat"
          },
          "default": "hex",
          "title": "Output Format",
          "description": "Format of the output content (non-streaming only)"
        },
        {
          "name": "pronunciation_dict",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Pronunciation Dict",
          "description": "Custom pronunciation dictionary for text replacement"
        },
        {
          "name": "voice_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Voice Setting",
          "description": "Voice configuration settings"
        },
        {
          "name": "normalization_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Normalization Setting",
          "description": "Loudness normalization settings for the audio"
        },
        {
          "name": "audio_setting",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Audio Setting",
          "description": "Audio configuration settings"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Minimax Voice Clone",
      "description": "Clone a voice from a sample audio and generate speech from text prompts using the MiniMax model, which leverages advanced AI techniques to create high-quality text-to-speech.\n    speech, synthesis, text-to-speech, tts\n\n    Use cases:\n    - Voice synthesis for applications\n    - Audiobook narration\n    - Virtual assistant voices\n    - Accessibility solutions\n    - Content localization",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.MinimaxVoiceClone",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "Hello, this is a preview of your cloned voice! I hope you like it!",
          "title": "Text",
          "description": "Text to generate a TTS preview with the cloned voice (optional)"
        },
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "speech-02-hd",
              "speech-02-turbo",
              "speech-01-hd",
              "speech-01-turbo"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.MinimaxVoiceClone.Model"
          },
          "default": "speech-02-hd",
          "title": "Model",
          "description": "TTS model to use for preview. Options: speech-02-hd, speech-02-turbo, speech-01-hd, speech-01-turbo"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the input audio file for voice cloning. Should be at least 10 seconds long. To retain the voice permanently, use it with a TTS (text-to-speech) endpoint at least once within 7 days. Otherwise, it will be automatically deleted."
        },
        {
          "name": "accuracy",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Accuracy",
          "description": "Text validation accuracy threshold (0-1)"
        },
        {
          "name": "noise_reduction",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Noise Reduction",
          "description": "Enable noise reduction for the cloned voice"
        },
        {
          "name": "need_volume_normalization",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Need Volume Normalization",
          "description": "Enable volume normalization for the cloned voice"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Minimax Voice Design",
      "description": "Design a personalized voice from a text description, and generate speech from text prompts using the MiniMax model, which leverages advanced AI techniques to create high-quality text-to-speech.\n    speech, synthesis, text-to-speech, tts\n\n    Use cases:\n    - Voice synthesis for applications\n    - Audiobook narration\n    - Virtual assistant voices\n    - Accessibility solutions\n    - Content localization",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.MinimaxVoiceDesign",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Voice description prompt for generating a personalized voice"
        },
        {
          "name": "preview_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Preview Text",
          "description": "Text for audio preview. Limited to 500 characters. A fee of $30 per 1M characters will be charged for the generation of the preview audio."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Orpheus Tts",
      "description": "Orpheus TTS is a state-of-the-art, Llama-based Speech-LLM designed for high-quality, empathetic text-to-speech generation. This model has been finetuned to deliver human-level speech synthesis, achieving exceptional clarity, expressiveness, and real-time performances.\n    speech, synthesis, text-to-speech, tts\n\n    Use cases:\n    - Voice synthesis for applications\n    - Audiobook narration\n    - Virtual assistant voices\n    - Accessibility solutions\n    - Content localization",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.OrpheusTts",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to be converted to speech. You can additionally add the following emotive tags: <laugh>, <chuckle>, <sigh>, <cough>, <sniffle>, <groan>, <yawn>, <gasp>"
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "tara",
              "leah",
              "jess",
              "leo",
              "dan",
              "mia",
              "zac",
              "zoe"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.OrpheusTts.Voice"
          },
          "default": "tara",
          "title": "Voice",
          "description": "Voice ID for the desired voice."
        },
        {
          "name": "repetition_penalty",
          "type": {
            "type": "float"
          },
          "default": 1.2,
          "title": "Repetition Penalty",
          "description": "Repetition penalty (>= 1.1 required for stable generations)."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.7,
          "title": "Temperature",
          "description": "Temperature for generation (higher = more creative)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Qwen 3 Tts Text To Speech 06B",
      "description": "Qwen-3 TTS 0.6B generates speech from text efficiently using the compact 600-million parameter model.\n    audio, tts, qwen, 0.6b, efficient, text-to-speech\n\n    Use cases:\n    - Generate speech efficiently from text\n    - Create fast voice-overs\n    - Produce quick audio narration\n    - Generate spoken content with low latency\n    - Create efficient text-to-speech",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.Qwen3TtsTextToSpeech06B",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional prompt to guide the style of the generated speech. This prompt will be ignored if a speaker embedding is provided."
        },
        {
          "name": "speaker_voice_embedding_file_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Speaker Voice Embedding File Url",
          "description": "URL to a speaker embedding file in safetensors format, from `fal-ai/qwen-3-tts/clone-voice/0.6b` endpoint. If provided, the TTS model will use the cloned voice for synthesis instead of the predefined voices."
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Top P",
          "description": "Top-p sampling parameter."
        },
        {
          "name": "repetition_penalty",
          "type": {
            "type": "float"
          },
          "default": 1.05,
          "title": "Repetition Penalty",
          "description": "Penalty to reduce repeated tokens/codes."
        },
        {
          "name": "subtalker_temperature",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Subtalker Temperature",
          "description": "Temperature for sub-talker sampling."
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Top K",
          "description": "Top-k sampling parameter."
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "Vivian",
              "Serena",
              "Uncle_Fu",
              "Dylan",
              "Eric",
              "Ryan",
              "Aiden",
              "Ono_Anna",
              "Sohee"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.Qwen3TtsTextToSpeech06B.Voice"
          },
          "default": null,
          "title": "Voice",
          "description": "The voice to be used for speech synthesis, will be ignored if a speaker embedding is provided. Check out the **[documentation](https://github.com/QwenLM/Qwen3-TTS/tree/main?tab=readme-ov-file#custom-voice-generate)** for each voice's details and which language they primarily support."
        },
        {
          "name": "reference_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Reference Text",
          "description": "Optional reference text that was used when creating the speaker embedding. Providing this can improve synthesis quality when using a cloned voice."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Temperature",
          "description": "Sampling temperature; higher => more random."
        },
        {
          "name": "language",
          "type": {
            "type": "enum",
            "values": [
              "Auto",
              "English",
              "Chinese",
              "Spanish",
              "French",
              "German",
              "Italian",
              "Japanese",
              "Korean",
              "Portuguese",
              "Russian"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.Qwen3TtsTextToSpeech06B.Language"
          },
          "default": "Auto",
          "title": "Language",
          "description": "The language of the voice."
        },
        {
          "name": "subtalker_top_k",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Subtalker Top K",
          "description": "Top-k for sub-talker sampling."
        },
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to be converted to speech."
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 200,
          "title": "Max New Tokens",
          "description": "Maximum number of new codec tokens to generate."
        },
        {
          "name": "subtalker_dosample",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Subtalker Dosample",
          "description": "Sampling switch for the sub-talker."
        },
        {
          "name": "subtalker_top_p",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Subtalker Top P",
          "description": "Top-p for sub-talker sampling."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Qwen 3 Tts Text To Speech 17B",
      "description": "Qwen-3 TTS 1.7B generates natural-sounding speech from text using the large 1.7-billion parameter model.\n    audio, tts, qwen, 1.7b, text-to-speech, speech-synthesis\n\n    Use cases:\n    - Generate natural-sounding speech from text\n    - Create voice-overs for videos\n    - Produce audiobook narration\n    - Generate spoken content for applications\n    - Create text-to-speech for accessibility",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.Qwen3TtsTextToSpeech17B",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional prompt to guide the style of the generated speech. This prompt will be ignored if a speaker embedding is provided."
        },
        {
          "name": "speaker_voice_embedding_file_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Speaker Voice Embedding File Url",
          "description": "URL to a speaker embedding file in safetensors format, from `fal-ai/qwen-3-tts/clone-voice` endpoint. If provided, the TTS model will use the cloned voice for synthesis instead of the predefined voices."
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Top P",
          "description": "Top-p sampling parameter."
        },
        {
          "name": "repetition_penalty",
          "type": {
            "type": "float"
          },
          "default": 1.05,
          "title": "Repetition Penalty",
          "description": "Penalty to reduce repeated tokens/codes."
        },
        {
          "name": "subtalker_temperature",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Subtalker Temperature",
          "description": "Temperature for sub-talker sampling."
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Top K",
          "description": "Top-k sampling parameter."
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "Vivian",
              "Serena",
              "Uncle_Fu",
              "Dylan",
              "Eric",
              "Ryan",
              "Aiden",
              "Ono_Anna",
              "Sohee"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.Qwen3TtsTextToSpeech17B.Voice"
          },
          "default": null,
          "title": "Voice",
          "description": "The voice to be used for speech synthesis, will be ignored if a speaker embedding is provided. Check out the **[documentation](https://github.com/QwenLM/Qwen3-TTS/tree/main?tab=readme-ov-file#custom-voice-generate)** for each voice's details and which language they primarily support."
        },
        {
          "name": "reference_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Reference Text",
          "description": "Optional reference text that was used when creating the speaker embedding. Providing this can improve synthesis quality when using a cloned voice."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Temperature",
          "description": "Sampling temperature; higher => more random."
        },
        {
          "name": "language",
          "type": {
            "type": "enum",
            "values": [
              "Auto",
              "English",
              "Chinese",
              "Spanish",
              "French",
              "German",
              "Italian",
              "Japanese",
              "Korean",
              "Portuguese",
              "Russian"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.Qwen3TtsTextToSpeech17B.Language"
          },
          "default": "Auto",
          "title": "Language",
          "description": "The language of the voice."
        },
        {
          "name": "subtalker_top_k",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Subtalker Top K",
          "description": "Top-k for sub-talker sampling."
        },
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to be converted to speech."
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 200,
          "title": "Max New Tokens",
          "description": "Maximum number of new codec tokens to generate."
        },
        {
          "name": "subtalker_dosample",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Subtalker Dosample",
          "description": "Sampling switch for the sub-talker."
        },
        {
          "name": "subtalker_top_p",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Subtalker Top P",
          "description": "Top-p for sub-talker sampling."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Qwen 3 Tts Voice Design 17B",
      "description": "Qwen-3 TTS Voice Design 1.7B creates custom voice characteristics for personalized speech synthesis.\n    audio, tts, qwen, voice-design, custom, 1.7b\n\n    Use cases:\n    - Design custom voice characteristics\n    - Create personalized speech synthesis\n    - Generate unique voice styles\n    - Produce custom voice-overs\n    - Create tailored speech synthesis",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.Qwen3TtsVoiceDesign17B",
      "properties": [
        {
          "name": "repetition_penalty",
          "type": {
            "type": "float"
          },
          "default": 1.05,
          "title": "Repetition Penalty",
          "description": "Penalty to reduce repeated tokens/codes."
        },
        {
          "name": "subtalker_top_k",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Subtalker Top K",
          "description": "Top-k for sub-talker sampling."
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Top P",
          "description": "Top-p sampling parameter."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional prompt to guide the style of the generated speech."
        },
        {
          "name": "max_new_tokens",
          "type": {
            "type": "int"
          },
          "default": 200,
          "title": "Max New Tokens",
          "description": "Maximum number of new codec tokens to generate."
        },
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to be converted to speech."
        },
        {
          "name": "language",
          "type": {
            "type": "enum",
            "values": [
              "Auto",
              "English",
              "Chinese",
              "Spanish",
              "French",
              "German",
              "Italian",
              "Japanese",
              "Korean",
              "Portuguese",
              "Russian"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.Qwen3TtsVoiceDesign17B.Language"
          },
          "default": "Auto",
          "title": "Language",
          "description": "The language of the voice to be designed."
        },
        {
          "name": "top_k",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Top K",
          "description": "Top-k sampling parameter."
        },
        {
          "name": "subtalker_dosample",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Subtalker Dosample",
          "description": "Sampling switch for the sub-talker."
        },
        {
          "name": "subtalker_temperature",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Subtalker Temperature",
          "description": "Temperature for sub-talker sampling."
        },
        {
          "name": "subtalker_top_p",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Subtalker Top P",
          "description": "Top-p for sub-talker sampling."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Temperature",
          "description": "Sampling temperature; higher => more random."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Resemble Ai Chatterboxhd Text To Speech",
      "description": "Generate expressive, natural speech with Resemble AI's Chatterbox. Features unique emotion control, instant voice cloning from short audio, and built-in watermarking.\n    speech, synthesis, text-to-speech, tts\n\n    Use cases:\n    - Voice synthesis for applications\n    - Audiobook narration\n    - Virtual assistant voices\n    - Accessibility solutions\n    - Content localization",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.ResembleAiChatterboxhdTextToSpeech",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "My name is Maximus Decimus Meridius, commander of the Armies of the North, General of the Felix Legions and loyal servant to the true emperor, Marcus Aurelius. Father to a murdered son, husband to a murdered wife. And I will have my vengeance, in this life or the next.",
          "title": "Text",
          "description": "Text to synthesize into speech."
        },
        {
          "name": "exaggeration",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Exaggeration",
          "description": "Controls emotion exaggeration. Range typically 0.25 to 2.0."
        },
        {
          "name": "high_quality_audio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "High Quality Audio",
          "description": "If True, the generated audio will be upscaled to 48kHz. The generation of the audio will take longer, but the quality will be higher. If False, the generated audio will be 24kHz."
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "Aurora",
              "Blade",
              "Britney",
              "Carl",
              "Cliff",
              "Richard",
              "Rico",
              "Siobhan",
              "Vicky"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.ResembleAiChatterboxhdTextToSpeech.Voice"
          },
          "default": null,
          "title": "Voice",
          "description": "The voice to use for the TTS request. If neither voice nor audio are provided, a random voice will be used."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL to the audio sample to use as a voice prompt for zero-shot TTS voice cloning. Providing a audio sample will override the voice setting. If neither voice nor audio_url are provided, a random voice will be used."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Temperature",
          "description": "Controls the randomness of generation. Range typically 0.05 to 5."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed",
          "description": "Useful to control the reproducibility of the generated audio. Assuming all other properties didn't change, a fixed seed should always generate the exact same audio file. Set to 0 for random seed."
        },
        {
          "name": "cfg",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg",
          "description": "Classifier-free guidance scale (CFG) controls the conditioning factor. Range typically 0.2 to 1.0. For expressive or dramatic speech, try lower cfg values (e.g. ~0.3) and increase exaggeration to around 0.7 or higher. If the reference speaker has a fast speaking style, lowering cfg to around 0.3 can improve pacing."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Vibevoice",
      "description": "Generate long, expressive multi-voice speech using Microsoft's powerful TTS\n    speech, synthesis, text-to-speech, tts\n\n    Use cases:\n    - Voice synthesis for applications\n    - Audiobook narration\n    - Virtual assistant voices\n    - Accessibility solutions\n    - Content localization",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.Vibevoice",
      "properties": [
        {
          "name": "script",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Script",
          "description": "The script to convert to speech. Can be formatted with 'Speaker X:' prefixes for multi-speaker dialogues."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation."
        },
        {
          "name": "speakers",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Speakers",
          "description": "List of speakers to use for the script. If not provided, will be inferred from the script or voice samples."
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 1.3,
          "title": "Cfg Scale",
          "description": "CFG (Classifier-Free Guidance) scale for generation. Higher values increase adherence to text."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Vibevoice 05B",
      "description": "VibeVoice 0.5B generates expressive and emotive speech from text with natural vocal characteristics.\n    audio, tts, vibevoice, 0.5b, expressive, text-to-speech\n\n    Use cases:\n    - Generate expressive speech from text\n    - Create emotive voice-overs\n    - Produce natural vocal narration\n    - Generate speech with personality\n    - Create engaging audio content",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.Vibevoice05B",
      "properties": [
        {
          "name": "script",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Script",
          "description": "The script to convert to speech."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation."
        },
        {
          "name": "speaker",
          "type": {
            "type": "enum",
            "values": [
              "Frank",
              "Wayne",
              "Carter",
              "Emma",
              "Grace",
              "Mike"
            ],
            "type_name": "nodetool.nodes.fal.text_to_speech.Vibevoice05B.Speaker"
          },
          "default": "",
          "title": "Speaker",
          "description": "Voice to use for speaking."
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 1.3,
          "title": "Cfg Scale",
          "description": "CFG (Classifier-Free Guidance) scale for generation. Higher values increase adherence to text."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Vibevoice 7 b",
      "description": "Generate long, expressive multi-voice speech using Microsoft's powerful TTS\n    speech, synthesis, text-to-speech, tts\n\n    Use cases:\n    - Voice synthesis for applications\n    - Audiobook narration\n    - Virtual assistant voices\n    - Accessibility solutions\n    - Content localization",
      "namespace": "fal.text_to_speech",
      "node_type": "fal.text_to_speech.Vibevoice7b",
      "properties": [
        {
          "name": "script",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Script",
          "description": "The script to convert to speech. Can be formatted with 'Speaker X:' prefixes for multi-speaker dialogues."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation."
        },
        {
          "name": "speakers",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Speakers",
          "description": "List of speakers to use for the script. If not provided, will be inferred from the script or voice samples."
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 1.3,
          "title": "Cfg Scale",
          "description": "CFG (Classifier-Free Guidance) scale for generation. Higher values increase adherence to text."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Open Router",
      "description": "OpenRouter provides unified access to any LLM (Large Language Model) through a single API.\n    llm, chat, openrouter, multimodel, language-model\n\n    Use cases:\n    - Run any LLM through unified interface\n    - Switch between models seamlessly\n    - Access multiple LLM providers\n    - Flexible model selection\n    - Unified LLM API access",
      "namespace": "fal.llm",
      "node_type": "fal.llm.OpenRouter",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model",
          "description": "Name of the model to use. Charged based on actual token usage."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to be used for the chat completion"
        },
        {
          "name": "max_tokens",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Max Tokens",
          "description": "This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Temperature",
          "description": "This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input."
        },
        {
          "name": "reasoning",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reasoning",
          "description": "Should reasoning be the part of the final answer."
        },
        {
          "name": "system_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "System Prompt",
          "description": "System prompt to provide context or instructions to the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "model"
      ]
    },
    {
      "title": "Open Router Chat Completions",
      "description": "OpenRouter Chat Completions provides OpenAI-compatible interface for any LLM.\n    llm, chat, openai-compatible, openrouter, chat-completions\n\n    Use cases:\n    - OpenAI-compatible LLM access\n    - Drop-in replacement for OpenAI API\n    - Multi-model chat completions\n    - Standardized chat interface\n    - Universal LLM chat API",
      "namespace": "fal.llm",
      "node_type": "fal.llm.OpenRouterChatCompletions",
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "messages",
        "model"
      ]
    },
    {
      "title": "Openrouter Router Openai V1 Embeddings",
      "description": "The OpenRouter Embeddings API with fal, powered by OpenRouter, provides unified access to a wide range of large language models - including GPT, Claude, Gemini, and many others through a single API interface.\n    llm, language-model, text-generation, ai\n\n    Use cases:\n    - Text generation and completion\n    - Conversational AI\n    - Content summarization\n    - Code generation\n    - Creative writing assistance",
      "namespace": "fal.llm",
      "node_type": "fal.llm.OpenrouterRouterOpenaiV1Embeddings",
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Openrouter Router Openai V1 Responses",
      "description": "The OpenRouter Responses API with fal, powered by OpenRouter, provides unified access to a wide range of large language models - including GPT, Claude, Gemini, and many others through a single API interface.\n    llm, language-model, text-generation, ai\n\n    Use cases:\n    - Text generation and completion\n    - Conversational AI\n    - Content summarization\n    - Code generation\n    - Creative writing assistance",
      "namespace": "fal.llm",
      "node_type": "fal.llm.OpenrouterRouterOpenaiV1Responses",
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Qwen 3 Guard",
      "description": "Qwen 3 Guard provides content safety and moderation using Qwen's LLM.\n    llm, safety, moderation, qwen, guard\n\n    Use cases:\n    - Content safety checking\n    - Moderation of text content\n    - Safety filtering for outputs\n    - Content policy enforcement\n    - Text safety analysis",
      "namespace": "fal.llm",
      "node_type": "fal.llm.Qwen3Guard",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The input text to be classified"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Video Prompt Generator",
      "description": "Generate video prompts using a variety of techniques including camera direction, style, pacing, special effects and more.\n    llm, language-model, text-generation, ai\n\n    Use cases:\n    - Text generation and completion\n    - Conversational AI\n    - Content summarization\n    - Code generation\n    - Creative writing assistance",
      "namespace": "fal.llm",
      "node_type": "fal.llm.VideoPromptGenerator",
      "properties": [
        {
          "name": "custom_elements",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Custom Elements",
          "description": "Custom technical elements (optional)"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "Minimalist",
              "Simple",
              "Detailed",
              "Descriptive",
              "Dynamic",
              "Cinematic",
              "Documentary",
              "Animation",
              "Action",
              "Experimental"
            ],
            "type_name": "nodetool.nodes.fal.llm.VideoPromptGenerator.Style"
          },
          "default": "Simple",
          "title": "Style",
          "description": "Style of the video prompt"
        },
        {
          "name": "camera_direction",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Zoom in",
              "Zoom out",
              "Pan left",
              "Pan right",
              "Tilt up",
              "Tilt down",
              "Orbital rotation",
              "Push in",
              "Pull out",
              "Track forward",
              "Track backward",
              "Spiral in",
              "Spiral out",
              "Arc movement",
              "Diagonal traverse",
              "Vertical rise",
              "Vertical descent"
            ],
            "type_name": "nodetool.nodes.fal.llm.VideoPromptGenerator.CameraDirection"
          },
          "default": "None",
          "title": "Camera Direction",
          "description": "Camera direction"
        },
        {
          "name": "pacing",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Slow burn",
              "Rhythmic pulse",
              "Frantic energy",
              "Ebb and flow",
              "Hypnotic drift",
              "Time-lapse rush",
              "Stop-motion staccato",
              "Gradual build",
              "Quick cut rhythm",
              "Long take meditation",
              "Jump cut energy",
              "Match cut flow",
              "Cross-dissolve dreamscape",
              "Parallel action",
              "Slow motion impact",
              "Ramping dynamics",
              "Montage tempo",
              "Continuous flow",
              "Episodic breaks"
            ],
            "type_name": "nodetool.nodes.fal.llm.VideoPromptGenerator.Pacing"
          },
          "default": "None",
          "title": "Pacing",
          "description": "Pacing rhythm"
        },
        {
          "name": "special_effects",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Practical effects",
              "CGI enhancement",
              "Analog glitches",
              "Light painting",
              "Projection mapping",
              "Nanosecond exposures",
              "Double exposure",
              "Smoke diffusion",
              "Lens flare artistry",
              "Particle systems",
              "Holographic overlay",
              "Chromatic aberration",
              "Digital distortion",
              "Wire removal",
              "Motion capture",
              "Miniature integration",
              "Weather simulation",
              "Color grading",
              "Mixed media composite",
              "Neural style transfer"
            ],
            "type_name": "nodetool.nodes.fal.llm.VideoPromptGenerator.SpecialEffects"
          },
          "default": "None",
          "title": "Special Effects",
          "description": "Special effects approach"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of an image to analyze and incorporate into the video prompt (optional)"
        },
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "anthropic/claude-3.5-sonnet",
              "anthropic/claude-3-5-haiku",
              "anthropic/claude-3-haiku",
              "google/gemini-2.5-flash-lite",
              "google/gemini-2.0-flash-001",
              "meta-llama/llama-3.2-1b-instruct",
              "meta-llama/llama-3.2-3b-instruct",
              "meta-llama/llama-3.1-8b-instruct",
              "meta-llama/llama-3.1-70b-instruct",
              "openai/gpt-4o-mini",
              "openai/gpt-4o",
              "deepseek/deepseek-r1"
            ],
            "type_name": "nodetool.nodes.fal.llm.VideoPromptGenerator.Model"
          },
          "default": "google/gemini-2.0-flash-001",
          "title": "Model",
          "description": "Model to use"
        },
        {
          "name": "camera_style",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "Steadicam flow",
              "Drone aerials",
              "Handheld urgency",
              "Crane elegance",
              "Dolly precision",
              "VR 360",
              "Multi-angle rig",
              "Static tripod",
              "Gimbal smoothness",
              "Slider motion",
              "Jib sweep",
              "POV immersion",
              "Time-slice array",
              "Macro extreme",
              "Tilt-shift miniature",
              "Snorricam character",
              "Whip pan dynamics",
              "Dutch angle tension",
              "Underwater housing",
              "Periscope lens"
            ],
            "type_name": "nodetool.nodes.fal.llm.VideoPromptGenerator.CameraStyle"
          },
          "default": "None",
          "title": "Camera Style",
          "description": "Camera movement style"
        },
        {
          "name": "input_concept",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Input Concept",
          "description": "Core concept or thematic input for the video prompt"
        },
        {
          "name": "prompt_length",
          "type": {
            "type": "enum",
            "values": [
              "Short",
              "Medium",
              "Long"
            ],
            "type_name": "nodetool.nodes.fal.llm.VideoPromptGenerator.PromptLength"
          },
          "default": "Medium",
          "title": "Prompt Length",
          "description": "Length of the prompt"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Openrouter Router Video",
      "description": "Run any VLM (Video Language Model) with fal, powered by OpenRouter.\n    video, transcription, analysis, video-understanding\n\n    Use cases:\n    - Video transcription\n    - Video content analysis\n    - Automated captioning\n    - Video understanding\n    - Content indexing",
      "namespace": "fal.video_to_text",
      "node_type": "fal.video_to_text.OpenrouterRouterVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to be used for the video processing"
        },
        {
          "name": "video_urls",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Urls",
          "description": "List of URLs or data URIs of video files to process. Supported formats: mp4, mpeg, mov, webm. For Google Gemini on AI Studio, YouTube links are also supported. Mutually exclusive with video_url."
        },
        {
          "name": "reasoning",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reasoning",
          "description": "Should reasoning be the part of the final answer."
        },
        {
          "name": "system_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "System Prompt",
          "description": "System prompt to provide context or instructions to the model"
        },
        {
          "name": "model",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model",
          "description": "Name of the model to use. Charged based on actual token usage."
        },
        {
          "name": "max_tokens",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Max Tokens",
          "description": "This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Temperature",
          "description": "This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "video_urls",
        "reasoning",
        "system_prompt",
        "model"
      ]
    },
    {
      "title": "Openrouter Router Video Enterprise",
      "description": "Run any VLM (Video Language Model) with fal, powered by OpenRouter.\n    video, transcription, analysis, video-understanding\n\n    Use cases:\n    - Video transcription\n    - Video content analysis\n    - Automated captioning\n    - Video understanding\n    - Content indexing",
      "namespace": "fal.video_to_text",
      "node_type": "fal.video_to_text.OpenrouterRouterVideoEnterprise",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to be used for the video processing"
        },
        {
          "name": "video_urls",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Urls",
          "description": "List of URLs or data URIs of video files to process. Supported formats: mp4, mpeg, mov, webm. For Google Gemini on AI Studio, YouTube links are also supported. Mutually exclusive with video_url."
        },
        {
          "name": "reasoning",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reasoning",
          "description": "Should reasoning be the part of the final answer."
        },
        {
          "name": "system_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "System Prompt",
          "description": "System prompt to provide context or instructions to the model"
        },
        {
          "name": "model",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model",
          "description": "Name of the model to use. Charged based on actual token usage."
        },
        {
          "name": "max_tokens",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Max Tokens",
          "description": "This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Temperature",
          "description": "This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "video_urls",
        "reasoning",
        "system_prompt",
        "model"
      ]
    },
    {
      "title": "Face Swap Video",
      "description": "Swap faces in videos using a source face image. Replaces faces in the target video with the source face while maintaining natural motion and expressions.\n    face-swap, video-editing, face-replacement, deep-fake, video-manipulation\n\n    Use cases:\n    - Create face-swapped video content\n    - Generate creative video edits\n    - Produce entertainment content\n    - Test different faces in video footage\n    - Create video memes and parodies",
      "namespace": "fal.video_processing",
      "node_type": "fal.video_processing.FaceSwapVideo",
      "properties": [
        {
          "name": "source_face",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Source Face",
          "description": "Source face image to swap into video"
        },
        {
          "name": "target_video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Target Video",
          "description": "Target video to swap face in (max 25 minutes)"
        },
        {
          "name": "enable_occlusion_prevention",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Occlusion Prevention",
          "description": "Enable occlusion prevention for faces covered by hands/objects (costs 2x more)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "source_face",
        "target_video"
      ]
    },
    {
      "title": "Live Avatar",
      "description": "Live Avatar creates animated avatars from images and audio.\n    video, avatar, animation, audio-driven\n\n    Use cases:\n    - Create talking avatars\n    - Generate animated presentations\n    - Produce video content from photos\n    - Create virtual presenters\n    - Generate video messages",
      "namespace": "fal.video_processing",
      "node_type": "fal.video_processing.LiveAvatar",
      "properties": [
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The avatar image"
        },
        {
          "name": "audio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Audio",
          "description": "URL to the driving audio"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "audio"
      ]
    },
    {
      "title": "RIFE",
      "description": "RIFE (Real-time Intermediate Flow Estimation) interpolates frames for smooth video.\n    video, interpolation, frame-rate, smoothing\n\n    Use cases:\n    - Increase video frame rate\n    - Create smooth slow motion\n    - Improve video fluidity\n    - Generate intermediate frames\n    - Enhance animation smoothness",
      "namespace": "fal.video_processing",
      "node_type": "fal.video_processing.RIFE",
      "properties": [
        {
          "name": "start_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Start Image",
          "description": "The start frame"
        },
        {
          "name": "end_image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image",
          "description": "The end frame"
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 2,
          "title": "Num Frames",
          "description": "Number of intermediate frames",
          "min": 1.0,
          "max": 16.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "list"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "start_image",
        "end_image",
        "num_frames"
      ]
    },
    {
      "title": "RIFEVideo",
      "description": "RIFE Video interpolates video frames for increased frame rate.\n    video, interpolation, frame-rate, enhancement\n\n    Use cases:\n    - Double video frame rate\n    - Create slow motion videos\n    - Improve video smoothness\n    - Enhance low-fps footage\n    - Generate high-fps content",
      "namespace": "fal.video_processing",
      "node_type": "fal.video_processing.RIFEVideo",
      "properties": [
        {
          "name": "video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video",
          "description": "The video to interpolate"
        },
        {
          "name": "multiplier",
          "type": {
            "type": "int"
          },
          "default": 2,
          "title": "Multiplier",
          "description": "Frame rate multiplier",
          "min": 2.0,
          "max": 4.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "multiplier"
      ]
    },
    {
      "title": "Sync Lipsync V2",
      "description": "Sync Lipsync V2 synchronizes lip movements to audio.\n    video, lipsync, audio, synchronization\n\n    Use cases:\n    - Sync lips to new audio\n    - Create talking head videos\n    - Dub videos in other languages\n    - Generate speaking animations\n    - Create video voice-overs",
      "namespace": "fal.video_processing",
      "node_type": "fal.video_processing.SyncLipsyncV2",
      "properties": [
        {
          "name": "video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video",
          "description": "The video with the face"
        },
        {
          "name": "audio",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Audio",
          "description": "URL to the audio file for lipsync"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "audio"
      ]
    },
    {
      "title": "Topaz Video Upscale",
      "description": "Topaz Video Upscale enhances video quality using advanced AI.\n    video, upscaling, enhancement, topaz, professional\n\n    Use cases:\n    - Professional video upscaling\n    - Restore archival footage\n    - Enhance video for broadcast\n    - Improve video quality\n    - Prepare videos for 4K display",
      "namespace": "fal.video_processing",
      "node_type": "fal.video_processing.TopazVideoUpscale",
      "properties": [
        {
          "name": "video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video",
          "description": "The video to upscale"
        },
        {
          "name": "scale",
          "type": {
            "type": "int"
          },
          "default": 2,
          "title": "Scale",
          "description": "Upscaling factor",
          "min": 2.0,
          "max": 4.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "scale"
      ]
    },
    {
      "title": "Video Upscaler",
      "description": "Video Upscaler enhances video resolution using AI.\n    video, upscaling, enhancement, super-resolution\n\n    Use cases:\n    - Upscale low-resolution videos\n    - Enhance video quality\n    - Improve video for larger displays\n    - Restore old videos\n    - Prepare videos for high-res output",
      "namespace": "fal.video_processing",
      "node_type": "fal.video_processing.VideoUpscaler",
      "properties": [
        {
          "name": "video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video",
          "description": "The video to upscale"
        },
        {
          "name": "scale",
          "type": {
            "type": "float"
          },
          "default": 2.0,
          "title": "Scale",
          "description": "Upscaling factor (1-8)",
          "min": 1.0,
          "max": 8.0
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "scale"
      ]
    },
    {
      "title": "Aura Flow",
      "description": "AuraFlow v0.3 is an open-source flow-based text-to-image generation model that achieves state-of-the-art results on GenEval. The model is currently in beta.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.AuraFlow",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate images from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to perform prompt expansion (recommended)"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier free guidance scale"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to take"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for generating images"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bagel",
      "description": "Bagel is a 7B parameter from Bytedance-Seed multimodal model that can generate both text and images.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Bagel",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation."
        },
        {
          "name": "use_thought",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Thought",
          "description": "Whether to use thought tokens for generation. If set to true, the model will \"think\" to potentially improve generation quality. Increases generation time and increases the cost by 20%."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Generate",
      "description": "Fibo\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.BriaFiboGenerate",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt for image generation."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "2:3",
              "3:2",
              "3:4",
              "4:3",
              "4:5",
              "5:4",
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.BriaFiboGenerate.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "Aspect ratio. Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9"
        },
        {
          "name": "steps_num",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Steps Num",
          "description": "Number of inference steps."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Reference image (file or URL)."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If true, returns the image directly in the response (increases latency)."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for text."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 5555,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for image generation."
        },
        {
          "name": "structured_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Structured Prompt",
          "description": "The structured prompt to generate an image from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Lite Generate",
      "description": "Fibo Lite\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.BriaFiboLiteGenerate",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt for image generation."
        },
        {
          "name": "steps_num",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Steps Num",
          "description": "Number of inference steps for Fibo Lite."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "2:3",
              "3:2",
              "3:4",
              "4:3",
              "4:5",
              "5:4",
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.BriaFiboLiteGenerate.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "Aspect ratio. Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Reference image (file or URL)."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If true, returns the image directly in the response (increases latency)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 5555,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "structured_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Structured Prompt",
          "description": "The structured prompt to generate an image from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bria Text To Image Base",
      "description": "Bria's Text-to-Image model, trained exclusively on licensed data for safe and risk-free commercial use. Available also as source code and weights. For access to weights: https://bria.ai/contact-us\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.BriaTextToImageBase",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt you would like to use to generate images."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Images",
          "description": "How many images you would like to generate. When using any Guidance Method, Value is set to 1."
        },
        {
          "name": "prompt_enhancement",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Prompt Enhancement",
          "description": "When set to true, enhances the provided prompt by generating additional, more descriptive variations, resulting in more diverse and creative output images."
        },
        {
          "name": "guidance",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Guidance",
          "description": "Guidance images to use for the generation. Up to 4 guidance methods can be combined during a single inference."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "2:3",
              "3:2",
              "3:4",
              "4:3",
              "4:5",
              "5:4",
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.BriaTextToImageBase.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the image. When a guidance method is being used, the aspect ratio is defined by the guidance image and this parameter is ignored."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "medium",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "photography",
              "art"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.BriaTextToImageBase.Medium"
          },
          "default": null,
          "title": "Medium",
          "description": "Which medium should be included in your generated images. This parameter is optional."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt you would like to use to generate images."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of iterations the model goes through to refine the generated image. This parameter is optional."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bria Text To Image Fast",
      "description": "Bria's Text-to-Image model with perfect harmony of latency and quality. Trained exclusively on licensed data for safe and risk-free commercial use. Available also as source code and weights. For access to weights: https://bria.ai/contact-us\n    generation, text-to-image, txt2img, ai-art, fast\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.BriaTextToImageFast",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt you would like to use to generate images."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Images",
          "description": "How many images you would like to generate. When using any Guidance Method, Value is set to 1."
        },
        {
          "name": "prompt_enhancement",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Prompt Enhancement",
          "description": "When set to true, enhances the provided prompt by generating additional, more descriptive variations, resulting in more diverse and creative output images."
        },
        {
          "name": "guidance",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Guidance",
          "description": "Guidance images to use for the generation. Up to 4 guidance methods can be combined during a single inference."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "2:3",
              "3:2",
              "3:4",
              "4:3",
              "4:5",
              "5:4",
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.BriaTextToImageFast.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the image. When a guidance method is being used, the aspect ratio is defined by the guidance image and this parameter is ignored."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "medium",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "photography",
              "art"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.BriaTextToImageFast.Medium"
          },
          "default": null,
          "title": "Medium",
          "description": "Which medium should be included in your generated images. This parameter is optional."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt you would like to use to generate images."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of iterations the model goes through to refine the generated image. This parameter is optional."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bria Text To Image Hd",
      "description": "Bria's Text-to-Image model for HD images. Trained exclusively on licensed data for safe and risk-free commercial use. Available also as source code and weights. For access to weights: https://bria.ai/contact-us\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.BriaTextToImageHd",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt you would like to use to generate images."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Images",
          "description": "How many images you would like to generate. When using any Guidance Method, Value is set to 1."
        },
        {
          "name": "prompt_enhancement",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Prompt Enhancement",
          "description": "When set to true, enhances the provided prompt by generating additional, more descriptive variations, resulting in more diverse and creative output images."
        },
        {
          "name": "guidance",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Guidance",
          "description": "Guidance images to use for the generation. Up to 4 guidance methods can be combined during a single inference."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "2:3",
              "3:2",
              "3:4",
              "4:3",
              "4:5",
              "5:4",
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.BriaTextToImageHd.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the image. When a guidance method is being used, the aspect ratio is defined by the guidance image and this parameter is ignored."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "medium",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "photography",
              "art"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.BriaTextToImageHd.Medium"
          },
          "default": null,
          "title": "Medium",
          "description": "Which medium should be included in your generated images. This parameter is optional."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt you would like to use to generate images."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of iterations the model goes through to refine the generated image. This parameter is optional."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bytedance Dreamina V31 Text To Image",
      "description": "Bytedance\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.BytedanceDreaminaV31TextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Width and height must be between 512 and 2048."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control the stochasticity of image generation."
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to use an LLM to enhance the prompt"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bytedance Seedream V3 Text To Image",
      "description": "Bytedance\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.BytedanceSeedreamV3TextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "Use for finer control over the output image size. Will be used over aspect_ratio, if both are provided. Width and height must be between 512 and 2048."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "Controls how closely the output image aligns with the input prompt. Higher values mean stronger prompt correlation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control the stochasticity of image generation."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bytedance Seedream V45 Text To Image",
      "description": "ByteDance SeeDream v4.5 generates advanced images from text with cutting-edge AI technology.\n    image, generation, bytedance, seedream, v4.5, text-to-image\n\n    Use cases:\n    - Generate images with SeeDream v4.5\n    - Create cutting-edge visual content\n    - Produce advanced AI artwork\n    - Generate images with latest tech\n    - Create modern AI visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.BytedanceSeedreamV45TextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of separate model generations to be run with the prompt."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Width and height must be between 1920 and 4096, or total number of pixels must be between 2560*1440 and 4096*4096."
        },
        {
          "name": "max_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Max Images",
          "description": "If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control the stochasticity of image generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bytedance Seedream V4 Text To Image",
      "description": "Bytedance Seedream v4\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.BytedanceSeedreamV4TextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of separate model generations to be run with the prompt."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Total pixels must be between 960x960 and 4096x4096."
        },
        {
          "name": "max_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Max Images",
          "description": "If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`."
        },
        {
          "name": "enhance_prompt_mode",
          "type": {
            "type": "enum",
            "values": [
              "standard",
              "fast"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.BytedanceSeedreamV4TextToImage.EnhancePromptMode"
          },
          "default": "standard",
          "title": "Enhance Prompt Mode",
          "description": "The mode to use for enhancing prompt enhancement. Standard mode provides higher quality results but takes longer to generate. Fast mode provides average quality results but takes less time to generate."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control the stochasticity of image generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Cogview 4",
      "description": "Generate high quality images from text prompts using CogView4. Longer text prompts will result in better quality images.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Cogview4",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Cogview4.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Diffusion Edge",
      "description": "Diffusion based high quality edge detection\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.DiffusionEdge",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The text prompt you would like to convert to speech."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Dreamo",
      "description": "DreamO is an image customization framework designed to support a wide range of tasks while facilitating seamless integration of multiple conditions.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Dreamo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "first_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "First Image Url",
          "description": "URL of first reference image to use for generation."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "second_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Second Image Url",
          "description": "URL of second reference image to use for generation."
        },
        {
          "name": "second_reference_task",
          "type": {
            "type": "enum",
            "values": [
              "ip",
              "id",
              "style"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Dreamo.SecondReferenceTask"
          },
          "default": "ip",
          "title": "Second Reference Task",
          "description": "Task for second reference image (ip/id/style)."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "first_reference_task",
          "type": {
            "type": "enum",
            "values": [
              "ip",
              "id",
              "style"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Dreamo.FirstReferenceTask"
          },
          "default": "ip",
          "title": "First Reference Task",
          "description": "Task for first reference image (ip/id/style)."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "ref_resolution",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Ref Resolution",
          "description": "Resolution for reference images."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "true_cfg",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "True Cfg",
          "description": "The weight of the CFG loss."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 12,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Dreamshaper",
      "description": "Dreamshaper model.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Dreamshaper",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size"
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The list of LoRA weights to use."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed, grayscale, bw, bad photo, bad photography, bad art:1.4), (watermark, signature, text font, username, error, logo, words, letters, digits, autograph, trademark, name:1.2), (blur, blurry, grainy), morbid, ugly, asymmetrical, mutated malformed, mutilated, poorly lit, bad shadow, draft, cropped, out of frame, cut off, censored, jpeg artifacts, out of focus, glitch, duplicate, (airbrushed, cartoon, anime, semi-realistic, cgi, render, blender, digital art, manga, amateur:1.3), (3D ,3D Game, 3D Game Scene, 3D Character:1.1), (bad hands, bad anatomy, bad body, bad face, bad teeth, bad arms, bad legs, deformities:1.3)",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Dreamshaper.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "model_name",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "Lykon/dreamshaper-xl-1-0",
              "Lykon/dreamshaper-xl-v2-turbo",
              "Lykon/dreamshaper-8"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Dreamshaper.ModelName"
          },
          "default": null,
          "title": "Model Name",
          "description": "The Dreamshaper model to use."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Dreamshaper.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 35,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Emu 35 Image Text To Image",
      "description": "Emu 3.5 Image\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Emu35ImageTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to create the image."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Emu35ImageTextToImage.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the output image."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Emu35ImageTextToImage.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the output image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Emu35ImageTextToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "Whether to return the image in sync mode."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the inference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "FLite Standard",
      "description": "F Lite is a 10B parameter diffusion model created by Fal and Freepik, trained exclusively on copyright-safe and SFW content.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FLiteStandard",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative Prompt for generation."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "FLite Texture",
      "description": "F Lite is a 10B parameter diffusion model created by Fal and Freepik, trained exclusively on copyright-safe and SFW content. This is a high texture density variant of the model.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FLiteTexture",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative Prompt for generation."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Fast Fooocus Sdxl",
      "description": "Fooocus extreme speed mode as a standalone app.\n    generation, text-to-image, txt2img, ai-art, fast\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FastFooocusSdxl",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "enable_refiner",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Refiner",
          "description": "If set to true, a smaller model will try to refine the output after it was processed."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "guidance_rescale",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Rescale",
          "description": "The rescale factor for the CFG."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FastFooocusSdxl.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FastFooocusSdxl.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Fast Fooocus Sdxl Image To Image",
      "description": "Fooocus extreme speed mode as a standalone app.\n    generation, text-to-image, txt2img, ai-art, fast\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FastFooocusSdxlImageToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "enable_refiner",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Refiner",
          "description": "If set to true, a smaller model will try to refine the output after it was processed."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Leave it none to automatically infer from the prompt image."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "guidance_rescale",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Rescale",
          "description": "The rescale factor for the CFG."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FastFooocusSdxlImageToImage.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Strength",
          "description": "determines how much the generated image resembles the initial image"
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FastFooocusSdxlImageToImage.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Fast Lcm Diffusion",
      "description": "Run SDXL at the speed of light\n    generation, text-to-image, txt2img, ai-art, fast\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FastLcmDiffusion",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "guidance_rescale",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Rescale",
          "description": "The rescale factor for the CFG."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FastLcmDiffusion.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "model_name",
          "type": {
            "type": "enum",
            "values": [
              "stabilityai/stable-diffusion-xl-base-1.0",
              "runwayml/stable-diffusion-v1-5"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FastLcmDiffusion.ModelName"
          },
          "default": "stabilityai/stable-diffusion-xl-base-1.0",
          "title": "Model Name",
          "description": "The name of the model to use."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FastLcmDiffusion.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Fast Lightning Sdxl",
      "description": "Run SDXL at the speed of light\n    generation, text-to-image, txt2img, ai-art, fast\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FastLightningSdxl",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FastLightningSdxl.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_rescale",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Rescale",
          "description": "The rescale factor for the CFG."
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FastLightningSdxl.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "4",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FastLightningSdxl.NumInferenceSteps"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Fast Sdxl",
      "description": "Run SDXL at the speed of light\n    generation, text-to-image, txt2img, ai-art, fast\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FastSdxl",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The list of LoRA weights to use."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FastSdxl.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FastSdxl.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Fast Sdxl Controlnet Canny",
      "description": "Generate Images with ControlNet.\n    generation, text-to-image, txt2img, ai-art, fast\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FastSdxlControlnetCanny",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Leave it none to automatically infer from the control image."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The list of LoRA weights to use."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale of the controlnet conditioning."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "control_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "enable_deep_cache",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Deep Cache",
          "description": "If set to true, DeepCache will be enabled. TBD"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 1 Dev",
      "description": "FLUX.1 [dev] is a 12 billion parameter flow transformer that generates high-quality images from text. It is suitable for personal and commercial use. \n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux1Dev",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux1Dev.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux1Dev.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 1 Krea",
      "description": "FLUX.1 Krea [dev]\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux1Krea",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux1Krea.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux1Krea.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 1 Schnell",
      "description": "Fastest inference in the world for the 12 billion parameter FLUX.1 [schnell] text-to-image model. \n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux1Schnell",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux1Schnell.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux1Schnell.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 1 Srpo",
      "description": "FLUX.1 SRPO [dev]\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux1Srpo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux1Srpo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux1Srpo.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Flash",
      "description": "FLUX.2 Flash is an ultra-fast variant of FLUX.2 designed for instant image generation with minimal latency.\n    image, generation, flux, ultra-fast, flash, text-to-image, txt2img\n\n    Use cases:\n    - Instant preview generation for user interfaces\n    - Real-time collaborative design tools\n    - Lightning-fast concept exploration\n    - High-speed batch processing\n    - Interactive gaming and entertainment applications",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Flash",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Flash.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "If set to true, the prompt will be expanded for better results."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image_size"
      ]
    },
    {
      "title": "Flux 2 Flex",
      "description": "Flux 2 Flex\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Flex",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Flex.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Flex.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to expand the prompt using the model's own knowledge."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 4B",
      "description": "FLUX-2 Klein 4B generates images with the efficient 4-billion parameter model for balanced quality and speed.\n    image, generation, flux-2, klein, 4b, text-to-image\n\n    Use cases:\n    - Generate images with 4B model\n    - Create balanced quality-speed content\n    - Produce efficient visual artwork\n    - Generate images with good performance\n    - Create optimized visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Klein4B",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the image to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein4B.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 4B Base",
      "description": "FLUX-2 Klein 4B Base provides foundation model generation with 4-billion parameters.\n    image, generation, flux-2, klein, 4b, base\n\n    Use cases:\n    - Generate with base 4B model\n    - Create foundation quality content\n    - Produce standard visual artwork\n    - Generate images with base model\n    - Create baseline visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Klein4BBase",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the image to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein4BBase.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for image generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein4BBase.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for classifier-free guidance. Describes what to avoid in the image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 4B Base Lora",
      "description": "FLUX-2 Klein 4B Base with LoRA enables custom-trained 4B models for specialized generation.\n    image, generation, flux-2, klein, 4b, base, lora\n\n    Use cases:\n    - Generate with custom 4B base model\n    - Create specialized foundation content\n    - Produce domain-specific visuals\n    - Generate with fine-tuned 4B model\n    - Create customized baseline visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Klein4BBaseLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the image to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein4BBaseLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for image generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein4BBaseLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for classifier-free guidance. Describes what to avoid in the image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 9B",
      "description": "FLUX-2 Klein 9B generates high-quality images with the powerful 9-billion parameter model.\n    image, generation, flux-2, klein, 9b, text-to-image\n\n    Use cases:\n    - Generate high-quality images with 9B model\n    - Create superior visual content\n    - Produce detailed artwork\n    - Generate images with powerful model\n    - Create premium quality visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Klein9B",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the image to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein9B.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 9B Base",
      "description": "FLUX-2 Klein 9B Base provides foundation generation with the full 9-billion parameter model.\n    image, generation, flux-2, klein, 9b, base\n\n    Use cases:\n    - Generate with base 9B model\n    - Create high-quality foundation content\n    - Produce superior baseline artwork\n    - Generate images with powerful base\n    - Create premium baseline visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Klein9BBase",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the image to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein9BBase.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for image generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein9BBase.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for classifier-free guidance. Describes what to avoid in the image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Klein 9B Base Lora",
      "description": "FLUX-2 Klein 9B Base with LoRA combines powerful generation with custom-trained models.\n    image, generation, flux-2, klein, 9b, base, lora\n\n    Use cases:\n    - Generate with custom 9B base model\n    - Create specialized high-quality content\n    - Produce custom superior visuals\n    - Generate with fine-tuned 9B model\n    - Create advanced customized visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Klein9BBaseLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the image to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein9BBaseLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for image generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Klein9BBaseLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI. Output is not stored when this is True."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for classifier-free guidance. Describes what to avoid in the image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Ballpoint Pen Sketch",
      "description": "Flux 2 Lora Gallery\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2LoraGalleryBallpointPenSketch",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a ballpoint pen sketch style image. Use 'b4llp01nt' trigger word for best results."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGalleryBallpointPenSketch.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the ballpoint pen sketch effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGalleryBallpointPenSketch.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Digital Comic Art",
      "description": "Flux 2 Lora Gallery\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2LoraGalleryDigitalComicArt",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a digital comic art style image. Use 'd1g1t4l' trigger word for best results."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGalleryDigitalComicArt.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the digital comic art effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGalleryDigitalComicArt.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Hdr Style",
      "description": "Flux 2 Lora Gallery\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2LoraGalleryHdrStyle",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an HDR style image. The trigger word 'Hyp3rRe4list1c' will be automatically prepended."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGalleryHdrStyle.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the HDR style effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGalleryHdrStyle.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Realism",
      "description": "Flux 2 Lora Gallery\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2LoraGalleryRealism",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a realistic image with natural lighting and authentic details."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGalleryRealism.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the realism effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGalleryRealism.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Satellite View Style",
      "description": "Flux 2 Lora Gallery\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2LoraGallerySatelliteViewStyle",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a satellite/aerial view style image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGallerySatelliteViewStyle.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the satellite view style effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGallerySatelliteViewStyle.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Lora Gallery Sepia Vintage",
      "description": "Flux 2 Lora Gallery\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2LoraGallerySepiaVintage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a sepia vintage photography style image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGallerySepiaVintage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. 'regular' balances speed and quality."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The strength of the sepia vintage photography effect."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2LoraGallerySepiaVintage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the output image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and won't be saved in history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. Same seed with same prompt will produce same result."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker for the generated image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Max",
      "description": "FLUX-2 Max generates maximum quality images with the most advanced FLUX-2 model for premium results.\n    image, generation, flux-2, max, premium, text-to-image\n\n    Use cases:\n    - Generate maximum quality images\n    - Create premium visual content\n    - Produce professional-grade artwork\n    - Generate images with best model\n    - Create superior quality visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Max",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Max.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Max.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux 2 Turbo",
      "description": "FLUX.2 Turbo is a blazing-fast image generation model optimized for speed without sacrificing quality, ideal for real-time applications.\n    image, generation, flux, fast, turbo, text-to-image, txt2img\n\n    Use cases:\n    - Real-time image generation for interactive apps\n    - Rapid prototyping of visual concepts\n    - Generate multiple variations instantly\n    - Live visual effects and augmented reality\n    - High-throughput batch image processing",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Flux2Turbo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Flux2Turbo.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "If set to true, the prompt will be expanded for better results."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image_size",
        "num_images"
      ]
    },
    {
      "title": "Flux Control Lora Canny",
      "description": "FLUX Control LoRA Canny is a high-performance endpoint that uses a control image to transfer structure to the generated image, using a Canny edge map.\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxControlLoraCanny",
      "properties": [
        {
          "name": "control_lora_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Control Lora Strength",
          "description": "The strength of the control lora."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxControlLoraCanny.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "control_lora_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Lora Image Url",
          "description": "The image to use for control lora. This is used to control the style of the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux Control Lora Depth",
      "description": "FLUX Control LoRA Depth is a high-performance endpoint that uses a control image to transfer structure to the generated image, using a depth map.\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxControlLoraDepth",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "control_lora_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Control Lora Strength",
          "description": "The strength of the control lora."
        },
        {
          "name": "preprocess_depth",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preprocess Depth",
          "description": "If set to true, the input image will be preprocessed to extract depth information. This is useful for generating depth maps from images."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxControlLoraDepth.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "control_lora_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Lora Image Url",
          "description": "The image to use for control lora. This is used to control the style of the generated image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux Dev",
      "description": "FLUX.1 [dev] is a powerful open-weight text-to-image model with 12 billion parameters. Optimized for prompt following and visual quality.\n    image, generation, flux, text-to-image, txt2img\n\n    Use cases:\n    - Generate high-quality images from text prompts\n    - Create detailed illustrations with precise control\n    - Produce professional artwork and designs\n    - Generate multiple variations from one prompt\n    - Create safe-for-work content with built-in safety checker",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxDev",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxDev.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxDev.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable safety checker to filter unsafe content"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "How strictly to follow the prompt. Higher values are more literal"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps. More steps typically improve quality"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image_size",
        "num_inference_steps"
      ]
    },
    {
      "title": "Flux Kontext Lora Text To Image",
      "description": "Flux Kontext Lora\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxKontextLoraTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxKontextLoraTextToImage.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxKontextLoraTextToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux Krea",
      "description": "FLUX.1 Krea [dev]\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxKrea",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxKrea.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxKrea.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux Krea Lora",
      "description": "FLUX.1 Krea [dev] with LoRAs\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxKreaLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxKreaLora.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux Krea Lora Stream",
      "description": "Flux Krea Lora\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxKreaLoraStream",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxKreaLoraStream.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux Lora",
      "description": "FLUX with LoRA support enables fine-tuned image generation using custom LoRA models for specific styles or subjects.\n    image, generation, flux, lora, fine-tuning, text-to-image, txt2img\n\n    Use cases:\n    - Generate images with custom artistic styles\n    - Create consistent characters across images\n    - Apply brand-specific visual styles\n    - Generate images with specialized subjects\n    - Combine multiple LoRA models for unique results",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxLora.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA models to apply with their weights"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "How strictly to follow the prompt"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable safety checker to filter unsafe content"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "loras",
        "image_size"
      ]
    },
    {
      "title": "Flux Lora Inpainting",
      "description": "Super fast endpoint for the FLUX.1 [dev] inpainting model with LoRA support, enabling rapid and high-quality image inpaingting using pre-trained LoRA adaptations for personalization, specific styles, brand identities, and product-specific outputs.\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxLoraInpainting",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxLoraInpainting.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image to use for inpainting. or img2img"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "The strength to use for inpainting/image-to-image. Only used if the image_url is provided. 1.0 is completely remakes the image while 0.0 preserves the original."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "The mask to area to Inpaint in."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux Lora Stream",
      "description": "Super fast endpoint for the FLUX.1 [dev] model with LoRA support, enabling rapid and high-quality image generation using pre-trained LoRA adaptations for personalization, specific styles, brand identities, and product-specific outputs.\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxLoraStream",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. This is always set to 1 for streaming output."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxLoraStream.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux Pro Kontext Max Text To Image",
      "description": "FLUX.1 Kontext [max] text-to-image is a new premium model brings maximum performance across all aspects \u2013 greatly improved prompt adherence.\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxProKontextMaxTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxProKontextMaxTextToImage.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxProKontextMaxTextToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxProKontextMaxTextToImage.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux Pro Kontext Text To Image",
      "description": "The FLUX.1 Kontext [pro] text-to-image delivers state-of-the-art image generation results with unprecedented prompt following, photorealistic rendering, and flawless typography.\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxProKontextTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "3:2",
              "1:1",
              "2:3",
              "3:4",
              "9:16",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxProKontextTextToImage.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxProKontextTextToImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxProKontextTextToImage.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux Pro New",
      "description": "FLUX.1 Pro New is the latest version of the professional FLUX model with enhanced capabilities and improved output quality.\n    image, generation, flux, professional, text-to-image, txt2img\n\n    Use cases:\n    - Generate professional-grade marketing visuals\n    - Create high-quality product renders\n    - Produce detailed architectural visualizations\n    - Design premium brand assets\n    - Generate photorealistic commercial imagery",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxProNew",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxProNew.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxProNew.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image_size"
      ]
    },
    {
      "title": "Flux Pro V11 Ultra Finetuned",
      "description": "FLUX1.1 [pro] ultra fine-tuned is the newest version of FLUX1.1 [pro] with a fine-tuned LoRA, maintaining professional-grade image quality while delivering up to 2K resolution with improved photo realism.\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxProV11UltraFinetuned",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "finetune_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Finetune Id",
          "description": "References your specific model"
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxProV11UltraFinetuned.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "image_prompt_strength",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Image Prompt Strength",
          "description": "The strength of the image prompt, between 0 and 1."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        },
        {
          "name": "raw",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Raw",
          "description": "Generate less processed, more natural-looking images."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxProV11UltraFinetuned.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "finetune_strength",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Finetune Strength",
          "description": "Controls finetune influence. Increase this value if your target concept isn't showing up strongly enough. The optimal setting depends on your finetune and prompt"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux Schnell",
      "description": "FLUX.1 [schnell] is a fast distilled version of FLUX.1 optimized for speed. Can generate high-quality images in 1-4 steps.\n    image, generation, flux, fast, text-to-image, txt2img\n\n    Use cases:\n    - Generate images quickly for rapid iteration\n    - Create concept art with minimal latency\n    - Produce preview images before final generation\n    - Generate multiple variations efficiently\n    - Real-time image generation applications",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxSchnell",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxSchnell.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxSchnell.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable safety checker to filter unsafe content"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps (1-4 recommended for schnell)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image_size",
        "num_inference_steps"
      ]
    },
    {
      "title": "Flux Srpo",
      "description": "FLUX.1 SRPO [dev]\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxSrpo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxSrpo.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The speed of the generation. The higher the speed, the faster the generation."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxSrpo.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux Subject",
      "description": "Super fast endpoint for the FLUX.1 [schnell] model with subject input capabilities, enabling rapid and high-quality image generation for personalization, specific styles, brand identities, and product-specific outputs.\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxSubject",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxSubject.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of image of the subject"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Flux V1 Pro",
      "description": "FLUX.1 Pro is a state-of-the-art image generation model with superior prompt following and image quality.\n    image, generation, flux, pro, text-to-image, txt2img\n\n    Use cases:\n    - Generate professional-grade images for commercial use\n    - Create highly detailed artwork with complex prompts\n    - Produce marketing materials and brand assets\n    - Generate photorealistic images\n    - Create custom visual content with precise control",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxV1Pro",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxV1Pro.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "Output image format (jpeg or png)"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxV1Pro.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "Safety checker tolerance level (1-6). Higher is more permissive"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable safety checker to filter unsafe content"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image_size",
        "guidance_scale"
      ]
    },
    {
      "title": "Flux V1 Pro Ultra",
      "description": "FLUX.1 Pro Ultra delivers the highest quality image generation with enhanced detail and realism.\n    image, generation, flux, pro, ultra, text-to-image, txt2img\n\n    Use cases:\n    - Generate ultra-high quality photorealistic images\n    - Create professional photography-grade visuals\n    - Produce detailed product renders\n    - Generate premium marketing materials\n    - Create artistic masterpieces with fine details",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FluxV1ProUltra",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio for the generated image"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxV1ProUltra.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FluxV1ProUltra.SafetyTolerance"
          },
          "default": "2",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."
        },
        {
          "name": "image_prompt_strength",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Image Prompt Strength",
          "description": "Strength of image prompt influence (0-1)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "raw",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Raw",
          "description": "Generate less processed, more natural results"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image_size",
        "aspect_ratio"
      ]
    },
    {
      "title": "Fooocus",
      "description": "Default parameters with automated optimizations and quality improvements.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Fooocus",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "performance",
          "type": {
            "type": "enum",
            "values": [
              "Speed",
              "Quality",
              "Extreme Speed",
              "Lightning"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Fooocus.Performance"
          },
          "default": "Extreme Speed",
          "title": "Performance",
          "description": "You can choose Speed or Quality"
        },
        {
          "name": "styles",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Styles",
          "description": "The style to use."
        },
        {
          "name": "control_type",
          "type": {
            "type": "enum",
            "values": [
              "ImagePrompt",
              "PyraCanny",
              "CPDS",
              "FaceSwap"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Fooocus.ControlType"
          },
          "default": "PyraCanny",
          "title": "Control Type",
          "description": "The type of image control"
        },
        {
          "name": "mask_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image Url",
          "description": "The image to use as a mask for the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 5 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to false, the safety checker will be disabled."
        },
        {
          "name": "sharpness",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Sharpness",
          "description": "The sharpness of the generated image. Use it to control how sharp the generated image should be. Higher value means image and texture are sharper."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "inpaint_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Inpaint Image Url",
          "description": "The image to use as a reference for inpainting."
        },
        {
          "name": "mixing_image_prompt_and_inpaint",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Mixing Image Prompt And Inpaint"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "1024x1024",
          "title": "Aspect Ratio",
          "description": "The size of the generated image. You can choose between some presets or custom height and width that **must be multiples of 8**."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate in one request"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Fooocus.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "refiner_model",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "realisticVisionV60B1_v51VAE.safetensors"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Fooocus.RefinerModel"
          },
          "default": "None",
          "title": "Refiner Model",
          "description": "Refiner (SDXL or SD 1.5)"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "control_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Control Image Url",
          "description": "The image to use as a reference for the generated image."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "refiner_switch",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Refiner Switch",
          "description": "Use 0.4 for SD1.5 realistic models; 0.667 for SD1.5 anime models 0.8 for XL-refiners; or any value for switching two SDXL models."
        },
        {
          "name": "control_image_weight",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Control Image Weight",
          "description": "The strength of the control image. Use it to control how much the generated image should look like the control image."
        },
        {
          "name": "control_image_stop_at",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Control Image Stop At",
          "description": "The stop at value of the control image. Use it to control how much the generated image should look like the control image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Fooocus Image Prompt",
      "description": "Default parameters with automated optimizations and quality improvements.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FooocusImagePrompt",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "uov_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Uov Image Url",
          "description": "The image to upscale or vary."
        },
        {
          "name": "performance",
          "type": {
            "type": "enum",
            "values": [
              "Speed",
              "Quality",
              "Extreme Speed",
              "Lightning"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FooocusImagePrompt.Performance"
          },
          "default": "Extreme Speed",
          "title": "Performance",
          "description": "You can choose Speed or Quality"
        },
        {
          "name": "image_prompt_3",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Prompt 3"
        },
        {
          "name": "styles",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Styles",
          "description": "The style to use."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 5 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "image_prompt_4",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Prompt 4"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "sharpness",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Sharpness",
          "description": "The sharpness of the generated image. Use it to control how sharp the generated image should be. Higher value means image and texture are sharper."
        },
        {
          "name": "mixing_image_prompt_and_inpaint",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Mixing Image Prompt And Inpaint",
          "description": "Mixing Image Prompt and Inpaint"
        },
        {
          "name": "outpaint_selections",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Outpaint Selections",
          "description": "The directions to outpaint."
        },
        {
          "name": "inpaint_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Inpaint Image Url",
          "description": "The image to use as a reference for inpainting."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FooocusImagePrompt.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "refiner_model",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "realisticVisionV60B1_v51VAE.safetensors"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FooocusImagePrompt.RefinerModel"
          },
          "default": "None",
          "title": "Refiner Model",
          "description": "Refiner (SDXL or SD 1.5)"
        },
        {
          "name": "image_prompt_2",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Prompt 2"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "inpaint_mode",
          "type": {
            "type": "enum",
            "values": [
              "Inpaint or Outpaint (default)",
              "Improve Detail (face, hand, eyes, etc.)",
              "Modify Content (add objects, change background, etc.)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FooocusImagePrompt.InpaintMode"
          },
          "default": "Inpaint or Outpaint (default)",
          "title": "Inpaint Mode",
          "description": "The mode to use for inpainting."
        },
        {
          "name": "uov_method",
          "type": {
            "type": "enum",
            "values": [
              "Disabled",
              "Vary (Subtle)",
              "Vary (Strong)",
              "Upscale (1.5x)",
              "Upscale (2x)",
              "Upscale (Fast 2x)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FooocusImagePrompt.UovMethod"
          },
          "default": "Disabled",
          "title": "Uov Method",
          "description": "The method to use for upscaling or varying."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "refiner_switch",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Refiner Switch",
          "description": "Use 0.4 for SD1.5 realistic models; 0.667 for SD1.5 anime models 0.8 for XL-refiners; or any value for switching two SDXL models."
        },
        {
          "name": "mixing_image_prompt_and_vary_upscale",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Mixing Image Prompt And Vary Upscale",
          "description": "Mixing Image Prompt and Vary/Upscale"
        },
        {
          "name": "mask_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image Url",
          "description": "The image to use as a mask for the generated image."
        },
        {
          "name": "image_prompt_1",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Prompt 1"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to false, the safety checker will be disabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate in one request"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "1024x1024",
          "title": "Aspect Ratio",
          "description": "The size of the generated image. You can choose between some presets or custom height and width that **must be multiples of 8**."
        },
        {
          "name": "inpaint_additional_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Inpaint Additional Prompt",
          "description": "Describe what you want to inpaint."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Fooocus Inpaint",
      "description": "Default parameters with automated optimizations and quality improvements.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FooocusInpaint",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "performance",
          "type": {
            "type": "enum",
            "values": [
              "Speed",
              "Quality",
              "Extreme Speed",
              "Lightning"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FooocusInpaint.Performance"
          },
          "default": "Extreme Speed",
          "title": "Performance",
          "description": "You can choose Speed or Quality"
        },
        {
          "name": "styles",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Styles",
          "description": "The style to use."
        },
        {
          "name": "image_prompt_3",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Prompt 3"
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 5 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "image_prompt_4",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Prompt 4"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "sharpness",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Sharpness",
          "description": "The sharpness of the generated image. Use it to control how sharp the generated image should be. Higher value means image and texture are sharper."
        },
        {
          "name": "mixing_image_prompt_and_inpaint",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Mixing Image Prompt And Inpaint",
          "description": "Mixing Image Prompt and Inpaint"
        },
        {
          "name": "outpaint_selections",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Outpaint Selections",
          "description": "The directions to outpaint."
        },
        {
          "name": "inpaint_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Inpaint Image Url",
          "description": "The image to use as a reference for inpainting."
        },
        {
          "name": "refiner_model",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "realisticVisionV60B1_v51VAE.safetensors"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FooocusInpaint.RefinerModel"
          },
          "default": "None",
          "title": "Refiner Model",
          "description": "Refiner (SDXL or SD 1.5)"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FooocusInpaint.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "image_prompt_2",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Prompt 2"
        },
        {
          "name": "inpaint_respective_field",
          "type": {
            "type": "float"
          },
          "default": 0.618,
          "title": "Inpaint Respective Field",
          "description": "The area to inpaint. Value 0 is same as \"Only Masked\" in A1111. Value 1 is same as \"Whole Image\" in A1111. Only used in inpaint, not used in outpaint. (Outpaint always use 1.0)"
        },
        {
          "name": "inpaint_mode",
          "type": {
            "type": "enum",
            "values": [
              "Inpaint or Outpaint (default)",
              "Improve Detail (face, hand, eyes, etc.)",
              "Modify Content (add objects, change background, etc.)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FooocusInpaint.InpaintMode"
          },
          "default": "Inpaint or Outpaint (default)",
          "title": "Inpaint Mode",
          "description": "The mode to use for inpainting."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "refiner_switch",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Refiner Switch",
          "description": "Use 0.4 for SD1.5 realistic models; 0.667 for SD1.5 anime models 0.8 for XL-refiners; or any value for switching two SDXL models."
        },
        {
          "name": "inpaint_disable_initial_latent",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Inpaint Disable Initial Latent",
          "description": "If set to true, the initial preprocessing will be disabled."
        },
        {
          "name": "mask_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image Url",
          "description": "The image to use as a mask for the generated image."
        },
        {
          "name": "invert_mask",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Invert Mask",
          "description": "If set to true, the mask will be inverted."
        },
        {
          "name": "image_prompt_1",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Prompt 1"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to false, the safety checker will be disabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate in one request"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "1024x1024",
          "title": "Aspect Ratio",
          "description": "The size of the generated image. You can choose between some presets or custom height and width that **must be multiples of 8**."
        },
        {
          "name": "inpaint_additional_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Inpaint Additional Prompt",
          "description": "Describe what you want to inpaint."
        },
        {
          "name": "inpaint_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Inpaint Strength",
          "description": "Same as the denoising strength in A1111 inpaint. Only used in inpaint, not used in outpaint. (Outpaint always use 1.0)"
        },
        {
          "name": "override_inpaint_options",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Override Inpaint Options",
          "description": "If set to true, the advanced inpaint options ('inpaint_disable_initial_latent', 'inpaint_engine', 'inpaint_strength', 'inpaint_respective_field', 'inpaint_erode_or_dilate') will be overridden. Otherwise, the default values will be used."
        },
        {
          "name": "inpaint_engine",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "v1",
              "v2.5",
              "v2.6"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FooocusInpaint.InpaintEngine"
          },
          "default": "v2.6",
          "title": "Inpaint Engine",
          "description": "Version of Fooocus inpaint model"
        },
        {
          "name": "inpaint_erode_or_dilate",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Inpaint Erode Or Dilate",
          "description": "Positive value will make white area in the mask larger, negative value will make white area smaller. (default is 0, always process before any mask invert)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Fooocus Upscale Or Vary",
      "description": "Default parameters with automated optimizations and quality improvements.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.FooocusUpscaleOrVary",
      "properties": [
        {
          "name": "styles",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Styles",
          "description": "The style to use."
        },
        {
          "name": "uov_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Uov Image Url",
          "description": "The image to upscale or vary."
        },
        {
          "name": "performance",
          "type": {
            "type": "enum",
            "values": [
              "Speed",
              "Quality",
              "Extreme Speed",
              "Lightning"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FooocusUpscaleOrVary.Performance"
          },
          "default": "Extreme Speed",
          "title": "Performance",
          "description": "You can choose Speed or Quality"
        },
        {
          "name": "mixing_image_prompt_and_vary_upscale",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Mixing Image Prompt And Vary Upscale",
          "description": "Mixing Image Prompt and Vary/Upscale"
        },
        {
          "name": "image_prompt_3",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Prompt 3"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 5 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "image_prompt_4",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Prompt 4"
        },
        {
          "name": "image_prompt_1",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Prompt 1"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to false, the safety checker will be disabled."
        },
        {
          "name": "sharpness",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Sharpness",
          "description": "The sharpness of the generated image. Use it to control how sharp the generated image should be. Higher value means image and texture are sharper."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "1024x1024",
          "title": "Aspect Ratio",
          "description": "The size of the generated image. You can choose between some presets or custom height and width that **must be multiples of 8**."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate in one request"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FooocusUpscaleOrVary.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "refiner_model",
          "type": {
            "type": "enum",
            "values": [
              "None",
              "realisticVisionV60B1_v51VAE.safetensors"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FooocusUpscaleOrVary.RefinerModel"
          },
          "default": "None",
          "title": "Refiner Model",
          "description": "Refiner (SDXL or SD 1.5)"
        },
        {
          "name": "image_prompt_2",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Prompt 2"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "uov_method",
          "type": {
            "type": "enum",
            "values": [
              "Disabled",
              "Vary (Subtle)",
              "Vary (Strong)",
              "Upscale (1.5x)",
              "Upscale (2x)",
              "Upscale (Fast 2x)"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.FooocusUpscaleOrVary.UovMethod"
          },
          "default": "Vary (Strong)",
          "title": "Uov Method",
          "description": "The method to use for upscaling or varying."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "refiner_switch",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Refiner Switch",
          "description": "Use 0.4 for SD1.5 realistic models; 0.667 for SD1.5 anime models 0.8 for XL-refiners; or any value for switching two SDXL models."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Gemini 25 Flash Image",
      "description": "Gemini 2.5 Flash Image\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Gemini25FlashImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to generate an image from."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "3:2",
              "4:3",
              "5:4",
              "1:1",
              "4:5",
              "3:4",
              "2:3",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Gemini25FlashImage.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Gemini25FlashImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "limit_generations",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Limit Generations",
          "description": "Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Gemini 3 Pro Image Preview",
      "description": "Gemini 3 Pro Image Preview\n    generation, text-to-image, txt2img, ai-art, professional\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Gemini3ProImagePreview",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "enable_web_search",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Web Search",
          "description": "Enable web search for the image generation task. This will allow the model to use the latest information from the web to generate the image."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image. Use \"auto\" to let the model decide based on the prompt."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "1K",
              "2K",
              "4K"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Gemini3ProImagePreview.Resolution"
          },
          "default": "1K",
          "title": "Resolution",
          "description": "The resolution of the image to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Gemini3ProImagePreview.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Gemini3ProImagePreview.SafetyTolerance"
          },
          "default": "4",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for content moderation. 1 is the most strict (blocks most content), 6 is the least strict."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "limit_generations",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Limit Generations",
          "description": "Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Glm Image",
      "description": "GLM Image generates images from text with advanced AI understanding and quality output.\n    image, generation, glm, ai, text-to-image\n\n    Use cases:\n    - Generate images with GLM AI\n    - Create intelligent visual content\n    - Produce AI-powered artwork\n    - Generate images with understanding\n    - Create smart visuals from text",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.GlmImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for image generation."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "Output image size."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GlmImage.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "Output image format."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If True, the image will be returned as a base64 data URI instead of a URL."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values make the model follow the prompt more closely."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. The same seed with the same prompt will produce the same image."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "If True, the prompt will be enhanced using an LLM for more detailed and higher quality results."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of diffusion denoising steps. More steps generally produce higher quality images."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable NSFW safety checking on the generated images."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Gpt Image 15",
      "description": "GPT Image 1.5 generates images from text with GPT-powered language understanding and visual creation.\n    image, generation, gpt, language-ai, text-to-image\n\n    Use cases:\n    - Generate images with GPT understanding\n    - Create language-aware visual content\n    - Produce intelligent artwork\n    - Generate images with natural language\n    - Create GPT-powered visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.GptImage15",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "1024x1024",
              "1536x1024",
              "1024x1536"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage15.ImageSize"
          },
          "default": "1024x1024",
          "title": "Image Size",
          "description": "Aspect ratio for the generated image"
        },
        {
          "name": "background",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "transparent",
              "opaque"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage15.Background"
          },
          "default": "auto",
          "title": "Background",
          "description": "Background for the generated image"
        },
        {
          "name": "quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage15.Quality"
          },
          "default": "high",
          "title": "Quality",
          "description": "Quality for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage15.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the images"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Gpt Image 1 Mini",
      "description": "GPT Image 1 Mini\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.GptImage1Mini",
      "properties": [
        {
          "name": "background",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "transparent",
              "opaque"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage1Mini.Background"
          },
          "default": "auto",
          "title": "Background",
          "description": "Background for the generated image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "1024x1024",
              "1536x1024",
              "1024x1536"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage1Mini.ImageSize"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "Aspect ratio for the generated image"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation"
        },
        {
          "name": "quality",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "low",
              "medium",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage1Mini.Quality"
          },
          "default": "auto",
          "title": "Quality",
          "description": "Quality for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage1Mini.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the images"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Gpt Image 1 Text To Image",
      "description": "OpenAI's latest image generation and editing model: gpt-1-image.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.GptImage1TextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "1024x1024",
              "1536x1024",
              "1024x1536"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage1TextToImage.ImageSize"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "Aspect ratio for the generated image"
        },
        {
          "name": "background",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "transparent",
              "opaque"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage1TextToImage.Background"
          },
          "default": "auto",
          "title": "Background",
          "description": "Background for the generated image"
        },
        {
          "name": "quality",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "low",
              "medium",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage1TextToImage.Quality"
          },
          "default": "auto",
          "title": "Quality",
          "description": "Quality for the generated image"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.GptImage1TextToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the images"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Image V21 Text To Image",
      "description": "Hunyuan Image\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.HunyuanImageV21TextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The desired size of the generated image."
        },
        {
          "name": "use_reprompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Reprompt",
          "description": "Enable prompt enhancement for potentially better results."
        },
        {
          "name": "use_refiner",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Refiner",
          "description": "Enable the refiner model for improved image quality."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.HunyuanImageV21TextToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Controls how much the model adheres to the prompt. Higher values mean stricter adherence."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible results. If None, a random seed is used."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide the image generation away from certain concepts."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Image V3 Instruct Text To Image",
      "description": "Hunyuan Image v3 Instruct generates high-quality images from text with advanced instruction understanding.\n    image, generation, hunyuan, v3, instruct, text-to-image\n\n    Use cases:\n    - Generate images with detailed instructions\n    - Create artwork with precise text control\n    - Produce high-quality visual content\n    - Generate images with advanced understanding\n    - Create professional visuals from text",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.HunyuanImageV3InstructTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Image Size",
          "description": "The desired size of the generated image. If auto, image size will be determined by the model."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.HunyuanImageV3InstructTextToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible results. If None, a random seed is used."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Controls how much the model adheres to the prompt. Higher values mean stricter adherence."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Image V3 Text To Image",
      "description": "Hunyuan Image\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.HunyuanImageV3TextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt for image-to-image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The desired size of the generated image."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.HunyuanImageV3TextToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "Controls how much the model adheres to the prompt. Higher values mean stricter adherence."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible results. If None, a random seed is used."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide the image generation away from certain concepts."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ideogram V2",
      "description": "Ideogram V2 is a state-of-the-art image generation model optimized for commercial and creative use, featuring exceptional typography handling and realistic outputs.\n    image, generation, ai, typography, realistic, text-to-image, txt2img\n\n    Use cases:\n    - Create commercial artwork and designs\n    - Generate realistic product visualizations\n    - Design marketing materials with text\n    - Produce high-quality illustrations\n    - Create brand assets and logos",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.IdeogramV2",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "10:16",
              "16:10",
              "9:16",
              "16:9",
              "4:3",
              "3:4",
              "1:1",
              "1:3",
              "3:1",
              "3:2",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.IdeogramV2.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "general",
              "realistic",
              "design",
              "render_3D",
              "anime"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.IdeogramV2.Style"
          },
          "default": "auto",
          "title": "Style",
          "description": "The style of the generated image"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt with MagicPrompt functionality"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to avoid in the generated image"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "aspect_ratio",
        "style"
      ]
    },
    {
      "title": "Ideogram V2 Turbo",
      "description": "Ideogram V2 Turbo offers faster image generation with the same exceptional quality and typography handling as V2.\n    image, generation, ai, typography, realistic, fast, text-to-image, txt2img\n\n    Use cases:\n    - Rapidly generate commercial designs\n    - Quick iteration on marketing materials\n    - Fast prototyping of visual concepts\n    - Real-time design exploration\n    - Efficient batch generation of branded content",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.IdeogramV2Turbo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "10:16",
              "16:10",
              "9:16",
              "16:9",
              "4:3",
              "3:4",
              "1:1",
              "1:3",
              "3:1",
              "3:2",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.IdeogramV2Turbo.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "general",
              "realistic",
              "design",
              "render_3D",
              "anime"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.IdeogramV2Turbo.Style"
          },
          "default": "auto",
          "title": "Style",
          "description": "The style of the generated image"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt with MagicPrompt functionality"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to avoid in the generated image"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "aspect_ratio",
        "style"
      ]
    },
    {
      "title": "Ideogram V2 a",
      "description": "Generate high-quality images, posters, and logos with Ideogram V2A. Features exceptional typography handling and realistic outputs optimized for commercial and creative use.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.IdeogramV2a",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "10:16",
              "16:10",
              "9:16",
              "16:9",
              "4:3",
              "3:4",
              "1:1",
              "1:3",
              "3:1",
              "3:2",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.IdeogramV2a.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "general",
              "realistic",
              "design",
              "render_3D",
              "anime"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.IdeogramV2a.Style"
          },
          "default": "auto",
          "title": "Style",
          "description": "The style of the generated image"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt with MagicPrompt functionality."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ideogram V2 a Turbo",
      "description": "Accelerated image generation with Ideogram V2A Turbo. Create high-quality visuals, posters, and logos with enhanced speed while maintaining Ideogram's signature quality.\n    generation, text-to-image, txt2img, ai-art, fast\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.IdeogramV2aTurbo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "10:16",
              "16:10",
              "9:16",
              "16:9",
              "4:3",
              "3:4",
              "1:1",
              "1:3",
              "3:1",
              "3:2",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.IdeogramV2aTurbo.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "general",
              "realistic",
              "design",
              "render_3D",
              "anime"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.IdeogramV2aTurbo.Style"
          },
          "default": "auto",
          "title": "Style",
          "description": "The style of the generated image"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt with MagicPrompt functionality."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ideogram V3",
      "description": "Ideogram V3 is the latest generation with enhanced text rendering, superior image quality, and expanded creative controls.\n    image, generation, ideogram, typography, text-rendering, text-to-image, txt2img\n\n    Use cases:\n    - Create professional graphics with embedded text\n    - Design social media posts with perfect typography\n    - Generate logos and brand identities\n    - Produce marketing materials with text overlays\n    - Create educational content with clear text",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.IdeogramV3",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The resolution of the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style",
          "description": "The style preset for the generated image"
        },
        {
          "name": "style_preset",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Preset",
          "description": "Style preset for generation. The chosen style preset will guide the generation."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Automatically enhance the prompt for better results"
        },
        {
          "name": "rendering_speed",
          "type": {
            "type": "enum",
            "values": [
              "TURBO",
              "BALANCED",
              "QUALITY"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.IdeogramV3.RenderingSpeed"
          },
          "default": "BALANCED",
          "title": "Rendering Speed",
          "description": "The rendering speed to use."
        },
        {
          "name": "style_codes",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Codes",
          "description": "A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style"
        },
        {
          "name": "color_palette",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Color Palette",
          "description": "A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the random number generator"
        },
        {
          "name": "image_urls",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Urls",
          "description": "A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "aspect_ratio",
        "style"
      ]
    },
    {
      "title": "Illusion Diffusion",
      "description": "Create illusions conditioned on image.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.IllusionDiffusion",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image. You can choose between some presets or custom height and width that **must be multiples of 8**."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale of the ControlNet."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Input image url."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPM++ Karras SDE",
              "Euler"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.IllusionDiffusion.Scheduler"
          },
          "default": "Euler",
          "title": "Scheduler",
          "description": "Scheduler / sampler to use for the image denoising process."
        },
        {
          "name": "control_guidance_start",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Control Guidance Start"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed of the generated Image. It will be the same value of the one passed in the input or the randomly generated that was used in case none was passed."
        },
        {
          "name": "control_guidance_end",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Control Guidance End"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "Increasing the amount of steps tells Stable Diffusion that it should take more steps to generate your final result which can increase the amount of detail in your image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Imagen 3",
      "description": "Imagen3 is a high-quality text-to-image model that generates realistic images from text prompts.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Imagen3",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing what you want to see"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "16:9",
              "9:16",
              "3:4",
              "4:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Imagen3.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate (1-4)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A description of what to discourage in the generated images"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Imagen 3 Fast",
      "description": "Imagen3 Fast is a high-quality text-to-image model that generates realistic images from text prompts.\n    generation, text-to-image, txt2img, ai-art, fast\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Imagen3Fast",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing what you want to see"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "16:9",
              "9:16",
              "3:4",
              "4:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Imagen3Fast.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate (1-4)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A description of what to discourage in the generated images"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Imagen 4 Preview Ultra",
      "description": "Google\u2019s highest quality image generation model\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Imagen4PreviewUltra",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "16:9",
              "9:16",
              "4:3",
              "3:4"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Imagen4PreviewUltra.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "1K",
              "2K"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Imagen4PreviewUltra.Resolution"
          },
          "default": "1K",
          "title": "Resolution",
          "description": "The resolution of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Imagen4PreviewUltra.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Imagineart Imagineart 15 Preview Text To Image",
      "description": "Imagineart 1.5 Preview\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.ImagineartImagineart15PreviewTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the desired image"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "3:1",
              "1:3",
              "3:2",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ImagineartImagineart15PreviewTextToImage.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "Image aspect ratio: 1:1, 3:1, 1:3, 16:9, 9:16, 4:3, 3:4, 3:2, 2:3"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the image generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Imagineart Imagineart 15 Pro Preview Text To Image",
      "description": "ImagineArt 1.5 Pro Preview\n    generation, text-to-image, txt2img, ai-art, professional\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.ImagineartImagineart15ProPreviewTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the desired image"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "3:1",
              "1:3",
              "3:2",
              "2:3"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ImagineartImagineart15ProPreviewTextToImage.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "Image aspect ratio: 1:1, 3:1, 1:3, 16:9, 9:16, 4:3, 3:4, 3:2, 2:3"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the image generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Janus",
      "description": "DeepSeek Janus-Pro is a novel text-to-image model that unifies multimodal understanding and generation through an autoregressive framework\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Janus",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate in parallel."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "cfg_weight",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Cfg Weight",
          "description": "Classifier Free Guidance scale - how closely to follow the prompt."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Temperature",
          "description": "Controls randomness in the generation. Higher values make output more random."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Kolors",
      "description": "Photorealistic Text-to-Image\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Kolors",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Kolors.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "EulerDiscreteScheduler",
              "EulerAncestralDiscreteScheduler",
              "DPMSolverMultistepScheduler",
              "DPMSolverMultistepScheduler_SDE_karras",
              "UniPCMultistepScheduler",
              "DEISMultistepScheduler"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Kolors.Scheduler"
          },
          "default": "EulerDiscreteScheduler",
          "title": "Scheduler",
          "description": "The scheduler to use for the model."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable safety checker."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Layer Diffusion",
      "description": "SDXL with an alpha channel.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.LayerDiffusion",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 8,
          "title": "Guidance Scale",
          "description": "The guidance scale for the model."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 20,
          "title": "Num Inference Steps",
          "description": "The number of inference steps for the model."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "text, watermark",
          "title": "Negative Prompt",
          "description": "The prompt to use for generating the negative image. Be as descriptive as possible for best results."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to false, the safety checker will be disabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Lcm",
      "description": "Produce high-quality images with minimal inference steps.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Lcm",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "controlnet_inpaint",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Controlnet Inpaint",
          "description": "If set to true, the inpainting pipeline will use controlnet inpainting. Only effective for inpainting pipelines."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. You can choose between some presets or custom height and width that **must be multiples of 8**. If not provided: - For text-to-image generations, the default size is 512x512. - For image-to-image generations, the default size is the same as the input image. - For inpainting generations, the default size is the same as the input image."
        },
        {
          "name": "enable_safety_checks",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checks",
          "description": "If set to true, the resulting image will be checked whether it includes any potentially unsafe content. If it does, it will be replaced with a black image."
        },
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "sdxl",
              "sdv1-5"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Lcm.Model"
          },
          "default": "sdv1-5",
          "title": "Model",
          "description": "The model to use for generating the image."
        },
        {
          "name": "lora_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Lora Url",
          "description": "The url of the lora server to use for image generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "inpaint_mask_only",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Inpaint Mask Only",
          "description": "If set to true, the inpainting pipeline will only inpaint the provided mask area. Only effective for inpainting pipelines."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate. The function will return a list of images with the same prompt and negative prompt but different seeds."
        },
        {
          "name": "lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Lora Scale",
          "description": "The scale of the lora server to use for image generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The base image to use for guiding the image generation on image-to-image generations. If the either width or height of the image is larger than 1024 pixels, the image will be resized to 1024 pixels while keeping the aspect ratio."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "The strength of the image that is passed as `image_url`. The strength determines how much the generated image will be similar to the image passed as `image_url`. The higher the strength the more model gets \"creative\" and generates an image that's different from the initial image. A strength of 1.0 means that the initial image is more or less ignored and the model will try to generate an image that's as close as possible to the prompt."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "The mask to use for guiding the image generation on image inpainting. The model will focus on the mask area and try to fill it with the most relevant content. The mask must be a black and white image where the white area is the area that needs to be filled and the black area is the area that should be ignored. The mask must have the same dimensions as the image passed as `image_url`."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for generating the image. The more steps the better the image will be but it will also take longer to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Lightning Models",
      "description": "Collection of SDXL Lightning models.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.LightningModels",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size"
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The list of LoRA weights to use."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "DPM++ 2M",
              "DPM++ 2M Karras",
              "DPM++ 2M SDE",
              "DPM++ 2M SDE Karras",
              "DPM++ SDE",
              "DPM++ SDE Karras",
              "KDPM 2A",
              "Euler",
              "Euler (trailing timesteps)",
              "Euler A",
              "LCM",
              "EDMDPMSolverMultistepScheduler",
              "TCDScheduler"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.LightningModels.Scheduler"
          },
          "default": null,
          "title": "Scheduler",
          "description": "Scheduler / sampler to use for the image denoising process."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed, grayscale, bw, bad photo, bad photography, bad art:1.4), (watermark, signature, text font, username, error, logo, words, letters, digits, autograph, trademark, name:1.2), (blur, blurry, grainy), morbid, ugly, asymmetrical, mutated malformed, mutilated, poorly lit, bad shadow, draft, cropped, out of frame, cut off, censored, jpeg artifacts, out of focus, glitch, duplicate, (airbrushed, cartoon, anime, semi-realistic, cgi, render, blender, digital art, manga, amateur:1.3), (3D ,3D Game, 3D Game Scene, 3D Character:1.1), (bad hands, bad anatomy, bad body, bad face, bad teeth, bad arms, bad legs, deformities:1.3)",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.LightningModels.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "model_name",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model Name",
          "description": "The Lightning model to use."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.LightningModels.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Longcat Image",
      "description": "Longcat Image generates creative and unique images from text with distinctive AI characteristics.\n    image, generation, longcat, creative, text-to-image\n\n    Use cases:\n    - Generate creative images\n    - Create unique visual content\n    - Produce distinctive artwork\n    - Generate images with character\n    - Create artistic visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.LongcatImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.LongcatImage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.LongcatImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Lora",
      "description": "Run Any Stable Diffusion model with customizable LoRA weights.\n    generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Lora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image. You can choose between some presets or custom height and width that **must be multiples of 8**."
        },
        {
          "name": "tile_height",
          "type": {
            "type": "int"
          },
          "default": 4096,
          "title": "Tile Height",
          "description": "The size of the tiles to be used for the image generation."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The embeddings to use for the image generation. Only a single embedding is supported at the moment. The embeddings will be used to map the tokens in the prompt to the embedding weights."
        },
        {
          "name": "ic_light_model_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ic Light Model Url",
          "description": "The URL of the IC Light model to use for the image generation."
        },
        {
          "name": "image_encoder_weight_name",
          "type": {
            "type": "str"
          },
          "default": "pytorch_model.bin",
          "title": "Image Encoder Weight Name",
          "description": "The weight name of the image encoder model to use for the image generation."
        },
        {
          "name": "ip_adapter",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Ip Adapter",
          "description": "The IP adapter to use for the image generation."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "DPM++ 2M",
              "DPM++ 2M Karras",
              "DPM++ 2M SDE",
              "DPM++ 2M SDE Karras",
              "Euler",
              "Euler A",
              "Euler (trailing timesteps)",
              "LCM",
              "LCM (trailing timesteps)",
              "DDIM",
              "TCD"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Lora.Scheduler"
          },
          "default": null,
          "title": "Scheduler",
          "description": "Scheduler / sampler to use for the image denoising process."
        },
        {
          "name": "sigmas",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Sigmas",
          "description": "Optionally override the sigmas to use for the denoising process. Only works with schedulers which support the `sigmas` argument in their `set_sigmas` method. Defaults to not overriding, in which case the scheduler automatically sets the sigmas based on the `num_inference_steps` parameter. If set to a custom sigma schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `timesteps` is set."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "tile_stride_width",
          "type": {
            "type": "int"
          },
          "default": 2048,
          "title": "Tile Stride Width",
          "description": "The stride of the tiles to be used for the image generation."
        },
        {
          "name": "debug_per_pass_latents",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Debug Per Pass Latents",
          "description": "If set to true, the latents will be saved for debugging per pass."
        },
        {
          "name": "timesteps",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Timesteps",
          "description": "Optionally override the timesteps to use for the denoising process. Only works with schedulers which support the `timesteps` argument in their `set_timesteps` method. Defaults to not overriding, in which case the scheduler automatically sets the timesteps based on the `num_inference_steps` parameter. If set to a custom timestep schedule, the `num_inference_steps` parameter will be ignored. Cannot be set if `sigmas` is set."
        },
        {
          "name": "image_encoder_subfolder",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Encoder Subfolder",
          "description": "The subfolder of the image encoder model to use for the image generation."
        },
        {
          "name": "prompt_weighting",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Prompt Weighting",
          "description": "If set to true, the prompt weighting syntax will be used. Additionally, this will lift the 77 token limit by averaging embeddings."
        },
        {
          "name": "variant",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Variant",
          "description": "The variant of the model to use for huggingface models, e.g. 'fp16'."
        },
        {
          "name": "model_name",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model Name",
          "description": "URL or HuggingFace ID of the base model to generate the image."
        },
        {
          "name": "controlnet_guess_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Controlnet Guess Mode",
          "description": "If set to true, the controlnet will be applied to only the conditional predictions."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "ic_light_model_background_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ic Light Model Background Image Url",
          "description": "The URL of the IC Light model background image to use for the image generation. Make sure to use a background compatible with the model."
        },
        {
          "name": "rescale_betas_snr_zero",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Rescale Betas Snr Zero",
          "description": "Whether to set the rescale_betas_snr_zero option or not for the sampler"
        },
        {
          "name": "tile_width",
          "type": {
            "type": "int"
          },
          "default": 4096,
          "title": "Tile Width",
          "description": "The size of the tiles to be used for the image generation."
        },
        {
          "name": "prediction_type",
          "type": {
            "type": "enum",
            "values": [
              "v_prediction",
              "epsilon"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Lora.PredictionType"
          },
          "default": "epsilon",
          "title": "Prediction Type",
          "description": "The type of prediction to use for the image generation. The `epsilon` is the default."
        },
        {
          "name": "eta",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Eta",
          "description": "The eta value to be used for the image generation."
        },
        {
          "name": "image_encoder_path",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Encoder Path",
          "description": "The path to the image encoder model to use for the image generation."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use.Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "image_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Lora.ImageFormat"
          },
          "default": "png",
          "title": "Image Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate in one request. Note that the higher the batch size, the longer it will take to generate the images."
        },
        {
          "name": "debug_latents",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Debug Latents",
          "description": "If set to true, the latents will be saved for debugging."
        },
        {
          "name": "ic_light_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ic Light Image Url",
          "description": "The URL of the IC Light model image to use for the image generation."
        },
        {
          "name": "unet_name",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Unet Name",
          "description": "URL or HuggingFace ID of the custom U-Net model to use for the image generation."
        },
        {
          "name": "clip_skip",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Clip Skip",
          "description": "Skips part of the image generation process, leading to slightly different results. This means the image renders faster, too."
        },
        {
          "name": "tile_stride_height",
          "type": {
            "type": "int"
          },
          "default": 2048,
          "title": "Tile Stride Height",
          "description": "The stride of the tiles to be used for the image generation."
        },
        {
          "name": "controlnets",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Controlnets",
          "description": "The control nets to use for the image generation. You can use any number of control nets and they will be applied to the image at the specified timesteps."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Increasing the amount of steps tells Stable Diffusion that it should take more steps to generate your final result which can increase the amount of detail in your image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Luma Photon Flash",
      "description": "Generate images from your prompts using Luma Photon Flash. Photon Flash is the most creative, personalizable, and intelligent visual models for creatives, bringing a step-function change in the cost of high-quality image generation.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.LumaPhotonFlash",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1",
              "4:3",
              "3:4",
              "21:9",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.LumaPhotonFlash.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Lumina Image V2",
      "description": "Lumina-Image-2.0 is a 2 billion parameter flow-based diffusion transforer which features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.LuminaImageV2",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "cfg_trunc_ratio",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Cfg Trunc Ratio",
          "description": "The ratio of the timestep interval to apply normalization-based guidance scale."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.LuminaImageV2.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "system_prompt",
          "type": {
            "type": "str"
          },
          "default": "You are an assistant designed to generate superior images with the superior degree of image-text alignment based on textual prompts or user prompts.",
          "title": "System Prompt",
          "description": "The system prompt to use."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "cfg_normalization",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Cfg Normalization",
          "description": "Whether to apply normalization-based guidance scale."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Minimax Image 01",
      "description": "Generate high quality images from text prompts using MiniMax Image-01. Longer text prompts will result in better quality images.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.MinimaxImage01",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Prompt Optimizer",
          "description": "Whether to enable automatic prompt optimization"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate (1-9)"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "16:9",
              "4:3",
              "3:2",
              "2:3",
              "3:4",
              "9:16",
              "21:9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.MinimaxImage01.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated image"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for image generation (max 1500 characters)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Nano Banana",
      "description": "Nano Banana\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.NanoBanana",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to generate an image from."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "3:2",
              "4:3",
              "5:4",
              "1:1",
              "4:5",
              "3:4",
              "2:3",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.NanoBanana.AspectRatio"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.NanoBanana.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "limit_generations",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Limit Generations",
          "description": "Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Nano Banana Pro",
      "description": "Nano Banana Pro\n    generation, text-to-image, txt2img, ai-art, professional\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.NanoBananaPro",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "enable_web_search",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Web Search",
          "description": "Enable web search for the image generation task. This will allow the model to use the latest information from the web to generate the image."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "1:1",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated image. Use \"auto\" to let the model decide based on the prompt."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "1K",
              "2K",
              "4K"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.NanoBananaPro.Resolution"
          },
          "default": "1K",
          "title": "Resolution",
          "description": "The resolution of the image to generate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.NanoBananaPro.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_tolerance",
          "type": {
            "type": "enum",
            "values": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "6"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.NanoBananaPro.SafetyTolerance"
          },
          "default": "4",
          "title": "Safety Tolerance",
          "description": "The safety tolerance level for content moderation. 1 is the most strict (blocks most content), 6 is the least strict."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "limit_generations",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Limit Generations",
          "description": "Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Omni Gen V1",
      "description": "OmniGen V1 is a versatile unified model for multi-modal image generation and editing with text, supporting complex compositional tasks.\n    image, generation, multi-modal, editing, unified, text-to-image, txt2img\n\n    Use cases:\n    - Generate images with multiple input modalities\n    - Edit existing images with text instructions\n    - Create complex compositional scenes\n    - Combine text and image inputs for generation\n    - Perform advanced image manipulations",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.OmniGenV1",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate or edit an image"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "img_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.6,
          "title": "Img Guidance Scale",
          "description": "The Image Guidance scale is a measure of how close you want the model to stick to your input image when looking for a related image to show you."
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "URL of images to use while generating the image, Use <img><|image_1|></img> for the first image and so on."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.OmniGenV1.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "How strictly to follow the prompt and inputs"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps for generation quality"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducible results. Use -1 for random"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "guidance_scale",
        "num_inference_steps"
      ]
    },
    {
      "title": "Omnigen V2",
      "description": "Omnigen V2\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.OmnigenV2",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate or edit an image. Use specific language like 'Add the bird from image 1 to the desk in image 2' for better results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "euler",
              "dpmsolver"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.OmnigenV2.Scheduler"
          },
          "default": "euler",
          "title": "Scheduler",
          "description": "The scheduler to use for the diffusion process."
        },
        {
          "name": "cfg_range_end",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Cfg Range End",
          "description": "CFG range end value."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(((deformed))), blurry, over saturation, bad anatomy, disfigured, poorly drawn face, mutation, mutated, (extra_limb), (ugly), (poorly drawn hands), fused fingers, messy drawing, broken legs censor, censored, censor_bar",
          "title": "Negative Prompt",
          "description": "Negative prompt to guide what should not be in the image."
        },
        {
          "name": "text_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Text Guidance Scale",
          "description": "The Text Guidance scale controls how closely the model follows the text prompt. Higher values make the model stick more closely to the prompt."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Image Guidance Scale",
          "description": "The Image Guidance scale controls how closely the model follows the input images. For image editing: 1.3-2.0, for in-context generation: 2.0-3.0"
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "URLs of input images to use for image editing or multi-image generation. Support up to 3 images."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.OmnigenV2.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "cfg_range_start",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Cfg Range Start",
          "description": "CFG range start value."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Ovis Image",
      "description": "Ovis Image\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.OvisImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.OvisImage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.OvisImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate an image from."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Piflow",
      "description": "Piflow\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Piflow",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image. You can choose between some presets or custom height and width that **must be multiples of 8**."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Piflow.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation. If set to None, a random seed will be used."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Pixart Sigma",
      "description": "Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.PixartSigma",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "(No style)",
              "Cinematic",
              "Photographic",
              "Anime",
              "Manga",
              "Digital Art",
              "Pixel art",
              "Fantasy art",
              "Neonpunk",
              "3D Model"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.PixartSigma.Style"
          },
          "default": "(No style)",
          "title": "Style",
          "description": "The style to apply to the image."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "DPM-SOLVER",
              "SA-SOLVER"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.PixartSigma.Scheduler"
          },
          "default": "DPM-SOLVER",
          "title": "Scheduler",
          "description": "The scheduler to use for the model."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 35,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Playground V25",
      "description": "State-of-the-art open-source model in aesthetic quality\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.PlaygroundV25",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "guidance_rescale",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Rescale",
          "description": "The rescale factor for the CFG."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.PlaygroundV25.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.PlaygroundV25.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Pony V7",
      "description": "Pony V7 is a finetuned text to image for superior aesthetics and prompt following.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.PonyV7",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate images from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.PonyV7.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "noise_source",
          "type": {
            "type": "enum",
            "values": [
              "gpu",
              "cpu"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.PonyV7.NoiseSource"
          },
          "default": "gpu",
          "title": "Noise Source",
          "description": "The source of the noise to use for generating images. If set to 'gpu', the noise will be generated on the GPU. If set to 'cpu', the noise will be generated on the CPU."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier free guidance scale"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to take"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for generating images"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Qwen Image",
      "description": "Qwen Image\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.QwenImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the image with"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.QwenImage.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "Acceleration level for image generation. Options: 'none', 'regular', 'high'. Higher acceleration increases speed. 'regular' balances speed and quality. 'high' is recommended for images without text."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.QwenImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "use_turbo",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Turbo",
          "description": "Enable turbo mode for faster generation with high quality. When enabled, uses optimized settings (10 steps, CFG=1.2)."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": " ",
          "title": "Negative Prompt",
          "description": "The negative prompt for the generation"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Qwen Image 2512",
      "description": "Qwen Image 2512 generates high-resolution images from text with excellent quality and detail.\n    image, generation, qwen, 2512, high-resolution, text-to-image\n\n    Use cases:\n    - Generate high-resolution images\n    - Create detailed visual content\n    - Produce quality artwork from text\n    - Generate images with fine details\n    - Create high-quality visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.QwenImage2512",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.QwenImage2512.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.QwenImage2512.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate an image from."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Qwen Image 2512 Lora",
      "description": "Qwen Image 2512 with LoRA support enables custom-trained models for specialized image generation.\n    image, generation, qwen, 2512, lora, custom\n\n    Use cases:\n    - Generate images with custom models\n    - Create specialized visual content\n    - Produce domain-specific artwork\n    - Generate images with fine-tuned models\n    - Create customized visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.QwenImage2512Lora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.QwenImage2512Lora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.QwenImage2512Lora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate an image from."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Qwen Image Max Text To Image",
      "description": "Qwen Image Max generates premium quality images from text with superior detail and accuracy.\n    image, generation, qwen, max, premium, text-to-image\n\n    Use cases:\n    - Generate premium quality images\n    - Create detailed artwork from text\n    - Produce high-fidelity visual content\n    - Generate professional-grade images\n    - Create superior quality visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.QwenImageMaxTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the desired image. Supports Chinese and English. Max 800 characters."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.QwenImageMaxTextToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Enable LLM prompt optimization for better results."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility (0-2147483647)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable content moderation for input and output."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Content to avoid in the generated image. Max 500 characters."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Realistic Vision",
      "description": "Generate realistic images.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.RealisticVision",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size"
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The list of LoRA weights to use."
        },
        {
          "name": "guidance_rescale",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Rescale",
          "description": "The rescale factor for the CFG."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed, grayscale, bw, bad photo, bad photography, bad art:1.4), (watermark, signature, text font, username, error, logo, words, letters, digits, autograph, trademark, name:1.2), (blur, blurry, grainy), morbid, ugly, asymmetrical, mutated malformed, mutilated, poorly lit, bad shadow, draft, cropped, out of frame, cut off, censored, jpeg artifacts, out of focus, glitch, duplicate, (airbrushed, cartoon, anime, semi-realistic, cgi, render, blender, digital art, manga, amateur:1.3), (3D ,3D Game, 3D Game Scene, 3D Character:1.1), (bad hands, bad anatomy, bad body, bad face, bad teeth, bad arms, bad legs, deformities:1.3)",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.RealisticVision.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "model_name",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model Name",
          "description": "The Realistic Vision model to use."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.RealisticVision.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 35,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Recraft 20 b",
      "description": "Recraft 20b is a new and affordable text-to-image model.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Recraft20b",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "colors",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Colors",
          "description": "An array of preferable colors"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "any",
              "realistic_image",
              "digital_illustration",
              "vector_illustration",
              "realistic_image/b_and_w",
              "realistic_image/enterprise",
              "realistic_image/hard_flash",
              "realistic_image/hdr",
              "realistic_image/motion_blur",
              "realistic_image/natural_light",
              "realistic_image/studio_portrait",
              "digital_illustration/2d_art_poster",
              "digital_illustration/2d_art_poster_2",
              "digital_illustration/3d",
              "digital_illustration/80s",
              "digital_illustration/engraving_color",
              "digital_illustration/glow",
              "digital_illustration/grain",
              "digital_illustration/hand_drawn",
              "digital_illustration/hand_drawn_outline",
              "digital_illustration/handmade_3d",
              "digital_illustration/infantile_sketch",
              "digital_illustration/kawaii",
              "digital_illustration/pixel_art",
              "digital_illustration/psychedelic",
              "digital_illustration/seamless",
              "digital_illustration/voxel",
              "digital_illustration/watercolor",
              "vector_illustration/cartoon",
              "vector_illustration/doodle_line_art",
              "vector_illustration/engraving",
              "vector_illustration/flat_2",
              "vector_illustration/kawaii",
              "vector_illustration/line_art",
              "vector_illustration/line_circuit",
              "vector_illustration/linocut",
              "vector_illustration/seamless"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Recraft20b.Style"
          },
          "default": "realistic_image",
          "title": "Style",
          "description": "The style of the generated images. Vector images cost 2X as much."
        },
        {
          "name": "style_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Id",
          "description": "The ID of the custom style reference (optional)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Recraft V3",
      "description": "Recraft V3 is a powerful image generation model with exceptional control over style and colors, ideal for brand consistency and design work.\n    image, generation, design, branding, style, text-to-image, txt2img\n\n    Use cases:\n    - Create brand-consistent visual assets\n    - Generate designs with specific color palettes\n    - Produce stylized illustrations and artwork\n    - Design marketing materials with brand colors\n    - Create cohesive visual content series",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.RecraftV3",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "values": [
              "any",
              "realistic_image",
              "digital_illustration",
              "vector_illustration",
              "realistic_image/b_and_w",
              "realistic_image/hard_flash",
              "realistic_image/hdr",
              "realistic_image/natural_light",
              "realistic_image/studio_portrait",
              "realistic_image/enterprise",
              "realistic_image/motion_blur",
              "realistic_image/evening_light",
              "realistic_image/faded_nostalgia",
              "realistic_image/forest_life",
              "realistic_image/mystic_naturalism",
              "realistic_image/natural_tones",
              "realistic_image/organic_calm",
              "realistic_image/real_life_glow",
              "realistic_image/retro_realism",
              "realistic_image/retro_snapshot",
              "realistic_image/urban_drama",
              "realistic_image/village_realism",
              "realistic_image/warm_folk",
              "digital_illustration/pixel_art",
              "digital_illustration/hand_drawn",
              "digital_illustration/grain",
              "digital_illustration/infantile_sketch",
              "digital_illustration/2d_art_poster",
              "digital_illustration/handmade_3d",
              "digital_illustration/hand_drawn_outline",
              "digital_illustration/engraving_color",
              "digital_illustration/2d_art_poster_2",
              "digital_illustration/antiquarian",
              "digital_illustration/bold_fantasy",
              "digital_illustration/child_book",
              "digital_illustration/child_books",
              "digital_illustration/cover",
              "digital_illustration/crosshatch",
              "digital_illustration/digital_engraving",
              "digital_illustration/expressionism",
              "digital_illustration/freehand_details",
              "digital_illustration/grain_20",
              "digital_illustration/graphic_intensity",
              "digital_illustration/hard_comics",
              "digital_illustration/long_shadow",
              "digital_illustration/modern_folk",
              "digital_illustration/multicolor",
              "digital_illustration/neon_calm",
              "digital_illustration/noir",
              "digital_illustration/nostalgic_pastel",
              "digital_illustration/outline_details",
              "digital_illustration/pastel_gradient",
              "digital_illustration/pastel_sketch",
              "digital_illustration/pop_art",
              "digital_illustration/pop_renaissance",
              "digital_illustration/street_art",
              "digital_illustration/tablet_sketch",
              "digital_illustration/urban_glow",
              "digital_illustration/urban_sketching",
              "digital_illustration/vanilla_dreams",
              "digital_illustration/young_adult_book",
              "digital_illustration/young_adult_book_2",
              "vector_illustration/bold_stroke",
              "vector_illustration/chemistry",
              "vector_illustration/colored_stencil",
              "vector_illustration/contour_pop_art",
              "vector_illustration/cosmics",
              "vector_illustration/cutout",
              "vector_illustration/depressive",
              "vector_illustration/editorial",
              "vector_illustration/emotional_flat",
              "vector_illustration/infographical",
              "vector_illustration/marker_outline",
              "vector_illustration/mosaic",
              "vector_illustration/naivector",
              "vector_illustration/roundish_flat",
              "vector_illustration/segmented_colors",
              "vector_illustration/sharp_contrast",
              "vector_illustration/thin",
              "vector_illustration/vector_photo",
              "vector_illustration/vivid_shapes",
              "vector_illustration/engraving",
              "vector_illustration/line_art",
              "vector_illustration/line_circuit",
              "vector_illustration/linocut"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.RecraftV3.RecraftV3Style"
          },
          "default": "realistic_image",
          "title": "Style",
          "description": "Visual style preset for the generated image"
        },
        {
          "name": "colors",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Colors",
          "description": "Specific color palette to use in the generation"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "style_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style Id",
          "description": "Custom style ID for brand-specific styles"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "style",
        "colors"
      ]
    },
    {
      "title": "Reve Text To Image",
      "description": "Reve\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.ReveTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text description of the desired image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "3:2",
              "2:3",
              "4:3",
              "3:4",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ReveTextToImage.AspectRatio"
          },
          "default": "3:2",
          "title": "Aspect Ratio",
          "description": "The desired aspect ratio of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ReveTextToImage.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "Output format for the generated image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Rundiffusion Fal Juggernaut Flux Base",
      "description": "Juggernaut Base Flux by RunDiffusion is a drop-in replacement for Flux [Dev] that delivers sharper details, richer colors, and enhanced realism, while instantly boosting LoRAs and LyCORIS with full compatibility.\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.RundiffusionFalJuggernautFluxBase",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.RundiffusionFalJuggernautFluxBase.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Rundiffusion Fal Juggernaut Flux Lightning",
      "description": "Juggernaut Lightning Flux by RunDiffusion provides blazing-fast, high-quality images rendered at five times the speed of Flux. Perfect for mood boards and mass ideation, this model excels in both realism and prompt adherence.\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.RundiffusionFalJuggernautFluxLightning",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.RundiffusionFalJuggernautFluxLightning.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Rundiffusion Fal Juggernaut Flux Lora",
      "description": "Juggernaut Base Flux LoRA by RunDiffusion is a drop-in replacement for Flux [Dev] that delivers sharper details, richer colors, and enhanced realism to all your LoRAs and LyCORIS with full compatibility.\n    flux, generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.RundiffusionFalJuggernautFluxLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.RundiffusionFalJuggernautFluxLora.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Rundiffusion Fal Juggernaut Flux Pro",
      "description": "Juggernaut Pro Flux by RunDiffusion is the flagship Juggernaut model rivaling some of the most advanced image models available, often surpassing them in realism. It combines Juggernaut Base with RunDiffusion Photo and features enhancements like reduced background blurriness.\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.RundiffusionFalJuggernautFluxPro",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.RundiffusionFalJuggernautFluxPro.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Rundiffusion Fal Rundiffusion Photo Flux",
      "description": "RunDiffusion Photo Flux provides insane realism. With this enhancer, textures and skin details burst to life, turning your favorite prompts into vivid, lifelike creations. Recommended to keep it at 0.65 to 0.80 weight. Supports resolutions up to 1536x1536.\n    flux, generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.RundiffusionFalRundiffusionPhotoFlux",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.RundiffusionFalRundiffusionPhotoFlux.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "photo_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Photo Lora Scale",
          "description": "LoRA Scale of the photo lora model"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Sana",
      "description": "Sana is an efficient high-resolution image generation model that balances quality and speed for practical applications.\n    image, generation, efficient, high-resolution, text-to-image, txt2img\n\n    Use cases:\n    - Generate high-resolution images efficiently\n    - Create detailed artwork with good performance\n    - Produce quality visuals with limited compute\n    - Generate images for web and mobile applications\n    - Balanced quality-speed image production",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Sana",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "enum",
            "values": [
              "square_hd",
              "square",
              "portrait_4_3",
              "portrait_16_9",
              "landscape_4_3",
              "landscape_16_9"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ImageSizePreset"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "Size preset for the generated image"
        },
        {
          "name": "style_name",
          "type": {
            "type": "enum",
            "values": [
              "(No style)",
              "Cinematic",
              "Photographic",
              "Anime",
              "Manga",
              "Digital Art",
              "Pixel art",
              "Fantasy art",
              "Neonpunk",
              "3D Model"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Sana.StyleName"
          },
          "default": "(No style)",
          "title": "Style Name",
          "description": "The style to generate the image in."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Sana.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "How strictly to follow the prompt"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 18,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Elements to avoid in the generated image"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "image_size",
        "guidance_scale"
      ]
    },
    {
      "title": "Sana Sprint",
      "description": "Sana Sprint is a text-to-image model capable of generating 4K images with exceptional speed.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.SanaSprint",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "style_name",
          "type": {
            "type": "enum",
            "values": [
              "(No style)",
              "Cinematic",
              "Photographic",
              "Anime",
              "Manga",
              "Digital Art",
              "Pixel art",
              "Fantasy art",
              "Neonpunk",
              "3D Model"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.SanaSprint.StyleName"
          },
          "default": "(No style)",
          "title": "Style Name",
          "description": "The style to generate the image in."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.SanaSprint.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 2,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Sana V1516 b",
      "description": "Sana v1.5 1.6B is a lightweight text-to-image model that delivers 4K image generation with impressive efficiency.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.SanaV1516b",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "style_name",
          "type": {
            "type": "enum",
            "values": [
              "(No style)",
              "Cinematic",
              "Photographic",
              "Anime",
              "Manga",
              "Digital Art",
              "Pixel art",
              "Fantasy art",
              "Neonpunk",
              "3D Model"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.SanaV1516b.StyleName"
          },
          "default": "(No style)",
          "title": "Style Name",
          "description": "The style to generate the image in."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.SanaV1516b.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 18,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Sana V1548 b",
      "description": "Sana v1.5 4.8B is a powerful text-to-image model that generates ultra-high quality 4K images with remarkable detail.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.SanaV1548b",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "style_name",
          "type": {
            "type": "enum",
            "values": [
              "(No style)",
              "Cinematic",
              "Photographic",
              "Anime",
              "Manga",
              "Digital Art",
              "Pixel art",
              "Fantasy art",
              "Neonpunk",
              "3D Model"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.SanaV1548b.StyleName"
          },
          "default": "(No style)",
          "title": "Style Name",
          "description": "The style to generate the image in."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.SanaV1548b.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 18,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Sdxl Controlnet Union",
      "description": "An efficent SDXL multi-controlnet text-to-image model.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.SdxlControlnetUnion",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "depth_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Depth Preprocess",
          "description": "Whether to preprocess the depth image."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Leave it none to automatically infer from the control image."
        },
        {
          "name": "normal_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Normal Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "teed_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Teed Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The list of LoRA weights to use."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "canny_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Canny Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "segmentation_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Segmentation Preprocess",
          "description": "Whether to preprocess the segmentation image."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.SdxlControlnetUnion.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "segmentation_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Segmentation Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "openpose_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Openpose Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "canny_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Canny Preprocess",
          "description": "Whether to preprocess the canny image."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "depth_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Depth Image Url",
          "description": "The URL of the control image."
        },
        {
          "name": "normal_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Normal Preprocess",
          "description": "Whether to preprocess the normal image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "teed_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Teed Preprocess",
          "description": "Whether to preprocess the teed image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "controlnet_conditioning_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Controlnet Conditioning Scale",
          "description": "The scale of the controlnet conditioning."
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.SdxlControlnetUnion.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "openpose_preprocess",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Openpose Preprocess",
          "description": "Whether to preprocess the openpose image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 35,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Sky Raccoon",
      "description": "Sky Raccoon\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.SkyRaccoon",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "turbo_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Turbo Mode",
          "description": "If true, the video will be generated faster with no noticeable degradation in the visual quality."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Stable Cascade",
      "description": "Stable Cascade: Image generation on a smaller & cheaper latent space.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.StableCascade",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "second_stage_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Second Stage Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the image will be returned as base64 encoded string."
        },
        {
          "name": "first_stage_steps",
          "type": {
            "type": "int"
          },
          "default": 20,
          "title": "First Stage Steps",
          "description": "Number of steps to run the first stage for."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Cascade will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to false, the safety checker will be disabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "second_stage_steps",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Second Stage Steps",
          "description": "Number of steps to run the second stage for."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Stable Cascade Sote Diffusion",
      "description": "Anime finetune of W\u00fcrstchen V3.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.StableCascadeSoteDiffusion",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "second_stage_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Second Stage Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the image will be returned as base64 encoded string."
        },
        {
          "name": "first_stage_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "First Stage Steps",
          "description": "Number of steps to run the first stage for."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 8,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Cascade will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to false, the safety checker will be disabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "second_stage_steps",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Second Stage Steps",
          "description": "Number of steps to run the second stage for."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Stable Diffusion V15",
      "description": "Stable Diffusion v1.5\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.StableDiffusionV15",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "embeddings",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Embeddings",
          "description": "The list of embeddings to use."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "If set to true, the prompt will be expanded with additional prompts."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The list of LoRA weights to use."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.StableDiffusionV15.Format"
          },
          "default": "jpeg",
          "title": "Format",
          "description": "The format of the generated image."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "safety_checker_version",
          "type": {
            "type": "enum",
            "values": [
              "v1",
              "v2"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.StableDiffusionV15.SafetyCheckerVersion"
          },
          "default": "v1",
          "title": "Safety Checker Version",
          "description": "The version of the safety checker to use. v1 is the default CompVis safety checker. v2 uses a custom ViT model."
        },
        {
          "name": "request_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Request Id",
          "description": "An id bound to a request, can be used with response to identify the request itself."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Stable Diffusion V35 Large",
      "description": "Stable Diffusion 3.5 Large is a powerful open-weight model with excellent prompt adherence and diverse output capabilities.\n    image, generation, stable-diffusion, open-source, text-to-image, txt2img\n\n    Use cases:\n    - Generate diverse artistic styles\n    - Create high-quality illustrations\n    - Produce photorealistic images\n    - Generate concept art and designs\n    - Create custom visual content",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.StableDiffusionV35Large",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from"
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "The size of the generated image. Defaults to landscape_4_3 if no controlnet has been passed, otherwise defaults to the size of the controlnet conditioning image."
        },
        {
          "name": "controlnet",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Controlnet",
          "description": "ControlNet for inference."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.StableDiffusionV35Large.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "ip_adapter",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Ip Adapter",
          "description": "IP-Adapter to use during inference."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Elements to avoid in the generated image"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "negative_prompt",
        "aspect_ratio"
      ]
    },
    {
      "title": "Stable Diffusion V35 Medium",
      "description": "Stable Diffusion 3.5 Medium is a Multimodal Diffusion Transformer (MMDiT) text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.StableDiffusionV35Medium",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.StableDiffusionV35Medium.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Stable Diffusion V3 Medium",
      "description": "Stable Diffusion 3 Medium (Text to Image) is a Multimodal Diffusion Transformer (MMDiT) model that improves image quality, typography, prompt understanding, and efficiency.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.StableDiffusionV3Medium",
      "properties": [
        {
          "name": "prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Prompt Expansion",
          "description": "If set to true, prompt will be upsampled with more details."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate an image from."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Switti",
      "description": "Switti is a scale-wise transformer for fast text-to-image generation that outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being faster than distilled diffusion models.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Switti",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "sampling_top_k",
          "type": {
            "type": "int"
          },
          "default": 400,
          "title": "Sampling Top K",
          "description": "The number of top-k tokens to sample from."
        },
        {
          "name": "turn_off_cfg_start_si",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Turn Off Cfg Start Si",
          "description": "Disable CFG starting scale"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 6,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "smooth_start_si",
          "type": {
            "type": "int"
          },
          "default": 2,
          "title": "Smooth Start Si",
          "description": "Smoothing starting scale"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "last_scale_temp",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Last Scale Temp",
          "description": "Temperature after disabling CFG"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Switti.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "more_diverse",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "More Diverse",
          "description": "More diverse sampling"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "more_smooth",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "More Smooth",
          "description": "Smoothing with Gumbel softmax sampling"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "sampling_top_p",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Sampling Top P",
          "description": "The top-p probability to sample from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Switti 512",
      "description": "Switti is a scale-wise transformer for fast text-to-image generation that outperforms existing T2I AR models and competes with state-of-the-art T2I diffusion models while being faster than distilled diffusion models.\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Switti512",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "sampling_top_k",
          "type": {
            "type": "int"
          },
          "default": 400,
          "title": "Sampling Top K",
          "description": "The number of top-k tokens to sample from."
        },
        {
          "name": "turn_off_cfg_start_si",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Turn Off Cfg Start Si",
          "description": "Disable CFG starting scale"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 6,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "smooth_start_si",
          "type": {
            "type": "int"
          },
          "default": 2,
          "title": "Smooth Start Si",
          "description": "Smoothing starting scale"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "last_scale_temp",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Last Scale Temp",
          "description": "Temperature after disabling CFG"
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.Switti512.OutputFormat"
          },
          "default": "jpeg",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "more_diverse",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "More Diverse",
          "description": "More diverse sampling"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "more_smooth",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "More Smooth",
          "description": "Smoothing with Gumbel softmax sampling"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "sampling_top_p",
          "type": {
            "type": "float"
          },
          "default": 0.95,
          "title": "Sampling Top P",
          "description": "The top-p probability to sample from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Vidu Q2 Text To Image",
      "description": "Vidu Q2 generates quality images from text with optimized performance and consistent results.\n    image, generation, vidu, q2, optimized, text-to-image\n\n    Use cases:\n    - Generate optimized quality images\n    - Create consistent visual content\n    - Produce balanced artwork\n    - Generate images efficiently\n    - Create reliable visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.ViduQ2TextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation, max 1500 characters"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ViduQ2TextToImage.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the output video"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan 25 Preview Text To Image",
      "description": "Wan 2.5 Text to Image\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.Wan25PreviewTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for image generation. Supports Chinese and English, max 2000 characters."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "Number of images to generate. Values from 1 to 4."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square",
          "title": "Image Size",
          "description": "The size of the generated image. Can use preset names like 'square', 'landscape_16_9', etc., or specific dimensions. Total pixels must be between 768\u00d7768 and 1440\u00d71440, with aspect ratio between [1:4, 4:1]."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to describe content to avoid. Max 500 characters."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan V225 BText To Image",
      "description": "Wan\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.WanV225BTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide image generation."
        },
        {
          "name": "image_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.WanV225BTextToImage.ImageFormat"
          },
          "default": "jpeg",
          "title": "Image Format",
          "description": "The format of the output image."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Shift",
          "description": "Shift value for the image. Must be between 1.0 and 10.0."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan V22 A14 BText To Image",
      "description": "Wan\n    generation, text-to-image, txt2img, ai-art\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.WanV22A14BTextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide image generation."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Shift",
          "description": "Shift value for the image. Must be between 1.0 and 10.0."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.WanV22A14BTextToImage.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "guidance_scale_2",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale 2",
          "description": "Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan V22 A14 BText To Image Lora",
      "description": "Wan v2.2 A14B Text-to-Image A14B with LoRAs\n    generation, text-to-image, txt2img, ai-art, lora\n\n    Use cases:\n    - AI-powered art generation\n    - Marketing and advertising visuals\n    - Concept art and ideation\n    - Social media content creation\n    - Rapid prototyping and mockups",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.WanV22A14BTextToImageLora",
      "properties": [
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Shift",
          "description": "Shift value for the image. Must be between 1.0 and 10.0."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide image generation."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "square_hd",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.WanV22A14BTextToImageLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'."
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "If true, the video will be reversed."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to be used in the inference."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "image_format",
          "type": {
            "type": "enum",
            "values": [
              "png",
              "jpeg"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.WanV22A14BTextToImageLora.ImageFormat"
          },
          "default": "jpeg",
          "title": "Image Format",
          "description": "The format of the output image."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "guidance_scale_2",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale 2",
          "description": "Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Wan V26 Text To Image",
      "description": "Wan v2.6 generates high-quality images from text with advanced capabilities and consistent results.\n    image, generation, wan, v2.6, quality, text-to-image\n\n    Use cases:\n    - Generate quality images with Wan v2.6\n    - Create consistent visual content\n    - Produce reliable artwork from text\n    - Generate images with advanced model\n    - Create high-quality visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.WanV26TextToImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the desired image. Supports Chinese and English. Max 2000 characters."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Image Size",
          "description": "Output image size. If not set: matches input image size (up to 1280*1280). Use presets like 'square_hd', 'landscape_16_9', or specify exact dimensions."
        },
        {
          "name": "max_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Max Images",
          "description": "Maximum number of images to generate (1-5). Actual count may be less depending on model inference."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional reference image (0 or 1). When provided, can be used for style guidance. Resolution: 384-5000px each dimension. Max size: 10MB. Formats: JPEG, JPG, PNG (no alpha), BMP, WEBP."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable content moderation for input and output."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility (0-2147483647)."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Content to avoid in the generated image. Max 500 characters."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "ZImage Base",
      "description": "Z-Image Base generates quality images from text with efficient processing and good results.\n    image, generation, z-image, base, efficient, text-to-image\n\n    Use cases:\n    - Generate images efficiently\n    - Create quality artwork from text\n    - Produce visual content quickly\n    - Generate images with good performance\n    - Create efficient visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.ZImageBase",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ZImageBase.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ZImageBase.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use for the image generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "ZImage Base Lora",
      "description": "Z-Image Base with LoRA enables efficient custom-trained models for specialized generation tasks.\n    image, generation, z-image, base, lora, custom\n\n    Use cases:\n    - Generate images with custom efficient models\n    - Create specialized content quickly\n    - Produce domain-specific visuals\n    - Generate with fine-tuned base model\n    - Create efficient custom visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.ZImageBaseLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ZImageBaseLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ZImageBaseLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use for the image generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "ZImage Turbo",
      "description": "Z-Image Turbo generates images from text with maximum speed for rapid iteration and prototyping.\n    image, generation, z-image, turbo, fast, text-to-image\n\n    Use cases:\n    - Generate images at maximum speed\n    - Create rapid prototypes from text\n    - Produce quick visual iterations\n    - Generate images for fast workflows\n    - Create instant visual content",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.ZImageTurbo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ZImageTurbo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ZImageTurbo.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "ZImage Turbo Lora",
      "description": "Z-Image Turbo with LoRA combines maximum speed with custom models for fast specialized generation.\n    image, generation, z-image, turbo, lora, fast\n\n    Use cases:\n    - Generate custom images at turbo speed\n    - Create specialized content rapidly\n    - Produce quick domain-specific visuals\n    - Generate with fast fine-tuned models\n    - Create instant custom visuals",
      "namespace": "fal.text_to_image",
      "node_type": "fal.text_to_image.ZImageTurboLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "num_images",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Images",
          "description": "The number of images to generate."
        },
        {
          "name": "image_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Image Size",
          "description": "The size of the generated image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ZImageTurboLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "jpeg",
              "png",
              "webp"
            ],
            "type_name": "nodetool.nodes.fal.text_to_image.ZImageTurboLora.OutputFormat"
          },
          "default": "png",
          "title": "Output Format",
          "description": "The format of the generated image."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "List of LoRA weights to apply (maximum 3)."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same image every time."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "image"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt"
      ]
    },
    {
      "title": "Bria Fibo Edit Edit Structured instruction",
      "description": "Structured Instructions Generation endpoint for Fibo Edit, Bria's newest editing model.\n    text, analysis, json, extraction\n\n    Use cases:\n    - Text analysis to structured data\n    - Content extraction\n    - Data structuring\n    - Information extraction\n    - Text classification",
      "namespace": "fal.text_to_json",
      "node_type": "fal.text_to_json.BriaFiboEditEditStructured_instruction",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If true, returns the image directly in the response (increases latency)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 5555,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Url",
          "description": "Reference image mask (file or URL). Optional."
        },
        {
          "name": "instruction",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Instruction",
          "description": "Instruction for image editing."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Reference image (file or URL)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "sync_mode",
        "seed",
        "mask_url",
        "instruction",
        "image_url"
      ]
    },
    {
      "title": "Bria Fibo Generate Structured prompt",
      "description": "Structured Prompt Generation endpoint for Fibo, Bria's SOTA Open source model\n    text, analysis, json, extraction\n\n    Use cases:\n    - Text analysis to structured data\n    - Content extraction\n    - Data structuring\n    - Information extraction\n    - Text classification",
      "namespace": "fal.text_to_json",
      "node_type": "fal.text_to_json.BriaFiboGenerateStructured_prompt",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt for image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 5555,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "structured_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Structured Prompt",
          "description": "The structured prompt to generate an image from."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Reference image (file or URL)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "seed",
        "structured_prompt",
        "image_url"
      ]
    },
    {
      "title": "Bria Fibo Lite Generate Structured prompt",
      "description": "Structured Prompt Generation endpoint for Fibo-Lite, Bria's SOTA Open source model\n    text, analysis, json, extraction\n\n    Use cases:\n    - Text analysis to structured data\n    - Content extraction\n    - Data structuring\n    - Information extraction\n    - Text classification",
      "namespace": "fal.text_to_json",
      "node_type": "fal.text_to_json.BriaFiboLiteGenerateStructured_prompt",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt for image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 5555,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "structured_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Structured Prompt",
          "description": "The structured prompt to generate an image from."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Reference image (file or URL)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "seed",
        "structured_prompt",
        "image_url"
      ]
    },
    {
      "title": "Bria Fibo Lite Generate Structured prompt Lite",
      "description": "Structured Prompt Generation endpoint for Fibo-Lite, Bria's SOTA Open source model\n    text, analysis, json, extraction\n\n    Use cases:\n    - Text analysis to structured data\n    - Content extraction\n    - Data structuring\n    - Information extraction\n    - Text classification",
      "namespace": "fal.text_to_json",
      "node_type": "fal.text_to_json.BriaFiboLiteGenerateStructured_promptLite",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt for image generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 5555,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "structured_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Structured Prompt",
          "description": "The structured prompt to generate an image from."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Reference image (file or URL)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "seed",
        "structured_prompt",
        "image_url"
      ]
    },
    {
      "title": "AIFace Swap Video",
      "description": "AI Face Swap replaces faces in videos with target faces while preserving expressions and movements.\n    video, face-swap, deepfake, face-replacement, video-to-video\n\n    Use cases:\n    - Replace faces in video content\n    - Create personalized video content\n    - Swap actors in video scenes\n    - Generate face replacement effects\n    - Create video with different faces",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.AIFaceSwapVideo",
      "properties": [
        {
          "name": "enable_occlusion_prevention",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Occlusion Prevention",
          "description": "Enable occlusion prevention for handling faces covered by hands/objects. Warning: Enabling this runs an occlusion-aware model which costs 2x more."
        },
        {
          "name": "source_face_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Source Face Url",
          "description": "Source face image. Allowed items: bmp, jpeg, png, tiff, webp"
        },
        {
          "name": "target_video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Target Video Url",
          "description": "Target video URL (max 25 minutes, will be truncated if longer; FPS capped at 25). Allowed items: avi, m4v, mkv, mp4, mpeg, mov, mxf, webm, wmv"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "target_face"
      ]
    },
    {
      "title": "AMTInterpolation",
      "description": "AMT (Any-to-Many Temporal) Interpolation creates smooth transitions between video frames.\n    video, interpolation, frame-generation, amt, video-to-video\n\n    Use cases:\n    - Increase video frame rate smoothly\n    - Create slow-motion effects\n    - Smooth out choppy video\n    - Generate intermediate frames\n    - Enhance video playback quality",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.AMTInterpolation",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video to be processed"
        },
        {
          "name": "recursive_interpolation_passes",
          "type": {
            "type": "int"
          },
          "default": 2,
          "title": "Recursive Interpolation Passes",
          "description": "Number of recursive interpolation passes"
        },
        {
          "name": "output_fps",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Output Fps",
          "description": "Output frames per second"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Animate Diff Turbo Video To Video",
      "description": "AnimateDiff Turbo re-animates videos quickly with reduced generation time.\n    video, style-transfer, animatediff, turbo, fast, video-to-video\n\n    Use cases:\n    - Quickly restyle videos\n    - Rapid video transformations\n    - Fast video effect application\n    - Efficient video processing\n    - Real-time video styling",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.AnimateDiffTurboVideoToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video."
        },
        {
          "name": "first_n_seconds",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "First N Seconds",
          "description": "The first N number of seconds of video to animate."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Fps",
          "description": "Number of frames per second to extract from the video."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.7,
          "title": "Strength",
          "description": "The strength of the input video in the final output."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform. 4-12 is recommended for turbo mode."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(bad quality, worst quality:1.2), ugly faces, bad anime",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "motions",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Motions",
          "description": "The motions to apply to the video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "prompt"
      ]
    },
    {
      "title": "Animate Diff Video To Video",
      "description": "AnimateDiff re-animates videos with new styles and effects using diffusion models.\n    video, style-transfer, animatediff, re-animation, video-to-video\n\n    Use cases:\n    - Restyle existing videos\n    - Apply artistic effects to videos\n    - Transform video aesthetics\n    - Create stylized video versions\n    - Generate video variations",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.AnimateDiffVideoToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for generating the image. Be as descriptive as possible for best results."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video."
        },
        {
          "name": "first_n_seconds",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "First N Seconds",
          "description": "The first N number of seconds of video to animate."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Fps",
          "description": "Number of frames per second to extract from the video."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.7,
          "title": "Strength",
          "description": "The strength of the input video in the final output."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "(bad quality, worst quality:1.2), ugly faces, bad anime",
          "title": "Negative Prompt",
          "description": "The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution)."
        },
        {
          "name": "motions",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Motions",
          "description": "The motions to apply to the video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "prompt"
      ]
    },
    {
      "title": "Auto Caption",
      "description": "Auto Caption automatically generates and adds captions to videos with speech recognition.\n    video, captions, subtitles, speech-to-text, video-to-video\n\n    Use cases:\n    - Add subtitles to videos automatically\n    - Generate captions for accessibility\n    - Create multilingual subtitles\n    - Transcribe video speech\n    - Add text overlays to videos",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.AutoCaption",
      "properties": [
        {
          "name": "txt_font",
          "type": {
            "type": "str"
          },
          "default": "Standard",
          "title": "Txt Font",
          "description": "Font for generated captions. Choose one in 'Arial','Standard','Garamond', 'Times New Roman','Georgia', or pass a url to a .ttf file"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the .mp4 video with audio. Only videos of size <100MB are allowed."
        },
        {
          "name": "top_align",
          "type": {
            "type": "str"
          },
          "default": "center",
          "title": "Top Align",
          "description": "Top-to-bottom alignment of the text. Can be a string ('top', 'center', 'bottom') or a float (0.0-1.0)"
        },
        {
          "name": "txt_color",
          "type": {
            "type": "str"
          },
          "default": "white",
          "title": "Txt Color",
          "description": "Colour of the text. Can be a RGB tuple, a color name, or an hexadecimal notation."
        },
        {
          "name": "stroke_width",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Stroke Width",
          "description": "Width of the text strokes in pixels"
        },
        {
          "name": "refresh_interval",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Refresh Interval",
          "description": "Number of seconds the captions should stay on screen. A higher number will also result in more text being displayed at once."
        },
        {
          "name": "font_size",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Font Size",
          "description": "Size of text in generated captions."
        },
        {
          "name": "left_align",
          "type": {
            "type": "str"
          },
          "default": "center",
          "title": "Left Align",
          "description": "Left-to-right alignment of the text. Can be a string ('left', 'center', 'right') or a float (0.0-1.0)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ben V2 Video",
      "description": "Ben v2 Video enhances and processes video content with advanced AI techniques.\n    video, enhancement, processing, ben, video-to-video\n\n    Use cases:\n    - Enhance video quality\n    - Process video content\n    - Improve video clarity\n    - Apply video enhancements\n    - Optimize video output",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BenV2Video",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of video to be used for background removal."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation."
        },
        {
          "name": "background_color",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Background Color",
          "description": "Optional RGB values (0-255) for the background color. If not provided, the background will be transparent. For ex: [0, 0, 0]"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Bi Ref Net V2 Video",
      "description": "BiRefNet v2 Video performs background removal from videos with high accuracy.\n    video, background-removal, segmentation, birefnet, video-to-video\n\n    Use cases:\n    - Remove backgrounds from videos\n    - Create transparent video backgrounds\n    - Isolate video subjects\n    - Generate video mattes\n    - Prepare videos for compositing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BiRefNetV2Video",
      "properties": [
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BiRefNetV2Video.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BiRefNetV2Video.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "operating_resolution",
          "type": {
            "type": "enum",
            "values": [
              "1024x1024",
              "2048x2048",
              "2304x2304"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BiRefNetV2Video.OperatingResolution"
          },
          "default": "1024x1024",
          "title": "Operating Resolution",
          "description": "The resolution to operate on. The higher the resolution, the more accurate the output will be for high res input images. The '2304x2304' option is only available for the 'General Use (Dynamic)' model."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video to remove background from"
        },
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "General Use (Light)",
              "General Use (Light 2K)",
              "General Use (Heavy)",
              "Matting",
              "Portrait",
              "General Use (Dynamic)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BiRefNetV2Video.Model"
          },
          "default": "General Use (Light)",
          "title": "Model",
          "description": "Model to use for background removal. The 'General Use (Light)' model is the original model used in the BiRefNet repository. The 'General Use (Light 2K)' model is the original model used in the BiRefNet repository but trained with 2K images. The 'General Use (Heavy)' model is a slower but more accurate model. The 'Matting' model is a model trained specifically for matting images. The 'Portrait' model is a model trained specifically for portrait images. The 'General Use (Dynamic)' model supports dynamic resolutions from 256x256 to 2304x2304. The 'General Use (Light)' model is recommended for most use cases. The corresponding models are as follows: - 'General Use (Light)': BiRefNet - 'General Use (Light 2K)': BiRefNet_lite-2K - 'General Use (Heavy)': BiRefNet_lite - 'Matting': BiRefNet-matting - 'Portrait': BiRefNet-portrait - 'General Use (Dynamic)': BiRefNet_dynamic"
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BiRefNetV2Video.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "output_mask",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Output Mask",
          "description": "Whether to output the mask used to remove the background"
        },
        {
          "name": "refine_foreground",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Refine Foreground",
          "description": "Whether to refine the foreground using the estimated mask"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Bria Video Erase Keypoints",
      "description": "Video\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BriaVideoEraseKeypoints",
      "properties": [
        {
          "name": "preserve_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Audio",
          "description": "If true, audio will be preserved in the output video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Input video to erase object from. duration must be less than 5s."
        },
        {
          "name": "output_container_and_codec",
          "type": {
            "type": "enum",
            "values": [
              "mp4_h265",
              "mp4_h264",
              "webm_vp9",
              "gif",
              "mov_h264",
              "mov_h265",
              "mov_proresks",
              "mkv_h264",
              "mkv_h265",
              "mkv_vp9",
              "mkv_mpeg4"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BriaVideoEraseKeypoints.OutputContainerAndCodec"
          },
          "default": "mp4_h264",
          "title": "Output Container And Codec",
          "description": "Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4."
        },
        {
          "name": "keypoints",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Keypoints",
          "description": "Input keypoints [x,y] to erase or keep from the video. Format like so: {'x':100, 'y':100, 'type':'positive/negative'}"
        },
        {
          "name": "auto_trim",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Auto Trim",
          "description": "auto trim the video, to working duration ( 5s )"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Bria Video Erase Mask",
      "description": "Video\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BriaVideoEraseMask",
      "properties": [
        {
          "name": "preserve_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Audio",
          "description": "If true, audio will be preserved in the output video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Input video to erase object from. duration must be less than 5s."
        },
        {
          "name": "output_container_and_codec",
          "type": {
            "type": "enum",
            "values": [
              "mp4_h265",
              "mp4_h264",
              "webm_vp9",
              "gif",
              "mov_h264",
              "mov_h265",
              "mov_proresks",
              "mkv_h264",
              "mkv_h265",
              "mkv_vp9",
              "mkv_mpeg4"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BriaVideoEraseMask.OutputContainerAndCodec"
          },
          "default": "mp4_h264",
          "title": "Output Container And Codec",
          "description": "Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4."
        },
        {
          "name": "mask_video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Mask Video Url",
          "description": "Input video to mask erase object from. duration must be less than 5s."
        },
        {
          "name": "auto_trim",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Auto Trim",
          "description": "auto trim the video, to working duration ( 5s )"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Bria Video Erase Prompt",
      "description": "Video\n    video, editing, video-to-video, vid2vid, professional\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BriaVideoErasePrompt",
      "properties": [
        {
          "name": "preserve_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Audio",
          "description": "If true, audio will be preserved in the output video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Input video to erase object from. duration must be less than 5s."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Input prompt to detect object to erase"
        },
        {
          "name": "output_container_and_codec",
          "type": {
            "type": "enum",
            "values": [
              "mp4_h265",
              "mp4_h264",
              "webm_vp9",
              "gif",
              "mov_h264",
              "mov_h265",
              "mov_proresks",
              "mkv_h264",
              "mkv_h265",
              "mkv_vp9",
              "mkv_mpeg4"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BriaVideoErasePrompt.OutputContainerAndCodec"
          },
          "default": "mp4_h264",
          "title": "Output Container And Codec",
          "description": "Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4."
        },
        {
          "name": "auto_trim",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Auto Trim",
          "description": "auto trim the video, to working duration ( 5s )"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Bria Video Eraser Keypoints",
      "description": "Bria Video Eraser removes objects from videos using keypoint-based selection.\n    video, object-removal, eraser, keypoints, bria, video-to-video\n\n    Use cases:\n    - Remove objects using keypoint selection\n    - Erase specific areas from videos\n    - Targeted video content removal\n    - Precision video editing\n    - Remove elements with point markers",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BriaVideoEraserKeypoints",
      "properties": [
        {
          "name": "preserve_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Audio",
          "description": "If true, audio will be preserved in the output video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Input video to erase object from. duration must be less than 5s."
        },
        {
          "name": "output_container_and_codec",
          "type": {
            "type": "enum",
            "values": [
              "mp4_h265",
              "mp4_h264",
              "webm_vp9",
              "gif",
              "mov_h264",
              "mov_h265",
              "mov_proresks",
              "mkv_h264",
              "mkv_h265",
              "mkv_vp9",
              "mkv_mpeg4"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BriaVideoEraserKeypoints.OutputContainerAndCodec"
          },
          "default": "mp4_h264",
          "title": "Output Container And Codec",
          "description": "Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4."
        },
        {
          "name": "keypoints",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Keypoints",
          "description": "Input keypoints [x,y] to erase or keep from the video. Format like so: {'x':100, 'y':100, 'type':'positive/negative'}"
        },
        {
          "name": "auto_trim",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Auto Trim",
          "description": "auto trim the video, to working duration ( 5s )"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "keypoints"
      ]
    },
    {
      "title": "Bria Video Eraser Mask",
      "description": "Bria Video Eraser removes objects from videos using mask-based selection.\n    video, object-removal, eraser, inpainting, bria, video-to-video\n\n    Use cases:\n    - Remove unwanted objects from videos\n    - Erase people or items from footage\n    - Clean up video backgrounds\n    - Remove watermarks from videos\n    - Edit video content seamlessly",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BriaVideoEraserMask",
      "properties": [
        {
          "name": "preserve_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Audio",
          "description": "If true, audio will be preserved in the output video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Input video to erase object from. duration must be less than 5s."
        },
        {
          "name": "output_container_and_codec",
          "type": {
            "type": "enum",
            "values": [
              "mp4_h265",
              "mp4_h264",
              "webm_vp9",
              "gif",
              "mov_h264",
              "mov_h265",
              "mov_proresks",
              "mkv_h264",
              "mkv_h265",
              "mkv_vp9",
              "mkv_mpeg4"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BriaVideoEraserMask.OutputContainerAndCodec"
          },
          "default": "mp4_h264",
          "title": "Output Container And Codec",
          "description": "Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4."
        },
        {
          "name": "mask_video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Mask Video Url",
          "description": "Input video to mask erase object from. duration must be less than 5s."
        },
        {
          "name": "auto_trim",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Auto Trim",
          "description": "auto trim the video, to working duration ( 5s )"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "mask"
      ]
    },
    {
      "title": "Bria Video Eraser Prompt",
      "description": "Bria Video Eraser removes objects from videos using text prompt descriptions.\n    video, object-removal, eraser, prompt, bria, video-to-video\n\n    Use cases:\n    - Remove objects by describing them\n    - Text-based video editing\n    - Natural language video cleanup\n    - Prompt-driven object removal\n    - Semantic video editing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BriaVideoEraserPrompt",
      "properties": [
        {
          "name": "preserve_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Audio",
          "description": "If true, audio will be preserved in the output video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Input video to erase object from. duration must be less than 5s."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Input prompt to detect object to erase"
        },
        {
          "name": "output_container_and_codec",
          "type": {
            "type": "enum",
            "values": [
              "mp4_h265",
              "mp4_h264",
              "webm_vp9",
              "gif",
              "mov_h264",
              "mov_h265",
              "mov_proresks",
              "mkv_h264",
              "mkv_h265",
              "mkv_vp9",
              "mkv_mpeg4"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BriaVideoEraserPrompt.OutputContainerAndCodec"
          },
          "default": "mp4_h264",
          "title": "Output Container And Codec",
          "description": "Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4."
        },
        {
          "name": "auto_trim",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Auto Trim",
          "description": "auto trim the video, to working duration ( 5s )"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "prompt"
      ]
    },
    {
      "title": "Bria Video Increase Resolution",
      "description": "Upscale videos up to 8K output resolution. Trained on fully licensed and commercially safe data.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BriaVideoIncreaseResolution",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Input video to increase resolution. Size should be less than 14142x14142 and duration less than 30s."
        },
        {
          "name": "output_container_and_codec",
          "type": {
            "type": "enum",
            "values": [
              "mp4_h265",
              "mp4_h264",
              "webm_vp9",
              "mov_h265",
              "mov_proresks",
              "mkv_h265",
              "mkv_h264",
              "mkv_vp9",
              "gif"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BriaVideoIncreaseResolution.OutputContainerAndCodec"
          },
          "default": "webm_vp9",
          "title": "Output Container And Codec",
          "description": "Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, mov_h265, mov_proresks, mkv_h265, mkv_h264, mkv_vp9, gif."
        },
        {
          "name": "desired_increase",
          "type": {
            "type": "enum",
            "values": [
              "2",
              "4"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BriaVideoIncreaseResolution.DesiredIncrease"
          },
          "default": "2",
          "title": "Desired Increase",
          "description": "desired_increase factor. Options: 2x, 4x."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Bytedance Upscaler Upscale Video",
      "description": "Bytedance Upscaler\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.BytedanceUpscalerUpscaleVideo",
      "properties": [
        {
          "name": "target_fps",
          "type": {
            "type": "enum",
            "values": [
              "30fps",
              "60fps"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BytedanceUpscalerUpscaleVideo.TargetFps"
          },
          "default": "30fps",
          "title": "Target Fps",
          "description": "The target FPS of the video to upscale."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to upscale."
        },
        {
          "name": "target_resolution",
          "type": {
            "type": "enum",
            "values": [
              "1080p",
              "2k",
              "4k"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.BytedanceUpscalerUpscaleVideo.TargetResolution"
          },
          "default": "1080p",
          "title": "Target Resolution",
          "description": "The target resolution of the video to upscale."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "CCSR",
      "description": "CCSR (Controllable Color Style Restoration) restores and enhances video colors.\n    video, color-restoration, enhancement, ccsr, video-to-video\n\n    Use cases:\n    - Restore video colors\n    - Enhance video color quality\n    - Fix color issues in videos\n    - Improve video color grading\n    - Restore faded video footage",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.CCSR",
      "properties": [
        {
          "name": "color_fix_type",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "wavelet",
              "adain"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.CCSR.ColorFixType"
          },
          "default": "adain",
          "title": "Color Fix Type",
          "description": "Type of color correction for samples."
        },
        {
          "name": "tile_diffusion_size",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Tile Diffusion Size",
          "description": "Size of patch."
        },
        {
          "name": "tile_vae_decoder_size",
          "type": {
            "type": "int"
          },
          "default": 226,
          "title": "Tile Vae Decoder Size",
          "description": "Size of VAE patch."
        },
        {
          "name": "tile_vae_encoder_size",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Tile Vae Encoder Size",
          "description": "Size of latent image"
        },
        {
          "name": "t_min",
          "type": {
            "type": "float"
          },
          "default": 0.3333,
          "title": "T Min",
          "description": "The starting point of uniform sampling strategy."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL or data URI of the image to upscale."
        },
        {
          "name": "tile_diffusion_stride",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Tile Diffusion Stride",
          "description": "Stride of sliding patch."
        },
        {
          "name": "tile_vae",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Tile Vae",
          "description": "If specified, a patch-based sampling strategy will be used for VAE decoding."
        },
        {
          "name": "scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Scale",
          "description": "The scale of the output image. The higher the scale, the bigger the output image will be."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for reproducibility. Different seeds will make slightly different results."
        },
        {
          "name": "t_max",
          "type": {
            "type": "float"
          },
          "default": 0.6667,
          "title": "T Max",
          "description": "The ending point of uniform sampling strategy."
        },
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Steps",
          "description": "The number of steps to run the model for. The higher the number the better the quality and longer it will take to generate."
        },
        {
          "name": "tile_diffusion",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "mix",
              "gaussian"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.CCSR.TileDiffusion"
          },
          "default": "none",
          "title": "Tile Diffusion",
          "description": "If specified, a patch-based sampling strategy will be used for sampling."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Cassetteai Video Sound Effects Generator",
      "description": "Add sound effects to your videos\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.CassetteaiVideoSoundEffectsGenerator",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "A video file to analyze & re-sound with generated SFX."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Clarityai Crystal Video Upscaler",
      "description": "Crystal Upscaler [Video]\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.ClarityaiCrystalVideoUpscaler",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the input video."
        },
        {
          "name": "scale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Scale Factor",
          "description": "Scale factor. The scale factor must be chosen such that the upscaled video does not exceed 5K resolution."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Cog Video X5 BVideo To Video",
      "description": "CogVideoX-5B transforms existing videos with new styles and effects.\n    video, transformation, cogvideo, style-transfer, video-to-video\n\n    Use cases:\n    - Transform video styles\n    - Apply effects to existing videos\n    - Restyle video content\n    - Generate video variations\n    - Create artistic video versions",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.CogVideoX5BVideoToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The video to generate the video from."
        },
        {
          "name": "use_rife",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Rife",
          "description": "Use RIFE for video interpolation"
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. We currently support one lora."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "The strength to use for Video to Video. 1.0 completely remakes the video while 0.0 preserves the original."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related video to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "export_fps",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Export Fps",
          "description": "The target FPS of the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate video from"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "prompt"
      ]
    },
    {
      "title": "Controlnext",
      "description": "Animate a reference image with a driving video using ControlNeXt.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Controlnext",
      "properties": [
        {
          "name": "controlnext_cond_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Controlnext Cond Scale",
          "description": "Condition scale for ControlNeXt."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 7,
          "title": "Fps",
          "description": "Frames per second for the output video."
        },
        {
          "name": "max_frame_num",
          "type": {
            "type": "int"
          },
          "default": 240,
          "title": "Max Frame Num",
          "description": "Maximum number of frames to process."
        },
        {
          "name": "width",
          "type": {
            "type": "int"
          },
          "default": 576,
          "title": "Width",
          "description": "Width of the output video."
        },
        {
          "name": "overlap",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Overlap",
          "description": "Number of overlapping frames between batches."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "Guidance scale for the diffusion process."
        },
        {
          "name": "batch_frames",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Batch Frames",
          "description": "Number of frames to process in each batch."
        },
        {
          "name": "height",
          "type": {
            "type": "int"
          },
          "default": 1024,
          "title": "Height",
          "description": "Height of the output video."
        },
        {
          "name": "sample_stride",
          "type": {
            "type": "int"
          },
          "default": 2,
          "title": "Sample Stride",
          "description": "Stride for sampling frames from the input video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the reference image."
        },
        {
          "name": "decode_chunk_size",
          "type": {
            "type": "int"
          },
          "default": 2,
          "title": "Decode Chunk Size",
          "description": "Chunk size for decoding frames."
        },
        {
          "name": "motion_bucket_id",
          "type": {
            "type": "float"
          },
          "default": 127,
          "title": "Motion Bucket Id",
          "description": "Motion bucket ID for the pipeline."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Inference Steps",
          "description": "Number of inference steps."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Decart Lucy Edit Dev",
      "description": "Edit outfits, objects, faces, or restyle your video - all with maximum detail retention.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.DecartLucyEditDev",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the video to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the video directly in the response without going through the CDN."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video to edit"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text description of the desired video content"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Decart Lucy Edit Fast",
      "description": "Lucy Edit [Fast]\n    video, editing, video-to-video, vid2vid, fast\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.DecartLucyEditFast",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the video to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the video directly in the response without going through the CDN."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video to edit"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text description of the desired video content"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Decart Lucy Edit Pro",
      "description": "Edit outfits, objects, faces, or restyle your video - all with maximum detail retention.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.DecartLucyEditPro",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the video to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the video directly in the response without going through the CDN."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video to edit"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text description of the desired video content"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.DecartLucyEditPro.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Decart Lucy Restyle",
      "description": "Lucy Restyle\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.DecartLucyRestyle",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the video to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the video directly in the response without going through the CDN."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video to edit"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.DecartLucyRestyle.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text description of the desired video content"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for video generation"
        },
        {
          "name": "enhance_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enhance Prompt",
          "description": "Whether to enhance the prompt for better results."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Dubbing",
      "description": "This endpoint delivers seamlessly localized videos by generating lip-synced dubs in multiple languages, ensuring natural and immersive multilingual experiences\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Dubbing",
      "properties": [
        {
          "name": "do_lipsync",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Do Lipsync",
          "description": "Whether to lip sync the audio to the video"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Input video URL to be dubbed."
        },
        {
          "name": "target_language",
          "type": {
            "type": "enum",
            "values": [
              "hindi",
              "turkish",
              "english"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Dubbing.TargetLanguage"
          },
          "default": "hindi",
          "title": "Target Language",
          "description": "Target language to dub the video to"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Dwpose Video",
      "description": "Predict poses from videos.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.DwposeVideo",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of video to be used for pose estimation"
        },
        {
          "name": "draw_mode",
          "type": {
            "type": "enum",
            "values": [
              "full-pose",
              "body-pose",
              "face-pose",
              "hand-pose",
              "face-hand-mask",
              "face-mask",
              "hand-mask"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.DwposeVideo.DrawMode"
          },
          "default": "body-pose",
          "title": "Draw Mode",
          "description": "Mode of drawing the pose on the video. Options are: 'full-pose', 'body-pose', 'face-pose', 'hand-pose', 'face-hand-mask', 'face-mask', 'hand-mask'."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Editto",
      "description": "Editto\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Editto",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the source video file. Required for inpainting."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "str"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between the original frames. A value of 0 means no interpolation."
        },
        {
          "name": "temporal_downsample_factor",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Temporal Downsample Factor",
          "description": "Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "str"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true."
        },
        {
          "name": "match_input_num_frames",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Num Frames",
          "description": "If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 241 (inclusive)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "sampler",
          "type": {
            "type": "enum",
            "values": [
              "unipc",
              "dpm++",
              "euler"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Editto.Sampler"
          },
          "default": "unipc",
          "title": "Sampler",
          "description": "Sampler to use for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Editto.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Editto.Resolution"
          },
          "default": "auto",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Editto.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "If true, also return a ZIP file containing all generated frames."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Editto.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "match_input_frames_per_second",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Frames Per Second",
          "description": "If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "enable_auto_downsample",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Auto Downsample",
          "description": "If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ffmpeg Api Compose",
      "description": "Compose videos from multiple media sources using FFmpeg API.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.FfmpegApiCompose",
      "properties": [
        {
          "name": "tracks",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Tracks",
          "description": "List of tracks to be combined into the final media"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ffmpeg Api Merge Audio Video",
      "description": "Merge videos with standalone audio files or audio from video files.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.FfmpegApiMergeAudioVideo",
      "properties": [
        {
          "name": "start_offset",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Start Offset",
          "description": "Offset in seconds for when the audio should start relative to the video"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video file to use as the video track"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to use as the audio track"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ffmpeg Api Merge Videos",
      "description": "Use ffmpeg capabilities to merge 2 or more videos.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.FfmpegApiMergeVideos",
      "properties": [
        {
          "name": "target_fps",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Target Fps",
          "description": "Target FPS for the output video. If not provided, uses the lowest FPS from input videos."
        },
        {
          "name": "video_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Video Urls",
          "description": "List of video URLs to merge in order"
        },
        {
          "name": "resolution",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Resolution",
          "description": "Resolution of the final video. Width and height must be between 512 and 2048."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Film Video",
      "description": "Interpolate videos with FILM - Frame Interpolation for Large Motion\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.FilmVideo",
      "properties": [
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.FilmVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the output video. Only applicable if output_type is 'video'."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to use for interpolation."
        },
        {
          "name": "use_calculated_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Calculated Fps",
          "description": "If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used."
        },
        {
          "name": "loop",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Loop",
          "description": "If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Fps",
          "description": "Frames per second for the output video. Only applicable if use_calculated_fps is False."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.FilmVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the output video. Only applicable if output_type is 'video'."
        },
        {
          "name": "use_scene_detection",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Scene Detection",
          "description": "If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Frames",
          "description": "The number of frames to generate between the input video frames."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Flashvsr Upscale Video",
      "description": "Flashvsr\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.FlashvsrUpscaleVideo",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The input video to be upscaled"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration mode for VAE decoding. Options: regular (best quality), high (balanced), full (fastest). More accerleation means longer duration videos can be processed too."
        },
        {
          "name": "quality",
          "type": {
            "type": "int"
          },
          "default": 70,
          "title": "Quality",
          "description": "Quality level for tile blending (0-100). Controls overlap between tiles to prevent grid artifacts. Higher values provide better quality with more overlap. Recommended: 70-85 for high-res videos, 50-70 for faster processing."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideo.OutputFormat"
          },
          "default": "X264 (.mp4)",
          "title": "Output Format",
          "description": "The format of the output video."
        },
        {
          "name": "color_fix",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Color Fix",
          "description": "Color correction enabled."
        },
        {
          "name": "output_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideo.OutputWriteMode"
          },
          "default": "balanced",
          "title": "Output Write Mode",
          "description": "The write mode of the output video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned inline and not stored in history."
        },
        {
          "name": "output_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideo.OutputQuality"
          },
          "default": "high",
          "title": "Output Quality",
          "description": "The quality of the output video."
        },
        {
          "name": "upscale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Upscale Factor",
          "description": "Upscaling factor to be used."
        },
        {
          "name": "preserve_audio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preserve Audio",
          "description": "Copy the original audio tracks into the upscaled video using FFmpeg when possible."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The random seed used for the generation process."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Hunyuan Video Foley",
      "description": "Use the capabilities of the hunyuan foley model to bring life to your videos by adding sound effect to them.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.HunyuanVideoFoley",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to generate audio for."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "Guidance scale for audio generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "noisy, harsh",
          "title": "Negative Prompt",
          "description": "Negative prompt to avoid certain audio characteristics."
        },
        {
          "name": "text_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Prompt",
          "description": "Text description of the desired audio (optional)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Hunyuan Video Lora Video To Video",
      "description": "Hunyuan Video is an Open video generation model with high visual quality, motion diversity, text-video alignment, and generation stability. Use this endpoint to generate videos from videos.\n    video, editing, video-to-video, vid2vid, lora\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.HunyuanVideoLoraVideoToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.HunyuanVideoLoraVideoToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video to generate."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.HunyuanVideoLoraVideoToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the video to generate."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video"
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.75,
          "title": "Strength",
          "description": "Strength of video-to-video"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for generating the video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "enum",
            "values": [
              "129",
              "85"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.HunyuanVideoLoraVideoToVideo.NumFrames"
          },
          "default": 129,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "pro_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Pro Mode",
          "description": "By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Hunyuan Video To Video",
      "description": "Hunyuan Video transforms existing videos with advanced AI-powered effects.\n    video, transformation, hunyuan, video-to-video\n\n    Use cases:\n    - Transform video content\n    - Apply AI effects to videos\n    - Restyle existing footage\n    - Generate video variations\n    - Create enhanced video versions",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.HunyuanVideoToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.HunyuanVideoToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video to generate."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.HunyuanVideoToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the video to generate."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video input."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "Strength for Video-to-Video"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to run. Lower gets faster results, higher gets better results."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for generating the video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "enum",
            "values": [
              "129",
              "85"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.HunyuanVideoToVideo.NumFrames"
          },
          "default": 129,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "pro_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Pro Mode",
          "description": "By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video",
        "prompt"
      ]
    },
    {
      "title": "Infinitalk",
      "description": "Infinitalk model generates a talking avatar video from an image and audio file. The avatar lip-syncs to the provided audio with natural facial expressions.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Infinitalk",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Infinitalk.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the video to generate. Must be either 480p or 720p."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Infinitalk.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 145,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 41 to 721."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Infinitalk Video To Video",
      "description": "Infinitalk\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.InfinitalkVideoToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.InfinitalkVideoToVideo.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the video to generate. Must be either 480p or 720p."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.InfinitalkVideoToVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 145,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Kling Video O1 Standard Video To Video Edit",
      "description": "Kling O1 Edit Video [Standard]\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.KlingVideoO1StandardVideoToVideoEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB. Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s"
        },
        {
          "name": "elements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Elements",
          "description": "Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video."
        },
        {
          "name": "keep_audio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Keep Audio",
          "description": "Whether to keep the original audio from the video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Kling Video O1 Standard Video To Video Reference",
      "description": "Kling O1 Reference Video to Video [Standard]\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.KlingVideoO1StandardVideoToVideoReference",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.KlingVideoO1StandardVideoToVideoReference.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame. If 'auto', the aspect ratio will be determined automatically based on the input video, and the closest aspect ratio to the input video will be used."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB. Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.KlingVideoO1StandardVideoToVideoReference.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Video duration in seconds."
        },
        {
          "name": "keep_audio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Keep Audio",
          "description": "Whether to keep the original audio from the video."
        },
        {
          "name": "elements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Elements",
          "description": "Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Kling Video O1 Video To Video Edit",
      "description": "Kling O1 Edit Video [Pro]\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.KlingVideoO1VideoToVideoEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB. Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s"
        },
        {
          "name": "elements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Elements",
          "description": "Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video."
        },
        {
          "name": "keep_audio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Keep Audio",
          "description": "Whether to keep the original audio from the video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Kling Video O1 Video To Video Reference",
      "description": "Kling O1 Reference Video to Video [Pro]\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.KlingVideoO1VideoToVideoReference",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.KlingVideoO1VideoToVideoReference.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame. If 'auto', the aspect ratio will be determined automatically based on the input video, and the closest aspect ratio to the input video will be used."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB. Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.KlingVideoO1VideoToVideoReference.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Video duration in seconds."
        },
        {
          "name": "keep_audio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Keep Audio",
          "description": "Whether to keep the original audio from the video."
        },
        {
          "name": "elements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Elements",
          "description": "Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Kling Video V26 Pro Motion Control",
      "description": "Kling Video v2.6 Motion Control [Pro]\n    video, editing, video-to-video, vid2vid, professional\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.KlingVideoV26ProMotionControl",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "video_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Video Url",
          "description": "Reference video URL. The character actions in the generated video will be consistent with this reference video. Should contain a realistic style character with entire body or upper body visible, including head, without obstruction. Duration limit depends on character_orientation: 10s max for 'image', 30s max for 'video'."
        },
        {
          "name": "character_orientation",
          "type": {
            "type": "enum",
            "values": [
              "image",
              "video"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.KlingVideoV26ProMotionControl.CharacterOrientation"
          },
          "default": "",
          "title": "Character Orientation",
          "description": "Controls whether the output character's orientation matches the reference image or video. 'video': orientation matches reference video - better for complex motions (max 30s). 'image': orientation matches reference image - better for following camera movements (max 10s)."
        },
        {
          "name": "keep_original_sound",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Keep Original Sound",
          "description": "Whether to keep the original sound from the reference video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Reference image URL. The characters, backgrounds, and other elements in the generated video are based on this reference image. Characters should have clear body proportions, avoid occlusion, and occupy more than 5% of the image area."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Kling Video V26 Standard Motion Control",
      "description": "Kling Video v2.6 Motion Control [Standard]\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.KlingVideoV26StandardMotionControl",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "video_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Video Url",
          "description": "Reference video URL. The character actions in the generated video will be consistent with this reference video. Should contain a realistic style character with entire body or upper body visible, including head, without obstruction. Duration limit depends on character_orientation: 10s max for 'image', 30s max for 'video'."
        },
        {
          "name": "character_orientation",
          "type": {
            "type": "enum",
            "values": [
              "image",
              "video"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.KlingVideoV26StandardMotionControl.CharacterOrientation"
          },
          "default": "",
          "title": "Character Orientation",
          "description": "Controls whether the output character's orientation matches the reference image or video. 'video': orientation matches reference video - better for complex motions (max 30s). 'image': orientation matches reference image - better for following camera movements (max 10s)."
        },
        {
          "name": "keep_original_sound",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Keep Original Sound",
          "description": "Whether to keep the original sound from the reference video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Reference image URL. The characters, backgrounds, and other elements in the generated video are based on this reference image. Characters should have clear body proportions, avoid occlusion, and occupy more than 5% of the image area."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Krea Wan 14B Video To Video",
      "description": "Krea Wan 14B\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.KreaWan14BVideoToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt for the video-to-video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video. Currently, only outputs of 16:9 aspect ratio and 480p resolution are supported. Video duration should be less than 1000 frames at 16fps, and output frames will be 6 plus a multiple of 12, for example 18, 30, 42, etc."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.85,
          "title": "Strength",
          "description": "Denoising strength for the video-to-video generation. 0.0 preserves the original, 1.0 completely remakes the video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Seed for the video-to-video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Latentsync",
      "description": "LatentSync is a video-to-video model that generates lip sync animations from audio using advanced algorithms for high-quality synchronization.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Latentsync",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to generate the lip sync for."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "Guidance scale for the model inference"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation. If None, a random seed will be used."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio to generate the lip sync for."
        },
        {
          "name": "loop_mode",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "pingpong",
              "loop"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Latentsync.LoopMode"
          },
          "default": null,
          "title": "Loop Mode",
          "description": "Video loop mode when audio is longer than video. Options: pingpong, loop"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Lightx Recamera",
      "description": "Lightx\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.LightxRecamera",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional text prompt. If omitted, Light-X will auto-caption the video."
        },
        {
          "name": "trajectory",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Trajectory",
          "description": "Camera trajectory parameters (required for recamera mode)."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video."
        },
        {
          "name": "camera",
          "type": {
            "type": "enum",
            "values": [
              "traj",
              "target"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LightxRecamera.Camera"
          },
          "default": "traj",
          "title": "Camera",
          "description": "Camera control mode."
        },
        {
          "name": "target_pose",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "float"
              }
            ]
          },
          "default": [],
          "title": "Target Pose",
          "description": "Target camera pose [theta, phi, radius, x, y] (required when camera='target')."
        },
        {
          "name": "mode",
          "type": {
            "type": "enum",
            "values": [
              "gradual",
              "bullet",
              "direct",
              "dolly-zoom"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LightxRecamera.Mode"
          },
          "default": "gradual",
          "title": "Mode",
          "description": "Camera motion mode."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Lightx Relight",
      "description": "Lightx\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.LightxRelight",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional text prompt. If omitted, Light-X will auto-caption the video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video."
        },
        {
          "name": "relight_parameters",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Relight Parameters",
          "description": "Relighting parameters (required for relight_condition_type='ic'). Not used for 'bg' (which expects a background image URL instead)."
        },
        {
          "name": "ref_id",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Ref Id",
          "description": "Frame index to use as referencen to relight the video with reference."
        },
        {
          "name": "relit_cond_img_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Relit Cond Img Url",
          "description": "URL of conditioning image. Required for relight_condition_type='ref'/'hdr'. Also required for relight_condition_type='bg' (background image)."
        },
        {
          "name": "relit_cond_type",
          "type": {
            "type": "enum",
            "values": [
              "ic",
              "ref",
              "hdr",
              "bg"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LightxRelight.RelitCondType"
          },
          "default": "ic",
          "title": "Relit Cond Type",
          "description": "Relight condition type."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 219B Distilled Extend Video",
      "description": "LTX-2 19B Distilled\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx219BDistilledExtendVideo",
      "properties": [
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to extend."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideo.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the extended video."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "extend_direction",
          "type": {
            "type": "enum",
            "values": [
              "forward",
              "backward"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideo.ExtendDirection"
          },
          "default": "forward",
          "title": "Extend Direction",
          "description": "Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "video_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Video Strength",
          "description": "Video conditioning strength. Lower values represent more freedom given to the model to change the video content."
        },
        {
          "name": "num_context_frames",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Context Frames",
          "description": "The number of frames to use as context for the extension."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "match_input_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Fps",
          "description": "When true, match the output FPS to the input video's FPS instead of using the default target FPS."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 219B Distilled Extend Video Lora",
      "description": "LTX-2 19B Distilled\n    video, editing, video-to-video, vid2vid, lora\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx219BDistilledExtendVideoLora",
      "properties": [
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to extend."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLora.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the extended video."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "extend_direction",
          "type": {
            "type": "enum",
            "values": [
              "forward",
              "backward"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLora.ExtendDirection"
          },
          "default": "forward",
          "title": "Extend Direction",
          "description": "Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "video_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Video Strength",
          "description": "Video conditioning strength. Lower values represent more freedom given to the model to change the video content."
        },
        {
          "name": "num_context_frames",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Context Frames",
          "description": "The number of frames to use as context for the extension."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "match_input_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Fps",
          "description": "When true, match the output FPS to the input video's FPS instead of using the default target FPS."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 219B Distilled Video To Video",
      "description": "LTX-2 19B Distilled\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx219BDistilledVideoToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to generate the video from."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "ic_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Ic Lora Scale",
          "description": "The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Video Strength",
          "description": "Video conditioning strength. Lower values represent more freedom given to the model to change the video content."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "An optional URL of an image to use as the first frame of the video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "match_video_length",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Video Length",
          "description": "When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideo.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "preprocessor",
          "type": {
            "type": "enum",
            "values": [
              "depth",
              "canny",
              "pose",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideo.Preprocessor"
          },
          "default": "none",
          "title": "Preprocessor",
          "description": "The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "ic_lora",
          "type": {
            "type": "enum",
            "values": [
              "match_preprocessor",
              "canny",
              "depth",
              "pose",
              "detailer",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideo.IcLora"
          },
          "default": "match_preprocessor",
          "title": "Ic Lora",
          "description": "The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "match_input_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Fps",
          "description": "When true, match the output FPS to the input video's FPS instead of using the default target FPS."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 219B Distilled Video To Video Lora",
      "description": "LTX-2 19B Distilled\n    video, editing, video-to-video, vid2vid, lora\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx219BDistilledVideoToVideoLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to generate the video from."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "ic_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Ic Lora Scale",
          "description": "The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Video Strength",
          "description": "Video conditioning strength. Lower values represent more freedom given to the model to change the video content."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "An optional URL of an image to use as the first frame of the video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "match_video_length",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Video Length",
          "description": "When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLora.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "preprocessor",
          "type": {
            "type": "enum",
            "values": [
              "depth",
              "canny",
              "pose",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLora.Preprocessor"
          },
          "default": "none",
          "title": "Preprocessor",
          "description": "The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "ic_lora",
          "type": {
            "type": "enum",
            "values": [
              "match_preprocessor",
              "canny",
              "depth",
              "pose",
              "detailer",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLora.IcLora"
          },
          "default": "match_preprocessor",
          "title": "Ic Lora",
          "description": "The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "match_input_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Fps",
          "description": "When true, match the output FPS to the input video's FPS instead of using the default target FPS."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 219B Extend Video",
      "description": "LTX-2 19B\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx219BExtendVideo",
      "properties": [
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to extend."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the extended video."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "extend_direction",
          "type": {
            "type": "enum",
            "values": [
              "forward",
              "backward"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideo.ExtendDirection"
          },
          "default": "forward",
          "title": "Extend Direction",
          "description": "Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "video_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Video Strength",
          "description": "Video conditioning strength. Lower values represent more freedom given to the model to change the video content."
        },
        {
          "name": "num_context_frames",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Context Frames",
          "description": "The number of frames to use as context for the extension."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "match_input_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Fps",
          "description": "When true, match the output FPS to the input video's FPS instead of using the default target FPS."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 219B Extend Video Lora",
      "description": "LTX-2 19B\n    video, editing, video-to-video, vid2vid, lora\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx219BExtendVideoLora",
      "properties": [
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to extend."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the extended video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "video_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Video Strength",
          "description": "Video conditioning strength. Lower values represent more freedom given to the model to change the video content."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "extend_direction",
          "type": {
            "type": "enum",
            "values": [
              "forward",
              "backward"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLora.ExtendDirection"
          },
          "default": "forward",
          "title": "Extend Direction",
          "description": "Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "num_context_frames",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Num Context Frames",
          "description": "The number of frames to use as context for the extension."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "match_input_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Fps",
          "description": "When true, match the output FPS to the input video's FPS instead of using the default target FPS."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 219B Video To Video",
      "description": "LTX-2 19B\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx219BVideoToVideo",
      "properties": [
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to generate the video from."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "ic_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Ic Lora Scale",
          "description": "The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Video Strength",
          "description": "Video conditioning strength. Lower values represent more freedom given to the model to change the video content."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "An optional URL of an image to use as the first frame of the video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "match_video_length",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Video Length",
          "description": "When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "preprocessor",
          "type": {
            "type": "enum",
            "values": [
              "depth",
              "canny",
              "pose",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideo.Preprocessor"
          },
          "default": "none",
          "title": "Preprocessor",
          "description": "The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "ic_lora",
          "type": {
            "type": "enum",
            "values": [
              "match_preprocessor",
              "canny",
              "depth",
              "pose",
              "detailer",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideo.IcLora"
          },
          "default": "match_preprocessor",
          "title": "Ic Lora",
          "description": "The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "match_input_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Fps",
          "description": "When true, match the output FPS to the input video's FPS instead of using the default target FPS."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 219B Video To Video Lora",
      "description": "LTX-2 19B\n    video, editing, video-to-video, vid2vid, lora\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx219BVideoToVideoLora",
      "properties": [
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to generate the video from."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "ic_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Ic Lora Scale",
          "description": "The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Video Strength",
          "description": "Video conditioning strength. Lower values represent more freedom given to the model to change the video content."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "An optional URL of an image to use as the first frame of the video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "match_video_length",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Video Length",
          "description": "When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "preprocessor",
          "type": {
            "type": "enum",
            "values": [
              "depth",
              "canny",
              "pose",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLora.Preprocessor"
          },
          "default": "none",
          "title": "Preprocessor",
          "description": "The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "ic_lora",
          "type": {
            "type": "enum",
            "values": [
              "match_preprocessor",
              "canny",
              "depth",
              "pose",
              "detailer",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLora.IcLora"
          },
          "default": "match_preprocessor",
          "title": "Ic Lora",
          "description": "The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "match_input_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Fps",
          "description": "When true, match the output FPS to the input video's FPS instead of using the default target FPS."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx 2 Retake Video",
      "description": "LTX Video 2.0 Retake\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltx2RetakeVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to retake the video with"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to retake"
        },
        {
          "name": "start_time",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Start Time",
          "description": "The start time of the video to retake in seconds"
        },
        {
          "name": "duration",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Duration",
          "description": "The duration of the video to retake in seconds"
        },
        {
          "name": "retake_mode",
          "type": {
            "type": "enum",
            "values": [
              "replace_audio",
              "replace_video",
              "replace_audio_and_video"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltx2RetakeVideo.RetakeMode"
          },
          "default": "replace_audio_and_video",
          "title": "Retake Mode",
          "description": "The retake mode to use for the retake"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx Video 13 b Dev Extend",
      "description": "Extend videos using LTX Video-0.9.7 13B and custom LoRA\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.LtxVideo13bDevExtend",
      "properties": [
        {
          "name": "second_pass_skip_initial_steps",
          "type": {
            "type": "int"
          },
          "default": 17,
          "title": "Second Pass Skip Initial Steps",
          "description": "The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes."
        },
        {
          "name": "first_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "First Pass Num Inference Steps",
          "description": "Number of inference steps during the first pass."
        },
        {
          "name": "frame_rate",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Frame Rate",
          "description": "The frame rate of the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide generation"
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "Whether to reverse the video."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt using a language model."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to use for generation"
        },
        {
          "name": "second_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Second Pass Num Inference Steps",
          "description": "Number of inference steps during the second pass."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames in the video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video",
          "description": "Video to be extended."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "worst quality, inconsistent motion, blurry, jittery, distorted",
          "title": "Negative Prompt",
          "description": "Negative prompt for generation"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LtxVideo13bDevExtend.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p or 720p)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "1:1",
              "16:9",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LtxVideo13bDevExtend.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video."
        },
        {
          "name": "constant_rate_factor",
          "type": {
            "type": "int"
          },
          "default": 35,
          "title": "Constant Rate Factor",
          "description": "The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality."
        },
        {
          "name": "first_pass_skip_final_steps",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "First Pass Skip Final Steps",
          "description": "Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx Video 13 b Dev Multiconditioning",
      "description": "Generate videos from prompts, images, and videos using LTX Video-0.9.7 13B and custom LoRA\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.LtxVideo13bDevMulticonditioning",
      "properties": [
        {
          "name": "second_pass_skip_initial_steps",
          "type": {
            "type": "int"
          },
          "default": 17,
          "title": "Second Pass Skip Initial Steps",
          "description": "The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes."
        },
        {
          "name": "first_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "First Pass Num Inference Steps",
          "description": "Number of inference steps during the first pass."
        },
        {
          "name": "frame_rate",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Frame Rate",
          "description": "The frame rate of the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide generation"
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "Whether to reverse the video."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt using a language model."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to use for generation"
        },
        {
          "name": "images",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Images",
          "description": "URL of images to use as conditioning"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames in the video."
        },
        {
          "name": "second_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Second Pass Num Inference Steps",
          "description": "Number of inference steps during the second pass."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "worst quality, inconsistent motion, blurry, jittery, distorted",
          "title": "Negative Prompt",
          "description": "Negative prompt for generation"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LtxVideo13bDevMulticonditioning.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p or 720p)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "1:1",
              "16:9",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LtxVideo13bDevMulticonditioning.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video."
        },
        {
          "name": "videos",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Videos",
          "description": "Videos to use as conditioning"
        },
        {
          "name": "constant_rate_factor",
          "type": {
            "type": "int"
          },
          "default": 35,
          "title": "Constant Rate Factor",
          "description": "The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality."
        },
        {
          "name": "first_pass_skip_final_steps",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "First Pass Skip Final Steps",
          "description": "Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx Video 13 b Distilled Extend",
      "description": "Extend videos using LTX Video-0.9.7 13B Distilled and custom LoRA\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.LtxVideo13bDistilledExtend",
      "properties": [
        {
          "name": "second_pass_skip_initial_steps",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Second Pass Skip Initial Steps",
          "description": "The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes."
        },
        {
          "name": "first_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "First Pass Num Inference Steps",
          "description": "Number of inference steps during the first pass."
        },
        {
          "name": "frame_rate",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Frame Rate",
          "description": "The frame rate of the video."
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "Whether to reverse the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide generation"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt using a language model."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to use for generation"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames in the video."
        },
        {
          "name": "second_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Second Pass Num Inference Steps",
          "description": "Number of inference steps during the second pass."
        },
        {
          "name": "video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video",
          "description": "Video to be extended."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "worst quality, inconsistent motion, blurry, jittery, distorted",
          "title": "Negative Prompt",
          "description": "Negative prompt for generation"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledExtend.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p or 720p)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "1:1",
              "16:9",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledExtend.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video."
        },
        {
          "name": "constant_rate_factor",
          "type": {
            "type": "int"
          },
          "default": 35,
          "title": "Constant Rate Factor",
          "description": "The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality."
        },
        {
          "name": "first_pass_skip_final_steps",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "First Pass Skip Final Steps",
          "description": "Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx Video 13 b Distilled Multiconditioning",
      "description": "Generate videos from prompts, images, and videos using LTX Video-0.9.7 13B Distilled and custom LoRA\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.LtxVideo13bDistilledMulticonditioning",
      "properties": [
        {
          "name": "second_pass_skip_initial_steps",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Second Pass Skip Initial Steps",
          "description": "The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes."
        },
        {
          "name": "first_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "First Pass Num Inference Steps",
          "description": "Number of inference steps during the first pass."
        },
        {
          "name": "frame_rate",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Frame Rate",
          "description": "The frame rate of the video."
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "Whether to reverse the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide generation"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt using a language model."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to use for generation"
        },
        {
          "name": "images",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Images",
          "description": "URL of images to use as conditioning"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames in the video."
        },
        {
          "name": "second_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Second Pass Num Inference Steps",
          "description": "Number of inference steps during the second pass."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "worst quality, inconsistent motion, blurry, jittery, distorted",
          "title": "Negative Prompt",
          "description": "Negative prompt for generation"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledMulticonditioning.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p or 720p)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "1:1",
              "16:9",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledMulticonditioning.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video."
        },
        {
          "name": "constant_rate_factor",
          "type": {
            "type": "int"
          },
          "default": 35,
          "title": "Constant Rate Factor",
          "description": "The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality."
        },
        {
          "name": "videos",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Videos",
          "description": "Videos to use as conditioning"
        },
        {
          "name": "first_pass_skip_final_steps",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "First Pass Skip Final Steps",
          "description": "Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx Video Lora Multiconditioning",
      "description": "Generate videos from prompts, images, and videos using LTX Video-0.9.7 and custom LoRA\n    video, editing, video-to-video, vid2vid, lora\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.LtxVideoLoraMulticonditioning",
      "properties": [
        {
          "name": "number_of_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Number Of Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "Whether to reverse the video."
        },
        {
          "name": "frame_rate",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Frame Rate",
          "description": "The frame rate of the video."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt using the LLM."
        },
        {
          "name": "number_of_frames",
          "type": {
            "type": "int"
          },
          "default": 89,
          "title": "Number Of Frames",
          "description": "The number of frames in the video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA weights to use for generation."
        },
        {
          "name": "images",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Images",
          "description": "The image conditions to use for generation."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, low quality, low resolution, inconsistent motion, jittery, distorted",
          "title": "Negative Prompt",
          "description": "The negative prompt to use."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "1:1",
              "9:16",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LtxVideoLoraMulticonditioning.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LtxVideoLoraMulticonditioning.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the video."
        },
        {
          "name": "videos",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Videos",
          "description": "The video conditions to use for generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx Video V095 Extend",
      "description": "Generate videos from prompts and videos using LTX Video-0.9.5\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.LtxVideoV095Extend",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide generation"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LtxVideoV095Extend.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p or 720p)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LtxVideoV095Extend.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video (16:9 or 9:16)."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt using the model's own capabilities."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "Number of inference steps"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "worst quality, inconsistent motion, blurry, jittery, distorted",
          "title": "Negative Prompt",
          "description": "Negative prompt for generation"
        },
        {
          "name": "video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video",
          "description": "Video to be extended."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltx Video V095 Multiconditioning",
      "description": "Generate videos from prompts,images, and videos using LTX Video-0.9.5\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.LtxVideoV095Multiconditioning",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide generation"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LtxVideoV095Multiconditioning.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p or 720p)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LtxVideoV095Multiconditioning.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video (16:9 or 9:16)."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt using the model's own capabilities."
        },
        {
          "name": "images",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Images",
          "description": "URL of images to use as conditioning"
        },
        {
          "name": "videos",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Videos",
          "description": "Videos to use as conditioning"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "Number of inference steps"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "worst quality, inconsistent motion, blurry, jittery, distorted",
          "title": "Negative Prompt",
          "description": "Negative prompt for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltxv 13 b 098 Distilled Extend",
      "description": "Extend videos using LTX Video-0.9.8 13B Distilled and custom LoRA\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltxv13b098DistilledExtend",
      "properties": [
        {
          "name": "second_pass_skip_initial_steps",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Second Pass Skip Initial Steps",
          "description": "The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes."
        },
        {
          "name": "first_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "First Pass Num Inference Steps",
          "description": "Number of inference steps during the first pass."
        },
        {
          "name": "frame_rate",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Frame Rate",
          "description": "The frame rate of the video."
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "Whether to reverse the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide generation"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt using a language model."
        },
        {
          "name": "temporal_adain_factor",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Temporal Adain Factor",
          "description": "The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to use for generation"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames in the video."
        },
        {
          "name": "second_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Second Pass Num Inference Steps",
          "description": "Number of inference steps during the second pass."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "worst quality, inconsistent motion, blurry, jittery, distorted",
          "title": "Negative Prompt",
          "description": "Negative prompt for generation"
        },
        {
          "name": "video",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video",
          "description": "Video to be extended."
        },
        {
          "name": "enable_detail_pass",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Detail Pass",
          "description": "Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledExtend.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "1:1",
              "16:9",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledExtend.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video."
        },
        {
          "name": "tone_map_compression_ratio",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Tone Map Compression Ratio",
          "description": "The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression."
        },
        {
          "name": "constant_rate_factor",
          "type": {
            "type": "int"
          },
          "default": 29,
          "title": "Constant Rate Factor",
          "description": "The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Ltxv 13 b 098 Distilled Multiconditioning",
      "description": "Generate long videos from prompts, images, and videos using LTX Video-0.9.8 13B Distilled and custom LoRA\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Ltxv13b098DistilledMulticonditioning",
      "properties": [
        {
          "name": "second_pass_skip_initial_steps",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Second Pass Skip Initial Steps",
          "description": "The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes."
        },
        {
          "name": "first_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "First Pass Num Inference Steps",
          "description": "Number of inference steps during the first pass."
        },
        {
          "name": "frame_rate",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Frame Rate",
          "description": "The frame rate of the video."
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "Whether to reverse the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide generation"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt using a language model."
        },
        {
          "name": "temporal_adain_factor",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Temporal Adain Factor",
          "description": "The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to use for generation"
        },
        {
          "name": "images",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Images",
          "description": "URL of images to use as conditioning"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames in the video."
        },
        {
          "name": "second_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Second Pass Num Inference Steps",
          "description": "Number of inference steps during the second pass."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "worst quality, inconsistent motion, blurry, jittery, distorted",
          "title": "Negative Prompt",
          "description": "Negative prompt for generation"
        },
        {
          "name": "enable_detail_pass",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Detail Pass",
          "description": "Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledMulticonditioning.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "1:1",
              "16:9",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledMulticonditioning.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video."
        },
        {
          "name": "tone_map_compression_ratio",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Tone Map Compression Ratio",
          "description": "The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression."
        },
        {
          "name": "videos",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Videos",
          "description": "Videos to use as conditioning"
        },
        {
          "name": "constant_rate_factor",
          "type": {
            "type": "int"
          },
          "default": 29,
          "title": "Constant Rate Factor",
          "description": "The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Luma Dream Machine Ray 2 Flash Modify",
      "description": "Ray2 Flash Modify is a video generative model capable of restyling or retexturing the entire shot, from turning live-action into CG or stylized animation, to changing wardrobe, props, or the overall aesthetic and swap environments or time periods, giving you control over background, location, or even weather.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.LumaDreamMachineRay2FlashModify",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Instruction for modifying the video"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video to modify"
        },
        {
          "name": "mode",
          "type": {
            "type": "enum",
            "values": [
              "adhere_1",
              "adhere_2",
              "adhere_3",
              "flex_1",
              "flex_2",
              "flex_3",
              "reimagine_1",
              "reimagine_2",
              "reimagine_3"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2FlashModify.Mode"
          },
          "default": "flex_1",
          "title": "Mode",
          "description": "Amount of modification to apply to the video, adhere_1 is the least amount of modification, reimagine_3 is the most"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of the first frame image for modification"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Luma Dream Machine Ray 2 Flash Reframe",
      "description": "Adjust and enhance videos with Ray-2 Reframe. This advanced tool seamlessly reframes videos to your desired aspect ratio, intelligently inpainting missing regions to ensure realistic visuals and coherent motion, delivering exceptional quality and creative flexibility.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.LumaDreamMachineRay2FlashReframe",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional prompt for reframing"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "21:9",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2FlashReframe.AspectRatio"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the reframed video"
        },
        {
          "name": "y_start",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Y Start",
          "description": "Start Y coordinate for reframing"
        },
        {
          "name": "x_end",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "X End",
          "description": "End X coordinate for reframing"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video to reframe"
        },
        {
          "name": "y_end",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Y End",
          "description": "End Y coordinate for reframing"
        },
        {
          "name": "x_start",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "X Start",
          "description": "Start X coordinate for reframing"
        },
        {
          "name": "grid_position_y",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Grid Position Y",
          "description": "Y position of the grid for reframing"
        },
        {
          "name": "grid_position_x",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Grid Position X",
          "description": "X position of the grid for reframing"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of the first frame image for reframing"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Luma Dream Machine Ray 2 Modify",
      "description": "Ray2 Modify is a video generative model capable of restyling or retexturing the entire shot, from turning live-action into CG or stylized animation, to changing wardrobe, props, or the overall aesthetic and swap environments or time periods, giving you control over background, location, or even weather.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.LumaDreamMachineRay2Modify",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Instruction for modifying the video"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video to modify"
        },
        {
          "name": "mode",
          "type": {
            "type": "enum",
            "values": [
              "adhere_1",
              "adhere_2",
              "adhere_3",
              "flex_1",
              "flex_2",
              "flex_3",
              "reimagine_1",
              "reimagine_2",
              "reimagine_3"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2Modify.Mode"
          },
          "default": "flex_1",
          "title": "Mode",
          "description": "Amount of modification to apply to the video, adhere_1 is the least amount of modification, reimagine_3 is the most"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of the first frame image for modification"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Luma Dream Machine Ray 2 Reframe",
      "description": "Adjust and enhance videos with Ray-2 Reframe. This advanced tool seamlessly reframes videos to your desired aspect ratio, intelligently inpainting missing regions to ensure realistic visuals and coherent motion, delivering exceptional quality and creative flexibility.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.LumaDreamMachineRay2Reframe",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional prompt for reframing"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "1:1",
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "21:9",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2Reframe.AspectRatio"
          },
          "default": "",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the reframed video"
        },
        {
          "name": "y_start",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Y Start",
          "description": "Start Y coordinate for reframing"
        },
        {
          "name": "x_end",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "X End",
          "description": "End X coordinate for reframing"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video to reframe"
        },
        {
          "name": "y_end",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Y End",
          "description": "End Y coordinate for reframing"
        },
        {
          "name": "x_start",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "X Start",
          "description": "Start X coordinate for reframing"
        },
        {
          "name": "grid_position_y",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Grid Position Y",
          "description": "Y position of the grid for reframing"
        },
        {
          "name": "grid_position_x",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Grid Position X",
          "description": "X position of the grid for reframing"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of the first frame image for reframing"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Mirelo Ai Sfx V15 Video To Video",
      "description": "Mirelo SFX V1.5\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.MireloAiSfxV15VideoToVideo",
      "properties": [
        {
          "name": "num_samples",
          "type": {
            "type": "str"
          },
          "default": 2,
          "title": "Num Samples",
          "description": "The number of samples to generate from the model"
        },
        {
          "name": "duration",
          "type": {
            "type": "str"
          },
          "default": 10,
          "title": "Duration",
          "description": "The duration of the generated audio in seconds"
        },
        {
          "name": "start_offset",
          "type": {
            "type": "str"
          },
          "default": 0,
          "title": "Start Offset",
          "description": "The start offset in seconds to start the audio generation from"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "A video url that can accessed from the API to process and add sound effects"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": 8069,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used"
        },
        {
          "name": "text_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Prompt",
          "description": "Additional description to guide the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Mirelo Ai Sfx V1 Video To Video",
      "description": "Generate synced sounds for any video, and return it with its new sound track (like MMAudio) \n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.MireloAiSfxV1VideoToVideo",
      "properties": [
        {
          "name": "num_samples",
          "type": {
            "type": "str"
          },
          "default": 2,
          "title": "Num Samples",
          "description": "The number of samples to generate from the model"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "A video url that can accessed from the API to process and add sound effects"
        },
        {
          "name": "duration",
          "type": {
            "type": "str"
          },
          "default": 10,
          "title": "Duration",
          "description": "The duration of the generated audio in seconds"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": 2105,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used"
        },
        {
          "name": "text_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Prompt",
          "description": "Additional description to guide the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Moonvalley Marey Motion Transfer",
      "description": "Pull motion from a reference video and apply it to new subjects or scenes.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.MoonvalleyMareyMotionTransfer",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a video from"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to use as the control video."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for random number generation. Use -1 for random seed each run."
        },
        {
          "name": "reference_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Reference Image Url",
          "description": "Optional reference image URL to use for pose control or as a starting frame"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "<synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts",
          "title": "Negative Prompt",
          "description": "Negative prompt used to guide the model away from undesirable features."
        },
        {
          "name": "first_frame_image_url",
          "type": {
            "type": "image"
          },
          "default": "https://video-editor-files-prod.s3.us-east-2.amazonaws.com/users/1e4d46df-0702-4491-95ce-763592f33f34/uploaded-images/9b9dce1c-abd0-46c0-bac9-9454f8893b06/original",
          "title": "First Frame Image Url",
          "description": "Optional first frame image URL to use as the first frame of the generated video"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Moonvalley Marey Pose Transfer",
      "description": "Ideal for matching human movement. Your input video determines human poses, gestures, and body movements that will appear in the generated video.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.MoonvalleyMareyPoseTransfer",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a video from"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to use as the control video."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for random number generation. Use -1 for random seed each run."
        },
        {
          "name": "reference_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Reference Image Url",
          "description": "Optional reference image URL to use for pose control or as a starting frame"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "<synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts",
          "title": "Negative Prompt",
          "description": "Negative prompt used to guide the model away from undesirable features."
        },
        {
          "name": "first_frame_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "First Frame Image Url",
          "description": "Optional first frame image URL to use as the first frame of the generated video"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "One To All Animation 13B",
      "description": "One To All Animation\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.OneToAllAnimation13B",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.OneToAllAnimation13B.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the video to generate."
        },
        {
          "name": "image_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Image Guidance Scale",
          "description": "The image guidance scale to use for the video generation."
        },
        {
          "name": "pose_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Pose Guidance Scale",
          "description": "The pose guidance scale to use for the video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to use as a reference for the video generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a reference for the video generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "One To All Animation 14B",
      "description": "One To All Animation\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.OneToAllAnimation14B",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.OneToAllAnimation14B.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the video to generate."
        },
        {
          "name": "image_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Image Guidance Scale",
          "description": "The image guidance scale to use for the video generation."
        },
        {
          "name": "pose_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Pose Guidance Scale",
          "description": "The pose guidance scale to use for the video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to use as a reference for the video generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a reference for the video generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Pika V2 Pikadditions",
      "description": "Pikadditions is a powerful video-to-video AI model that allows you to add anyone or anything to any video with seamless integration.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.PikaV2Pikadditions",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing what to add"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to guide the model"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to add"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Pixverse Extend",
      "description": "PixVerse Extend model is a video extending tool for your videos using with high-quality video extending techniques \n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.PixverseExtend",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt describing how to extend the video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.PixverseExtend.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.PixverseExtend.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. 1080p videos are limited to 5 seconds"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.PixverseExtend.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the extended video"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video to extend"
        },
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "v3.5",
              "v4",
              "v4.5",
              "v5",
              "v5.5",
              "v5.6"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.PixverseExtend.Model"
          },
          "default": "v4.5",
          "title": "Model",
          "description": "The model version to use for generation"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Pixverse Extend Fast",
      "description": "PixVerse Extend model is a video extending tool for your videos using with high-quality video extending techniques \n    video, editing, video-to-video, vid2vid, fast\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.PixverseExtendFast",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt describing how to extend the video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.PixverseExtendFast.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video. Fast mode doesn't support 1080p"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video to extend"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.PixverseExtendFast.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the extended video"
        },
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "v3.5",
              "v4",
              "v4.5",
              "v5",
              "v5.5",
              "v5.6"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.PixverseExtendFast.Model"
          },
          "default": "v4.5",
          "title": "Model",
          "description": "The model version to use for generation"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Pixverse Lipsync",
      "description": "Generate realistic lipsync animations from audio using advanced algorithms for high-quality synchronization with PixVerse Lipsync model\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.PixverseLipsync",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "Text content for TTS when audio_url is not provided"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the input audio. If not provided, TTS will be used."
        },
        {
          "name": "voice_id",
          "type": {
            "type": "enum",
            "values": [
              "Emily",
              "James",
              "Isabella",
              "Liam",
              "Chloe",
              "Adrian",
              "Harper",
              "Ava",
              "Sophia",
              "Julia",
              "Mason",
              "Jack",
              "Oliver",
              "Ethan",
              "Auto"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.PixverseLipsync.VoiceId"
          },
          "default": "Auto",
          "title": "Voice Id",
          "description": "Voice to use for TTS when audio_url is not provided"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Pixverse Sound Effects",
      "description": "Add immersive sound effects and background music to your videos using PixVerse sound effects  generation\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.PixverseSoundEffects",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Description of the sound effect to generate. If empty, a random sound effect will be generated"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video to add sound effects to"
        },
        {
          "name": "original_sound_switch",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Original Sound Switch",
          "description": "Whether to keep the original audio from the video"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Rife Video",
      "description": "Interpolate videos with RIFE - Real-Time Intermediate Flow Estimation\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.RifeVideo",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to use for interpolation."
        },
        {
          "name": "use_scene_detection",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Scene Detection",
          "description": "If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene."
        },
        {
          "name": "loop",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Loop",
          "description": "If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Frames",
          "description": "The number of frames to generate between the input video frames."
        },
        {
          "name": "use_calculated_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Calculated Fps",
          "description": "If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Fps",
          "description": "Frames per second for the output video. Only applicable if use_calculated_fps is False."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Sam 2 Video",
      "description": "SAM 2 is a model for segmenting images and videos in real-time.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Sam2Video",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to be segmented."
        },
        {
          "name": "prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Prompts",
          "description": "List of prompts to segment the video"
        },
        {
          "name": "boundingbox_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Boundingbox Zip",
          "description": "Return per-frame bounding box overlays as a zip archive."
        },
        {
          "name": "box_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Box Prompts",
          "description": "Coordinates for boxes"
        },
        {
          "name": "apply_mask",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Apply Mask",
          "description": "Apply the mask on the video."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "The URL of the mask to be applied initially."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Sam 3 Video",
      "description": "Sam 3\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Sam3Video",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth')."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to be segmented."
        },
        {
          "name": "detection_threshold",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detection Threshold",
          "description": "Detection confidence threshold (0.0-1.0). Lower = more detections but less precise."
        },
        {
          "name": "box_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Box Prompts",
          "description": "List of box prompt coordinates (x_min, y_min, x_max, y_max)."
        },
        {
          "name": "point_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Point Prompts",
          "description": "List of point prompts"
        },
        {
          "name": "apply_mask",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Apply Mask",
          "description": "Apply the mask on the video."
        },
        {
          "name": "text_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Prompt",
          "description": "[DEPRECATED] Use 'prompt' instead. Kept for backward compatibility."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Sam 3 Video Rle",
      "description": "Sam 3\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Sam3VideoRle",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth')."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to be segmented."
        },
        {
          "name": "detection_threshold",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Detection Threshold",
          "description": "Detection confidence threshold (0.0-1.0). Lower = more detections but less precise. Defaults: 0.5 for existing, 0.7 for new objects. Try 0.2-0.3 if text prompts fail."
        },
        {
          "name": "box_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Box Prompts",
          "description": "List of box prompts with optional frame_index."
        },
        {
          "name": "point_prompts",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Point Prompts",
          "description": "List of point prompts with frame indices."
        },
        {
          "name": "boundingbox_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Boundingbox Zip",
          "description": "Return per-frame bounding box overlays as a zip archive."
        },
        {
          "name": "frame_index",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Frame Index",
          "description": "Frame index used for initial interaction when mask_url is provided."
        },
        {
          "name": "mask_url",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Mask Url",
          "description": "The URL of the mask to be applied initially."
        },
        {
          "name": "apply_mask",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Apply Mask",
          "description": "Apply the mask on the video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Scail",
      "description": "Scail\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Scail",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to guide video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to use as a reference for the video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "512p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Scail.Resolution"
          },
          "default": "512p",
          "title": "Resolution",
          "description": "Output resolution. Outputs 896x512 (landscape) or 512x896 (portrait) based on the input image aspect ratio."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for the video generation."
        },
        {
          "name": "multi_character",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Multi Character",
          "description": "Enable multi-character mode. Use when driving video has multiple people."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a reference for the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Seedvr Upscale Video",
      "description": "SeedVR2\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.SeedvrUpscaleVideo",
      "properties": [
        {
          "name": "upscale_mode",
          "type": {
            "type": "enum",
            "values": [
              "target",
              "factor"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideo.UpscaleMode"
          },
          "default": "factor",
          "title": "Upscale Mode",
          "description": "The mode to use for the upscale. If 'target', the upscale factor will be calculated based on the target resolution. If 'factor', the upscale factor will be used directly."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The input video to be processed"
        },
        {
          "name": "noise_scale",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Noise Scale",
          "description": "The noise scale to use for the generation process."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideo.OutputFormat"
          },
          "default": "X264 (.mp4)",
          "title": "Output Format",
          "description": "The format of the output video."
        },
        {
          "name": "output_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideo.OutputWriteMode"
          },
          "default": "balanced",
          "title": "Output Write Mode",
          "description": "The write mode of the output video."
        },
        {
          "name": "target_resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "1440p",
              "2160p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideo.TargetResolution"
          },
          "default": "1080p",
          "title": "Target Resolution",
          "description": "The target resolution to upscale to when `upscale_mode` is `target`."
        },
        {
          "name": "output_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideo.OutputQuality"
          },
          "default": "high",
          "title": "Output Quality",
          "description": "The quality of the output video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "upscale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Upscale Factor",
          "description": "Upscaling factor to be used. Will multiply the dimensions with this factor when `upscale_mode` is `factor`."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The random seed used for the generation process."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Sora 2 Video To Video Remix",
      "description": "Sora 2\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Sora2VideoToVideoRemix",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Updated text prompt that directs the remix generation"
        },
        {
          "name": "video_id",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Video Id",
          "description": "The video_id from a previous Sora 2 generation. Note: You can only remix videos that were generated by Sora (via text-to-video or image-to-video endpoints), not arbitrary uploaded videos."
        },
        {
          "name": "delete_video",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Delete Video",
          "description": "Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Steady Dancer",
      "description": "Steady Dancer\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.SteadyDancer",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A person dancing with smooth and natural movements.",
          "title": "Prompt",
          "description": "Text prompt describing the desired animation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "image"
          },
          "default": "https://v3b.fal.media/files/b/0a84de68/jXDWywjhagRfR-GuZjoRs_video.mp4",
          "title": "Video Url",
          "description": "URL of the driving pose video. The motion from this video will be transferred to the reference image."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "light",
              "moderate",
              "aggressive"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SteadyDancer.Acceleration"
          },
          "default": "aggressive",
          "title": "Acceleration",
          "description": "Acceleration levels."
        },
        {
          "name": "pose_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Pose Guidance Scale",
          "description": "Pose guidance scale for pose control strength."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "pose_guidance_end",
          "type": {
            "type": "float"
          },
          "default": 0.4,
          "title": "Pose Guidance End",
          "description": "End ratio for pose guidance. Controls when pose guidance ends."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 24. If not specified, uses the FPS from the input video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale for prompt adherence."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Frames",
          "description": "Number of frames to generate. If not specified, uses the frame count from the input video (capped at 241). Will be adjusted to nearest valid value (must satisfy 4k+1 pattern)."
        },
        {
          "name": "use_turbo",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Turbo",
          "description": "If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized (num_inference_steps=6, guidance_scale=1.0) and uses the LightX2V distillation LoRA."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurred, distorted face, bad anatomy, extra limbs, poorly drawn hands, poorly drawn feet, disfigured, out of frame, duplicate, watermark, signature, text",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SteadyDancer.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video. If 'auto', will be determined from the reference image."
        },
        {
          "name": "pose_guidance_start",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Pose Guidance Start",
          "description": "Start ratio for pose guidance. Controls when pose guidance begins."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "576p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SteadyDancer.Resolution"
          },
          "default": "576p",
          "title": "Resolution",
          "description": "Resolution of the generated video. 576p is default, 720p for higher quality. 480p is lower quality."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": "https://v3b.fal.media/files/b/0a85edaa/GDUCMPrdvOMcI5JpEcU7f.png",
          "title": "Image Url",
          "description": "URL of the reference image to animate. This is the person/character whose appearance will be preserved."
        },
        {
          "name": "preserve_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preserve Audio",
          "description": "If enabled, copies audio from the input driving video to the output video."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 6,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Sync Lipsync",
      "description": "Generate realistic lipsync animations from audio using advanced algorithms for high-quality synchronization.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.SyncLipsync",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "lipsync-1.8.0",
              "lipsync-1.7.1",
              "lipsync-1.9.0-beta"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SyncLipsync.Model"
          },
          "default": "lipsync-1.9.0-beta",
          "title": "Model",
          "description": "The model to use for lipsyncing"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "enum",
            "values": [
              "cut_off",
              "loop",
              "bounce",
              "silence",
              "remap"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SyncLipsync.SyncMode"
          },
          "default": "cut_off",
          "title": "Sync Mode",
          "description": "Lipsync mode when audio and video durations are out of sync."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the input audio"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Sync Lipsync React 1",
      "description": "Sync React-1\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.SyncLipsyncReact1",
      "properties": [
        {
          "name": "emotion",
          "type": {
            "type": "enum",
            "values": [
              "happy",
              "angry",
              "sad",
              "neutral",
              "disgusted",
              "surprised"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SyncLipsyncReact1.Emotion"
          },
          "default": "",
          "title": "Emotion",
          "description": "Emotion prompt for the generation. Currently supports single-word emotions only."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the input video. Must be **15 seconds or shorter**."
        },
        {
          "name": "lipsync_mode",
          "type": {
            "type": "enum",
            "values": [
              "cut_off",
              "loop",
              "bounce",
              "silence",
              "remap"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SyncLipsyncReact1.LipsyncMode"
          },
          "default": "bounce",
          "title": "Lipsync Mode",
          "description": "Lipsync mode when audio and video durations are out of sync."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL to the input audio. Must be **15 seconds or shorter**."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Temperature",
          "description": "Controls the expresiveness of the lipsync."
        },
        {
          "name": "model_mode",
          "type": {
            "type": "enum",
            "values": [
              "lips",
              "face",
              "head"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SyncLipsyncReact1.ModelMode"
          },
          "default": "face",
          "title": "Model Mode",
          "description": "Controls the edit region and movement scope for the model. Available options: - `lips`: Only lipsync using react-1 (minimal facial changes). - `face`: Lipsync + facial expressions without head movements. - `head`: Lipsync + facial expressions + natural talking head movements."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Sync Lipsync V2",
      "description": "Generate realistic lipsync animations from audio using advanced algorithms for high-quality synchronization with Sync Lipsync 2.0 model\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.SyncLipsyncV2",
      "properties": [
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "lipsync-2",
              "lipsync-2-pro"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SyncLipsyncV2.Model"
          },
          "default": "lipsync-2",
          "title": "Model",
          "description": "The model to use for lipsyncing. `lipsync-2-pro` will cost roughly 1.67 times as much as `lipsync-2` for the same duration."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "enum",
            "values": [
              "cut_off",
              "loop",
              "bounce",
              "silence",
              "remap"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SyncLipsyncV2.SyncMode"
          },
          "default": "cut_off",
          "title": "Sync Mode",
          "description": "Lipsync mode when audio and video durations are out of sync."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the input audio"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Sync Lipsync V2 Pro",
      "description": "Generate high-quality realistic lipsync animations from audio while preserving unique details like natural teeth and unique facial features using the state-of-the-art Sync Lipsync 2 Pro model.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.SyncLipsyncV2Pro",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "enum",
            "values": [
              "cut_off",
              "loop",
              "bounce",
              "silence",
              "remap"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.SyncLipsyncV2Pro.SyncMode"
          },
          "default": "cut_off",
          "title": "Sync Mode",
          "description": "Lipsync mode when audio and video durations are out of sync."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the input audio"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Thinksound",
      "description": "Generate realistic audio for a video with an optional text prompt and combine\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Thinksound",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "A prompt to guide the audio generation. If not provided, it will be extracted from the video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to generate the audio for."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Num Inference Steps",
          "description": "The number of inference steps for audio generation."
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Cfg Scale",
          "description": "The classifier-free guidance scale for audio generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Thinksound Audio",
      "description": "Generate realistic audio from a video with an optional text prompt\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.ThinksoundAudio",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "A prompt to guide the audio generation. If not provided, it will be extracted from the video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to generate the audio for."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Num Inference Steps",
          "description": "The number of inference steps for audio generation."
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Cfg Scale",
          "description": "The classifier-free guidance scale for audio generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Topaz Upscale Video",
      "description": "Professional-grade video upscaling using Topaz technology. Enhance your videos with high-quality upscaling.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.TopazUpscaleVideo",
      "properties": [
        {
          "name": "H264_output",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "H264 Output",
          "description": "Whether to use H264 codec for output video. Default is H265."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video to upscale"
        },
        {
          "name": "upscale_factor",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Upscale Factor",
          "description": "Factor to upscale the video by (e.g. 2.0 doubles width and height)"
        },
        {
          "name": "target_fps",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Target Fps",
          "description": "Target FPS for frame interpolation. If set, frame interpolation will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Veed Lipsync",
      "description": "Generate realistic lipsync from any audio using VEED's latest model\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.VeedLipsync",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Veed Video Background Removal",
      "description": "Video Background Removal\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.VeedVideoBackgroundRemoval",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url"
        },
        {
          "name": "subject_is_person",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Subject Is Person",
          "description": "Set to False if the subject is not a person."
        },
        {
          "name": "output_codec",
          "type": {
            "type": "enum",
            "values": [
              "vp9",
              "h264"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemoval.OutputCodec"
          },
          "default": "vp9",
          "title": "Output Codec",
          "description": "Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality."
        },
        {
          "name": "refine_foreground_edges",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Refine Foreground Edges",
          "description": "Improves the quality of the extracted object's edges."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Veed Video Background Removal Fast",
      "description": "Video Background Removal\n    video, editing, video-to-video, vid2vid, fast\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.VeedVideoBackgroundRemovalFast",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url"
        },
        {
          "name": "subject_is_person",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Subject Is Person",
          "description": "Set to False if the subject is not a person."
        },
        {
          "name": "output_codec",
          "type": {
            "type": "enum",
            "values": [
              "vp9",
              "h264"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemovalFast.OutputCodec"
          },
          "default": "vp9",
          "title": "Output Codec",
          "description": "Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality."
        },
        {
          "name": "refine_foreground_edges",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Refine Foreground Edges",
          "description": "Improves the quality of the extracted object's edges."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Veed Video Background Removal Green Screen",
      "description": "Video Background Removal\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.VeedVideoBackgroundRemovalGreenScreen",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url"
        },
        {
          "name": "output_codec",
          "type": {
            "type": "enum",
            "values": [
              "vp9",
              "h264"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemovalGreenScreen.OutputCodec"
          },
          "default": "vp9",
          "title": "Output Codec",
          "description": "Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality."
        },
        {
          "name": "spill_suppression_strength",
          "type": {
            "type": "str"
          },
          "default": 0.8,
          "title": "Spill Suppression Strength",
          "description": "Increase the value if green spots remain in the video, decrease if color changes are noticed on the extracted subject."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Veo 31 Extend Video",
      "description": "Veo 3.1\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Veo31ExtendVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing how the video should be extended"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "7s"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Veo31ExtendVideo.Duration"
          },
          "default": "7s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Veo31ExtendVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Veo31ExtendVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Veo 31 Fast Extend Video",
      "description": "Veo 3.1 Fast\n    video, editing, video-to-video, vid2vid, fast\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Veo31FastExtendVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing how the video should be extended"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "7s"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Veo31FastExtendVideo.Duration"
          },
          "default": "7s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Veo31FastExtendVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Veo31FastExtendVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Video As Prompt",
      "description": "Video As Prompt\n    video, editing, video-to-video, vid2vid, professional\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.VideoAsPrompt",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate an image from."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.VideoAsPrompt.AspectRatio"
          },
          "default": "9:16",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.VideoAsPrompt.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "reference video to generate effect video from."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Input image to generate the effect video for."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Fps",
          "description": "Frames per second for the output video. Only applicable if output_type is 'video'."
        },
        {
          "name": "video_description",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Video Description",
          "description": "A brief description of the input video content."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducible generation. If set none, a random seed will be used."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 49,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Video Upscaler",
      "description": "Video Upscaler enhances video resolution and quality using AI.\n    video, upscaling, enhancement, resolution, video-to-video\n\n    Use cases:\n    - Upscale low resolution videos\n    - Enhance video quality\n    - Increase video resolution\n    - Improve video clarity\n    - Restore old video footage",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.VideoUpscaler",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to upscale"
        },
        {
          "name": "scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Scale",
          "description": "The scale factor"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan 22 Vace Fun A14 b Depth",
      "description": "VACE Fun for Wan 2.2 A14B from Alibaba-PAI\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Wan22VaceFunA14bDepth",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the source video file. Required for depth task."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between the original frames. A value of 0 means no interpolation."
        },
        {
          "name": "temporal_downsample_factor",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Temporal Downsample Factor",
          "description": "Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied."
        },
        {
          "name": "first_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "First Frame Url",
          "description": "URL to the first frame of the video. If provided, the model will use this frame as a reference."
        },
        {
          "name": "ref_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Ref Image Urls",
          "description": "URLs to source reference image. If provided, the model will use this image as reference."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 241 (inclusive)."
        },
        {
          "name": "auto_downsample_min_fps",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Auto Downsample Min Fps",
          "description": "The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences."
        },
        {
          "name": "transparency_mode",
          "type": {
            "type": "enum",
            "values": [
              "content_aware",
              "white",
              "black"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepth.TransparencyMode"
          },
          "default": "content_aware",
          "title": "Transparency Mode",
          "description": "The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled."
        },
        {
          "name": "sampler",
          "type": {
            "type": "enum",
            "values": [
              "unipc",
              "dpm++",
              "euler"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepth.Sampler"
          },
          "default": "unipc",
          "title": "Sampler",
          "description": "Sampler to use for video generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepth.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "rife",
              "film"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepth.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. Options are 'rife' or 'film'."
        },
        {
          "name": "enable_auto_downsample",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Auto Downsample",
          "description": "If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length."
        },
        {
          "name": "preprocess",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preprocess",
          "description": "Whether to preprocess the input video."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "str"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "str"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true."
        },
        {
          "name": "match_input_num_frames",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Num Frames",
          "description": "If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepth.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepth.Resolution"
          },
          "default": "auto",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "If true, also return a ZIP file containing all generated frames."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepth.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "match_input_frames_per_second",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Frames Per Second",
          "description": "If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "last_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Last Frame Url",
          "description": "URL to the last frame of the video. If provided, the model will use this frame as a reference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan 22 Vace Fun A14 b Inpainting",
      "description": "VACE Fun for Wan 2.2 A14B from Alibaba-PAI\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Wan22VaceFunA14bInpainting",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the source video file. Required for inpainting."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between the original frames. A value of 0 means no interpolation."
        },
        {
          "name": "temporal_downsample_factor",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Temporal Downsample Factor",
          "description": "Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied."
        },
        {
          "name": "first_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "First Frame Url",
          "description": "URL to the first frame of the video. If provided, the model will use this frame as a reference."
        },
        {
          "name": "ref_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Ref Image Urls",
          "description": "Urls to source reference image. If provided, the model will use this image as reference."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 241 (inclusive)."
        },
        {
          "name": "auto_downsample_min_fps",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Auto Downsample Min Fps",
          "description": "The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences."
        },
        {
          "name": "transparency_mode",
          "type": {
            "type": "enum",
            "values": [
              "content_aware",
              "white",
              "black"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpainting.TransparencyMode"
          },
          "default": "content_aware",
          "title": "Transparency Mode",
          "description": "The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled."
        },
        {
          "name": "sampler",
          "type": {
            "type": "enum",
            "values": [
              "unipc",
              "dpm++",
              "euler"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpainting.Sampler"
          },
          "default": "unipc",
          "title": "Sampler",
          "description": "Sampler to use for video generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpainting.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "mask_video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Mask Video Url",
          "description": "URL to the source mask file. Required for inpainting."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "rife",
              "film"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpainting.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. Options are 'rife' or 'film'."
        },
        {
          "name": "enable_auto_downsample",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Auto Downsample",
          "description": "If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "preprocess",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preprocess",
          "description": "Whether to preprocess the input video."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "str"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster."
        },
        {
          "name": "mask_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image Url",
          "description": "URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video using salient mask tracking. Will be ignored if mask_video_url is provided."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "str"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true."
        },
        {
          "name": "match_input_num_frames",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Num Frames",
          "description": "If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpainting.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpainting.Resolution"
          },
          "default": "auto",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "If true, also return a ZIP file containing all generated frames."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpainting.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "match_input_frames_per_second",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Frames Per Second",
          "description": "If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "last_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Last Frame Url",
          "description": "URL to the last frame of the video. If provided, the model will use this frame as a reference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan 22 Vace Fun A14 b Outpainting",
      "description": "VACE Fun for Wan 2.2 A14B from Alibaba-PAI\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Wan22VaceFunA14bOutpainting",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the source video file. Required for outpainting."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between the original frames. A value of 0 means no interpolation."
        },
        {
          "name": "temporal_downsample_factor",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Temporal Downsample Factor",
          "description": "Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied."
        },
        {
          "name": "first_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "First Frame Url",
          "description": "URL to the first frame of the video. If provided, the model will use this frame as a reference."
        },
        {
          "name": "ref_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Ref Image Urls",
          "description": "URLs to source reference image. If provided, the model will use this image as reference."
        },
        {
          "name": "expand_ratio",
          "type": {
            "type": "float"
          },
          "default": 0.25,
          "title": "Expand Ratio",
          "description": "Amount of expansion. This is a float value between 0 and 1, where 0.25 adds 25% to the original video size on the specified sides."
        },
        {
          "name": "transparency_mode",
          "type": {
            "type": "enum",
            "values": [
              "content_aware",
              "white",
              "black"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpainting.TransparencyMode"
          },
          "default": "content_aware",
          "title": "Transparency Mode",
          "description": "The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 241 (inclusive)."
        },
        {
          "name": "auto_downsample_min_fps",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Auto Downsample Min Fps",
          "description": "The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt."
        },
        {
          "name": "sampler",
          "type": {
            "type": "enum",
            "values": [
              "unipc",
              "dpm++",
              "euler"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpainting.Sampler"
          },
          "default": "unipc",
          "title": "Sampler",
          "description": "Sampler to use for video generation."
        },
        {
          "name": "expand_bottom",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Bottom",
          "description": "Whether to expand the video to the bottom."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpainting.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "expand_left",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Left",
          "description": "Whether to expand the video to the left."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "rife",
              "film"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpainting.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. Options are 'rife' or 'film'."
        },
        {
          "name": "enable_auto_downsample",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Auto Downsample",
          "description": "If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length."
        },
        {
          "name": "expand_top",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Top",
          "description": "Whether to expand the video to the top."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "str"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "str"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true."
        },
        {
          "name": "match_input_num_frames",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Num Frames",
          "description": "If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpainting.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "expand_right",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Right",
          "description": "Whether to expand the video to the right."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpainting.Resolution"
          },
          "default": "auto",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "If true, also return a ZIP file containing all generated frames."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpainting.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "match_input_frames_per_second",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Frames Per Second",
          "description": "If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "last_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Last Frame Url",
          "description": "URL to the last frame of the video. If provided, the model will use this frame as a reference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan 22 Vace Fun A14 b Pose",
      "description": "VACE Fun for Wan 2.2 A14B from Alibaba-PAI\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Wan22VaceFunA14bPose",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation. For pose task, the prompt should describe the desired pose and action of the subject in the video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the source video file. Required for pose task."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between the original frames. A value of 0 means no interpolation."
        },
        {
          "name": "temporal_downsample_factor",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Temporal Downsample Factor",
          "description": "Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied."
        },
        {
          "name": "first_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "First Frame Url",
          "description": "URL to the first frame of the video. If provided, the model will use this frame as a reference."
        },
        {
          "name": "ref_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Ref Image Urls",
          "description": "URLs to source reference image. If provided, the model will use this image as reference."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 241 (inclusive)."
        },
        {
          "name": "auto_downsample_min_fps",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Auto Downsample Min Fps",
          "description": "The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences."
        },
        {
          "name": "transparency_mode",
          "type": {
            "type": "enum",
            "values": [
              "content_aware",
              "white",
              "black"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPose.TransparencyMode"
          },
          "default": "content_aware",
          "title": "Transparency Mode",
          "description": "The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled."
        },
        {
          "name": "sampler",
          "type": {
            "type": "enum",
            "values": [
              "unipc",
              "dpm++",
              "euler"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPose.Sampler"
          },
          "default": "unipc",
          "title": "Sampler",
          "description": "Sampler to use for video generation."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPose.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "rife",
              "film"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPose.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. Options are 'rife' or 'film'."
        },
        {
          "name": "enable_auto_downsample",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Auto Downsample",
          "description": "If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length."
        },
        {
          "name": "preprocess",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preprocess",
          "description": "Whether to preprocess the input video."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "str"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "str"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true."
        },
        {
          "name": "match_input_num_frames",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Num Frames",
          "description": "If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPose.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPose.Resolution"
          },
          "default": "auto",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "If true, also return a ZIP file containing all generated frames."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPose.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "match_input_frames_per_second",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Frames Per Second",
          "description": "If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "last_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Last Frame Url",
          "description": "URL to the last frame of the video. If provided, the model will use this frame as a reference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan 22 Vace Fun A14 b Reframe",
      "description": "VACE Fun for Wan 2.2 A14B from Alibaba-PAI\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.Wan22VaceFunA14bReframe",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation. Optional for reframing."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the source video file. This video will be used as a reference for the reframe task."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between the original frames. A value of 0 means no interpolation."
        },
        {
          "name": "temporal_downsample_factor",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Temporal Downsample Factor",
          "description": "Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied."
        },
        {
          "name": "first_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "First Frame Url",
          "description": "URL to the first frame of the video. If provided, the model will use this frame as a reference."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 241 (inclusive)."
        },
        {
          "name": "auto_downsample_min_fps",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Auto Downsample Min Fps",
          "description": "The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences."
        },
        {
          "name": "trim_borders",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Trim Borders",
          "description": "Whether to trim borders from the video."
        },
        {
          "name": "sampler",
          "type": {
            "type": "enum",
            "values": [
              "unipc",
              "dpm++",
              "euler"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframe.Sampler"
          },
          "default": "unipc",
          "title": "Sampler",
          "description": "Sampler to use for video generation."
        },
        {
          "name": "transparency_mode",
          "type": {
            "type": "enum",
            "values": [
              "content_aware",
              "white",
              "black"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframe.TransparencyMode"
          },
          "default": "content_aware",
          "title": "Transparency Mode",
          "description": "The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframe.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "rife",
              "film"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframe.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. Options are 'rife' or 'film'."
        },
        {
          "name": "enable_auto_downsample",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Auto Downsample",
          "description": "If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "str"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster."
        },
        {
          "name": "zoom_factor",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Zoom Factor",
          "description": "Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "str"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true."
        },
        {
          "name": "match_input_num_frames",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Num Frames",
          "description": "If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframe.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframe.Resolution"
          },
          "default": "auto",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "If true, also return a ZIP file containing all generated frames."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframe.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "match_input_frames_per_second",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Frames Per Second",
          "description": "If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "last_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Last Frame Url",
          "description": "URL to the last frame of the video. If provided, the model will use this frame as a reference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan Fun Control",
      "description": "Generate pose or depth controlled video using Alibaba-PAI's Wan 2.2 Fun\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanFunControl",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "The shift for the scheduler."
        },
        {
          "name": "preprocess_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preprocess Video",
          "description": "Whether to preprocess the video. If True, the video will be preprocessed to depth or pose."
        },
        {
          "name": "reference_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Reference Image Url",
          "description": "The URL of the reference image to use as a reference for the video generation."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Fps",
          "description": "The fps to generate. Only used when match_input_fps is False."
        },
        {
          "name": "match_input_num_frames",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Num Frames",
          "description": "Whether to match the number of frames in the input video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 6,
          "title": "Guidance Scale",
          "description": "The guidance scale."
        },
        {
          "name": "preprocess_type",
          "type": {
            "type": "enum",
            "values": [
              "depth",
              "pose"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanFunControl.PreprocessType"
          },
          "default": "depth",
          "title": "Preprocess Type",
          "description": "The type of preprocess to apply to the video. Only used when preprocess_video is True."
        },
        {
          "name": "control_video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Control Video Url",
          "description": "The URL of the control video to use as a reference for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "The number of frames to generate. Only used when match_input_num_frames is False."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Num Inference Steps",
          "description": "The number of inference steps."
        },
        {
          "name": "match_input_fps",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Fps",
          "description": "Whether to match the fps in the input video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan V2214 b Animate Move",
      "description": "Wan-Animate is a video model that generates high-fidelity character videos by replicating the expressions and movements of characters from reference videos.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanV2214bAnimateMove",
      "properties": [
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV2214bAnimateMove.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV2214bAnimateMove.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p, 580p, or 720p)."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift value for the video. Must be between 1.0 and 10.0."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "If true, also return a ZIP archive containing per-frame images generated on GPU (lossless)."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV2214bAnimateMove.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the output video. Higher quality means better visual quality but larger file size."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 20,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "use_turbo",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Turbo",
          "description": "If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized for best results."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan V2214 b Animate Replace",
      "description": "Wan-Animate Replace is a model that can integrate animated characters into reference videos, replacing the original character while preserving the scene\u2019s lighting and color tone for seamless environmental integration.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanV2214bAnimateReplace",
      "properties": [
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV2214bAnimateReplace.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV2214bAnimateReplace.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p, 580p, or 720p)."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift value for the video. Must be between 1.0 and 10.0."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "If true, also return a ZIP archive containing per-frame images generated on GPU (lossless)."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV2214bAnimateReplace.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the output video. Higher quality means better visual quality but larger file size."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 20,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "use_turbo",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Turbo",
          "description": "If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized for best results."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan V22 A14 b Video To Video",
      "description": "Wan-2.2 video-to-video is a video model that generates high-quality videos with high visual quality and motion diversity from text prompts and source videos.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanV22A14bVideoToVideo",
      "properties": [
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift value for the video. Must be between 1.0 and 10.0."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resample_fps",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Resample Fps",
          "description": "If true, the video will be resampled to the passed frames per second. If false, the video will not be resampled."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 17 to 161 (inclusive)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p, 580p, or 720p)."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "guidance_scale_2",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale 2",
          "description": "Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.9,
          "title": "Strength",
          "description": "Strength of the video transformation. A value of 1.0 means the output will be completely based on the prompt, while a value of 0.0 means the output will be identical to the input video."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the output video. Higher quality means better visual quality but larger file size."
        },
        {
          "name": "adjust_fps_for_interpolation",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Adjust Fps For Interpolation",
          "description": "If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "film",
              "rife"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideo.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. If None, no interpolation is applied."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan V26 Reference To Video",
      "description": "Wan v2.6 Reference to Video\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanV26ReferenceToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Use @Video1, @Video2, @Video3 to reference subjects from your videos. Works for people, animals, or objects. For multi-shot prompts: '[0-3s] Shot 1. [3-6s] Shot 2.' Max 800 characters."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV26ReferenceToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the generated video in seconds. R2V supports only 5 or 10 seconds (no 15s)."
        },
        {
          "name": "video_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Video Urls",
          "description": "Reference videos for subject consistency (1-3 videos). Videos' FPS must be at least 16 FPS.Reference in prompt as @Video1, @Video2, @Video3. Works for people, animals, or objects."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV26ReferenceToVideo.Resolution"
          },
          "default": "1080p",
          "title": "Resolution",
          "description": "Video resolution tier. R2V only supports 720p and 1080p (no 480p)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1",
              "4:3",
              "3:4"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanV26ReferenceToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt rewriting using LLM."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "multi_shots",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Multi Shots",
          "description": "When true (default), enables intelligent multi-shot segmentation for coherent narrative videos with multiple shots. When false, generates single continuous shot. Only active when enable_prompt_expansion is True."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to describe content to avoid. Max 500 characters."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan Vace",
      "description": "Vace a video generation model that uses a source image, mask, and video to create prompted videos with controllable sources.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanVace",
      "properties": [
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the source video file. If provided, the model will use this video as a reference."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "ref_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Ref Image Urls",
          "description": "Urls to source reference image. If provided, the model will use this image as reference."
        },
        {
          "name": "task",
          "type": {
            "type": "enum",
            "values": [
              "depth",
              "inpainting"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace.Task"
          },
          "default": "depth",
          "title": "Task",
          "description": "Task type for the model."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 24."
        },
        {
          "name": "mask_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image Url",
          "description": "URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video (16:9 or 9:16)."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p,580p, or 720p)."
        },
        {
          "name": "mask_video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Mask Video Url",
          "description": "URL to the source mask file. If provided, the model will use this mask as a reference."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "preprocess",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preprocess",
          "description": "Whether to preprocess the input video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan Vace 13 b",
      "description": "Vace a video generation model that uses a source image, mask, and video to create prompted videos with controllable sources.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanVace13b",
      "properties": [
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the source video file. If provided, the model will use this video as a reference."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "mask_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image Url",
          "description": "URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored."
        },
        {
          "name": "task",
          "type": {
            "type": "enum",
            "values": [
              "depth",
              "inpainting",
              "pose"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace13b.Task"
          },
          "default": "depth",
          "title": "Task",
          "description": "Task type for the model."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 24."
        },
        {
          "name": "ref_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Ref Image Urls",
          "description": "Urls to source reference image. If provided, the model will use this image as reference."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace13b.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p,580p, or 720p)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace13b.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video (16:9 or 9:16)."
        },
        {
          "name": "mask_video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Mask Video Url",
          "description": "URL to the source mask file. If provided, the model will use this mask as a reference."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "preprocess",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preprocess",
          "description": "Whether to preprocess the input video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan Vace 14 b",
      "description": "VACE is a video generation model that uses a source image, mask, and video to create prompted videos with controllable sources.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanVace14b",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the source video file. If provided, the model will use this video as a reference."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between the original frames. A value of 0 means no interpolation."
        },
        {
          "name": "temporal_downsample_factor",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Temporal Downsample Factor",
          "description": "Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied."
        },
        {
          "name": "first_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "First Frame Url",
          "description": "URL to the first frame of the video. If provided, the model will use this frame as a reference."
        },
        {
          "name": "ref_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Ref Image Urls",
          "description": "URLs to source reference image. If provided, the model will use this image as reference."
        },
        {
          "name": "transparency_mode",
          "type": {
            "type": "enum",
            "values": [
              "content_aware",
              "white",
              "black"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14b.TransparencyMode"
          },
          "default": "content_aware",
          "title": "Transparency Mode",
          "description": "The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 241 (inclusive)."
        },
        {
          "name": "auto_downsample_min_fps",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Auto Downsample Min Fps",
          "description": "The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt."
        },
        {
          "name": "sampler",
          "type": {
            "type": "enum",
            "values": [
              "unipc",
              "dpm++",
              "euler"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14b.Sampler"
          },
          "default": "unipc",
          "title": "Sampler",
          "description": "Sampler to use for video generation."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14b.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "mask_video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Mask Video Url",
          "description": "URL to the source mask file. If provided, the model will use this mask as a reference."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "rife",
              "film"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14b.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. Options are 'rife' or 'film'."
        },
        {
          "name": "enable_auto_downsample",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Auto Downsample",
          "description": "If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length."
        },
        {
          "name": "preprocess",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preprocess",
          "description": "Whether to preprocess the input video."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "str"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster."
        },
        {
          "name": "mask_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image Url",
          "description": "URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored."
        },
        {
          "name": "task",
          "type": {
            "type": "enum",
            "values": [
              "depth",
              "pose",
              "inpainting",
              "outpainting",
              "reframe"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14b.Task"
          },
          "default": "depth",
          "title": "Task",
          "description": "Task type for the model."
        },
        {
          "name": "match_input_num_frames",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Num Frames",
          "description": "If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "str"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14b.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "If true, also return a ZIP file containing all generated frames."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14b.Resolution"
          },
          "default": "auto",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14b.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "match_input_frames_per_second",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Frames Per Second",
          "description": "If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "last_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Last Frame Url",
          "description": "URL to the last frame of the video. If provided, the model will use this frame as a reference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan Vace 14 b Depth",
      "description": "VACE is a video generation model that uses a source image, mask, and video to create prompted videos with controllable sources.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanVace14bDepth",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the source video file. Required for depth task."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between the original frames. A value of 0 means no interpolation."
        },
        {
          "name": "temporal_downsample_factor",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Temporal Downsample Factor",
          "description": "Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied."
        },
        {
          "name": "first_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "First Frame Url",
          "description": "URL to the first frame of the video. If provided, the model will use this frame as a reference."
        },
        {
          "name": "ref_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Ref Image Urls",
          "description": "URLs to source reference image. If provided, the model will use this image as reference."
        },
        {
          "name": "transparency_mode",
          "type": {
            "type": "enum",
            "values": [
              "content_aware",
              "white",
              "black"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bDepth.TransparencyMode"
          },
          "default": "content_aware",
          "title": "Transparency Mode",
          "description": "The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 241 (inclusive)."
        },
        {
          "name": "auto_downsample_min_fps",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Auto Downsample Min Fps",
          "description": "The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt."
        },
        {
          "name": "sampler",
          "type": {
            "type": "enum",
            "values": [
              "unipc",
              "dpm++",
              "euler"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bDepth.Sampler"
          },
          "default": "unipc",
          "title": "Sampler",
          "description": "Sampler to use for video generation."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bDepth.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "rife",
              "film"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bDepth.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. Options are 'rife' or 'film'."
        },
        {
          "name": "enable_auto_downsample",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Auto Downsample",
          "description": "If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length."
        },
        {
          "name": "preprocess",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preprocess",
          "description": "Whether to preprocess the input video."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "str"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster."
        },
        {
          "name": "match_input_num_frames",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Num Frames",
          "description": "If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "str"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bDepth.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "If true, also return a ZIP file containing all generated frames."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bDepth.Resolution"
          },
          "default": "auto",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bDepth.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "match_input_frames_per_second",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Frames Per Second",
          "description": "If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "last_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Last Frame Url",
          "description": "URL to the last frame of the video. If provided, the model will use this frame as a reference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan Vace 14 b Inpainting",
      "description": "VACE is a video generation model that uses a source image, mask, and video to create prompted videos with controllable sources.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanVace14bInpainting",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the source video file. Required for inpainting."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between the original frames. A value of 0 means no interpolation."
        },
        {
          "name": "temporal_downsample_factor",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Temporal Downsample Factor",
          "description": "Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied."
        },
        {
          "name": "first_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "First Frame Url",
          "description": "URL to the first frame of the video. If provided, the model will use this frame as a reference."
        },
        {
          "name": "ref_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Ref Image Urls",
          "description": "Urls to source reference image. If provided, the model will use this image as reference."
        },
        {
          "name": "transparency_mode",
          "type": {
            "type": "enum",
            "values": [
              "content_aware",
              "white",
              "black"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bInpainting.TransparencyMode"
          },
          "default": "content_aware",
          "title": "Transparency Mode",
          "description": "The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 241 (inclusive)."
        },
        {
          "name": "auto_downsample_min_fps",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Auto Downsample Min Fps",
          "description": "The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt."
        },
        {
          "name": "sampler",
          "type": {
            "type": "enum",
            "values": [
              "unipc",
              "dpm++",
              "euler"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bInpainting.Sampler"
          },
          "default": "unipc",
          "title": "Sampler",
          "description": "Sampler to use for video generation."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bInpainting.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "mask_video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Mask Video Url",
          "description": "URL to the source mask file. Required for inpainting."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "rife",
              "film"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bInpainting.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. Options are 'rife' or 'film'."
        },
        {
          "name": "enable_auto_downsample",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Auto Downsample",
          "description": "If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "preprocess",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preprocess",
          "description": "Whether to preprocess the input video."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "str"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster."
        },
        {
          "name": "mask_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Mask Image Url",
          "description": "URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video using salient mask tracking. Will be ignored if mask_video_url is provided."
        },
        {
          "name": "match_input_num_frames",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Num Frames",
          "description": "If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "str"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bInpainting.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "If true, also return a ZIP file containing all generated frames."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bInpainting.Resolution"
          },
          "default": "auto",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bInpainting.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "match_input_frames_per_second",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Frames Per Second",
          "description": "If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "last_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Last Frame Url",
          "description": "URL to the last frame of the video. If provided, the model will use this frame as a reference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan Vace 14 b Outpainting",
      "description": "VACE is a video generation model that uses a source image, mask, and video to create prompted videos with controllable sources.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanVace14bOutpainting",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the source video file. Required for outpainting."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between the original frames. A value of 0 means no interpolation."
        },
        {
          "name": "temporal_downsample_factor",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Temporal Downsample Factor",
          "description": "Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied."
        },
        {
          "name": "first_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "First Frame Url",
          "description": "URL to the first frame of the video. If provided, the model will use this frame as a reference."
        },
        {
          "name": "ref_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Ref Image Urls",
          "description": "URLs to source reference image. If provided, the model will use this image as reference."
        },
        {
          "name": "expand_ratio",
          "type": {
            "type": "float"
          },
          "default": 0.25,
          "title": "Expand Ratio",
          "description": "Amount of expansion. This is a float value between 0 and 1, where 0.25 adds 25% to the original video size on the specified sides."
        },
        {
          "name": "transparency_mode",
          "type": {
            "type": "enum",
            "values": [
              "content_aware",
              "white",
              "black"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bOutpainting.TransparencyMode"
          },
          "default": "content_aware",
          "title": "Transparency Mode",
          "description": "The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 241 (inclusive)."
        },
        {
          "name": "auto_downsample_min_fps",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Auto Downsample Min Fps",
          "description": "The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences."
        },
        {
          "name": "expand_bottom",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Bottom",
          "description": "Whether to expand the video to the bottom."
        },
        {
          "name": "sampler",
          "type": {
            "type": "enum",
            "values": [
              "unipc",
              "dpm++",
              "euler"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bOutpainting.Sampler"
          },
          "default": "unipc",
          "title": "Sampler",
          "description": "Sampler to use for video generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bOutpainting.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "rife",
              "film"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bOutpainting.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. Options are 'rife' or 'film'."
        },
        {
          "name": "enable_auto_downsample",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Auto Downsample",
          "description": "If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length."
        },
        {
          "name": "expand_top",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Top",
          "description": "Whether to expand the video to the top."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "expand_left",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Left",
          "description": "Whether to expand the video to the left."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "str"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster."
        },
        {
          "name": "match_input_num_frames",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Num Frames",
          "description": "If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "str"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bOutpainting.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "If true, also return a ZIP file containing all generated frames."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bOutpainting.Resolution"
          },
          "default": "auto",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "expand_right",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Right",
          "description": "Whether to expand the video to the right."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bOutpainting.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "match_input_frames_per_second",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Frames Per Second",
          "description": "If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "last_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Last Frame Url",
          "description": "URL to the last frame of the video. If provided, the model will use this frame as a reference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan Vace 14 b Pose",
      "description": "VACE is a video generation model that uses a source image, mask, and video to create prompted videos with controllable sources.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanVace14bPose",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation. For pose task, the prompt should describe the desired pose and action of the subject in the video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the source video file. Required for pose task."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between the original frames. A value of 0 means no interpolation."
        },
        {
          "name": "temporal_downsample_factor",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Temporal Downsample Factor",
          "description": "Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied."
        },
        {
          "name": "first_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "First Frame Url",
          "description": "URL to the first frame of the video. If provided, the model will use this frame as a reference."
        },
        {
          "name": "ref_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Ref Image Urls",
          "description": "URLs to source reference image. If provided, the model will use this image as reference."
        },
        {
          "name": "transparency_mode",
          "type": {
            "type": "enum",
            "values": [
              "content_aware",
              "white",
              "black"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bPose.TransparencyMode"
          },
          "default": "content_aware",
          "title": "Transparency Mode",
          "description": "The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 241 (inclusive)."
        },
        {
          "name": "auto_downsample_min_fps",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Auto Downsample Min Fps",
          "description": "The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt."
        },
        {
          "name": "sampler",
          "type": {
            "type": "enum",
            "values": [
              "unipc",
              "dpm++",
              "euler"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bPose.Sampler"
          },
          "default": "unipc",
          "title": "Sampler",
          "description": "Sampler to use for video generation."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bPose.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "rife",
              "film"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bPose.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. Options are 'rife' or 'film'."
        },
        {
          "name": "enable_auto_downsample",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Auto Downsample",
          "description": "If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length."
        },
        {
          "name": "preprocess",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preprocess",
          "description": "Whether to preprocess the input video."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "str"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster."
        },
        {
          "name": "match_input_num_frames",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Num Frames",
          "description": "If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "str"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bPose.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "If true, also return a ZIP file containing all generated frames."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bPose.Resolution"
          },
          "default": "auto",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bPose.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "match_input_frames_per_second",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Match Input Frames Per Second",
          "description": "If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "last_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Last Frame Url",
          "description": "URL to the last frame of the video. If provided, the model will use this frame as a reference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan Vace 14 b Reframe",
      "description": "VACE is a video generation model that uses a source image, mask, and video to create prompted videos with controllable sources.\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanVace14bReframe",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation. Optional for reframing."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the source video file. This video will be used as a reference for the reframe task."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between the original frames. A value of 0 means no interpolation."
        },
        {
          "name": "temporal_downsample_factor",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Temporal Downsample Factor",
          "description": "Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied."
        },
        {
          "name": "first_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "First Frame Url",
          "description": "URL to the first frame of the video. If provided, the model will use this frame as a reference."
        },
        {
          "name": "transparency_mode",
          "type": {
            "type": "enum",
            "values": [
              "content_aware",
              "white",
              "black"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bReframe.TransparencyMode"
          },
          "default": "content_aware",
          "title": "Transparency Mode",
          "description": "The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 241 (inclusive)."
        },
        {
          "name": "trim_borders",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Trim Borders",
          "description": "Whether to trim borders from the video."
        },
        {
          "name": "auto_downsample_min_fps",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Auto Downsample Min Fps",
          "description": "The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences."
        },
        {
          "name": "sampler",
          "type": {
            "type": "enum",
            "values": [
              "unipc",
              "dpm++",
              "euler"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bReframe.Sampler"
          },
          "default": "unipc",
          "title": "Sampler",
          "description": "Sampler to use for video generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bReframe.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "rife",
              "film"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bReframe.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. Options are 'rife' or 'film'."
        },
        {
          "name": "enable_auto_downsample",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Auto Downsample",
          "description": "If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "str"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster."
        },
        {
          "name": "zoom_factor",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Zoom Factor",
          "description": "Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom."
        },
        {
          "name": "match_input_num_frames",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Num Frames",
          "description": "If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "str"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bReframe.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "If true, also return a ZIP file containing all generated frames."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bReframe.Resolution"
          },
          "default": "auto",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVace14bReframe.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "match_input_frames_per_second",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Input Frames Per Second",
          "description": "If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "last_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Last Frame Url",
          "description": "URL to the last frame of the video. If provided, the model will use this frame as a reference."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan Vace Apps Long Reframe",
      "description": "Wan 2.1 VACE Long Reframe\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanVaceAppsLongReframe",
      "properties": [
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL to the source video file. This video will be used as a reference for the reframe task."
        },
        {
          "name": "zoom_factor",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Zoom Factor",
          "description": "Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom."
        },
        {
          "name": "paste_back",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Paste Back",
          "description": "Whether to paste back the reframed scene to the original video."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "low",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframe.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation. Optional for reframing."
        },
        {
          "name": "scene_threshold",
          "type": {
            "type": "float"
          },
          "default": 30,
          "title": "Scene Threshold",
          "description": "Threshold for scene detection sensitivity (0-100). Lower values detect more scenes."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "auto_downsample_min_fps",
          "type": {
            "type": "float"
          },
          "default": 6,
          "title": "Auto Downsample Min Fps",
          "description": "Minimum FPS for auto downsample."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "sampler",
          "type": {
            "type": "enum",
            "values": [
              "unipc",
              "dpm++",
              "euler"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframe.Sampler"
          },
          "default": "unipc",
          "title": "Sampler",
          "description": "Sampler to use for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframe.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "If true, also return a ZIP file containing all generated frames."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "1:1",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframe.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframe.Resolution"
          },
          "default": "auto",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "transparency_mode",
          "type": {
            "type": "enum",
            "values": [
              "content_aware",
              "white",
              "black"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframe.TransparencyMode"
          },
          "default": "content_aware",
          "title": "Transparency Mode",
          "description": "The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled."
        },
        {
          "name": "trim_borders",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Trim Borders",
          "description": "Whether to trim borders from the video."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframe.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "rife",
              "film"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframe.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. Options are 'rife' or 'film'."
        },
        {
          "name": "enable_auto_downsample",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Auto Downsample",
          "description": "Whether to enable auto downsample."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan Vace Apps Video Edit",
      "description": "Wan VACE Video Edit\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanVaceAppsVideoEdit",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to edit the video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the input video."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "low",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEdit.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "240p",
              "360p",
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEdit.Resolution"
          },
          "default": "auto",
          "title": "Resolution",
          "description": "Resolution of the edited video."
        },
        {
          "name": "return_frames_zip",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Return Frames Zip",
          "description": "Whether to include a ZIP archive containing all generated frames."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEdit.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the edited video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "video_type",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "general",
              "human"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEdit.VideoType"
          },
          "default": "auto",
          "title": "Video Type",
          "description": "The type of video you're editing. Use 'general' for most videos, and 'human' for videos emphasizing human subjects and motions. The default value 'auto' means the model will guess based on the first frame of the video."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "URLs of the input images to use as a reference for the generation."
        },
        {
          "name": "enable_auto_downsample",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Auto Downsample",
          "description": "Whether to enable automatic downsampling. If your video has a high frame rate or is long, enabling longer sequences to be generated. The video will be interpolated back to the original frame rate after generation."
        },
        {
          "name": "auto_downsample_min_fps",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Auto Downsample Min Fps",
          "description": "The minimum frames per second to downsample the video to."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Wan Vision Enhancer",
      "description": "Wan Vision Enhancer\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WanVisionEnhancer",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Optional prompt to prepend to the VLM-generated description. Leave empty to use only the auto-generated description from the video."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the video to enhance with Wan Video. Maximum 200MB file size. Videos longer than 500 frames will have only the first 500 frames processed (~8-21 seconds depending on fps)."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If not provided, a random seed will be used."
        },
        {
          "name": "target_resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WanVisionEnhancer.TargetResolution"
          },
          "default": "720p",
          "title": "Target Resolution",
          "description": "Target output resolution for the enhanced video. 720p (native, fast) or 1080p (upscaled, slower). Processing is always done at 720p, then upscaled if 1080p selected."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "oversaturated, overexposed, static, blurry details, subtitles, stylized, artwork, painting, still frame, overall gray, worst quality, low quality, JPEG artifacts, ugly, mutated, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fused fingers, static motion, cluttered background, three legs, crowded background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt to avoid unwanted features."
        },
        {
          "name": "creativity",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Creativity",
          "description": "Controls how much the model enhances/changes the video. 0 = Minimal change (preserves original), 1 = Subtle enhancement (default), 2 = Medium enhancement, 3 = Strong enhancement, 4 = Maximum enhancement."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "Workflow Utilities Auto Subtitle",
      "description": "Workflow Utilities\n    video, editing, video-to-video, vid2vid\n\n    Use cases:\n    - Video style transfer\n    - Video enhancement and restoration\n    - Automated video editing\n    - Special effects generation\n    - Content repurposing",
      "namespace": "fal.video_to_video",
      "node_type": "fal.video_to_video.WorkflowUtilitiesAutoSubtitle",
      "properties": [
        {
          "name": "font_weight",
          "type": {
            "type": "enum",
            "values": [
              "normal",
              "bold",
              "black"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitle.FontWeight"
          },
          "default": "bold",
          "title": "Font Weight",
          "description": "Font weight (TikTok style typically uses bold or black)"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video file to add automatic subtitles to Max file size: 95.4MB, Timeout: 30.0s"
        },
        {
          "name": "stroke_width",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "Stroke Width",
          "description": "Text stroke/outline width in pixels (0 for no stroke)"
        },
        {
          "name": "font_color",
          "type": {
            "type": "enum",
            "values": [
              "white",
              "black",
              "red",
              "green",
              "blue",
              "yellow",
              "orange",
              "purple",
              "pink",
              "brown",
              "gray",
              "cyan",
              "magenta"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitle.FontColor"
          },
          "default": "white",
          "title": "Font Color",
          "description": "Subtitle text color for non-active words"
        },
        {
          "name": "font_size",
          "type": {
            "type": "int"
          },
          "default": 100,
          "title": "Font Size",
          "description": "Font size for subtitles (TikTok style uses larger text)"
        },
        {
          "name": "language",
          "type": {
            "type": "str"
          },
          "default": "en",
          "title": "Language",
          "description": "Language code for transcription (e.g., 'en', 'es', 'fr', 'de', 'it', 'pt', 'nl', 'ja', 'zh', 'ko') or 3-letter ISO code (e.g., 'eng', 'spa', 'fra')"
        },
        {
          "name": "y_offset",
          "type": {
            "type": "int"
          },
          "default": 75,
          "title": "Y Offset",
          "description": "Vertical offset in pixels (positive = move down, negative = move up)"
        },
        {
          "name": "background_opacity",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Background Opacity",
          "description": "Background opacity (0.0 = fully transparent, 1.0 = fully opaque)"
        },
        {
          "name": "stroke_color",
          "type": {
            "type": "enum",
            "values": [
              "black",
              "white",
              "red",
              "green",
              "blue",
              "yellow",
              "orange",
              "purple",
              "pink",
              "brown",
              "gray",
              "cyan",
              "magenta"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitle.StrokeColor"
          },
          "default": "black",
          "title": "Stroke Color",
          "description": "Text stroke/outline color"
        },
        {
          "name": "highlight_color",
          "type": {
            "type": "enum",
            "values": [
              "white",
              "black",
              "red",
              "green",
              "blue",
              "yellow",
              "orange",
              "purple",
              "pink",
              "brown",
              "gray",
              "cyan",
              "magenta"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitle.HighlightColor"
          },
          "default": "purple",
          "title": "Highlight Color",
          "description": "Color for the currently speaking word (karaoke-style highlight)"
        },
        {
          "name": "enable_animation",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Animation",
          "description": "Enable animation effects for subtitles (bounce style entrance)"
        },
        {
          "name": "font_name",
          "type": {
            "type": "str"
          },
          "default": "Montserrat",
          "title": "Font Name",
          "description": "Any Google Font name from fonts.google.com (e.g., 'Montserrat', 'Poppins', 'BBH Sans Hegarty')"
        },
        {
          "name": "position",
          "type": {
            "type": "enum",
            "values": [
              "top",
              "center",
              "bottom"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitle.Position"
          },
          "default": "bottom",
          "title": "Position",
          "description": "Vertical position of subtitles"
        },
        {
          "name": "words_per_subtitle",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "Words Per Subtitle",
          "description": "Maximum number of words per subtitle segment. Use 1 for single-word display, 2-3 for short phrases, or 8-12 for full sentences."
        },
        {
          "name": "background_color",
          "type": {
            "type": "enum",
            "values": [
              "black",
              "white",
              "red",
              "green",
              "blue",
              "yellow",
              "orange",
              "purple",
              "pink",
              "brown",
              "gray",
              "cyan",
              "magenta",
              "none",
              "transparent"
            ],
            "type_name": "nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitle.BackgroundColor"
          },
          "default": "none",
          "title": "Background Color",
          "description": "Background color behind text ('none' or 'transparent' for no background)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video"
      ]
    },
    {
      "title": "AIAvatar",
      "description": "MultiTalk generates talking avatar videos from images and audio files.\n    video, avatar, talking-head, multitalk, image-to-video\n\n    Use cases:\n    - Create talking avatar videos\n    - Animate portrait photos with audio\n    - Generate spokesperson videos\n    - Produce avatar presentations\n    - Create personalized video messages",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.AIAvatar",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatar.AIAvatarResolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the video to generate. Must be either 480p or 720p."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatar.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 145,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "audio"
      ]
    },
    {
      "title": "AIAvatar Multi",
      "description": "MultiTalk generates multi-speaker avatar videos with audio synchronization.\n    video, avatar, multi-speaker, talking-head, image-to-video\n\n    Use cases:\n    - Create multi-speaker videos with audio\n    - Generate synchronized dialogue\n    - Produce conversation videos\n    - Create interactive characters\n    - Generate multi-avatar content",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.AIAvatarMulti",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarMulti.AIAvatarMultiResolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the video to generate. Must be either 480p or 720p."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarMulti.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for generation."
        },
        {
          "name": "first_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "First Audio Url",
          "description": "The URL of the Person 1 audio file."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "second_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Second Audio Url",
          "description": "The URL of the Person 2 audio file."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "use_only_first_audio",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Use Only First Audio",
          "description": "Whether to use only the first audio file."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 181,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "images",
        "audio"
      ]
    },
    {
      "title": "AIAvatar Multi Text",
      "description": "MultiTalk generates multi-speaker avatar videos from images and text.\n    video, avatar, multi-speaker, talking-head, image-to-video\n\n    Use cases:\n    - Create multi-speaker conversations\n    - Generate dialogue between avatars\n    - Produce interactive presentations\n    - Create conversational content\n    - Generate multi-character scenes",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.AIAvatarMultiText",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "second_text_input",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Second Text Input",
          "description": "The text input to guide video generation."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarMultiText.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarMultiText.AIAvatarMultiTextResolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the video to generate. Must be either 480p or 720p."
        },
        {
          "name": "first_text_input",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "First Text Input",
          "description": "The text input to guide video generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "voice2",
          "type": {
            "type": "enum",
            "values": [
              "Aria",
              "Roger",
              "Sarah",
              "Laura",
              "Charlie",
              "George",
              "Callum",
              "River",
              "Liam",
              "Charlotte",
              "Alice",
              "Matilda",
              "Will",
              "Jessica",
              "Eric",
              "Chris",
              "Brian",
              "Daniel",
              "Lily",
              "Bill"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarMultiText.Voice2"
          },
          "default": "Roger",
          "title": "Voice2",
          "description": "The second person's voice to use for speech generation"
        },
        {
          "name": "voice1",
          "type": {
            "type": "enum",
            "values": [
              "Aria",
              "Roger",
              "Sarah",
              "Laura",
              "Charlie",
              "George",
              "Callum",
              "River",
              "Liam",
              "Charlotte",
              "Alice",
              "Matilda",
              "Will",
              "Jessica",
              "Eric",
              "Chris",
              "Brian",
              "Daniel",
              "Lily",
              "Bill"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarMultiText.Voice1"
          },
          "default": "Sarah",
          "title": "Voice1",
          "description": "The first person's voice to use for speech generation"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 191,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "images",
        "texts"
      ]
    },
    {
      "title": "AIAvatar Single Text",
      "description": "MultiTalk generates talking avatar videos from an image and text input.\n    video, avatar, talking-head, text-to-speech, image-to-video\n\n    Use cases:\n    - Create avatar videos from text\n    - Generate talking heads with TTS\n    - Produce text-driven avatars\n    - Create virtual presenters\n    - Generate automated spokesperson videos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.AIAvatarSingleText",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarSingleText.AIAvatarSingleTextResolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the video to generate. Must be either 480p or 720p."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarSingleText.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for generation."
        },
        {
          "name": "text_input",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Input",
          "description": "The text input to guide video generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "voice",
          "type": {
            "type": "enum",
            "values": [
              "Aria",
              "Roger",
              "Sarah",
              "Laura",
              "Charlie",
              "George",
              "Callum",
              "River",
              "Liam",
              "Charlotte",
              "Alice",
              "Matilda",
              "Will",
              "Jessica",
              "Eric",
              "Chris",
              "Brian",
              "Daniel",
              "Lily",
              "Bill"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.AIAvatarSingleText.Voice"
          },
          "default": "",
          "title": "Voice",
          "description": "The voice to use for speech generation"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 42,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 136,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "text"
      ]
    },
    {
      "title": "AMTFrame Interpolation",
      "description": "AMT Frame Interpolation creates smooth transitions between image frames.\n    video, interpolation, frame-generation, amt, image-to-video\n\n    Use cases:\n    - Create smooth transitions between images\n    - Generate intermediate frames\n    - Animate image sequences\n    - Create video from image pairs\n    - Produce smooth motion effects",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.AMTFrameInterpolation",
      "properties": [
        {
          "name": "frames",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Frames",
          "description": "Frames to interpolate"
        },
        {
          "name": "recursive_interpolation_passes",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Recursive Interpolation Passes",
          "description": "Number of recursive interpolation passes"
        },
        {
          "name": "output_fps",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Output Fps",
          "description": "Output frames per second"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Byte Dance Video Stylize",
      "description": "ByteDance Video Stylize applies artistic styles to image-based video generation.\n    video, style-transfer, artistic, bytedance, image-to-video\n\n    Use cases:\n    - Apply artistic styles to videos\n    - Create stylized video content\n    - Generate artistic animations\n    - Produce style-transferred videos\n    - Create visually unique content",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.ByteDanceVideoStylize",
      "properties": [
        {
          "name": "style",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Style",
          "description": "The style for your character in the video. Please use a short description."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to make the stylized video from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "style"
      ]
    },
    {
      "title": "Bytedance Lynx",
      "description": "Lynx\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.BytedanceLynx",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide video generation"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.BytedanceLynx.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p, 580p, or 720p)"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.BytedanceLynx.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video (16:9, 9:16, or 1:1)"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "guidance_scale_2",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Guidance Scale 2",
          "description": "Image guidance scale. Controls how closely the generated video follows the reference image. Higher values increase adherence to the reference image but may decrease quality."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Strength",
          "description": "Reference image scale. Controls the influence of the reference image on the generated video."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 30."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the subject image to be used for video generation"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames in the generated video. Must be between 9 to 100."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Bright tones, overexposed, blurred background, static, subtitles, style, works, paintings, images, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt to guide what should not appear in the generated video"
        },
        {
          "name": "ip_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Ip Scale",
          "description": "Identity preservation scale. Controls how closely the generated video preserves the subject's identity from the reference image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bytedance Omnihuman",
      "description": "OmniHuman generates video using an image of a human figure paired with an audio file. It produces vivid, high-quality videos where the character\u2019s emotions and movements maintain a strong correlation with the audio.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.BytedanceOmnihuman",
      "properties": [
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file to generate the video. Audio must be under 30s long."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image used to generate the video"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Bytedance Seedance V1 Lite Image To Video",
      "description": "Seedance 1.0 Lite\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.BytedanceSeedanceV1LiteImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.BytedanceSeedanceV1LiteImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Video resolution - 480p for faster generation, 720p for higher quality"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "2",
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.BytedanceSeedanceV1LiteImageToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the video in seconds"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.BytedanceSeedanceV1LiteImageToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image used to generate video"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "camera_fixed",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Camera Fixed",
          "description": "Whether to fix the camera position"
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image the video ends with. Defaults to None."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control video generation. Use -1 for random."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Cog Video X5 BImage To Video",
      "description": "CogVideoX-5B generates high-quality videos from images with advanced motion.\n    video, generation, cogvideo, image-to-video, img2vid\n\n    Use cases:\n    - Generate videos from images\n    - Create dynamic image animations\n    - Produce high-quality video content\n    - Animate static images\n    - Generate motion from photos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.CogVideoX5BImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "use_rife",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Rife",
          "description": "Use RIFE for video interpolation"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL to the image to generate the video from."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the image generation. We currently support one lora."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 7,
          "title": "Guidance Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related video to show you."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to perform."
        },
        {
          "name": "export_fps",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Export Fps",
          "description": "The target FPS of the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate video from"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Creatify Aurora",
      "description": "Creatify Aurora generates creative and visually stunning videos from images with unique effects.\n    video, generation, creatify, aurora, creative, effects\n\n    Use cases:\n    - Generate creative visual effects videos\n    - Create stunning video animations\n    - Produce artistic video content\n    - Generate unique video effects\n    - Create visually impressive videos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.CreatifyAurora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "A text prompt to guide the video generation process."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.CreatifyAurora.CreatifyAuroraResolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "Guidance scale to be used for text prompt adherence."
        },
        {
          "name": "audio_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2,
          "title": "Audio Guidance Scale",
          "description": "Guidance scale to be used for audio adherence."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file to be used for video generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image file to be used for video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Decart Lucy 14B Image To Video",
      "description": "Decart Lucy 14b\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.DecartLucy14BImageToVideo",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Sync Mode",
          "description": "If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.DecartLucy14BImageToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text description of the desired video content"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.DecartLucy14BImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Decart Lucy 5 b Image To Video",
      "description": "Lucy-5B is a model that can create 5-second I2V videos in under 5 seconds, achieving >1x RTF end-to-end\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.DecartLucy5bImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text description of the desired video content"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "16:9"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.DecartLucy5bImageToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.DecartLucy5bImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Fast Svd Lcm",
      "description": "Generate short video clips from your images using SVD v1.1 at Lightning Speed\n    video, animation, image-to-video, img2vid, fast\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.FastSvdLcm",
      "properties": [
        {
          "name": "motion_bucket_id",
          "type": {
            "type": "int"
          },
          "default": 127,
          "title": "Motion Bucket Id",
          "description": "The motion bucket id determines the motion of the generated video. The higher the number, the more motion there will be."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Fps",
          "description": "The FPS of the generated video. The higher the number, the faster the video will play. Total video length is 25 frames."
        },
        {
          "name": "steps",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Steps",
          "description": "The number of steps to run the model for. The higher the number the better the quality and longer it will take to generate."
        },
        {
          "name": "cond_aug",
          "type": {
            "type": "float"
          },
          "default": 0.02,
          "title": "Cond Aug",
          "description": "The conditoning augmentation determines the amount of noise that will be added to the conditioning frame. The higher the number, the more noise there will be, and the less the video will look like the initial image. Increase it for more motion."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Framepack",
      "description": "Framepack is an efficient Image-to-video model that autoregressively generates videos.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Framepack",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation (max 500 characters)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Framepack.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video to generate."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "480p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Framepack.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 180,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image input."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 10,
          "title": "Guidance Scale",
          "description": "Guidance scale for the generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed to use for generating the video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Cfg Scale",
          "description": "Classifier-Free Guidance scale for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Framepack F1",
      "description": "Framepack is an efficient Image-to-video model that autoregressively generates videos.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.FramepackF1",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation (max 500 characters)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.FramepackF1.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video to generate."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "480p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.FramepackF1.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 180,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image input."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 10,
          "title": "Guidance Scale",
          "description": "Guidance scale for the generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed to use for generating the video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Cfg Scale",
          "description": "Classifier-Free Guidance scale for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Framepack Flf 2 v",
      "description": "Framepack is an efficient Image-to-video model that autoregressively generates videos.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.FramepackFlf2v",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation (max 500 characters)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.FramepackFlf2v.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video to generate."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "480p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.FramepackFlf2v.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 240,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image input."
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Determines the influence of the final frame on the generated video. Higher values result in the output being more heavily influenced by the last frame."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 10,
          "title": "Guidance Scale",
          "description": "Guidance scale for the generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed to use for generating the video."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the end image input."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Cfg Scale",
          "description": "Classifier-Free Guidance scale for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Avatar",
      "description": "HunyuanAvatar is a High-Fidelity Audio-Driven Human Animation model for Multiple Characters .\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.HunyuanAvatar",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "A cat is singing.",
          "title": "Text",
          "description": "Text prompt describing the scene."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the reference image."
        },
        {
          "name": "turbo_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Turbo Mode",
          "description": "If true, the video will be generated faster with no noticeable degradation in the visual quality."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 129,
          "title": "Num Frames",
          "description": "Number of video frames to generate at 25 FPS. If greater than the input audio length, it will capped to the length of the input audio."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Custom",
      "description": "HunyuanCustom revolutionizes video generation with unmatched identity consistency across multiple input types. Its innovative fusion modules and alignment networks outperform competitors, maintaining subject integrity while responding flexibly to text, image, audio, and video conditions.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.HunyuanCustom",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation (max 500 characters)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.HunyuanCustom.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video to generate."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "512p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.HunyuanCustom.Resolution"
          },
          "default": "512p",
          "title": "Resolution",
          "description": "The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 129,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image input."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for generating the video."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to run. Lower gets faster results, higher gets better results."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion, blurring, text, subtitles, static, picture, black border.",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 7.5,
          "title": "Cfg Scale",
          "description": "Classifier-Free Guidance scale for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Portrait",
      "description": "HunyuanPortrait is a diffusion-based framework for generating lifelike, temporally consistent portrait animations.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.HunyuanPortrait",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the driving video."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation. If None, a random seed will be used."
        },
        {
          "name": "use_arcface",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Arcface",
          "description": "Whether to use ArcFace for face recognition."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the source image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Video Image To Video",
      "description": "Image to Video for the high-quality Hunyuan Video I2V model.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.HunyuanVideoImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.HunyuanVideoImageToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video to generate."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.HunyuanVideoImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the video to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image input."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for generating the video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "enum",
            "values": [
              "129"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.HunyuanVideoImageToVideo.NumFrames"
          },
          "default": 129,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "i2v_stability",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "I2V Stability",
          "description": "Turning on I2V Stability reduces hallucination but also reduces motion."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Video Img 2 vid Lora",
      "description": "Image to Video for the Hunyuan Video model using a custom trained LoRA.\n    video, animation, image-to-video, img2vid, lora\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.HunyuanVideoImg2vidLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for generating the video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL to the image to generate the video from. The image must be 960x544 or it will get cropped and resized to that size."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Hunyuan Video V15 Image To Video",
      "description": "Hunyuan Video v1.5 generates high-quality videos from images with advanced AI capabilities.\n    video, generation, hunyuan, v1.5, advanced\n\n    Use cases:\n    - Generate advanced quality videos\n    - Create sophisticated animations\n    - Produce high-fidelity video content\n    - Generate videos with AI excellence\n    - Create cutting-edge video animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.HunyuanVideoV15ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.HunyuanVideoV15ImageToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.HunyuanVideoV15ImageToVideo.HunyuanVideoV15Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "The resolution of the video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the reference image for image-to-video generation."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Enable prompt expansion to enhance the input prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps",
          "description": "The number of inference steps."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to guide what not to generate."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kandinsky 5 Pro Image To Video",
      "description": "Kandinsky5 Pro generates professional quality videos from images with artistic style and control.\n    video, generation, kandinsky, pro, artistic\n\n    Use cases:\n    - Generate artistic videos from images\n    - Create stylized video animations\n    - Produce creative video content\n    - Generate videos with artistic flair\n    - Create professional artistic videos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Kandinsky5ProImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "512P",
              "1024P"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Kandinsky5ProImageToVideo.Kandinsky5ProResolution"
          },
          "default": "512P",
          "title": "Resolution",
          "description": "Video resolution: 512p or 1024p."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Kandinsky5ProImageToVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level for faster generation."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5s"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Kandinsky5ProImageToVideo.Kandinsky5ProDuration"
          },
          "default": "5s",
          "title": "Duration",
          "description": "Video duration."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 28,
          "title": "Num Inference Steps"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a reference for the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video Ai Avatar V2 Pro",
      "description": "Kling Video AI Avatar v2 Pro creates professional quality animated talking avatars with enhanced realism.\n    video, avatar, kling, v2, pro, talking-head\n\n    Use cases:\n    - Create professional talking avatars\n    - Animate portraits with high quality\n    - Generate realistic avatar videos\n    - Produce premium speaking characters\n    - Create pro-grade AI avatars",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoAiAvatarV2Pro",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": ".",
          "title": "Prompt",
          "description": "The prompt to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as your avatar"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video Ai Avatar V2 Standard",
      "description": "Kling Video AI Avatar v2 Standard creates animated talking avatars with standard quality.\n    video, avatar, kling, v2, standard, talking-head\n\n    Use cases:\n    - Create standard quality talking avatars\n    - Animate portraits with speech\n    - Generate avatar presentations\n    - Produce speaking character videos\n    - Create AI-driven avatars",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoAiAvatarV2Standard",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": ".",
          "title": "Prompt",
          "description": "The prompt to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as your avatar"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video O1 Image To Video",
      "description": "Kling O1 First Frame Last Frame to Video [Pro]\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoO1ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Use @Image1 to reference the start frame, @Image2 to reference the end frame."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoO1ImageToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Video duration in seconds."
        },
        {
          "name": "start_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Start Image Url",
          "description": "Image to use as the first frame of the video. Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s"
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "Image to use as the last frame of the video. Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video O1 Reference To Video",
      "description": "Kling O1 Reference Image to Video [Pro]\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoO1ReferenceToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Take @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoO1ReferenceToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoO1ReferenceToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Video duration in seconds."
        },
        {
          "name": "elements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Elements",
          "description": "Elements (characters/objects) to include in the video. Reference in prompt as @Element1, @Element2, etc. Maximum 7 total (elements + reference images + start image)."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Additional reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 7 total (elements + reference images + start image)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video O1 Standard Image To Video",
      "description": "Kling Video O1 Standard generates videos with optimized standard quality from images.\n    video, generation, kling, o1, standard\n\n    Use cases:\n    - Generate standard O1 quality videos\n    - Create optimized video animations\n    - Produce efficient video content\n    - Generate balanced quality videos\n    - Create standard tier animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoO1StandardImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Use @Image1 to reference the start frame, @Image2 to reference the end frame."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoO1StandardImageToVideo.KlingVideoO1StandardDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Video duration in seconds."
        },
        {
          "name": "start_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Start Image Url",
          "description": "Image to use as the first frame of the video. Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s"
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "Image to use as the last frame of the video. Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video O1 Standard Reference To Video",
      "description": "Kling Video O1 Standard generates videos using reference images for style consistency.\n    video, generation, kling, o1, standard, reference\n\n    Use cases:\n    - Generate videos from reference images\n    - Create style-consistent animations\n    - Produce reference-guided content\n    - Generate videos matching examples\n    - Create standardized reference videos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoO1StandardReferenceToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Take @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoO1StandardReferenceToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoO1StandardReferenceToVideo.KlingVideoO1StandardReferenceToVideoDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Video duration in seconds."
        },
        {
          "name": "elements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Elements",
          "description": "Elements (characters/objects) to include in the video. Reference in prompt as @Element1, @Element2, etc. Maximum 7 total (elements + reference images + start image)."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "Additional reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 7 total (elements + reference images + start image)."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video V15 Pro Image To Video",
      "description": "Generate video clips from your images using Kling 1.5 (pro)\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoV15ProImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV15ProImageToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "tail_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Tail Image Url",
          "description": "URL of the image to be used for the end of the video"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV15ProImageToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url"
        },
        {
          "name": "static_mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Static Mask Url",
          "description": "URL of the image for Static Brush Application Area (Mask image created by users using the motion brush)"
        },
        {
          "name": "dynamic_masks",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Dynamic Masks",
          "description": "List of dynamic masks"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video V16 Pro Elements",
      "description": "Generate video clips from your multiple image references using Kling 1.6 (pro)\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoV16ProElements",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV16ProElements.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV16ProElements.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame"
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "List of image URLs to use for video generation. Supports up to 4 images."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video V16 Standard Elements",
      "description": "Generate video clips from your multiple image references using Kling 1.6 (standard)\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoV16StandardElements",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV16StandardElements.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV16StandardElements.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame"
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "List of image URLs to use for video generation. Supports up to 4 images."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video V16 Standard Image To Video",
      "description": "Generate video clips from your images using Kling 1.6 (std)\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoV16StandardImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV16StandardImageToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video V1 Pro Ai Avatar",
      "description": "Kling AI Avatar Pro\n    video, animation, image-to-video, img2vid, professional\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoV1ProAiAvatar",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": ".",
          "title": "Prompt",
          "description": "The prompt to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as your avatar"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video V1 Standard Ai Avatar",
      "description": "Kling AI Avatar\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoV1StandardAiAvatar",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": ".",
          "title": "Prompt",
          "description": "The prompt to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as your avatar"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video V1 Standard Image To Video",
      "description": "Kling Video v1 Standard generates videos from images with balanced quality.\n    video, generation, kling, standard, image-to-video\n\n    Use cases:\n    - Generate standard quality videos\n    - Create balanced video animations\n    - Produce efficient video content\n    - Generate videos for web use\n    - Create moderate quality outputs",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoV1StandardImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for the video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV1StandardImageToVideo.KlingVideoV1StandardDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "tail_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Tail Image Url",
          "description": "URL of the image to be used for the end of the video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be used for the video"
        },
        {
          "name": "static_mask_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Static Mask Url",
          "description": "URL of the image for Static Brush Application Area (Mask image created by users using the motion brush)"
        },
        {
          "name": "dynamic_masks",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Dynamic Masks",
          "description": "List of dynamic masks"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video V21 Pro Image To Video",
      "description": "Kling 2.1 Pro is an advanced endpoint for the Kling 2.1 model, offering professional-grade videos with enhanced visual fidelity, precise camera movements, and dynamic motion control, perfect for cinematic storytelling.  \n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoV21ProImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV21ProImageToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be used for the video"
        },
        {
          "name": "tail_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Tail Image Url",
          "description": "URL of the image to be used for the end of the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video V25 Turbo Standard Image To Video",
      "description": "Kling Video\n    video, animation, image-to-video, img2vid, fast\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoV25TurboStandardImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV25TurboStandardImageToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be used for the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video V26 Pro Image To Video",
      "description": "Kling Video v2.6 Pro generates professional quality videos with latest model improvements.\n    video, generation, kling, v2.6, pro\n\n    Use cases:\n    - Generate professional v2.6 videos\n    - Create latest quality animations\n    - Produce premium video content\n    - Generate advanced videos\n    - Create pro-tier animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoV26ProImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV26ProImageToVideo.KlingVideoV26ProDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "voice_ids",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Voice Ids",
          "description": "Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice"
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase."
        },
        {
          "name": "start_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Start Image Url",
          "description": "URL of the image to be used for the video"
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the image to be used for the end of the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Kling Video V3 Pro Image To Video",
      "description": "Kling Video V3 Pro generates professional quality videos from images with enhanced visual fidelity using the latest V3 model.\n    video, generation, kling, v3, pro, image-to-video\n\n    Use cases:\n    - Create professional-grade video animations from images\n    - Generate cinematic video content with precise motion\n    - Produce high-fidelity product showcase videos\n    - Animate images with enhanced visual quality\n    - Create premium video content for advertising",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoV3ProImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV3ProImageToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12",
              "13",
              "14",
              "15"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV3ProImageToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate native audio for the video."
        },
        {
          "name": "start_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Start Image Url",
          "description": "URL of the image to be used for the video"
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the image to be used for the end of the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "start_image_url",
        "prompt",
        "duration"
      ]
    },
    {
      "title": "Kling Video V3 Standard Image To Video",
      "description": "Kling Video V3 Standard generates videos from images with balanced quality and speed using the latest V3 model.\n    video, generation, kling, v3, standard, image-to-video\n\n    Use cases:\n    - Animate static images into short video clips\n    - Create engaging social media content from photos\n    - Generate product demonstration videos\n    - Produce marketing and promotional videos\n    - Transform images into cinematic animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.KlingVideoV3StandardImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV3StandardImageToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video frame"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12",
              "13",
              "14",
              "15"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.KlingVideoV3StandardImageToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate native audio for the video."
        },
        {
          "name": "start_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Start Image Url",
          "description": "URL of the image to be used for the video"
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the image to be used for the end of the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blur, distort, and low quality",
          "title": "Negative Prompt"
        },
        {
          "name": "cfg_scale",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Cfg Scale",
          "description": "The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "start_image_url",
        "prompt",
        "duration"
      ]
    },
    {
      "title": "LTXImage To Video",
      "description": "LTX Video generates temporally consistent videos from images.\n    video, generation, ltx, temporal, image-to-video\n\n    Use cases:\n    - Generate temporally consistent videos\n    - Create smooth image animations\n    - Produce coherent video sequences\n    - Animate with temporal awareness\n    - Generate fluid motion videos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LTXImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for random number generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to take."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate the video from."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Live Avatar",
      "description": "Live Avatar creates animated talking avatars from portrait images with realistic lip-sync and expressions.\n    video, avatar, talking-head, animation, portrait\n\n    Use cases:\n    - Create talking avatar videos\n    - Animate portrait images\n    - Generate lip-synced avatars\n    - Produce speaking character videos\n    - Create animated presenters",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LiveAvatar",
      "properties": [
        {
          "name": "frames_per_clip",
          "type": {
            "type": "int"
          },
          "default": 48,
          "title": "Frames Per Clip",
          "description": "Number of frames per clip. Must be a multiple of 4. Higher values = smoother but slower generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "A text prompt describing the scene and character. Helps guide the video generation style and context."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "light",
              "regular",
              "high"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LiveAvatar.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "Acceleration level for faster video decoding"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the reference image for avatar generation. The character in this image will be animated."
        },
        {
          "name": "num_clips",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Num Clips",
          "description": "Number of video clips to generate. Each clip is approximately 3 seconds. Set higher for longer videos."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the driving audio file (WAV or MP3). The avatar will be animated to match this audio."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducible generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values follow the prompt more closely."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Enable safety checker for content moderation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Live Portrait",
      "description": "Transfer expression from a video to a portrait.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LivePortrait",
      "properties": [
        {
          "name": "smile",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Smile",
          "description": "Amount to smile"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video to drive the lip syncing."
        },
        {
          "name": "flag_stitching",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Flag Stitching",
          "description": "Whether to enable stitching. Recommended to set to True."
        },
        {
          "name": "eyebrow",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Eyebrow",
          "description": "Amount to raise or lower eyebrows"
        },
        {
          "name": "wink",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Wink",
          "description": "Amount to wink"
        },
        {
          "name": "rotate_pitch",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Rotate Pitch",
          "description": "Amount to rotate the face in pitch"
        },
        {
          "name": "blink",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Blink",
          "description": "Amount to blink the eyes"
        },
        {
          "name": "scale",
          "type": {
            "type": "float"
          },
          "default": 2.3,
          "title": "Scale",
          "description": "Scaling factor for the face crop."
        },
        {
          "name": "eee",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Eee",
          "description": "Amount to shape mouth in 'eee' position"
        },
        {
          "name": "flag_pasteback",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Flag Pasteback",
          "description": "Whether to paste-back/stitch the animated face cropping from the face-cropping space to the original image space."
        },
        {
          "name": "pupil_y",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Pupil Y",
          "description": "Amount to move pupils vertically"
        },
        {
          "name": "rotate_yaw",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Rotate Yaw",
          "description": "Amount to rotate the face in yaw"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be animated"
        },
        {
          "name": "woo",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Woo",
          "description": "Amount to shape mouth in 'woo' position"
        },
        {
          "name": "aaa",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Aaa",
          "description": "Amount to open mouth in 'aaa' shape"
        },
        {
          "name": "flag_do_rot",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Flag Do Rot",
          "description": "Whether to conduct the rotation when flag_do_crop is True."
        },
        {
          "name": "flag_relative",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Flag Relative",
          "description": "Whether to use relative motion."
        },
        {
          "name": "flag_eye_retargeting",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Flag Eye Retargeting",
          "description": "Whether to enable eye retargeting."
        },
        {
          "name": "flag_lip_zero",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Flag Lip Zero",
          "description": "Whether to set the lip to closed state before animation. Only takes effect when flag_eye_retargeting and flag_lip_retargeting are False."
        },
        {
          "name": "batch_size",
          "type": {
            "type": "int"
          },
          "default": 32,
          "title": "Batch Size",
          "description": "Batch size for the model. The larger the batch size, the faster the model will run, but the more memory it will consume."
        },
        {
          "name": "rotate_roll",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Rotate Roll",
          "description": "Amount to rotate the face in roll"
        },
        {
          "name": "dsize",
          "type": {
            "type": "int"
          },
          "default": 512,
          "title": "Dsize",
          "description": "Size of the output image."
        },
        {
          "name": "vy_ratio",
          "type": {
            "type": "float"
          },
          "default": -0.125,
          "title": "Vy Ratio",
          "description": "Vertical offset ratio for face crop. Positive values move up, negative values move down."
        },
        {
          "name": "pupil_x",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Pupil X",
          "description": "Amount to move pupils horizontally"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker. If enabled, the model will check if the input image contains a face before processing it. The safety checker will process the input image"
        },
        {
          "name": "vx_ratio",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Vx Ratio",
          "description": "Horizontal offset ratio for face crop."
        },
        {
          "name": "flag_lip_retargeting",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Flag Lip Retargeting",
          "description": "Whether to enable lip retargeting."
        },
        {
          "name": "flag_do_crop",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Flag Do Crop",
          "description": "Whether to crop the source portrait to the face-cropping space."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Longcat Video Distilled Image To Video 480P",
      "description": "LongCat Video Distilled\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LongcatVideoDistilledImageToVideo480P",
      "properties": [
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo480P.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo480P.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 15,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate a video from."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo480P.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 12,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 162,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Longcat Video Distilled Image To Video 720P",
      "description": "LongCat Video Distilled\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LongcatVideoDistilledImageToVideo720P",
      "properties": [
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo720P.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo720P.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "num_refine_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 12,
          "title": "Num Refine Inference Steps",
          "description": "The number of inference steps to use for refinement."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate a video from."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 12,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 162,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo720P.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Longcat Video Image To Video 480P",
      "description": "LongCat Video\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LongcatVideoImageToVideo480P",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for the video generation."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 15,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the video generation."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 162,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "The negative prompt to use for the video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate a video from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for the video generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Longcat Video Image To Video 720P",
      "description": "LongCat Video\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LongcatVideoImageToVideo720P",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use for the video generation."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Fps",
          "description": "The frame rate of the generated video."
        },
        {
          "name": "num_refine_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Refine Inference Steps",
          "description": "The number of inference steps to use for refinement."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the video generation."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 162,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "The negative prompt to use for the video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate a video from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for the video generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ltx 219B Distilled Image To Video",
      "description": "LTX-2 19B Distilled generates videos efficiently using knowledge distillation from the 19B model.\n    video, generation, ltx-2, 19b, distilled, efficient\n\n    Use cases:\n    - Generate videos efficiently with distilled model\n    - Create fast quality video animations\n    - Produce optimized video content\n    - Generate videos with good performance\n    - Create balanced quality-speed videos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Ltx219BDistilledImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt used for the generation."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate the video from."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "interpolation_direction",
          "type": {
            "type": "enum",
            "values": [
              "forward",
              "backward"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.InterpolationDirection"
          },
          "default": "forward",
          "title": "Interpolation Direction",
          "description": "The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ltx 219B Distilled Image To Video Lora",
      "description": "LTX-2 19B Distilled with LoRA combines efficient generation with custom-trained models.\n    video, generation, ltx-2, 19b, distilled, lora\n\n    Use cases:\n    - Generate videos with custom distilled model\n    - Create efficient specialized content\n    - Produce fast domain-specific videos\n    - Generate with optimized custom model\n    - Create quick customized animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Ltx219BDistilledImageToVideoLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt used for the generation."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate the video from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "interpolation_direction",
          "type": {
            "type": "enum",
            "values": [
              "forward",
              "backward"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.InterpolationDirection"
          },
          "default": "forward",
          "title": "Interpolation Direction",
          "description": "The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ltx 219B Image To Video",
      "description": "LTX-2 19B generates high-quality videos from images using the powerful 19-billion parameter model.\n    video, generation, ltx-2, 19b, large-model\n\n    Use cases:\n    - Generate high-quality videos with large model\n    - Create detailed video animations\n    - Produce superior video content\n    - Generate videos with powerful AI\n    - Create premium video animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Ltx219BImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt used for the generation."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate the video from."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "interpolation_direction",
          "type": {
            "type": "enum",
            "values": [
              "forward",
              "backward"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.InterpolationDirection"
          },
          "default": "forward",
          "title": "Interpolation Direction",
          "description": "The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ltx 219B Image To Video Lora",
      "description": "LTX-2 19B with LoRA enables custom-trained 19B models for specialized video generation.\n    video, generation, ltx-2, 19b, lora, custom\n\n    Use cases:\n    - Generate videos with custom 19B model\n    - Create specialized video content\n    - Produce domain-specific animations\n    - Generate with fine-tuned large model\n    - Create customized video animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Ltx219BImageToVideoLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt used for the generation."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "auto",
          "title": "Video Size",
          "description": "The size of the generated video."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to generate the video from."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "interpolation_direction",
          "type": {
            "type": "enum",
            "values": [
              "forward",
              "backward"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.InterpolationDirection"
          },
          "default": "forward",
          "title": "Interpolation Direction",
          "description": "The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ltx Video 13 b Dev Image To Video",
      "description": "Generate videos from prompts and images using LTX Video-0.9.7 13B and custom LoRA\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LtxVideo13bDevImageToVideo",
      "properties": [
        {
          "name": "second_pass_skip_initial_steps",
          "type": {
            "type": "int"
          },
          "default": 17,
          "title": "Second Pass Skip Initial Steps",
          "description": "The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes."
        },
        {
          "name": "first_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "First Pass Num Inference Steps",
          "description": "Number of inference steps during the first pass."
        },
        {
          "name": "frame_rate",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Frame Rate",
          "description": "The frame rate of the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide generation"
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "Whether to reverse the video."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt using a language model."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to use for generation"
        },
        {
          "name": "second_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Second Pass Num Inference Steps",
          "description": "Number of inference steps during the second pass."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames in the video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "worst quality, inconsistent motion, blurry, jittery, distorted",
          "title": "Negative Prompt",
          "description": "Negative prompt for generation"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LtxVideo13bDevImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p or 720p)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "1:1",
              "16:9",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LtxVideo13bDevImageToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL for Image-to-Video task"
        },
        {
          "name": "constant_rate_factor",
          "type": {
            "type": "int"
          },
          "default": 35,
          "title": "Constant Rate Factor",
          "description": "The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality."
        },
        {
          "name": "first_pass_skip_final_steps",
          "type": {
            "type": "int"
          },
          "default": 3,
          "title": "First Pass Skip Final Steps",
          "description": "Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ltx Video 13 b Distilled Image To Video",
      "description": "Generate videos from prompts and images using LTX Video-0.9.7 13B Distilled and custom LoRA\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LtxVideo13bDistilledImageToVideo",
      "properties": [
        {
          "name": "second_pass_skip_initial_steps",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Second Pass Skip Initial Steps",
          "description": "The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes."
        },
        {
          "name": "first_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "First Pass Num Inference Steps",
          "description": "Number of inference steps during the first pass."
        },
        {
          "name": "frame_rate",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Frame Rate",
          "description": "The frame rate of the video."
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "Whether to reverse the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide generation"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt using a language model."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to use for generation"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames in the video."
        },
        {
          "name": "second_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Second Pass Num Inference Steps",
          "description": "Number of inference steps during the second pass."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "worst quality, inconsistent motion, blurry, jittery, distorted",
          "title": "Negative Prompt",
          "description": "Negative prompt for generation"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LtxVideo13bDistilledImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p or 720p)."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "1:1",
              "16:9",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LtxVideo13bDistilledImageToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL for Image-to-Video task"
        },
        {
          "name": "constant_rate_factor",
          "type": {
            "type": "int"
          },
          "default": 35,
          "title": "Constant Rate Factor",
          "description": "The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality."
        },
        {
          "name": "first_pass_skip_final_steps",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "First Pass Skip Final Steps",
          "description": "Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ltx Video Lora Image To Video",
      "description": "Generate videos from prompts and images using LTX Video-0.9.7 and custom LoRA\n    video, animation, image-to-video, img2vid, lora\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LtxVideoLoraImageToVideo",
      "properties": [
        {
          "name": "number_of_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Number Of Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LtxVideoLoraImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the video."
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "Whether to reverse the video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "1:1",
              "9:16",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LtxVideoLoraImageToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video."
        },
        {
          "name": "frame_rate",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Frame Rate",
          "description": "The frame rate of the video."
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt using the LLM."
        },
        {
          "name": "number_of_frames",
          "type": {
            "type": "int"
          },
          "default": 89,
          "title": "Number Of Frames",
          "description": "The number of frames in the video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as input."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRA weights to use for generation."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, low quality, low resolution, inconsistent motion, jittery, distorted",
          "title": "Negative Prompt",
          "description": "The negative prompt to use."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ltxv 13 b 098 Distilled Image To Video",
      "description": "Generate long videos from prompts and images using LTX Video-0.9.8 13B Distilled and custom LoRA\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Ltxv13b098DistilledImageToVideo",
      "properties": [
        {
          "name": "second_pass_skip_initial_steps",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Second Pass Skip Initial Steps",
          "description": "The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes."
        },
        {
          "name": "first_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "First Pass Num Inference Steps",
          "description": "Number of inference steps during the first pass."
        },
        {
          "name": "frame_rate",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Frame Rate",
          "description": "The frame rate of the video."
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "Whether to reverse the video."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide generation"
        },
        {
          "name": "expand_prompt",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Expand Prompt",
          "description": "Whether to expand the prompt using a language model."
        },
        {
          "name": "temporal_adain_factor",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Temporal Adain Factor",
          "description": "The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to use for generation"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames in the video."
        },
        {
          "name": "second_pass_num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Second Pass Num Inference Steps",
          "description": "Number of inference steps during the second pass."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "worst quality, inconsistent motion, blurry, jittery, distorted",
          "title": "Negative Prompt",
          "description": "Negative prompt for generation"
        },
        {
          "name": "enable_detail_pass",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Detail Pass",
          "description": "Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltxv13b098DistilledImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "9:16",
              "1:1",
              "16:9",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Ltxv13b098DistilledImageToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video."
        },
        {
          "name": "tone_map_compression_ratio",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Tone Map Compression Ratio",
          "description": "The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL for Image-to-Video task"
        },
        {
          "name": "constant_rate_factor",
          "type": {
            "type": "int"
          },
          "default": 29,
          "title": "Constant Rate Factor",
          "description": "The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Luma Dream Machine",
      "description": "Generate video clips from your images using Luma Dream Machine v1.5. Supports various aspect ratios and optional end-frame blending.\n    video, generation, animation, blending, aspect-ratio, img2vid, image-to-video\n\n    Use cases:\n    - Create seamless video loops\n    - Generate video transitions\n    - Transform images into animations\n    - Create motion graphics\n    - Produce video content",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LumaDreamMachine",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "21:9",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LumaDreamMachine.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "loop",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Loop",
          "description": "Whether the video should loop (end of video is blended with the beginning)"
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "An image to blend the end of the video with"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt",
        "resolution"
      ]
    },
    {
      "title": "Luma Dream Machine Ray 2 Flash Image To Video",
      "description": "Ray2 Flash is a fast video generative model capable of creating realistic visuals with natural, coherent motion.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LumaDreamMachineRay2FlashImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "21:9",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2FlashImageToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2FlashImageToVideo.Resolution"
          },
          "default": "540p",
          "title": "Resolution",
          "description": "The resolution of the generated video (720p costs 2x more, 1080p costs 4x more)"
        },
        {
          "name": "loop",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Loop",
          "description": "Whether the video should loop (end of video is blended with the beginning)"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5s",
              "9s"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2FlashImageToVideo.Duration"
          },
          "default": "5s",
          "title": "Duration",
          "description": "The duration of the generated video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Initial image to start the video from. Can be used together with end_image_url."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "Final image to end the video with. Can be used together with image_url."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Luma Dream Machine Ray 2 Image To Video",
      "description": "Ray2 is a large-scale video generative model capable of creating realistic visuals with natural, coherent motion.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.LumaDreamMachineRay2ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "4:3",
              "3:4",
              "21:9",
              "9:21"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2ImageToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2ImageToVideo.Resolution"
          },
          "default": "540p",
          "title": "Resolution",
          "description": "The resolution of the generated video (720p costs 2x more, 1080p costs 4x more)"
        },
        {
          "name": "loop",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Loop",
          "description": "Whether the video should loop (end of video is blended with the beginning)"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5s",
              "9s"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2ImageToVideo.Duration"
          },
          "default": "5s",
          "title": "Duration",
          "description": "The duration of the generated video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Initial image to start the video from. Can be used together with end_image_url."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "Final image to end the video with. Can be used together with image_url."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Minimax Hailuo 02 Fast Image To Video",
      "description": "Create blazing fast and economical videos with MiniMax Hailuo-02 Image To Video API at 512p resolution\n    video, animation, image-to-video, img2vid, fast\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.MinimaxHailuo02FastImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "6",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.MinimaxHailuo02FastImageToVideo.Duration"
          },
          "default": "6",
          "title": "Duration",
          "description": "The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution."
        },
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Minimax Hailuo 02 Pro Image To Video",
      "description": "MiniMax Hailuo-02 Image To Video API (Pro, 1080p): Advanced image-to-video generation model with 1080p resolution\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.MinimaxHailuo02ProImageToVideo",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "Optional URL of the image to use as the last frame of the video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Minimax Hailuo 23 Fast Pro Image To Video",
      "description": "MiniMax Hailuo 2.3 Fast [Pro] (Image to Video)\n    video, animation, image-to-video, img2vid, fast, professional\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.MinimaxHailuo23FastProImageToVideo",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Minimax Hailuo 23 Fast Standard Image To Video",
      "description": "MiniMax Hailuo 2.3 Fast [Standard] (Image to Video)\n    video, animation, image-to-video, img2vid, fast, professional\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.MinimaxHailuo23FastStandardImageToVideo",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "6",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.MinimaxHailuo23FastStandardImageToVideo.Duration"
          },
          "default": "6",
          "title": "Duration",
          "description": "The duration of the video in seconds."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Minimax Hailuo 23 Standard Image To Video",
      "description": "MiniMax Hailuo 2.3 [Standard] (Image to Video)\n    video, animation, image-to-video, img2vid, professional\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.MinimaxHailuo23StandardImageToVideo",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "6",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.MinimaxHailuo23StandardImageToVideo.Duration"
          },
          "default": "6",
          "title": "Duration",
          "description": "The duration of the video in seconds."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Minimax Video 01 Director Image To Video",
      "description": "Generate video clips more accurately with respect to initial image, natural language descriptions, and using camera movement instructions for shot control.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.MinimaxVideo01DirectorImageToVideo",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation. Camera movement instructions can be added using square brackets (e.g. [Pan left] or [Zoom in]). You can use up to 3 combined movements per prompt. Supported movements: Truck left/right, Pan left/right, Push in/Pull out, Pedestal up/down, Tilt up/down, Zoom in/out, Shake, Tracking shot, Static shot. For example: [Truck left, Pan right, Zoom in]. For a more detailed guide, refer https://sixth-switch-2ac.notion.site/T2V-01-Director-Model-Tutorial-with-camera-movement-1886c20a98eb80f395b8e05291ad8645"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Minimax Video 01 Live Image To Video",
      "description": "Generate video clips from your images using MiniMax Video model\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.MinimaxVideo01LiveImageToVideo",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Minimax Video 01 Subject Reference",
      "description": "Generate video clips maintaining consistent, realistic facial features and identity across dynamic video content\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.MinimaxVideo01SubjectReference",
      "properties": [
        {
          "name": "prompt_optimizer",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Prompt Optimizer",
          "description": "Whether to use the model's prompt optimizer"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "subject_reference_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Subject Reference Image Url",
          "description": "URL of the subject reference image to use for consistent subject appearance"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Moonvalley Marey I2 v",
      "description": "Generate a video starting from an image as the first frame with Marey, a generative video model trained exclusively on fully licensed data.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.MoonvalleyMareyI2v",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate a video from"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5s",
              "10s"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.MoonvalleyMareyI2v.Duration"
          },
          "default": "5s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as the first frame of the video."
        },
        {
          "name": "dimensions",
          "type": {
            "type": "enum",
            "values": [
              "1920x1080",
              "1080x1920",
              "1152x1152",
              "1536x1152",
              "1152x1536"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.MoonvalleyMareyI2v.Dimensions"
          },
          "default": "1920x1080",
          "title": "Dimensions",
          "description": "The dimensions of the generated video in width x height format."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Guidance Scale",
          "description": "Controls how strongly the generation is guided by the prompt (0-20). Higher values follow the prompt more closely."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for random number generation. Use -1 for random seed each run."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "<synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts",
          "title": "Negative Prompt",
          "description": "Negative prompt used to guide the model away from undesirable features."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Musetalk",
      "description": "MuseTalk is a real-time high quality audio-driven lip-syncing model. Use MuseTalk to animate a face with your own audio.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Musetalk",
      "properties": [
        {
          "name": "source_video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Source Video Url",
          "description": "URL of the source video"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Omni Human V15",
      "description": "OmniHuman v1.5 generates realistic human videos from images.\n    video, human, realistic, bytedance, image-to-video\n\n    Use cases:\n    - Generate realistic human videos\n    - Create human motion animations\n    - Produce lifelike character videos\n    - Generate human performances\n    - Create realistic human content",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.OmniHumanV15",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to guide the video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.OmniHumanV15.OmniHumanV15Resolution"
          },
          "default": "1080p",
          "title": "Resolution",
          "description": "The resolution of the generated video. Defaults to 1080p. 720p generation is faster and higher in quality. 1080p generation is limited to 30s audio and 720p generation is limited to 60s audio."
        },
        {
          "name": "turbo_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Turbo Mode",
          "description": "Generate a video at a faster rate with a slight quality trade-off."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file to generate the video. Audio must be under 30s long for 1080p generation and under 60s long for 720p generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image used to generate the video"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Ovi Image To Video",
      "description": "Ovi\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.OviImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps."
        },
        {
          "name": "audio_negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "robotic, muffled, echo, distorted",
          "title": "Audio Negative Prompt",
          "description": "Negative prompt for audio generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "jitter, bad hands, blur, distortion",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The image URL to guide video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pika V15 Pikaffects",
      "description": "Pika Effects are AI-powered video effects designed to modify objects, characters, and environments in a fun, engaging, and visually compelling manner.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PikaV15Pikaffects",
      "properties": [
        {
          "name": "pikaffect",
          "type": {
            "type": "enum",
            "values": [
              "Cake-ify",
              "Crumble",
              "Crush",
              "Decapitate",
              "Deflate",
              "Dissolve",
              "Explode",
              "Eye-pop",
              "Inflate",
              "Levitate",
              "Melt",
              "Peel",
              "Poke",
              "Squish",
              "Ta-da",
              "Tear"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PikaV15Pikaffects.Pikaffect"
          },
          "default": "",
          "title": "Pikaffect",
          "description": "The Pikaffect to apply"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide the effect"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to guide the model"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pika V21 Image To Video",
      "description": "Turn photos into mind-blowing, dynamic videos. Your images can can come to life with sharp details, impressive character control and cinematic camera moves.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PikaV21ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PikaV21ImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the model"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pika V22 Pikaframes",
      "description": "Pika\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PikaV22Pikaframes",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Default prompt for all transitions. Individual transition prompts override this."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PikaV22Pikaframes.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "transitions",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Transitions",
          "description": "Configuration for each transition. Length must be len(image_urls) - 1. Total duration of all transitions must not exceed 25 seconds. If not provided, uses default 5-second transitions with the global prompt."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator"
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "URLs of keyframe images (2-5 images) to create transitions between"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pika V2 Turbo Image To Video",
      "description": "Turbo is the model to use when you feel the need for speed. Turn your image to stunning video up to 3x faster \u2013 all with high quality outputs. \n    video, animation, image-to-video, img2vid, fast\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PikaV2TurboImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PikaV2TurboImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "int"
          },
          "default": 5,
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the model"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse Swap",
      "description": "Pixverse\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseSwap",
      "properties": [
        {
          "name": "original_sound_switch",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Original Sound Switch",
          "description": "Whether to keep the original audio"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the external video to swap"
        },
        {
          "name": "keyframe_id",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Keyframe Id",
          "description": "The keyframe ID (from 1 to the last frame position)"
        },
        {
          "name": "mode",
          "type": {
            "type": "enum",
            "values": [
              "person",
              "object",
              "background"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseSwap.Mode"
          },
          "default": "person",
          "title": "Mode",
          "description": "The swap mode to use"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseSwap.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The output resolution (1080p not supported)"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the target image for swapping"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V35 Effects",
      "description": "Generate high quality video clips with different effects using PixVerse v3.5\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV35Effects",
      "properties": [
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV35Effects.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV35Effects.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "effect",
          "type": {
            "type": "enum",
            "values": [
              "Kiss Me AI",
              "Kiss",
              "Muscle Surge",
              "Warmth of Jesus",
              "Anything, Robot",
              "The Tiger Touch",
              "Hug",
              "Holy Wings",
              "Microwave",
              "Zombie Mode",
              "Squid Game",
              "Baby Face",
              "Black Myth: Wukong",
              "Long Hair Magic",
              "Leggy Run",
              "Fin-tastic Mermaid",
              "Punch Face",
              "Creepy Devil Smile",
              "Thunder God",
              "Eye Zoom Challenge",
              "Who's Arrested?",
              "Baby Arrived",
              "Werewolf Rage",
              "Bald Swipe",
              "BOOM DROP",
              "Huge Cutie",
              "Liquid Metal",
              "Sharksnap!",
              "Dust Me Away",
              "3D Figurine Factor",
              "Bikini Up",
              "My Girlfriends",
              "My Boyfriends",
              "Subject 3 Fever",
              "Earth Zoom",
              "Pole Dance",
              "Vroom Dance",
              "GhostFace Terror",
              "Dragon Evoker",
              "Skeletal Bae",
              "Summoning succubus",
              "Halloween Voodoo Doll",
              "3D Naked-Eye AD",
              "Package Explosion",
              "Dishes Served",
              "Ocean ad",
              "Supermarket AD",
              "Tree doll",
              "Come Feel My Abs",
              "The Bicep Flex",
              "London Elite Vibe",
              "Flora Nymph Gown",
              "Christmas Costume",
              "It's Snowy",
              "Reindeer Cruiser",
              "Snow Globe Maker",
              "Pet Christmas Outfit",
              "Adopt a Polar Pal",
              "Cat Christmas Box",
              "Starlight Gift Box",
              "Xmas Poster",
              "Pet Christmas Tree",
              "City Santa Hat",
              "Stocking Sweetie",
              "Christmas Night",
              "Xmas Front Page Karma",
              "Grinch's Xmas Hijack",
              "Giant Product",
              "Truck Fashion Shoot",
              "Beach AD",
              "Shoal Surround",
              "Mechanical Assembly",
              "Lighting AD",
              "Billboard AD",
              "Product close-up",
              "Parachute Delivery",
              "Dreamlike Cloud",
              "Macaron Machine",
              "Poster AD",
              "Truck AD",
              "Graffiti AD",
              "3D Figurine Factory",
              "The Exclusive First Class",
              "Art Zoom Challenge",
              "I Quit",
              "Hitchcock Dolly Zoom",
              "Smell the Lens",
              "I believe I can fly",
              "Strikout Dance",
              "Pixel World",
              "Mint in Box",
              "Hands up, Hand",
              "Flora Nymph Go",
              "Somber Embrace",
              "Beam me up",
              "Suit Swagger"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV35Effects.Effect"
          },
          "default": "",
          "title": "Effect",
          "description": "The effect to apply to the video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of the image to use as the first frame. If not provided, generates from text"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V35 Image To Video",
      "description": "Generate high quality video clips from text and image prompts using PixVerse v3.5\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV35ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideo.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V35 Image To Video Fast",
      "description": "Generate high quality video clips from text and image prompts quickly using PixVerse v3.5 Fast\n    video, animation, image-to-video, img2vid, fast\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV35ImageToVideoFast",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideoFast.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideoFast.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V35 Transition",
      "description": "Create seamless transition between images using PixVerse v3.5\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV35Transition",
      "properties": [
        {
          "name": "first_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "First Image Url",
          "description": "URL of the image to use as the first frame"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV35Transition.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV35Transition.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV35Transition.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for the transition"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV35Transition.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the image to use as the last frame"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V45 Effects",
      "description": "Generate high quality video clips with different effects using PixVerse v4.5\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV45Effects",
      "properties": [
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV45Effects.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV45Effects.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "effect",
          "type": {
            "type": "enum",
            "values": [
              "Kiss Me AI",
              "Kiss",
              "Muscle Surge",
              "Warmth of Jesus",
              "Anything, Robot",
              "The Tiger Touch",
              "Hug",
              "Holy Wings",
              "Microwave",
              "Zombie Mode",
              "Squid Game",
              "Baby Face",
              "Black Myth: Wukong",
              "Long Hair Magic",
              "Leggy Run",
              "Fin-tastic Mermaid",
              "Punch Face",
              "Creepy Devil Smile",
              "Thunder God",
              "Eye Zoom Challenge",
              "Who's Arrested?",
              "Baby Arrived",
              "Werewolf Rage",
              "Bald Swipe",
              "BOOM DROP",
              "Huge Cutie",
              "Liquid Metal",
              "Sharksnap!",
              "Dust Me Away",
              "3D Figurine Factor",
              "Bikini Up",
              "My Girlfriends",
              "My Boyfriends",
              "Subject 3 Fever",
              "Earth Zoom",
              "Pole Dance",
              "Vroom Dance",
              "GhostFace Terror",
              "Dragon Evoker",
              "Skeletal Bae",
              "Summoning succubus",
              "Halloween Voodoo Doll",
              "3D Naked-Eye AD",
              "Package Explosion",
              "Dishes Served",
              "Ocean ad",
              "Supermarket AD",
              "Tree doll",
              "Come Feel My Abs",
              "The Bicep Flex",
              "London Elite Vibe",
              "Flora Nymph Gown",
              "Christmas Costume",
              "It's Snowy",
              "Reindeer Cruiser",
              "Snow Globe Maker",
              "Pet Christmas Outfit",
              "Adopt a Polar Pal",
              "Cat Christmas Box",
              "Starlight Gift Box",
              "Xmas Poster",
              "Pet Christmas Tree",
              "City Santa Hat",
              "Stocking Sweetie",
              "Christmas Night",
              "Xmas Front Page Karma",
              "Grinch's Xmas Hijack",
              "Giant Product",
              "Truck Fashion Shoot",
              "Beach AD",
              "Shoal Surround",
              "Mechanical Assembly",
              "Lighting AD",
              "Billboard AD",
              "Product close-up",
              "Parachute Delivery",
              "Dreamlike Cloud",
              "Macaron Machine",
              "Poster AD",
              "Truck AD",
              "Graffiti AD",
              "3D Figurine Factory",
              "The Exclusive First Class",
              "Art Zoom Challenge",
              "I Quit",
              "Hitchcock Dolly Zoom",
              "Smell the Lens",
              "I believe I can fly",
              "Strikout Dance",
              "Pixel World",
              "Mint in Box",
              "Hands up, Hand",
              "Flora Nymph Go",
              "Somber Embrace",
              "Beam me up",
              "Suit Swagger"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV45Effects.Effect"
          },
          "default": "",
          "title": "Effect",
          "description": "The effect to apply to the video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of the image to use as the first frame. If not provided, generates from text"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V45 Image To Video Fast",
      "description": "Generate fast high quality video clips from text and image prompts using PixVerse v4.5\n    video, animation, image-to-video, img2vid, fast\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV45ImageToVideoFast",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV45ImageToVideoFast.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV45ImageToVideoFast.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "camera_movement",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "horizontal_left",
              "horizontal_right",
              "vertical_up",
              "vertical_down",
              "zoom_in",
              "zoom_out",
              "crane_up",
              "quickly_zoom_in",
              "quickly_zoom_out",
              "smooth_zoom_in",
              "camera_rotation",
              "robo_arm",
              "super_dolly_out",
              "whip_pan",
              "hitchcock",
              "left_follow",
              "right_follow",
              "pan_left",
              "pan_right",
              "fix_bg"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV45ImageToVideoFast.CameraMovement"
          },
          "default": null,
          "title": "Camera Movement",
          "description": "The type of camera movement to apply to the video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V45 Transition",
      "description": "Create seamless transition between images using PixVerse v4.5\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV45Transition",
      "properties": [
        {
          "name": "first_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "First Image Url",
          "description": "URL of the image to use as the first frame"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV45Transition.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV45Transition.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV45Transition.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for the transition"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV45Transition.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the image to use as the last frame"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V4 Effects",
      "description": "Generate high quality video clips with different effects using PixVerse v4\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV4Effects",
      "properties": [
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV4Effects.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV4Effects.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "effect",
          "type": {
            "type": "enum",
            "values": [
              "Kiss Me AI",
              "Kiss",
              "Muscle Surge",
              "Warmth of Jesus",
              "Anything, Robot",
              "The Tiger Touch",
              "Hug",
              "Holy Wings",
              "Microwave",
              "Zombie Mode",
              "Squid Game",
              "Baby Face",
              "Black Myth: Wukong",
              "Long Hair Magic",
              "Leggy Run",
              "Fin-tastic Mermaid",
              "Punch Face",
              "Creepy Devil Smile",
              "Thunder God",
              "Eye Zoom Challenge",
              "Who's Arrested?",
              "Baby Arrived",
              "Werewolf Rage",
              "Bald Swipe",
              "BOOM DROP",
              "Huge Cutie",
              "Liquid Metal",
              "Sharksnap!",
              "Dust Me Away",
              "3D Figurine Factor",
              "Bikini Up",
              "My Girlfriends",
              "My Boyfriends",
              "Subject 3 Fever",
              "Earth Zoom",
              "Pole Dance",
              "Vroom Dance",
              "GhostFace Terror",
              "Dragon Evoker",
              "Skeletal Bae",
              "Summoning succubus",
              "Halloween Voodoo Doll",
              "3D Naked-Eye AD",
              "Package Explosion",
              "Dishes Served",
              "Ocean ad",
              "Supermarket AD",
              "Tree doll",
              "Come Feel My Abs",
              "The Bicep Flex",
              "London Elite Vibe",
              "Flora Nymph Gown",
              "Christmas Costume",
              "It's Snowy",
              "Reindeer Cruiser",
              "Snow Globe Maker",
              "Pet Christmas Outfit",
              "Adopt a Polar Pal",
              "Cat Christmas Box",
              "Starlight Gift Box",
              "Xmas Poster",
              "Pet Christmas Tree",
              "City Santa Hat",
              "Stocking Sweetie",
              "Christmas Night",
              "Xmas Front Page Karma",
              "Grinch's Xmas Hijack",
              "Giant Product",
              "Truck Fashion Shoot",
              "Beach AD",
              "Shoal Surround",
              "Mechanical Assembly",
              "Lighting AD",
              "Billboard AD",
              "Product close-up",
              "Parachute Delivery",
              "Dreamlike Cloud",
              "Macaron Machine",
              "Poster AD",
              "Truck AD",
              "Graffiti AD",
              "3D Figurine Factory",
              "The Exclusive First Class",
              "Art Zoom Challenge",
              "I Quit",
              "Hitchcock Dolly Zoom",
              "Smell the Lens",
              "I believe I can fly",
              "Strikout Dance",
              "Pixel World",
              "Mint in Box",
              "Hands up, Hand",
              "Flora Nymph Go",
              "Somber Embrace",
              "Beam me up",
              "Suit Swagger"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV4Effects.Effect"
          },
          "default": "",
          "title": "Effect",
          "description": "The effect to apply to the video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of the image to use as the first frame. If not provided, generates from text"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V4 Image To Video",
      "description": "Generate high quality video clips from text and image prompts using PixVerse v4\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV4ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideo.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "camera_movement",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "horizontal_left",
              "horizontal_right",
              "vertical_up",
              "vertical_down",
              "zoom_in",
              "zoom_out",
              "crane_up",
              "quickly_zoom_in",
              "quickly_zoom_out",
              "smooth_zoom_in",
              "camera_rotation",
              "robo_arm",
              "super_dolly_out",
              "whip_pan",
              "hitchcock",
              "left_follow",
              "right_follow",
              "pan_left",
              "pan_right",
              "fix_bg"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideo.CameraMovement"
          },
          "default": null,
          "title": "Camera Movement",
          "description": "The type of camera movement to apply to the video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V4 Image To Video Fast",
      "description": "Generate fast high quality video clips from text and image prompts using PixVerse v4\n    video, animation, image-to-video, img2vid, fast\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV4ImageToVideoFast",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideoFast.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideoFast.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "camera_movement",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "horizontal_left",
              "horizontal_right",
              "vertical_up",
              "vertical_down",
              "zoom_in",
              "zoom_out",
              "crane_up",
              "quickly_zoom_in",
              "quickly_zoom_out",
              "smooth_zoom_in",
              "camera_rotation",
              "robo_arm",
              "super_dolly_out",
              "whip_pan",
              "hitchcock",
              "left_follow",
              "right_follow",
              "pan_left",
              "pan_right",
              "fix_bg"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideoFast.CameraMovement"
          },
          "default": null,
          "title": "Camera Movement",
          "description": "The type of camera movement to apply to the video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V55 Effects",
      "description": "Pixverse\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV55Effects",
      "properties": [
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV55Effects.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV55Effects.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "thinking_type",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "enabled",
              "disabled",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV55Effects.ThinkingType"
          },
          "default": null,
          "title": "Thinking Type",
          "description": "Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision"
        },
        {
          "name": "effect",
          "type": {
            "type": "enum",
            "values": [
              "Kiss Me AI",
              "Kiss",
              "Muscle Surge",
              "Warmth of Jesus",
              "Anything, Robot",
              "The Tiger Touch",
              "Hug",
              "Holy Wings",
              "Microwave",
              "Zombie Mode",
              "Squid Game",
              "Baby Face",
              "Black Myth: Wukong",
              "Long Hair Magic",
              "Leggy Run",
              "Fin-tastic Mermaid",
              "Punch Face",
              "Creepy Devil Smile",
              "Thunder God",
              "Eye Zoom Challenge",
              "Who's Arrested?",
              "Baby Arrived",
              "Werewolf Rage",
              "Bald Swipe",
              "BOOM DROP",
              "Huge Cutie",
              "Liquid Metal",
              "Sharksnap!",
              "Dust Me Away",
              "3D Figurine Factor",
              "Bikini Up",
              "My Girlfriends",
              "My Boyfriends",
              "Subject 3 Fever",
              "Earth Zoom",
              "Pole Dance",
              "Vroom Dance",
              "GhostFace Terror",
              "Dragon Evoker",
              "Skeletal Bae",
              "Summoning succubus",
              "Halloween Voodoo Doll",
              "3D Naked-Eye AD",
              "Package Explosion",
              "Dishes Served",
              "Ocean ad",
              "Supermarket AD",
              "Tree doll",
              "Come Feel My Abs",
              "The Bicep Flex",
              "London Elite Vibe",
              "Flora Nymph Gown",
              "Christmas Costume",
              "It's Snowy",
              "Reindeer Cruiser",
              "Snow Globe Maker",
              "Pet Christmas Outfit",
              "Adopt a Polar Pal",
              "Cat Christmas Box",
              "Starlight Gift Box",
              "Xmas Poster",
              "Pet Christmas Tree",
              "City Santa Hat",
              "Stocking Sweetie",
              "Christmas Night",
              "Xmas Front Page Karma",
              "Grinch's Xmas Hijack",
              "Giant Product",
              "Truck Fashion Shoot",
              "Beach AD",
              "Shoal Surround",
              "Mechanical Assembly",
              "Lighting AD",
              "Billboard AD",
              "Product close-up",
              "Parachute Delivery",
              "Dreamlike Cloud",
              "Macaron Machine",
              "Poster AD",
              "Truck AD",
              "Graffiti AD",
              "3D Figurine Factory",
              "The Exclusive First Class",
              "Art Zoom Challenge",
              "I Quit",
              "Hitchcock Dolly Zoom",
              "Smell the Lens",
              "I believe I can fly",
              "Strikout Dance",
              "Pixel World",
              "Mint in Box",
              "Hands up, Hand",
              "Flora Nymph Go",
              "Somber Embrace",
              "Beam me up",
              "Suit Swagger"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV55Effects.Effect"
          },
          "default": "",
          "title": "Effect",
          "description": "The effect to apply to the video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of the image to use as the first frame. If not provided, generates from text"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V55 Image To Video",
      "description": "Pixverse\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV55ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV55ImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV55ImageToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV55ImageToVideo.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "thinking_type",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "enabled",
              "disabled",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV55ImageToVideo.ThinkingType"
          },
          "default": null,
          "title": "Thinking Type",
          "description": "Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision"
        },
        {
          "name": "generate_multi_clip_switch",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Generate Multi Clip Switch",
          "description": "Enable multi-clip generation with dynamic camera changes"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        },
        {
          "name": "generate_audio_switch",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Generate Audio Switch",
          "description": "Enable audio generation (BGM, SFX, dialogue)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V55 Transition",
      "description": "Pixverse\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV55Transition",
      "properties": [
        {
          "name": "first_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "First Image Url",
          "description": "URL of the image to use as the first frame"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV55Transition.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV55Transition.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV55Transition.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "thinking_type",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "enabled",
              "disabled",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV55Transition.ThinkingType"
          },
          "default": null,
          "title": "Thinking Type",
          "description": "Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for the transition"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV55Transition.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds"
        },
        {
          "name": "generate_audio_switch",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Generate Audio Switch",
          "description": "Enable audio generation (BGM, SFX, dialogue)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the image to use as the last frame"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V56 Image To Video",
      "description": "Generate high-quality videos from images with Pixverse v5.6.\n    video, generation, pixverse, v5.6, image-to-video, img2vid\n\n    Use cases:\n    - Animate photos into professional video clips\n    - Create dynamic product showcase videos\n    - Generate stylized video content from artwork\n    - Produce high-resolution social media animations\n    - Transform static images with various visual styles",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV56ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the desired video motion"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution quality of the output video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56Style"
          },
          "default": null,
          "title": "Style",
          "description": "Optional visual style for the video"
        },
        {
          "name": "thinking_type",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "enabled",
              "disabled",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56ThinkingType"
          },
          "default": null,
          "title": "Thinking Type",
          "description": "Thinking mode for video generation"
        },
        {
          "name": "image",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image",
          "description": "The image to transform into a video"
        },
        {
          "name": "generate_audio_switch",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Generate Audio Switch",
          "description": "Whether to generate audio for the video"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Optional seed for reproducible generation"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "What to avoid in the generated video"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt",
        "resolution"
      ]
    },
    {
      "title": "Pixverse V56 Transition",
      "description": "Pixverse v5.6 Transition creates smooth video transitions between two images with professional effects.\n    video, transition, pixverse, v5.6, effects\n\n    Use cases:\n    - Create smooth transitions between images\n    - Generate professional video effects\n    - Produce seamless image morphing\n    - Create transition animations\n    - Generate video connecting two scenes",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV56Transition",
      "properties": [
        {
          "name": "first_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "First Image Url",
          "description": "URL of the image to use as the first frame"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56Transition.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56Transition.PixverseV56TransitionResolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56Transition.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "thinking_type",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "enabled",
              "disabled",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56Transition.ThinkingType"
          },
          "default": null,
          "title": "Thinking Type",
          "description": "Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for the transition"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8",
              "10"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV56Transition.PixverseV56TransitionDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. 1080p videos are limited to 5 or 8 seconds"
        },
        {
          "name": "generate_audio_switch",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Generate Audio Switch",
          "description": "Enable audio generation (BGM, SFX, dialogue)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the image to use as the last frame"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V5 Effects",
      "description": "Generate high quality video clips with different effects using PixVerse v5\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV5Effects",
      "properties": [
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5Effects.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5Effects.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "effect",
          "type": {
            "type": "enum",
            "values": [
              "Kiss Me AI",
              "Kiss",
              "Muscle Surge",
              "Warmth of Jesus",
              "Anything, Robot",
              "The Tiger Touch",
              "Hug",
              "Holy Wings",
              "Microwave",
              "Zombie Mode",
              "Squid Game",
              "Baby Face",
              "Black Myth: Wukong",
              "Long Hair Magic",
              "Leggy Run",
              "Fin-tastic Mermaid",
              "Punch Face",
              "Creepy Devil Smile",
              "Thunder God",
              "Eye Zoom Challenge",
              "Who's Arrested?",
              "Baby Arrived",
              "Werewolf Rage",
              "Bald Swipe",
              "BOOM DROP",
              "Huge Cutie",
              "Liquid Metal",
              "Sharksnap!",
              "Dust Me Away",
              "3D Figurine Factor",
              "Bikini Up",
              "My Girlfriends",
              "My Boyfriends",
              "Subject 3 Fever",
              "Earth Zoom",
              "Pole Dance",
              "Vroom Dance",
              "GhostFace Terror",
              "Dragon Evoker",
              "Skeletal Bae",
              "Summoning succubus",
              "Halloween Voodoo Doll",
              "3D Naked-Eye AD",
              "Package Explosion",
              "Dishes Served",
              "Ocean ad",
              "Supermarket AD",
              "Tree doll",
              "Come Feel My Abs",
              "The Bicep Flex",
              "London Elite Vibe",
              "Flora Nymph Gown",
              "Christmas Costume",
              "It's Snowy",
              "Reindeer Cruiser",
              "Snow Globe Maker",
              "Pet Christmas Outfit",
              "Adopt a Polar Pal",
              "Cat Christmas Box",
              "Starlight Gift Box",
              "Xmas Poster",
              "Pet Christmas Tree",
              "City Santa Hat",
              "Stocking Sweetie",
              "Christmas Night",
              "Xmas Front Page Karma",
              "Grinch's Xmas Hijack",
              "Giant Product",
              "Truck Fashion Shoot",
              "Beach AD",
              "Shoal Surround",
              "Mechanical Assembly",
              "Lighting AD",
              "Billboard AD",
              "Product close-up",
              "Parachute Delivery",
              "Dreamlike Cloud",
              "Macaron Machine",
              "Poster AD",
              "Truck AD",
              "Graffiti AD",
              "3D Figurine Factory",
              "The Exclusive First Class",
              "Art Zoom Challenge",
              "I Quit",
              "Hitchcock Dolly Zoom",
              "Smell the Lens",
              "I believe I can fly",
              "Strikout Dance",
              "Pixel World",
              "Mint in Box",
              "Hands up, Hand",
              "Flora Nymph Go",
              "Somber Embrace",
              "Beam me up",
              "Suit Swagger"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5Effects.Effect"
          },
          "default": "",
          "title": "Effect",
          "description": "The effect to apply to the video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of the image to use as the first frame. If not provided, generates from text"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V5 Image To Video",
      "description": "Generate high quality video clips from text and image prompts using PixVerse v5\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV5ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5ImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5ImageToVideo.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5ImageToVideo.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Pixverse V5 Transition",
      "description": "Create seamless transition between images using PixVerse v5\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.PixverseV5Transition",
      "properties": [
        {
          "name": "first_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "First Image Url",
          "description": "URL of the image to use as the first frame"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5Transition.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "360p",
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5Transition.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video"
        },
        {
          "name": "style",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "anime",
              "3d_animation",
              "clay",
              "comic",
              "cyberpunk"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5Transition.Style"
          },
          "default": null,
          "title": "Style",
          "description": "The style of the generated video"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt for the transition"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "8"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.PixverseV5Transition.Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "The duration of the generated video in seconds"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of the model will output the same video every time."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the image to use as the last frame"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to be used for the generation"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Sadtalker",
      "description": "Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Sadtalker",
      "properties": [
        {
          "name": "pose_style",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Pose Style",
          "description": "The style of the pose"
        },
        {
          "name": "source_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Source Image Url",
          "description": "URL of the source image"
        },
        {
          "name": "driven_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Driven Audio Url",
          "description": "URL of the driven audio"
        },
        {
          "name": "face_enhancer",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "gfpgan"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Sadtalker.FaceEnhancer"
          },
          "default": null,
          "title": "Face Enhancer",
          "description": "The type of face enhancer to use"
        },
        {
          "name": "expression_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Expression Scale",
          "description": "The scale of the expression"
        },
        {
          "name": "face_model_resolution",
          "type": {
            "type": "enum",
            "values": [
              "256",
              "512"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Sadtalker.FaceModelResolution"
          },
          "default": "256",
          "title": "Face Model Resolution",
          "description": "The resolution of the face model"
        },
        {
          "name": "still_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Still Mode",
          "description": "Whether to use still mode. Fewer head motion, works with preprocess `full`."
        },
        {
          "name": "preprocess",
          "type": {
            "type": "enum",
            "values": [
              "crop",
              "extcrop",
              "resize",
              "full",
              "extfull"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Sadtalker.Preprocess"
          },
          "default": "crop",
          "title": "Preprocess",
          "description": "The type of preprocessing to use"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Sadtalker Reference",
      "description": "Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.SadtalkerReference",
      "properties": [
        {
          "name": "pose_style",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Pose Style",
          "description": "The style of the pose"
        },
        {
          "name": "source_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Source Image Url",
          "description": "URL of the source image"
        },
        {
          "name": "reference_pose_video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Reference Pose Video Url",
          "description": "URL of the reference video"
        },
        {
          "name": "driven_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Driven Audio Url",
          "description": "URL of the driven audio"
        },
        {
          "name": "face_enhancer",
          "type": {
            "type": "enum",
            "optional": true,
            "values": [
              "gfpgan"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SadtalkerReference.FaceEnhancer"
          },
          "default": null,
          "title": "Face Enhancer",
          "description": "The type of face enhancer to use"
        },
        {
          "name": "expression_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Expression Scale",
          "description": "The scale of the expression"
        },
        {
          "name": "face_model_resolution",
          "type": {
            "type": "enum",
            "values": [
              "256",
              "512"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SadtalkerReference.FaceModelResolution"
          },
          "default": "256",
          "title": "Face Model Resolution",
          "description": "The resolution of the face model"
        },
        {
          "name": "still_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Still Mode",
          "description": "Whether to use still mode. Fewer head motion, works with preprocess `full`."
        },
        {
          "name": "preprocess",
          "type": {
            "type": "enum",
            "values": [
              "crop",
              "extcrop",
              "resize",
              "full",
              "extfull"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SadtalkerReference.Preprocess"
          },
          "default": "crop",
          "title": "Preprocess",
          "description": "The type of preprocessing to use"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "See Dance V15 Pro Image To Video",
      "description": "SeeDance v1.5 Pro generates high-quality dance videos from images.\n    video, dance, animation, seedance, bytedance, image-to-video\n\n    Use cases:\n    - Animate photos into dance videos\n    - Create dance choreography from images\n    - Generate dance performances\n    - Produce music video content\n    - Create dance training materials",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.SeeDanceV15ProImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV15ProImageToVideo.SeeDanceV15ProResolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV15ProImageToVideo.SeeDanceV15ProDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the video in seconds"
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV15ProImageToVideo.SeeDanceV15ProAspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image used to generate video"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "camera_fixed",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Camera Fixed",
          "description": "Whether to fix the camera position"
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image the video ends with. Defaults to None."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control video generation. Use -1 for random."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "See Dance V1 Lite Reference To Video",
      "description": "SeeDance v1 Lite generates lightweight dance videos using reference images.\n    video, dance, lite, reference, seedance, image-to-video\n\n    Use cases:\n    - Generate efficient dance videos\n    - Create reference-based animations\n    - Produce lightweight dance content\n    - Generate quick dance outputs\n    - Create optimized dance videos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.SeeDanceV1LiteReferenceToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV1LiteReferenceToVideo.SeeDanceV1LiteResolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Video resolution - 480p for faster generation, 720p for higher quality"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "2",
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV1LiteReferenceToVideo.SeeDanceV1LiteDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the video in seconds"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV1LiteReferenceToVideo.SeeDanceV1LiteAspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "reference_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Image Urls",
          "description": "Reference images to generate the video with."
        },
        {
          "name": "camera_fixed",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Camera Fixed",
          "description": "Whether to fix the camera position"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control video generation. Use -1 for random."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "reference"
      ]
    },
    {
      "title": "See Dance V1 Pro Fast Image To Video",
      "description": "SeeDance v1 Pro Fast generates dance videos quickly from images.\n    video, dance, fast, seedance, bytedance, image-to-video\n\n    Use cases:\n    - Rapidly generate dance videos\n    - Quick dance animation\n    - Fast dance prototypes\n    - Create dance previews\n    - Efficient dance video generation",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.SeeDanceV1ProFastImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used to generate the video"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV1ProFastImageToVideo.SeeDanceV1ProFastResolution"
          },
          "default": "1080p",
          "title": "Resolution",
          "description": "Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "2",
              "3",
              "4",
              "5",
              "6",
              "7",
              "8",
              "9",
              "10",
              "11",
              "12"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV1ProFastImageToVideo.SeeDanceV1ProFastDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the video in seconds"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "21:9",
              "16:9",
              "4:3",
              "1:1",
              "3:4",
              "9:16",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SeeDanceV1ProFastImageToVideo.SeeDanceV1ProFastAspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image used to generate video"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "camera_fixed",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Camera Fixed",
          "description": "Whether to fix the camera position"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed to control video generation. Use -1 for random."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Skyreels I2 v",
      "description": "SkyReels V1 is the first and most advanced open-source human-centric video foundation model. By fine-tuning HunyuanVideo on O(10M) high-quality film and television clips\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.SkyreelsI2v",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.SkyreelsI2v.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the output video"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image input."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 6,
          "title": "Guidance Scale",
          "description": "Guidance scale for generation (between 1.0 and 20.0)"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation. If not provided, a random seed will be used."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of denoising steps (between 1 and 50). Higher values give better quality but take longer."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to guide generation away from certain attributes."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Stable Video Image To Video",
      "description": "Stable Video generates consistent video animations from images.\n    video, generation, stable, consistent, image-to-video\n\n    Use cases:\n    - Generate stable video animations\n    - Create consistent motion\n    - Produce reliable video outputs\n    - Animate images consistently\n    - Generate predictable videos",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.StableVideoImageToVideo",
      "properties": [
        {
          "name": "motion_bucket_id",
          "type": {
            "type": "int"
          },
          "default": 127,
          "title": "Motion Bucket Id",
          "description": "The motion bucket id determines the motion of the generated video. The higher the number, the more motion there will be."
        },
        {
          "name": "fps",
          "type": {
            "type": "int"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "cond_aug",
          "type": {
            "type": "float"
          },
          "default": 0.02,
          "title": "Cond Aug",
          "description": "The conditoning augmentation determines the amount of noise that will be added to the conditioning frame. The higher the number, the more noise there will be, and the less the video will look like the initial image. Increase it for more motion."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a starting point for the generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Veed Fabric 10",
      "description": "Fabric 1.0\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.VeedFabric10",
      "properties": [
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "480p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.VeedFabric10.Resolution"
          },
          "default": "",
          "title": "Resolution",
          "description": "Resolution"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Veed Fabric 10 Fast",
      "description": "Fabric 1.0 Fast\n    video, animation, image-to-video, img2vid, fast\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.VeedFabric10Fast",
      "properties": [
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "480p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.VeedFabric10Fast.Resolution"
          },
          "default": "",
          "title": "Resolution",
          "description": "Resolution"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Veo 31 Fast First Last Frame To Video",
      "description": "Veo 3.1 Fast\n    video, animation, image-to-video, img2vid, fast\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Veo31FastFirstLastFrameToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the video you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4s",
              "6s",
              "8s"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo31FastFirstLastFrameToVideo.Duration"
          },
          "default": "8s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo31FastFirstLastFrameToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "4k"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo31FastFirstLastFrameToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "first_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "First Frame Url",
          "description": "URL of the first frame of the video"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "last_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Last Frame Url",
          "description": "URL of the last frame of the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Veo 31 Fast Image To Video",
      "description": "Veo 3.1 Fast\n    video, animation, image-to-video, img2vid, fast\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Veo31FastImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the video you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4s",
              "6s",
              "8s"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo31FastImageToVideo.Duration"
          },
          "default": "8s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo31FastImageToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video. Only 16:9 and 9:16 are supported."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "4k"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo31FastImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Veo 31 First Last Frame To Video",
      "description": "Veo 3.1\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Veo31FirstLastFrameToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the video you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4s",
              "6s",
              "8s"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo31FirstLastFrameToVideo.Duration"
          },
          "default": "8s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo31FirstLastFrameToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "4k"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo31FirstLastFrameToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "first_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "First Frame Url",
          "description": "URL of the first frame of the video"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "last_frame_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Last Frame Url",
          "description": "URL of the last frame of the video"
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Veo 31 Image To Video",
      "description": "Veo 3.1\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Veo31ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the video you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4s",
              "6s",
              "8s"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo31ImageToVideo.Duration"
          },
          "default": "8s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo31ImageToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video. Only 16:9 and 9:16 are supported."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "4k"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo31ImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "A negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Veo 31 Reference To Video",
      "description": "Veo 3.1\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Veo31ReferenceToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the video you want to generate"
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "8s"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo31ReferenceToVideo.Duration"
          },
          "default": "8s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo31ReferenceToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p",
              "4k"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo31ReferenceToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "URLs of the reference images to use for consistent subject appearance"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Veo 3 Fast Image To Video",
      "description": "Now with a 50% price drop. Generate videos from your image prompts using Veo 3 fast.\n    video, animation, image-to-video, img2vid, fast\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Veo3FastImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing how the image should be animated"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3FastImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3FastImageToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4s",
              "6s",
              "8s"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3FastImageToVideo.Duration"
          },
          "default": "8s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Veo 3 Image To Video",
      "description": "Veo 3 is the latest state-of-the art video generation model from Google DeepMind\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.Veo3ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing how the image should be animated"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3ImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "The resolution of the generated video."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3ImageToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the generated video."
        },
        {
          "name": "generate_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Generate Audio",
          "description": "Whether to generate audio for the video."
        },
        {
          "name": "auto_fix",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Auto Fix",
          "description": "Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "4s",
              "6s",
              "8s"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.Veo3ImageToVideo.Duration"
          },
          "default": "8s",
          "title": "Duration",
          "description": "The duration of the generated video."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Vidu Image To Video",
      "description": "Vidu Image to Video generates high-quality videos with exceptional visual quality and motion diversity from a single image\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.ViduImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation, max 1500 characters"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        },
        {
          "name": "movement_amplitude",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "small",
              "medium",
              "large"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.ViduImageToVideo.MovementAmplitude"
          },
          "default": "auto",
          "title": "Movement Amplitude",
          "description": "The movement amplitude of objects in the frame"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Vidu Q1 Image To Video",
      "description": "Vidu Q1 Image to Video generates high-quality 1080p videos with exceptional visual quality and motion diversity from a single image\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.ViduQ1ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation, max 1500 characters"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator"
        },
        {
          "name": "movement_amplitude",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "small",
              "medium",
              "large"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.ViduQ1ImageToVideo.MovementAmplitude"
          },
          "default": "auto",
          "title": "Movement Amplitude",
          "description": "The movement amplitude of objects in the frame"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Vidu Q1 Reference To Video",
      "description": "Generate video clips from your multiple image references using Vidu Q1\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.ViduQ1ReferenceToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation, max 1500 characters"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.ViduQ1ReferenceToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the output video"
        },
        {
          "name": "bgm",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Bgm",
          "description": "Whether to add background music to the generated video"
        },
        {
          "name": "reference_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Image Urls",
          "description": "URLs of the reference images to use for consistent subject appearance. Q1 model supports up to 7 reference images."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        },
        {
          "name": "movement_amplitude",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "small",
              "medium",
              "large"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.ViduQ1ReferenceToVideo.MovementAmplitude"
          },
          "default": "auto",
          "title": "Movement Amplitude",
          "description": "The movement amplitude of objects in the frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Vidu Q1 Start End To Video",
      "description": "Vidu Q1 Start-End to Video generates smooth transition 1080p videos between specified start and end images.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.ViduQ1StartEndToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation, max 1500 characters"
        },
        {
          "name": "start_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Start Image Url",
          "description": "URL of the image to use as the first frame"
        },
        {
          "name": "movement_amplitude",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "small",
              "medium",
              "large"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.ViduQ1StartEndToVideo.MovementAmplitude"
          },
          "default": "auto",
          "title": "Movement Amplitude",
          "description": "The movement amplitude of objects in the frame"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Seed for the random number generator"
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the image to use as the last frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Vidu Q2 Reference To Video Pro",
      "description": "Vidu Q2 Reference-to-Video Pro generates professional quality videos using reference images for style and content.\n    video, generation, vidu, q2, pro, reference\n\n    Use cases:\n    - Generate pro videos from references\n    - Create style-consistent animations\n    - Produce reference-guided videos\n    - Generate videos matching examples\n    - Create professional reference-based content",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.ViduQ2ReferenceToVideoPro",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation, max 2000 characters"
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "540p",
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.ViduQ2ReferenceToVideoPro.ViduQ2ReferenceToVideoProResolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Output video resolution"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "str"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the output video (e.g., auto, 16:9, 9:16, 1:1, or any W:H)"
        },
        {
          "name": "duration",
          "type": {
            "type": "int"
          },
          "default": 4,
          "title": "Duration",
          "description": "Duration of the video in seconds (0 for automatic duration)"
        },
        {
          "name": "reference_video_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Video Urls",
          "description": "URLs of the reference videos for video editing or motion reference. Supports up to 2 videos."
        },
        {
          "name": "bgm",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Bgm",
          "description": "Whether to add background music to the generated video"
        },
        {
          "name": "reference_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Image Urls",
          "description": "URLs of the reference images for subject appearance. If videos are provided, up to 4 images are allowed; otherwise up to 7 images."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "movement_amplitude",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "small",
              "medium",
              "large"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.ViduQ2ReferenceToVideoPro.MovementAmplitude"
          },
          "default": "auto",
          "title": "Movement Amplitude",
          "description": "The movement amplitude of objects in the frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Vidu Reference To Video",
      "description": "Vidu Reference to Video creates videos by using a reference images and combining them with a prompt.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.ViduReferenceToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation, max 1500 characters"
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.ViduReferenceToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the output video"
        },
        {
          "name": "reference_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Reference Image Urls",
          "description": "URLs of the reference images to use for consistent subject appearance"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        },
        {
          "name": "movement_amplitude",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "small",
              "medium",
              "large"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.ViduReferenceToVideo.MovementAmplitude"
          },
          "default": "auto",
          "title": "Movement Amplitude",
          "description": "The movement amplitude of objects in the frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Vidu Start End To Video",
      "description": "Vidu Start-End to Video generates smooth transition videos between specified start and end images.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.ViduStartEndToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt for video generation, max 1500 characters"
        },
        {
          "name": "start_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Start Image Url",
          "description": "URL of the image to use as the first frame"
        },
        {
          "name": "movement_amplitude",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "small",
              "medium",
              "large"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.ViduStartEndToVideo.MovementAmplitude"
          },
          "default": "auto",
          "title": "Movement Amplitude",
          "description": "The movement amplitude of objects in the frame"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the image to use as the last frame"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Vidu Template To Video",
      "description": "Vidu Template to Video lets you create different effects by applying motion templates to your images.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.ViduTemplateToVideo",
      "properties": [
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "9:16"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.ViduTemplateToVideo.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the output video"
        },
        {
          "name": "template",
          "type": {
            "type": "enum",
            "values": [
              "dreamy_wedding",
              "romantic_lift",
              "sweet_proposal",
              "couple_arrival",
              "cupid_arrow",
              "pet_lovers",
              "lunar_newyear",
              "hug",
              "kiss",
              "dynasty_dress",
              "wish_sender",
              "love_pose",
              "hair_swap",
              "youth_rewind",
              "morphlab",
              "live_photo",
              "emotionlab",
              "live_memory",
              "interaction",
              "christmas",
              "pet_finger",
              "eat_mushrooms",
              "beast_chase_library",
              "beast_chase_supermarket",
              "petal_scattered",
              "emoji_figure",
              "hair_color_change",
              "multiple_people_kissing",
              "beast_chase_amazon",
              "beast_chase_mountain",
              "balloonman_explodes_pro",
              "get_thinner",
              "jump2pool",
              "bodyshake",
              "jiggle_up",
              "shake_it_dance",
              "subject_3",
              "pubg_winner_hit",
              "shake_it_down",
              "blueprint_supreme",
              "hip_twist",
              "motor_dance",
              "rat_dance",
              "kwok_dance",
              "leg_sweep_dance",
              "heeseung_march",
              "shake_to_max",
              "dame_un_grrr",
              "i_know",
              "lit_bounce",
              "wave_dance",
              "chill_dance",
              "hip_flicking",
              "sakura_season",
              "zongzi_wrap",
              "zongzi_drop",
              "dragonboat_shot",
              "rain_kiss",
              "child_memory",
              "couple_drop",
              "couple_walk",
              "flower_receive",
              "love_drop",
              "cheek_kiss",
              "carry_me",
              "blow_kiss",
              "love_fall",
              "french_kiss_8s",
              "workday_feels",
              "love_story",
              "bloom_magic",
              "ghibli",
              "minecraft",
              "box_me",
              "claw_me",
              "clayshot",
              "manga_meme",
              "quad_meme",
              "pixel_me",
              "clayshot_duo",
              "irasutoya",
              "american_comic",
              "simpsons_comic",
              "yayoi_kusama_style",
              "pop_art",
              "jojo_style",
              "slice_therapy",
              "balloon_flyaway",
              "flying",
              "paperman",
              "pinch",
              "bloom_doorobear",
              "gender_swap",
              "nap_me",
              "sexy_me",
              "spin360",
              "smooth_shift",
              "paper_fall",
              "jump_to_cloud",
              "pilot",
              "sweet_dreams",
              "soul_depart",
              "punch_hit",
              "watermelon_hit",
              "split_stance_pet",
              "make_face",
              "break_glass",
              "split_stance_human",
              "covered_liquid_metal",
              "fluffy_plunge",
              "pet_belly_dance",
              "water_float",
              "relax_cut",
              "head_to_balloon",
              "cloning",
              "across_the_universe_jungle",
              "clothes_spinning_remnant",
              "across_the_universe_jurassic",
              "across_the_universe_moon",
              "fisheye_pet",
              "hitchcock_zoom",
              "cute_bangs",
              "earth_zoom_out",
              "fisheye_human",
              "drive_yacht",
              "virtual_singer",
              "earth_zoom_in",
              "aliens_coming",
              "drive_ferrari",
              "bjd_style",
              "virtual_fitting",
              "orbit",
              "zoom_in",
              "ai_outfit",
              "spin180",
              "orbit_dolly",
              "orbit_dolly_fast",
              "auto_spin",
              "walk_forward",
              "outfit_show",
              "zoom_in_fast",
              "zoom_out_image",
              "zoom_out_startend",
              "muscling",
              "captain_america",
              "hulk",
              "cap_walk",
              "hulk_dive",
              "exotic_princess",
              "beast_companion",
              "cartoon_doll",
              "golden_epoch",
              "oscar_gala",
              "fashion_stride",
              "star_carpet",
              "flame_carpet",
              "frost_carpet",
              "mecha_x",
              "style_me",
              "tap_me",
              "saber_warrior",
              "pet2human",
              "graduation",
              "fishermen",
              "happy_birthday",
              "fairy_me",
              "ladudu_me",
              "ladudu_me_random",
              "squid_game",
              "superman",
              "grow_wings",
              "clevage",
              "fly_with_doraemon",
              "creatice_product_down",
              "pole_dance",
              "hug_from_behind",
              "creatice_product_up_cybercity",
              "creatice_product_up_bluecircuit",
              "creatice_product_up",
              "run_fast",
              "background_explosion"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.ViduTemplateToVideo.Template"
          },
          "default": "hug",
          "title": "Template",
          "description": "AI video template to use. Pricing varies by template: Standard templates (hug, kiss, love_pose, etc.) cost 4 credits ($0.20), Premium templates (lunar_newyear, dynasty_dress, dreamy_wedding, etc.) cost 6 credits ($0.30), and Advanced templates (live_photo) cost 10 credits ($0.50)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for generation"
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "URLs of the images to use with the template. Number of images required varies by template: 'dynasty_dress' and 'shop_frame' accept 1-2 images, 'wish_sender' requires exactly 3 images, all other templates accept only 1 image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan Ati",
      "description": "Wan Ati\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.WanAti",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanAti.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p, 580p, 720p)."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image."
        },
        {
          "name": "track",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "list",
                "type_args": [
                  {
                    "type": "str"
                  }
                ]
              }
            ]
          },
          "default": [],
          "title": "Track",
          "description": "Motion tracks to guide video generation. Each track is a sequence of points defining a motion trajectory. Multiple tracks can control different elements or objects in the video. Expected format: array of tracks, where each track is an array of points with 'x' and 'y' coordinates (up to 121 points per track). Points will be automatically padded to 121 if fewer are provided. Coordinates should be within the image dimensions."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan Flf 2 v",
      "description": "Wan-2.1 flf2v generates dynamic videos by intelligently bridging a given first frame to a desired end frame through smooth, coherent motion sequences.\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.WanFlf2v",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanFlf2v.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 24."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "start_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Start Image Url",
          "description": "URL of the starting image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the ending image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 100 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanFlf2v.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanFlf2v.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "guide_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guide Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan I2 v Lora",
      "description": "Add custom LoRAs to Wan-2.1 is a image-to-video model that generates high-quality videos with high visual quality and motion diversity from images\n    video, animation, image-to-video, img2vid, lora\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.WanI2vLora",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift parameter for video generation."
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "If true, the video will be reversed."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to be used in the inference."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 5 to 24."
        },
        {
          "name": "turbo_mode",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Turbo Mode",
          "description": "If true, the video will be generated faster with no noticeable degradation in the visual quality."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 81 to 100 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanI2vLora.AspectRatio"
          },
          "default": "16:9",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the output video."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanI2vLora.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "guide_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guide Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan Move",
      "description": "Wan Move generates videos with natural motion and movement from static images.\n    video, generation, wan, motion, animation\n\n    Use cases:\n    - Add natural motion to images\n    - Create animated movements\n    - Produce dynamic video content\n    - Generate moving scenes from stills\n    - Create motion animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.WanMove",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to guide the video generation."
        },
        {
          "name": "trajectories",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "list",
                "type_args": [
                  {
                    "type": "str"
                  }
                ]
              }
            ]
          },
          "default": [],
          "title": "Trajectories",
          "description": "A list of trajectories. Each trajectory list means the movement of one object."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "\u8272\u8c03\u8273\u4e3d\uff0c\u8fc7\u66dd\uff0c\u9759\u6001\uff0c\u7ec6\u8282\u6a21\u7cca\u4e0d\u6e05\uff0c\u5b57\u5e55\uff0c\u98ce\u683c\uff0c\u4f5c\u54c1\uff0c\u753b\u4f5c\uff0c\u753b\u9762\uff0c\u9759\u6b62\uff0c\u6574\u4f53\u53d1\u7070\uff0c\u6700\u5dee\u8d28\u91cf\uff0c\u4f4e\u8d28\u91cf\uff0cJPEG\u538b\u7f29\u6b8b\u7559\uff0c\u4e11\u964b\u7684\uff0c\u6b8b\u7f3a\u7684\uff0c\u591a\u4f59\u7684\u624b\u6307\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u624b\u90e8\uff0c\u753b\u5f97\u4e0d\u597d\u7684\u8138\u90e8\uff0c\u7578\u5f62\u7684\uff0c\u6bc1\u5bb9\u7684\uff0c\u5f62\u6001\u7578\u5f62\u7684\u80a2\u4f53\uff0c\u624b\u6307\u878d\u5408\uff0c\u9759\u6b62\u4e0d\u52a8\u7684\u753b\u9762\uff0c\u6742\u4e71\u7684\u80cc\u666f\uff0c\u4e09\u6761\u817f\uff0c\u80cc\u666f\u4eba\u5f88\u591a\uff0c\u5012\u7740\u8d70",
          "title": "Negative Prompt",
          "description": "Negative prompt to guide the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan V225 b Image To Video",
      "description": "Wan 2.2's 5B model produces up to 5 seconds of video 720p at 24FPS with fluid motion and powerful prompt understanding\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.WanV225bImageToVideo",
      "properties": [
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift value for the video. Must be between 1.0 and 10.0."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 24,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 17 to 161 (inclusive)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (580p or 720p)."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the output video. Higher quality means better visual quality but larger file size."
        },
        {
          "name": "adjust_fps_for_interpolation",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Adjust Fps For Interpolation",
          "description": "If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "film",
              "rife"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. If None, no interpolation is applied."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan V22 A14 b Image To Video",
      "description": "fal-ai/wan/v2.2-A14B/image-to-video\n    video, animation, image-to-video, img2vid\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.WanV22A14bImageToVideo",
      "properties": [
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift value for the video. Must be between 1.0 and 10.0."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 17 to 161 (inclusive)."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the end image."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p, 580p, or 720p)."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "guidance_scale_2",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale 2",
          "description": "Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the output video. Higher quality means better visual quality but larger file size."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "adjust_fps_for_interpolation",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Adjust Fps For Interpolation",
          "description": "If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "film",
              "rife"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. If None, no interpolation is applied."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan V22 A14 b Image To Video Lora",
      "description": "Wan-2.2 image-to-video is a video model that generates high-quality videos with high visual quality and motion diversity from text prompts and images. This endpoint supports LoRAs made for Wan 2.2\n    video, animation, image-to-video, img2vid, lora\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.WanV22A14bImageToVideoLora",
      "properties": [
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift value for the video. Must be between 1.0 and 10.0."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "num_interpolated_frames",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Interpolated Frames",
          "description": "Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'."
        },
        {
          "name": "reverse_video",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reverse Video",
          "description": "If true, the video will be reversed."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "LoRA weights to be used in the inference."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 81,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 17 to 161 (inclusive)."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the end image."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p, 580p, or 720p)."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "guidance_scale_2",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Guidance Scale 2",
          "description": "Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the output video. Higher quality means better visual quality but larger file size."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "adjust_fps_for_interpolation",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Adjust Fps For Interpolation",
          "description": "If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "interpolator_model",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "film",
              "rife"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.InterpolatorModel"
          },
          "default": "film",
          "title": "Interpolator Model",
          "description": "The model to use for frame interpolation. If None, no interpolation is applied."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan V22 A14 b Image To Video Turbo",
      "description": "Wan-2.2 Turbo image-to-video is a video model that generates high-quality videos with high visual quality and motion diversity from text prompts. \n    video, animation, image-to-video, img2vid, fast\n\n    Use cases:\n    - Animate static images\n    - Create engaging social media content\n    - Product demonstrations\n    - Marketing and promotional videos\n    - Visual storytelling",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.WanV22A14bImageToVideoTurbo",
      "properties": [
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.Resolution"
          },
          "default": "720p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p, 580p, or 720p)."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "auto",
              "16:9",
              "9:16",
              "1:1"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt to guide video generation."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the output video. Higher quality means better visual quality but larger file size."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "URL of the end image."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan V26 Image To Video",
      "description": "Wan v2.6 generates high-quality videos from images with balanced quality and performance.\n    video, generation, wan, v2.6, image-to-video\n\n    Use cases:\n    - Generate quality videos from images\n    - Create balanced video animations\n    - Produce reliable video content\n    - Generate consistent videos\n    - Create professional animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.WanV26ImageToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the desired video motion. Max 800 characters."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10",
              "15"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV26ImageToVideo.WanV26Duration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the generated video in seconds. Choose between 5, 10 or 15 seconds."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV26ImageToVideo.WanV26Resolution"
          },
          "default": "1080p",
          "title": "Resolution",
          "description": "Video resolution. Valid values: 720p, 1080p"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame. Must be publicly accessible or base64 data URI. Image dimensions must be between 240 and 7680."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "URL of the audio to use as the background music. Must be publicly accessible. Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds), the audio is truncated to the first N seconds, and the rest is discarded. If the audio is shorter than the video, the remaining part of the video will be silent. For example, if the audio is 3 seconds long and the video duration is 5 seconds, the first 3 seconds of the output video will have sound, and the last 2 seconds will be silent. - Format: WAV, MP3. - Duration: 3 to 30 s. - File size: Up to 15 MB."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "multi_shots",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Multi Shots",
          "description": "When true, enables intelligent multi-shot segmentation. Only active when enable_prompt_expansion is True. Set to false for single-shot generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to describe content to avoid. Max 500 characters."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt rewriting using LLM."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "Wan V26 Image To Video Flash",
      "description": "Wan v2.6 Flash generates videos from images with ultra-fast processing for rapid iteration.\n    video, generation, wan, v2.6, flash, fast\n\n    Use cases:\n    - Generate videos at maximum speed\n    - Create rapid video prototypes\n    - Produce instant video previews\n    - Generate quick video iterations\n    - Create fast video animations",
      "namespace": "fal.image_to_video",
      "node_type": "fal.image_to_video.WanV26ImageToVideoFlash",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt describing the desired video motion. Max 800 characters."
        },
        {
          "name": "duration",
          "type": {
            "type": "enum",
            "values": [
              "5",
              "10",
              "15"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV26ImageToVideoFlash.WanV26FlashDuration"
          },
          "default": "5",
          "title": "Duration",
          "description": "Duration of the generated video in seconds. Choose between 5, 10 or 15 seconds."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "720p",
              "1080p"
            ],
            "type_name": "nodetool.nodes.fal.image_to_video.WanV26ImageToVideoFlash.WanV26FlashResolution"
          },
          "default": "1080p",
          "title": "Resolution",
          "description": "Video resolution. Valid values: 720p, 1080p"
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "If set to true, the safety checker will be enabled."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to use as the first frame. Must be publicly accessible or base64 data URI. Image dimensions must be between 240 and 7680."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "URL of the audio to use as the background music. Must be publicly accessible. Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds), the audio is truncated to the first N seconds, and the rest is discarded. If the audio is shorter than the video, the remaining part of the video will be silent. For example, if the audio is 3 seconds long and the video duration is 5 seconds, the first 3 seconds of the output video will have sound, and the last 2 seconds will be silent. - Format: WAV, MP3. - Duration: 3 to 30 s. - File size: Up to 15 MB."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        },
        {
          "name": "multi_shots",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Multi Shots",
          "description": "When true, enables intelligent multi-shot segmentation. Only active when enable_prompt_expansion is True. Set to false for single-shot generation."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt to describe content to avoid. Max 500 characters."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt rewriting using LLM."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "prompt"
      ]
    },
    {
      "title": "AIDetector Image",
      "description": "AI Detector analyzes images to determine if they were generated by AI or are real photos.\n    vision, ai-detection, analysis, classification\n\n    Use cases:\n    - Detect AI-generated images\n    - Verify image authenticity\n    - Filter synthetic content\n    - Content moderation for AI images\n    - Analyze image provenance",
      "namespace": "fal.vision",
      "node_type": "fal.vision.AIDetectorImage",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL pointing to an image to analyze for AI generation.(Max: 3000 characters)"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Arbiter Image",
      "description": "Arbiter provides comprehensive image analysis and quality metrics.\n    vision, analysis, quality, metrics, image-evaluation\n\n    Use cases:\n    - Analyze image quality\n    - Extract image metrics\n    - Evaluate visual properties\n    - Assess image characteristics\n    - Generate quality reports",
      "namespace": "fal.vision",
      "node_type": "fal.vision.ArbiterImage",
      "properties": [
        {
          "name": "measurements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Measurements",
          "description": "The measurements to use for the measurement."
        },
        {
          "name": "inputs",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Inputs",
          "description": "The inputs to use for the measurement."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Arbiter Image Image",
      "description": "Arbiter measures similarity and alignment between reference images.\n    vision, similarity, comparison, image-matching, analysis\n\n    Use cases:\n    - Compare image similarity\n    - Measure visual alignment\n    - Find duplicate images\n    - Rank image variations\n    - Evaluate image consistency",
      "namespace": "fal.vision",
      "node_type": "fal.vision.ArbiterImageImage",
      "properties": [
        {
          "name": "measurements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Measurements",
          "description": "The measurements to use for the measurement."
        },
        {
          "name": "inputs",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Inputs",
          "description": "The inputs to use for the measurement."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image1",
        "image2"
      ]
    },
    {
      "title": "Arbiter Image Text",
      "description": "Arbiter measures semantic alignment between images and text descriptions.\n    vision, alignment, similarity, text-image, analysis\n\n    Use cases:\n    - Measure image-text alignment\n    - Verify prompt accuracy\n    - Quality control for generated images\n    - Rank images by text relevance\n    - Evaluate caption accuracy",
      "namespace": "fal.vision",
      "node_type": "fal.vision.ArbiterImageText",
      "properties": [
        {
          "name": "measurements",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Measurements",
          "description": "The measurements to use for the measurement."
        },
        {
          "name": "inputs",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Inputs",
          "description": "The inputs to use for the measurement."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "text"
      ]
    },
    {
      "title": "Florence 2 Caption",
      "description": "Florence-2 Large generates concise, accurate captions for images.\n    vision, captioning, description, florence, analysis\n\n    Use cases:\n    - Generate image captions\n    - Create alt text for images\n    - Describe images concisely\n    - Automate image descriptions\n    - Produce accessibility captions",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Florence2Caption",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Florence 2 Detailed Caption",
      "description": "Florence-2 Large generates detailed captions with rich contextual information.\n    vision, captioning, detailed-description, florence, analysis\n\n    Use cases:\n    - Generate detailed captions\n    - Create rich image descriptions\n    - Produce comprehensive captions\n    - Analyze image context\n    - Generate informative descriptions",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Florence2DetailedCaption",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Florence 2 More Detailed Caption",
      "description": "Florence-2 Large generates highly detailed, comprehensive image captions.\n    vision, captioning, detailed-description, florence, analysis\n\n    Use cases:\n    - Generate detailed image descriptions\n    - Create comprehensive captions\n    - Produce rich image narratives\n    - Analyze image content thoroughly\n    - Generate long-form descriptions",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Florence2MoreDetailedCaption",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Florence 2OCR",
      "description": "Florence-2 Large performs optical character recognition to extract text from images.\n    vision, ocr, text-extraction, florence, reading\n\n    Use cases:\n    - Extract text from images\n    - Read document images\n    - Digitize printed text\n    - Parse image text content\n    - Convert images to text",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Florence2OCR",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Florence 2 Region To Category",
      "description": "Florence-2 Large classifies image regions into semantic categories.\n    vision, classification, region-analysis, florence, categorization\n\n    Use cases:\n    - Classify image regions\n    - Categorize image areas\n    - Label image segments\n    - Identify region types\n    - Semantic region analysis",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Florence2RegionToCategory",
      "properties": [
        {
          "name": "region",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Region",
          "description": "The user input coordinates"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "region"
      ]
    },
    {
      "title": "Florence 2 Region To Description",
      "description": "Florence-2 Large generates detailed descriptions of specific image regions.\n    vision, captioning, region-description, florence, ocr\n\n    Use cases:\n    - Describe specific image regions\n    - Generate region captions\n    - Extract region information\n    - Annotate image areas\n    - Create detailed region descriptions",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Florence2RegionToDescription",
      "properties": [
        {
          "name": "region",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Region",
          "description": "The user input coordinates"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to be processed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image",
        "region"
      ]
    },
    {
      "title": "Got Ocr V2",
      "description": "GOT-OCR2 works on a wide range of tasks, including plain document OCR, scene text OCR, formatted document OCR, and even OCR for tables, charts, mathematical formulas, geometric shapes, molecular formulas and sheet music.\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Image analysis and understanding\n    - Object detection and recognition\n    - Visual content moderation\n    - Automated image captioning\n    - Scene understanding",
      "namespace": "fal.vision",
      "node_type": "fal.vision.GotOcrV2",
      "properties": [
        {
          "name": "do_format",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Do Format",
          "description": "Generate the output in formatted mode."
        },
        {
          "name": "multi_page",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Multi Page",
          "description": "Use provided images to generate a single output."
        },
        {
          "name": "input_image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Input Image Urls",
          "description": "URL of images."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Imageutils Nsfw",
      "description": "Predict the probability of an image being NSFW.\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Image analysis and understanding\n    - Object detection and recognition\n    - Visual content moderation\n    - Automated image captioning\n    - Scene understanding",
      "namespace": "fal.vision",
      "node_type": "fal.vision.ImageutilsNsfw",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Input image url."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Llava Next",
      "description": "Vision\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Image analysis and understanding\n    - Object detection and recognition\n    - Visual content moderation\n    - Automated image captioning\n    - Scene understanding",
      "namespace": "fal.vision",
      "node_type": "fal.vision.LlavaNext",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to be used for the image"
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Top P",
          "description": "Top P for sampling"
        },
        {
          "name": "max_tokens",
          "type": {
            "type": "int"
          },
          "default": 64,
          "title": "Max Tokens",
          "description": "Maximum number of tokens to generate"
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.2,
          "title": "Temperature",
          "description": "Temperature for sampling"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be processed"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Moondream 2",
      "description": "Moondream2 is a highly efficient open-source vision language model that combines powerful image understanding capabilities with a remarkably small footprint. \n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Image analysis and understanding\n    - Object detection and recognition\n    - Visual content moderation\n    - Automated image captioning\n    - Scene understanding",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Moondream2",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be processed"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Moondream 2 Object Detection",
      "description": "Moondream2 is a highly efficient open-source vision language model that combines powerful image understanding capabilities with a remarkably small footprint.\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Image analysis and understanding\n    - Object detection and recognition\n    - Visual content moderation\n    - Automated image captioning\n    - Scene understanding",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Moondream2ObjectDetection",
      "properties": [
        {
          "name": "object",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Object",
          "description": "Object to be detected in the image"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be processed"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Moondream 2 Point Object Detection",
      "description": "Moondream2 is a highly efficient open-source vision language model that combines powerful image understanding capabilities with a remarkably small footprint.\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Image analysis and understanding\n    - Object detection and recognition\n    - Visual content moderation\n    - Automated image captioning\n    - Scene understanding",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Moondream2PointObjectDetection",
      "properties": [
        {
          "name": "object",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Object",
          "description": "Object to be detected in the image"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be processed"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Moondream 2 Visual Query",
      "description": "Moondream2 is a highly efficient open-source vision language model that combines powerful image understanding capabilities with a remarkably small footprint.\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Image analysis and understanding\n    - Object detection and recognition\n    - Visual content moderation\n    - Automated image captioning\n    - Scene understanding",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Moondream2VisualQuery",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Query to be asked in the image"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be processed"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Moondream 3 Preview Caption",
      "description": "Moondream3 Preview [Caption]\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Moondream3PreviewCaption",
      "properties": [
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Top P",
          "description": "Nucleus sampling probability mass to use, between 0 and 1."
        },
        {
          "name": "length",
          "type": {
            "type": "enum",
            "values": [
              "short",
              "normal",
              "long"
            ],
            "type_name": "nodetool.nodes.fal.vision.Moondream3PreviewCaption.Length"
          },
          "default": "normal",
          "title": "Length",
          "description": "Length of the caption to generate"
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Temperature",
          "description": "Sampling temperature to use, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If not set, defaults to 0."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be processed Max width: 7000px, Max height: 7000px, Timeout: 20.0s"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Moondream 3 Preview Detect",
      "description": "Moondream3 Preview [Detect]\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Moondream3PreviewDetect",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Object to be detected in the image"
        },
        {
          "name": "preview",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preview",
          "description": "Whether to preview the output"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be processed Max width: 7000px, Max height: 7000px, Timeout: 20.0s"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Moondream 3 Preview Point",
      "description": "Moondream3 Preview [Point]\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Moondream3PreviewPoint",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Object to be located in the image"
        },
        {
          "name": "preview",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Preview",
          "description": "Whether to preview the output"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be processed Max width: 7000px, Max height: 7000px, Timeout: 20.0s"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Moondream 3 Preview Query",
      "description": "Moondream 3 Preview [Query]\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Moondream3PreviewQuery",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Query to be asked in the image"
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Top P",
          "description": "Nucleus sampling probability mass to use, between 0 and 1."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.0,
          "title": "Temperature",
          "description": "Sampling temperature to use, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If not set, defaults to 0."
        },
        {
          "name": "reasoning",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Reasoning",
          "description": "Whether to include detailed reasoning behind the answer"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to be processed Max width: 7000px, Max height: 7000px, Timeout: 20.0s"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Moondream Batched",
      "description": "Answer questions from the images.\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Image analysis and understanding\n    - Object detection and recognition\n    - Visual content moderation\n    - Automated image captioning\n    - Scene understanding",
      "namespace": "fal.vision",
      "node_type": "fal.vision.MoondreamBatched",
      "properties": [
        {
          "name": "model_id",
          "type": {
            "type": "enum",
            "values": [
              "vikhyatk/moondream2",
              "fal-ai/moondream2-docci"
            ],
            "type_name": "nodetool.nodes.fal.vision.MoondreamBatched.ModelId"
          },
          "default": "vikhyatk/moondream2",
          "title": "Model Id",
          "description": "Model ID to use for inference"
        },
        {
          "name": "repetition_penalty",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Repetition Penalty",
          "description": "Repetition penalty for sampling"
        },
        {
          "name": "inputs",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Inputs",
          "description": "List of input prompts and image URLs"
        },
        {
          "name": "max_tokens",
          "type": {
            "type": "int"
          },
          "default": 64,
          "title": "Max Tokens",
          "description": "Maximum number of new tokens to generate"
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 0.2,
          "title": "Temperature",
          "description": "Temperature for sampling"
        },
        {
          "name": "top_p",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Top P",
          "description": "Top P for sampling"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Moondream Next",
      "description": "MoonDreamNext is a multimodal vision-language model for captioning, gaze detection, bbox detection, point detection, and more.\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Image analysis and understanding\n    - Object detection and recognition\n    - Visual content moderation\n    - Automated image captioning\n    - Scene understanding",
      "namespace": "fal.vision",
      "node_type": "fal.vision.MoondreamNext",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt for query task"
        },
        {
          "name": "task_type",
          "type": {
            "type": "enum",
            "values": [
              "caption",
              "query"
            ],
            "type_name": "nodetool.nodes.fal.vision.MoondreamNext.TaskType"
          },
          "default": "caption",
          "title": "Task Type",
          "description": "Type of task to perform"
        },
        {
          "name": "max_tokens",
          "type": {
            "type": "int"
          },
          "default": 64,
          "title": "Max Tokens",
          "description": "Maximum number of tokens to generate"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL to be processed"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Moondream Next Batch",
      "description": "MoonDreamNext Batch is a multimodal vision-language model for batch captioning.\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Image analysis and understanding\n    - Object detection and recognition\n    - Visual content moderation\n    - Automated image captioning\n    - Scene understanding",
      "namespace": "fal.vision",
      "node_type": "fal.vision.MoondreamNextBatch",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Single prompt to apply to all images"
        },
        {
          "name": "images_data_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Images Data Url",
          "description": "List of image URLs to be processed (maximum 32 images)"
        },
        {
          "name": "max_tokens",
          "type": {
            "type": "int"
          },
          "default": 64,
          "title": "Max Tokens",
          "description": "Maximum number of tokens to generate"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Openrouter Router Vision",
      "description": "OpenRouter [Vision]\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.vision",
      "node_type": "fal.vision.OpenrouterRouterVision",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to be used for the image"
        },
        {
          "name": "reasoning",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Reasoning",
          "description": "Should reasoning be the part of the final answer."
        },
        {
          "name": "system_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "System Prompt",
          "description": "System prompt to provide context or instructions to the model"
        },
        {
          "name": "model",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Model",
          "description": "Name of the model to use. Charged based on actual token usage."
        },
        {
          "name": "max_tokens",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Max Tokens",
          "description": "This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length."
        },
        {
          "name": "temperature",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Temperature",
          "description": "This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input."
        },
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of image URLs to be processed"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Perceptron Isaac 01",
      "description": "Isaac-01 is a multimodal vision-language model from Perceptron for various vision language tasks.\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Image analysis and understanding\n    - Object detection and recognition\n    - Visual content moderation\n    - Automated image captioning\n    - Scene understanding",
      "namespace": "fal.vision",
      "node_type": "fal.vision.PerceptronIsaac01",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to be used for the image"
        },
        {
          "name": "response_style",
          "type": {
            "type": "enum",
            "values": [
              "text",
              "box",
              "point",
              "polygon"
            ],
            "type_name": "nodetool.nodes.fal.vision.PerceptronIsaac01.ResponseStyle"
          },
          "default": "text",
          "title": "Response Style",
          "description": "Response style to be used for the image. - text: Model will output text. Good for descriptions and captioning. - box: Model will output a combination of text and bounding boxes. Good for localization. - point: Model will output a combination of text and points. Good for counting many objects. - polygon: Model will output a combination of text and polygons. Good for granular segmentation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Image URL to be processed"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Perceptron Isaac 01 Openai V1 Chat Completions",
      "description": "Isaac 0.1 [OpenAI Compatible Endpoint]\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.vision",
      "node_type": "fal.vision.PerceptronIsaac01OpenaiV1ChatCompletions",
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Sa 2 va 4 b Image",
      "description": "Sa2VA is an MLLM capable of question answering, visual prompt understanding, and dense object segmentation at both image and video levels\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Image analysis and understanding\n    - Object detection and recognition\n    - Visual content moderation\n    - Automated image captioning\n    - Scene understanding",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Sa2va4bImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to be used for the chat completion"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Url for the Input image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Sa 2 va 4 b Video",
      "description": "Sa2VA is an MLLM capable of question answering, visual prompt understanding, and dense object segmentation at both image and video levels\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Image analysis and understanding\n    - Object detection and recognition\n    - Visual content moderation\n    - Automated image captioning\n    - Scene understanding",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Sa2va4bVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to be used for the chat completion"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the input video."
        },
        {
          "name": "num_frames_to_sample",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Frames To Sample",
          "description": "Number of frames to sample from the video. If not provided, all frames are sampled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Sa 2 va 8 b Image",
      "description": "Sa2VA is an MLLM capable of question answering, visual prompt understanding, and dense object segmentation at both image and video levels\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Image analysis and understanding\n    - Object detection and recognition\n    - Visual content moderation\n    - Automated image captioning\n    - Scene understanding",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Sa2va8bImage",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to be used for the chat completion"
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Url for the Input image."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Sa 2 va 8 b Video",
      "description": "Sa2VA is an MLLM capable of question answering, visual prompt understanding, and dense object segmentation at both image and video levels\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Image analysis and understanding\n    - Object detection and recognition\n    - Visual content moderation\n    - Automated image captioning\n    - Scene understanding",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Sa2va8bVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Prompt to be used for the chat completion"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The URL of the input video."
        },
        {
          "name": "num_frames_to_sample",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Num Frames To Sample",
          "description": "Number of frames to sample from the video. If not provided, all frames are sampled."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Sam 3 Image Embed",
      "description": "Sam 3\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.vision",
      "node_type": "fal.vision.Sam3ImageEmbed",
      "properties": [
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the image to embed."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Video Understanding",
      "description": "A video understanding model to analyze video content and answer questions about what's happening in the video based on user prompts.\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Image analysis and understanding\n    - Object detection and recognition\n    - Visual content moderation\n    - Automated image captioning\n    - Scene understanding",
      "namespace": "fal.vision",
      "node_type": "fal.vision.VideoUnderstanding",
      "properties": [
        {
          "name": "detailed_analysis",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Detailed Analysis",
          "description": "Whether to request a more detailed analysis of the video"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video to analyze"
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The question or prompt about the video content."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "XAilab Nsfw",
      "description": "Predict whether an image is NSFW or SFW.\n    vision, analysis, image-understanding, detection\n\n    Use cases:\n    - Image analysis and understanding\n    - Object detection and recognition\n    - Visual content moderation\n    - Automated image captioning\n    - Scene understanding",
      "namespace": "fal.vision",
      "node_type": "fal.vision.XAilabNsfw",
      "properties": [
        {
          "name": "image_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Image Urls",
          "description": "List of image URLs to check. If more than 10 images are provided, only the first 10 will be checked."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "image"
      ]
    },
    {
      "title": "Argil Avatars Audio To Video",
      "description": "High-quality avatar videos that feel real, generated from your audio\n    video, generation, audio-to-video, visualization\n\n    Use cases:\n    - Audio-driven video generation\n    - Music visualization\n    - Talking head animation\n    - Audio-synced content creation\n    - Podcast video generation",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.ArgilAvatarsAudioToVideo",
      "properties": [
        {
          "name": "avatar",
          "type": {
            "type": "enum",
            "values": [
              "Mia outdoor (UGC)",
              "Lara (Masterclass)",
              "Ines (UGC)",
              "Maria (Masterclass)",
              "Emma (UGC)",
              "Sienna (Masterclass)",
              "Elena (UGC)",
              "Jasmine (Masterclass)",
              "Amara (Masterclass)",
              "Ryan podcast (UGC)",
              "Tyler (Masterclass)",
              "Jayse (Masterclass)",
              "Paul (Masterclass)",
              "Matteo (UGC)",
              "Daniel car (UGC)",
              "Dario (Masterclass)",
              "Viva (Masterclass)",
              "Chen (Masterclass)",
              "Alex (Masterclass)",
              "Vanessa (UGC)",
              "Laurent (UGC)",
              "Noemie car (UGC)",
              "Brandon (UGC)",
              "Byron (Masterclass)",
              "Calista (Masterclass)",
              "Milo (Masterclass)",
              "Fabien (Masterclass)",
              "Rose (UGC)"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.ArgilAvatarsAudioToVideo.Avatar"
          },
          "default": "",
          "title": "Avatar"
        },
        {
          "name": "remove_background",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Remove Background",
          "description": "Enabling the remove background feature will result in a 50% increase in the price."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "avatar",
        "remove_background",
        "audio_url"
      ]
    },
    {
      "title": "Echomimic V3",
      "description": "EchoMimic V3 generates a talking avatar model from a picture, audio and text prompt.\n    video, generation, audio-to-video, visualization\n\n    Use cases:\n    - Audio-driven video generation\n    - Music visualization\n    - Talking head animation\n    - Audio-synced content creation\n    - Podcast video generation",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.EchomimicV3",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio to use as a reference for the video generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a reference for the video generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4.5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the video generation."
        },
        {
          "name": "audio_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 2.5,
          "title": "Audio Guidance Scale",
          "description": "The audio guidance scale to use for the video generation."
        },
        {
          "name": "num_frames_per_generation",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames Per Generation",
          "description": "The number of frames to generate at once."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "The negative prompt to use for the video generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "audio_url",
        "image_url",
        "guidance_scale",
        "audio_guidance_scale"
      ]
    },
    {
      "title": "Elevenlabs Dubbing",
      "description": "ElevenLabs Dubbing\n    video, generation, audio-to-video, visualization\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.ElevenlabsDubbing",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video file to dub. Either audio_url or video_url must be provided. If both are provided, video_url takes priority."
        },
        {
          "name": "highest_resolution",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Highest Resolution",
          "description": "Whether to use the highest resolution for dubbing."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to dub. Either audio_url or video_url must be provided."
        },
        {
          "name": "target_lang",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Target Lang",
          "description": "Target language code for dubbing (ISO 639-1)"
        },
        {
          "name": "num_speakers",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Num Speakers",
          "description": "Number of speakers in the audio. If not provided, will be auto-detected."
        },
        {
          "name": "source_lang",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Source Lang",
          "description": "Source language code. If not provided, will be auto-detected."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video_url",
        "highest_resolution",
        "audio_url",
        "target_lang",
        "num_speakers"
      ]
    },
    {
      "title": "Longcat Multi Avatar Image Audio To Video",
      "description": "Longcat Multi Avatar\n    video, generation, audio-to-video, visualization\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.LongcatMultiAvatarImageAudioToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "Two people are having a conversation with natural expressions and movements.",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "audio_url_person2",
          "type": {
            "type": "str"
          },
          "default": "https://raw.githubusercontent.com/meituan-longcat/LongCat-Video/refs/heads/main/assets/avatar/multi/sing_woman.WAV",
          "title": "Audio Url Person2",
          "description": "The URL of the audio file for person 2 (right side)."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "bbox_person1",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Bbox Person1",
          "description": "Bounding box for person 1. If not provided, defaults to left half of image."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Close-up, Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "The negative prompt to avoid in the video generation."
        },
        {
          "name": "text_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Text Guidance Scale",
          "description": "The text guidance scale for classifier-free guidance."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.LongcatMultiAvatarImageAudioToVideo.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p or 720p). Billing is per video-second (16 frames): 480p is 1 unit per second and 720p is 4 units per second."
        },
        {
          "name": "audio_type",
          "type": {
            "type": "enum",
            "values": [
              "para",
              "add"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.LongcatMultiAvatarImageAudioToVideo.AudioType"
          },
          "default": "para",
          "title": "Audio Type",
          "description": "How to combine the two audio tracks. 'para' (parallel) plays both simultaneously, 'add' (sequential) plays person 1 first then person 2."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image containing two speakers."
        },
        {
          "name": "audio_url_person1",
          "type": {
            "type": "str"
          },
          "default": "https://raw.githubusercontent.com/meituan-longcat/LongCat-Video/refs/heads/main/assets/avatar/multi/sing_man.WAV",
          "title": "Audio Url Person1",
          "description": "The URL of the audio file for person 1 (left side)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "audio_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Audio Guidance Scale",
          "description": "The audio guidance scale. Higher values may lead to exaggerated mouth movements."
        },
        {
          "name": "bbox_person2",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Bbox Person2",
          "description": "Bounding box for person 2. If not provided, defaults to right half of image."
        },
        {
          "name": "num_segments",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Segments",
          "description": "Number of video segments to generate. Each segment adds ~5 seconds of video. First segment is ~5.8s, additional segments are 5s each."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "num_inference_steps",
        "audio_url_person2",
        "enable_safety_checker",
        "bbox_person1"
      ]
    },
    {
      "title": "Longcat Single Avatar Audio To Video",
      "description": "LongCat-Video-Avatar is an audio-driven video generation model that can generates super-realistic, lip-synchronized long video generation with natural dynamics and consistent identity.\n    video, generation, audio-to-video, visualization\n\n    Use cases:\n    - Audio-driven video generation\n    - Music visualization\n    - Talking head animation\n    - Audio-synced content creation\n    - Podcast video generation",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.LongcatSingleAvatarAudioToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "A person is talking naturally with natural expressions and movements.",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.LongcatSingleAvatarAudioToVideo.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p or 720p). Billing is per video-second (16 frames): 480p is 1 unit per second and 720p is 4 units per second."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "audio_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Audio Guidance Scale",
          "description": "The audio guidance scale. Higher values may lead to exaggerated mouth movements."
        },
        {
          "name": "num_segments",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Segments",
          "description": "Number of video segments to generate. Each segment adds ~5 seconds of video. First segment is ~5.8s, additional segments are 5s each."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file to drive the avatar."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Close-up, Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "The negative prompt to avoid in the video generation."
        },
        {
          "name": "text_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Text Guidance Scale",
          "description": "The text guidance scale for classifier-free guidance."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "resolution",
        "enable_safety_checker",
        "audio_guidance_scale",
        "num_segments"
      ]
    },
    {
      "title": "Longcat Single Avatar Image Audio To Video",
      "description": "LongCat-Video-Avatar is an audio-driven video generation model that can generates super-realistic, lip-synchronized long video generation with natural dynamics and consistent identity.\n    video, generation, audio-to-video, visualization\n\n    Use cases:\n    - Audio-driven video generation\n    - Music visualization\n    - Talking head animation\n    - Audio-synced content creation\n    - Podcast video generation",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.LongcatSingleAvatarImageAudioToVideo",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to guide the video generation."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.LongcatSingleAvatarImageAudioToVideo.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p or 720p). Billing is per video-second (16 frames): 480p is 1 unit per second and 720p is 4 units per second."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable safety checker."
        },
        {
          "name": "audio_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Audio Guidance Scale",
          "description": "The audio guidance scale. Higher values may lead to exaggerated mouth movements."
        },
        {
          "name": "num_segments",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Segments",
          "description": "Number of video segments to generate. Each segment adds ~5 seconds of video. First segment is ~5.8s, additional segments are 5s each."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to animate."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file to drive the avatar."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed for the random number generator."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "Close-up, Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
          "title": "Negative Prompt",
          "description": "The negative prompt to avoid in the video generation."
        },
        {
          "name": "text_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Text Guidance Scale",
          "description": "The text guidance scale for classifier-free guidance."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "resolution",
        "enable_safety_checker",
        "audio_guidance_scale",
        "num_segments"
      ]
    },
    {
      "title": "Ltx 219B Audio To Video",
      "description": "LTX-2 19B\n    video, generation, audio-to-video, visualization\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.Ltx219BAudioToVideo",
      "properties": [
        {
          "name": "match_audio_length",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Audio Length",
          "description": "When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Video Size",
          "description": "The size of the generated video. Use 'auto' to match the input image dimensions if provided."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "preprocess_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preprocess Audio",
          "description": "Whether to preprocess the audio before using it as conditioning."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of an image to use as the first frame of the video."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio to generate the video from."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "match_audio_length",
        "prompt",
        "acceleration",
        "use_multiscale",
        "num_inference_steps"
      ]
    },
    {
      "title": "Ltx 219B Audio To Video Lora",
      "description": "LTX-2 19B\n    video, generation, audio-to-video, visualization, lora\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.Ltx219BAudioToVideoLora",
      "properties": [
        {
          "name": "match_audio_length",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Audio Length",
          "description": "When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.Acceleration"
          },
          "default": "regular",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 40,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Video Size",
          "description": "The size of the generated video. Use 'auto' to match the input image dimensions if provided."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Guidance Scale",
          "description": "The guidance scale to use."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "preprocess_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preprocess Audio",
          "description": "Whether to preprocess the audio before using it as conditioning."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of an image to use as the first frame of the video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio to generate the video from."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "match_audio_length",
        "prompt",
        "acceleration",
        "use_multiscale",
        "num_inference_steps"
      ]
    },
    {
      "title": "Ltx 219B Distilled Audio To Video",
      "description": "LTX-2 19B Distilled\n    video, generation, audio-to-video, visualization\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.Ltx219BDistilledAudioToVideo",
      "properties": [
        {
          "name": "match_audio_length",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Audio Length",
          "description": "When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Video Size",
          "description": "The size of the generated video. Use 'auto' to match the input image dimensions if provided."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "preprocess_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preprocess Audio",
          "description": "Whether to preprocess the audio before using it as conditioning."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of an image to use as the first frame of the video."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio to generate the video from."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "match_audio_length",
        "prompt",
        "acceleration",
        "use_multiscale",
        "fps"
      ]
    },
    {
      "title": "Ltx 219B Distilled Audio To Video Lora",
      "description": "LTX-2 19B Distilled\n    video, generation, audio-to-video, visualization, lora\n\n    Use cases:\n    - Automated content generation\n    - Creative workflows\n    - Batch processing\n    - Professional applications\n    - Rapid prototyping",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.Ltx219BDistilledAudioToVideoLora",
      "properties": [
        {
          "name": "match_audio_length",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Match Audio Length",
          "description": "When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to generate the video from."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "none",
              "regular",
              "high",
              "full"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.Acceleration"
          },
          "default": "none",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "use_multiscale",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Use Multiscale",
          "description": "Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details."
        },
        {
          "name": "fps",
          "type": {
            "type": "float"
          },
          "default": 25,
          "title": "Fps",
          "description": "The frames per second of the generated video."
        },
        {
          "name": "loras",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Loras",
          "description": "The LoRAs to use for the generation."
        },
        {
          "name": "camera_lora",
          "type": {
            "type": "enum",
            "values": [
              "dolly_in",
              "dolly_out",
              "dolly_left",
              "dolly_right",
              "jib_up",
              "jib_down",
              "static",
              "none"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.CameraLora"
          },
          "default": "none",
          "title": "Camera Lora",
          "description": "The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "video_size",
          "type": {
            "type": "str"
          },
          "default": "landscape_4_3",
          "title": "Video Size",
          "description": "The size of the generated video. Use 'auto' to match the input image dimensions if provided."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Safety Checker",
          "description": "Whether to enable the safety checker."
        },
        {
          "name": "camera_lora_scale",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Camera Lora Scale",
          "description": "The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera."
        },
        {
          "name": "image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Image Strength",
          "description": "The strength of the image to use for the video generation."
        },
        {
          "name": "preprocess_audio",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Preprocess Audio",
          "description": "Whether to preprocess the audio before using it as conditioning."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
          "title": "Negative Prompt",
          "description": "The negative prompt to generate the video from."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the generated video."
        },
        {
          "name": "video_output_type",
          "type": {
            "type": "enum",
            "values": [
              "X264 (.mp4)",
              "VP9 (.webm)",
              "PRORES4444 (.mov)",
              "GIF (.gif)"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.VideoOutputType"
          },
          "default": "X264 (.mp4)",
          "title": "Video Output Type",
          "description": "The output type of the generated video."
        },
        {
          "name": "end_image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "End Image Url",
          "description": "The URL of the image to use as the end of the video."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 121,
          "title": "Num Frames",
          "description": "The number of frames to generate."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "Optional URL of an image to use as the first frame of the video."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the generated video."
        },
        {
          "name": "enable_prompt_expansion",
          "type": {
            "type": "bool"
          },
          "default": true,
          "title": "Enable Prompt Expansion",
          "description": "Whether to enable prompt expansion."
        },
        {
          "name": "audio_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "Audio Strength",
          "description": "Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification."
        },
        {
          "name": "end_image_strength",
          "type": {
            "type": "float"
          },
          "default": 1,
          "title": "End Image Strength",
          "description": "The strength of the end image to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio to generate the video from."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed for the random number generator."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "match_audio_length",
        "prompt",
        "acceleration",
        "use_multiscale",
        "fps"
      ]
    },
    {
      "title": "Stable Avatar",
      "description": "Stable Avatar generates audio-driven video avatars up to five minutes long\n    video, generation, audio-to-video, visualization\n\n    Use cases:\n    - Audio-driven video generation\n    - Music visualization\n    - Talking head animation\n    - Audio-synced content creation\n    - Podcast video generation",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.StableAvatar",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to use for the video generation."
        },
        {
          "name": "aspect_ratio",
          "type": {
            "type": "enum",
            "values": [
              "16:9",
              "1:1",
              "9:16",
              "auto"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.StableAvatar.AspectRatio"
          },
          "default": "auto",
          "title": "Aspect Ratio",
          "description": "The aspect ratio of the video to generate. If 'auto', the aspect ratio will be determined by the reference image."
        },
        {
          "name": "perturbation",
          "type": {
            "type": "float"
          },
          "default": 0.1,
          "title": "Perturbation",
          "description": "The amount of perturbation to use for the video generation. 0.0 means no perturbation, 1.0 means full perturbation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "The URL of the image to use as a reference for the video generation."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Guidance Scale",
          "description": "The guidance scale to use for the video generation."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "The seed to use for the video generation."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 50,
          "title": "Num Inference Steps",
          "description": "The number of inference steps to use for the video generation."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio to use as a reference for the video generation."
        },
        {
          "name": "audio_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 4,
          "title": "Audio Guidance Scale",
          "description": "The audio guidance scale to use for the video generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "aspect_ratio",
        "perturbation",
        "image_url",
        "guidance_scale"
      ]
    },
    {
      "title": "Veed Avatars Audio To Video",
      "description": "Generate high-quality videos with UGC-like avatars from audio\n    video, generation, audio-to-video, visualization\n\n    Use cases:\n    - Audio-driven video generation\n    - Music visualization\n    - Talking head animation\n    - Audio-synced content creation\n    - Podcast video generation",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.VeedAvatarsAudioToVideo",
      "properties": [
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url"
        },
        {
          "name": "avatar_id",
          "type": {
            "type": "enum",
            "values": [
              "emily_vertical_primary",
              "emily_vertical_secondary",
              "marcus_vertical_primary",
              "marcus_vertical_secondary",
              "mira_vertical_primary",
              "mira_vertical_secondary",
              "jasmine_vertical_primary",
              "jasmine_vertical_secondary",
              "jasmine_vertical_walking",
              "aisha_vertical_walking",
              "elena_vertical_primary",
              "elena_vertical_secondary",
              "any_male_vertical_primary",
              "any_female_vertical_primary",
              "any_male_vertical_secondary",
              "any_female_vertical_secondary",
              "any_female_vertical_walking",
              "emily_primary",
              "emily_side",
              "marcus_primary",
              "marcus_side",
              "aisha_walking",
              "elena_primary",
              "elena_side",
              "any_male_primary",
              "any_female_primary",
              "any_male_side",
              "any_female_side"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.VeedAvatarsAudioToVideo.AvatarId"
          },
          "default": "",
          "title": "Avatar Id",
          "description": "The avatar to use for the video"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio_url",
        "avatar_id"
      ]
    },
    {
      "title": "Wan V2214 b Speech To Video",
      "description": "Wan-S2V is a video model that generates high-quality videos from static images and audio, with realistic facial expressions, body movements, and professional camera work for film and television applications\n    video, generation, audio-to-video, visualization\n\n    Use cases:\n    - Audio-driven video generation\n    - Music visualization\n    - Talking head animation\n    - Audio-synced content creation\n    - Podcast video generation",
      "namespace": "fal.audio_to_video",
      "node_type": "fal.audio_to_video.WanV2214bSpeechToVideo",
      "properties": [
        {
          "name": "shift",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Shift",
          "description": "Shift value for the video. Must be between 1.0 and 10.0."
        },
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The text prompt used for video generation."
        },
        {
          "name": "frames_per_second",
          "type": {
            "type": "int"
          },
          "default": 16,
          "title": "Frames Per Second",
          "description": "Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is."
        },
        {
          "name": "enable_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Safety Checker",
          "description": "If set to true, input data will be checked for safety before processing."
        },
        {
          "name": "num_frames",
          "type": {
            "type": "int"
          },
          "default": 80,
          "title": "Num Frames",
          "description": "Number of frames to generate. Must be between 40 to 120, (must be multiple of 4)."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3.5,
          "title": "Guidance Scale",
          "description": "Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality."
        },
        {
          "name": "negative_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Negative Prompt",
          "description": "Negative prompt for video generation."
        },
        {
          "name": "video_write_mode",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "small"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.WanV2214bSpeechToVideo.VideoWriteMode"
          },
          "default": "balanced",
          "title": "Video Write Mode",
          "description": "The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size."
        },
        {
          "name": "resolution",
          "type": {
            "type": "enum",
            "values": [
              "480p",
              "580p",
              "720p"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.WanV2214bSpeechToVideo.Resolution"
          },
          "default": "480p",
          "title": "Resolution",
          "description": "Resolution of the generated video (480p, 580p, or 720p)."
        },
        {
          "name": "enable_output_safety_checker",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Enable Output Safety Checker",
          "description": "If set to true, output video will be checked for safety after generation."
        },
        {
          "name": "image_url",
          "type": {
            "type": "image"
          },
          "default": {
            "type": "image",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Image Url",
          "description": "URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped."
        },
        {
          "name": "video_quality",
          "type": {
            "type": "enum",
            "values": [
              "low",
              "medium",
              "high",
              "maximum"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_video.WanV2214bSpeechToVideo.VideoQuality"
          },
          "default": "high",
          "title": "Video Quality",
          "description": "The quality of the output video. Higher quality means better visual quality but larger file size."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Num Inference Steps",
          "description": "Number of inference steps for sampling. Higher values give better quality but take longer."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If None, a random seed is chosen."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "shift",
        "prompt",
        "frames_per_second",
        "enable_safety_checker",
        "num_frames"
      ]
    },
    {
      "title": "Ace Step Audio Inpaint",
      "description": "Modify a portion of provided audio with lyrics and/or style using ACE-Step\n    audio, processing, audio-to-audio, transformation\n\n    Use cases:\n    - Audio enhancement and processing\n    - Voice transformation\n    - Audio style transfer\n    - Sound quality improvement\n    - Audio effect application",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.AceStepAudioInpaint",
      "properties": [
        {
          "name": "number_of_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Number Of Steps",
          "description": "Number of steps to generate the audio."
        },
        {
          "name": "start_time",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Start Time",
          "description": "start time in seconds for the inpainting process."
        },
        {
          "name": "tags",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Tags",
          "description": "Comma-separated list of genre tags to control the style of the generated audio."
        },
        {
          "name": "minimum_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Minimum Guidance Scale",
          "description": "Minimum guidance scale for the generation after the decay."
        },
        {
          "name": "lyrics",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Lyrics",
          "description": "Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song."
        },
        {
          "name": "end_time_relative_to",
          "type": {
            "type": "enum",
            "values": [
              "start",
              "end"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.AceStepAudioInpaint.EndTimeRelativeTo"
          },
          "default": "start",
          "title": "End Time Relative To",
          "description": "Whether the end time is relative to the start or end of the audio."
        },
        {
          "name": "tag_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Tag Guidance Scale",
          "description": "Tag guidance scale for the generation."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "euler",
              "heun"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.AceStepAudioInpaint.Scheduler"
          },
          "default": "euler",
          "title": "Scheduler",
          "description": "Scheduler to use for the generation process."
        },
        {
          "name": "end_time",
          "type": {
            "type": "float"
          },
          "default": 30,
          "title": "End Time",
          "description": "end time in seconds for the inpainting process."
        },
        {
          "name": "guidance_type",
          "type": {
            "type": "enum",
            "values": [
              "cfg",
              "apg",
              "cfg_star"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.AceStepAudioInpaint.GuidanceType"
          },
          "default": "apg",
          "title": "Guidance Type",
          "description": "Type of CFG to use for the generation process."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Guidance Scale",
          "description": "Guidance scale for the generation."
        },
        {
          "name": "lyric_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Lyric Guidance Scale",
          "description": "Lyric guidance scale for the generation."
        },
        {
          "name": "guidance_interval",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Guidance Interval",
          "description": "Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)"
        },
        {
          "name": "variance",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Variance",
          "description": "Variance for the inpainting process. Higher values can lead to more diverse results."
        },
        {
          "name": "guidance_interval_decay",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Interval Decay",
          "description": "Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay."
        },
        {
          "name": "start_time_relative_to",
          "type": {
            "type": "enum",
            "values": [
              "start",
              "end"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.AceStepAudioInpaint.StartTimeRelativeTo"
          },
          "default": "start",
          "title": "Start Time Relative To",
          "description": "Whether the start time is relative to the start or end of the audio."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to be inpainted."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If not provided, a random seed will be used."
        },
        {
          "name": "granularity_scale",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Granularity Scale",
          "description": "Granularity scale for the generation process. Higher values can reduce artifacts."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Ace Step Audio Outpaint",
      "description": "Extend the beginning or end of provided audio with lyrics and/or style using ACE-Step\n    audio, processing, audio-to-audio, transformation\n\n    Use cases:\n    - Audio enhancement and processing\n    - Voice transformation\n    - Audio style transfer\n    - Sound quality improvement\n    - Audio effect application",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.AceStepAudioOutpaint",
      "properties": [
        {
          "name": "number_of_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Number Of Steps",
          "description": "Number of steps to generate the audio."
        },
        {
          "name": "tags",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Tags",
          "description": "Comma-separated list of genre tags to control the style of the generated audio."
        },
        {
          "name": "minimum_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Minimum Guidance Scale",
          "description": "Minimum guidance scale for the generation after the decay."
        },
        {
          "name": "extend_after_duration",
          "type": {
            "type": "float"
          },
          "default": 30,
          "title": "Extend After Duration",
          "description": "Duration in seconds to extend the audio from the end."
        },
        {
          "name": "lyrics",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Lyrics",
          "description": "Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song."
        },
        {
          "name": "tag_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Tag Guidance Scale",
          "description": "Tag guidance scale for the generation."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "euler",
              "heun"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.AceStepAudioOutpaint.Scheduler"
          },
          "default": "euler",
          "title": "Scheduler",
          "description": "Scheduler to use for the generation process."
        },
        {
          "name": "extend_before_duration",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Extend Before Duration",
          "description": "Duration in seconds to extend the audio from the start."
        },
        {
          "name": "guidance_type",
          "type": {
            "type": "enum",
            "values": [
              "cfg",
              "apg",
              "cfg_star"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.AceStepAudioOutpaint.GuidanceType"
          },
          "default": "apg",
          "title": "Guidance Type",
          "description": "Type of CFG to use for the generation process."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Guidance Scale",
          "description": "Guidance scale for the generation."
        },
        {
          "name": "lyric_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Lyric Guidance Scale",
          "description": "Lyric guidance scale for the generation."
        },
        {
          "name": "guidance_interval",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Guidance Interval",
          "description": "Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)"
        },
        {
          "name": "guidance_interval_decay",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Interval Decay",
          "description": "Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to be outpainted."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If not provided, a random seed will be used."
        },
        {
          "name": "granularity_scale",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Granularity Scale",
          "description": "Granularity scale for the generation process. Higher values can reduce artifacts."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Ace Step Audio To Audio",
      "description": "Generate music from a lyrics and example audio using ACE-Step\n    audio, processing, audio-to-audio, transformation\n\n    Use cases:\n    - Audio enhancement and processing\n    - Voice transformation\n    - Audio style transfer\n    - Sound quality improvement\n    - Audio effect application",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.AceStepAudioToAudio",
      "properties": [
        {
          "name": "number_of_steps",
          "type": {
            "type": "int"
          },
          "default": 27,
          "title": "Number Of Steps",
          "description": "Number of steps to generate the audio."
        },
        {
          "name": "tags",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Tags",
          "description": "Comma-separated list of genre tags to control the style of the generated audio."
        },
        {
          "name": "minimum_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 3,
          "title": "Minimum Guidance Scale",
          "description": "Minimum guidance scale for the generation after the decay."
        },
        {
          "name": "lyrics",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Lyrics",
          "description": "Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song."
        },
        {
          "name": "tag_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 5,
          "title": "Tag Guidance Scale",
          "description": "Tag guidance scale for the generation."
        },
        {
          "name": "original_lyrics",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Original Lyrics",
          "description": "Original lyrics of the audio file."
        },
        {
          "name": "scheduler",
          "type": {
            "type": "enum",
            "values": [
              "euler",
              "heun"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.AceStepAudioToAudio.Scheduler"
          },
          "default": "euler",
          "title": "Scheduler",
          "description": "Scheduler to use for the generation process."
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 15,
          "title": "Guidance Scale",
          "description": "Guidance scale for the generation."
        },
        {
          "name": "guidance_type",
          "type": {
            "type": "enum",
            "values": [
              "cfg",
              "apg",
              "cfg_star"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.AceStepAudioToAudio.GuidanceType"
          },
          "default": "apg",
          "title": "Guidance Type",
          "description": "Type of CFG to use for the generation process."
        },
        {
          "name": "lyric_guidance_scale",
          "type": {
            "type": "float"
          },
          "default": 1.5,
          "title": "Lyric Guidance Scale",
          "description": "Lyric guidance scale for the generation."
        },
        {
          "name": "guidance_interval",
          "type": {
            "type": "float"
          },
          "default": 0.5,
          "title": "Guidance Interval",
          "description": "Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)"
        },
        {
          "name": "edit_mode",
          "type": {
            "type": "enum",
            "values": [
              "lyrics",
              "remix"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.AceStepAudioToAudio.EditMode"
          },
          "default": "remix",
          "title": "Edit Mode",
          "description": "Whether to edit the lyrics only or remix the audio."
        },
        {
          "name": "guidance_interval_decay",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Guidance Interval Decay",
          "description": "Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to be outpainted."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility. If not provided, a random seed will be used."
        },
        {
          "name": "granularity_scale",
          "type": {
            "type": "int"
          },
          "default": 10,
          "title": "Granularity Scale",
          "description": "Granularity scale for the generation process. Higher values can reduce artifacts."
        },
        {
          "name": "original_tags",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Original Tags",
          "description": "Original tags of the audio file."
        },
        {
          "name": "original_seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Original Seed",
          "description": "Original seed of the audio file."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Audio Understanding",
      "description": "A audio understanding model to analyze audio content and answer questions about what's happening in the audio based on user prompts.\n    audio, processing, audio-to-audio, transformation\n\n    Use cases:\n    - Audio enhancement and processing\n    - Voice transformation\n    - Audio style transfer\n    - Sound quality improvement\n    - Audio effect application",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.AudioUnderstanding",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The question or prompt about the audio content."
        },
        {
          "name": "detailed_analysis",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Detailed Analysis",
          "description": "Whether to request a more detailed analysis of the audio"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to analyze"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Deepfilternet 3",
      "description": "DeepFilterNet3 removes noise and improves audio quality with advanced deep learning filtering.\n    audio, noise-reduction, filtering, cleaning, audio-to-audio\n\n    Use cases:\n    - Remove noise from audio\n    - Clean audio recordings\n    - Filter unwanted sounds\n    - Improve audio clarity\n    - Generate clean audio",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.Deepfilternet3",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "audio_format",
          "type": {
            "type": "enum",
            "values": [
              "mp3",
              "aac",
              "m4a",
              "ogg",
              "opus",
              "flac",
              "wav"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.Deepfilternet3.AudioFormat"
          },
          "default": "mp3",
          "title": "Audio Format",
          "description": "The format for the output audio."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio to enhance."
        },
        {
          "name": "bitrate",
          "type": {
            "type": "str"
          },
          "default": "192k",
          "title": "Bitrate",
          "description": "The bitrate of the output audio."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Demucs",
      "description": "Demucs separates music into vocals, drums, bass, and other instruments with high quality.\n    audio, music-separation, stems, demucs, audio-to-audio\n\n    Use cases:\n    - Separate music into stems\n    - Extract vocals from songs\n    - Isolate instruments in music\n    - Create karaoke tracks\n    - Generate individual audio stems",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.Demucs",
      "properties": [
        {
          "name": "segment_length",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Segment Length",
          "description": "Length in seconds of each segment for processing. Smaller values use less memory but may reduce quality. Default is model-specific."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "wav",
              "mp3"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.Demucs.OutputFormat"
          },
          "default": "mp3",
          "title": "Output Format",
          "description": "Output audio format for the separated stems"
        },
        {
          "name": "stems",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Stems",
          "description": "Specific stems to extract. If None, extracts all available stems. Available stems depend on model: vocals, drums, bass, other, guitar, piano (for 6s model)"
        },
        {
          "name": "overlap",
          "type": {
            "type": "float"
          },
          "default": 0.25,
          "title": "Overlap",
          "description": "Overlap between segments (0.0 to 1.0). Higher values may improve quality but increase processing time."
        },
        {
          "name": "model",
          "type": {
            "type": "enum",
            "values": [
              "htdemucs",
              "htdemucs_ft",
              "htdemucs_6s",
              "hdemucs_mmi",
              "mdx",
              "mdx_extra",
              "mdx_q",
              "mdx_extra_q"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.Demucs.Model"
          },
          "default": "htdemucs_6s",
          "title": "Model",
          "description": "Demucs model to use for separation"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to separate into stems"
        },
        {
          "name": "shifts",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Shifts",
          "description": "Number of random shifts for equivariant stabilization. Higher values improve quality but increase processing time."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Dia Tts Voice Clone",
      "description": "Clone dialog voices from a sample audio and generate dialogs from text prompts using the Dia TTS which leverages advanced AI techniques to create high-quality text-to-speech.\n    audio, processing, audio-to-audio, transformation\n\n    Use cases:\n    - Audio enhancement and processing\n    - Voice transformation\n    - Audio style transfer\n    - Sound quality improvement\n    - Audio effect application",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.DiaTtsVoiceClone",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "The text to be converted to speech."
        },
        {
          "name": "ref_text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Ref Text",
          "description": "The reference text to be used for TTS."
        },
        {
          "name": "ref_audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Ref Audio Url",
          "description": "The URL of the reference audio file."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Elevenlabs Audio Isolation",
      "description": "Isolate audio tracks using ElevenLabs advanced audio isolation technology.\n    audio, processing, audio-to-audio, transformation\n\n    Use cases:\n    - Audio enhancement and processing\n    - Voice transformation\n    - Audio style transfer\n    - Sound quality improvement\n    - Audio effect application",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.ElevenlabsAudioIsolation",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "Video file to use for audio isolation. Either `audio_url` or `video_url` must be provided."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to isolate voice from"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Elevenlabs Voice Changer",
      "description": "ElevenLabs Voice Changer transforms voice characteristics in audio with AI-powered voice conversion.\n    audio, voice-change, elevenlabs, transformation, audio-to-audio\n\n    Use cases:\n    - Change voice characteristics in audio\n    - Transform vocal qualities\n    - Create voice variations\n    - Modify speaker identity\n    - Generate voice-changed audio",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.ElevenlabsVoiceChanger",
      "properties": [
        {
          "name": "voice",
          "type": {
            "type": "str"
          },
          "default": "Rachel",
          "title": "Voice",
          "description": "The voice to use for speech generation"
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The input audio file"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": -1,
          "title": "Seed",
          "description": "Random seed for reproducibility."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "mp3_22050_32",
              "mp3_44100_32",
              "mp3_44100_64",
              "mp3_44100_96",
              "mp3_44100_128",
              "mp3_44100_192",
              "pcm_8000",
              "pcm_16000",
              "pcm_22050",
              "pcm_24000",
              "pcm_44100",
              "pcm_48000",
              "ulaw_8000",
              "alaw_8000",
              "opus_48000_32",
              "opus_48000_64",
              "opus_48000_96",
              "opus_48000_128",
              "opus_48000_192"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.ElevenlabsVoiceChanger.OutputFormat"
          },
          "default": "mp3_44100_128",
          "title": "Output Format",
          "description": "Output format of the generated audio. Formatted as codec_sample_rate_bitrate."
        },
        {
          "name": "remove_background_noise",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Remove Background Noise",
          "description": "If set, will remove the background noise from your audio input using our audio isolation model."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Ffmpeg Api Merge Audios",
      "description": "FFmpeg API Merge Audios combines multiple audio files into a single output.\n    audio, processing, audio-to-audio, merging, ffmpeg\n\n    Use cases:\n    - Combine multiple audio tracks\n    - Merge audio segments\n    - Create audio compilations\n    - Join split audio files\n    - Generate combined audio output",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.FfmpegApiMergeAudios",
      "properties": [
        {
          "name": "audio_urls",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Audio Urls",
          "description": "List of audio URLs to merge in order. The 0th stream of the audio will be considered as the merge candidate."
        },
        {
          "name": "output_format",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Output Format",
          "description": "Output format of the combined audio. If not used, will be determined automatically using FFMPEG. Formatted as codec_sample_rate_bitrate."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Kling Video Create Voice",
      "description": "Create Voices to be used with Kling 2.6 Voice Control\n    audio, processing, audio-to-audio, transformation\n\n    Use cases:\n    - Audio enhancement and processing\n    - Voice transformation\n    - Audio style transfer\n    - Sound quality improvement\n    - Audio effect application",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.KlingVideoCreateVoice",
      "properties": [
        {
          "name": "voice_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Voice Url",
          "description": "URL of the voice audio file. Supports .mp3/.wav audio or .mp4/.mov video. Duration must be 5-30 seconds with clean, single-voice audio."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "any"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Nova Sr",
      "description": "Nova SR enhances audio quality through super-resolution processing for clearer and richer sound.\n    audio, enhancement, super-resolution, quality, audio-to-audio\n\n    Use cases:\n    - Enhance audio quality\n    - Improve sound clarity\n    - Upscale audio resolution\n    - Restore degraded audio\n    - Generate high-quality audio",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.NovaSr",
      "properties": [
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "bitrate",
          "type": {
            "type": "str"
          },
          "default": "192k",
          "title": "Bitrate",
          "description": "The bitrate of the output audio."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file to enhance."
        },
        {
          "name": "audio_format",
          "type": {
            "type": "enum",
            "values": [
              "mp3",
              "aac",
              "m4a",
              "ogg",
              "opus",
              "flac",
              "wav"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.NovaSr.AudioFormat"
          },
          "default": "mp3",
          "title": "Audio Format",
          "description": "The format for the output audio."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Sam Audio Separate",
      "description": "SAM Audio Separate isolates and extracts different audio sources from mixed recordings.\n    audio, separation, source-extraction, isolation, audio-to-audio\n\n    Use cases:\n    - Separate audio sources\n    - Extract vocals from music\n    - Isolate instruments\n    - Remove background sounds\n    - Generate separated audio tracks",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.SamAudioSeparate",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the sound to isolate."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "quality"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.SamAudioSeparate.Acceleration"
          },
          "default": "balanced",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to process (WAV, MP3, FLAC supported)"
        },
        {
          "name": "predict_spans",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Predict Spans",
          "description": "Automatically predict temporal spans where the target sound occurs."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "wav",
              "mp3"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.SamAudioSeparate.OutputFormat"
          },
          "default": "wav",
          "title": "Output Format",
          "description": "Output audio format."
        },
        {
          "name": "reranking_candidates",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Reranking Candidates",
          "description": "Number of candidates to generate and rank. Higher improves quality but increases latency and cost."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Sam Audio Span Separate",
      "description": "SAM Audio Span Separate isolates audio sources across time spans with precise temporal control.\n    audio, separation, temporal, span, audio-to-audio\n\n    Use cases:\n    - Separate audio by time spans\n    - Extract sources in specific periods\n    - Isolate temporal audio segments\n    - Remove sounds in time ranges\n    - Generate time-based separations",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.SamAudioSpanSeparate",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt describing the sound to isolate. Optional but recommended - helps the model identify what type of sound to extract from the span."
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "quality"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.SamAudioSpanSeparate.Acceleration"
          },
          "default": "balanced",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "spans",
          "type": {
            "type": "list",
            "type_args": [
              {
                "type": "str"
              }
            ]
          },
          "default": [],
          "title": "Spans",
          "description": "Time spans where the target sound occurs which should be isolated."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "wav",
              "mp3"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.SamAudioSpanSeparate.OutputFormat"
          },
          "default": "wav",
          "title": "Output Format",
          "description": "Output audio format."
        },
        {
          "name": "trim_to_span",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Trim To Span",
          "description": "Trim output audio to only include the specified span time range. If False, returns the full audio length with the target sound isolated throughout."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "URL of the audio file to process."
        },
        {
          "name": "reranking_candidates",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Reranking Candidates",
          "description": "Number of candidates to generate and rank. Higher improves quality but increases latency and cost. Requires text prompt; ignored for span-only separation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Sonauto V2 Extend",
      "description": "Extend an existing song\n    audio, processing, audio-to-audio, transformation\n\n    Use cases:\n    - Audio enhancement and processing\n    - Voice transformation\n    - Audio style transfer\n    - Sound quality improvement\n    - Audio effect application",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.SonautoV2Extend",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "A description of the track you want to generate. This prompt will be used to automatically generate the tags and lyrics unless you manually set them. For example, if you set prompt and tags, then the prompt will be used to generate only the lyrics."
        },
        {
          "name": "lyrics_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Lyrics Prompt",
          "description": "The lyrics sung in the generated song. An empty string will generate an instrumental track."
        },
        {
          "name": "tags",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Tags",
          "description": "Tags/styles of the music to generate. You can view a list of all available tags at https://sonauto.ai/tag-explorer."
        },
        {
          "name": "prompt_strength",
          "type": {
            "type": "float"
          },
          "default": 1.8,
          "title": "Prompt Strength",
          "description": "Controls how strongly your prompt influences the output. Greater values adhere more to the prompt but sound less natural. (This is CFG.)"
        },
        {
          "name": "output_bit_rate",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Output Bit Rate",
          "description": "The bit rate to use for mp3 and m4a formats. Not available for other formats."
        },
        {
          "name": "num_songs",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Num Songs",
          "description": "Generating 2 songs costs 1.5x the price of generating 1 song. Also, note that using the same seed may not result in identical songs if the number of songs generated is changed."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "flac",
              "mp3",
              "wav",
              "ogg",
              "m4a"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.SonautoV2Extend.OutputFormat"
          },
          "default": "wav",
          "title": "Output Format"
        },
        {
          "name": "side",
          "type": {
            "type": "enum",
            "values": [
              "left",
              "right"
            ],
            "type_name": "nodetool.nodes.fal.audio_to_audio.SonautoV2Extend.Side"
          },
          "default": "",
          "title": "Side",
          "description": "Add more to the beginning (left) or end (right) of the song"
        },
        {
          "name": "balance_strength",
          "type": {
            "type": "float"
          },
          "default": 0.7,
          "title": "Balance Strength",
          "description": "Greater means more natural vocals. Lower means sharper instrumentals. We recommend 0.7."
        },
        {
          "name": "crop_duration",
          "type": {
            "type": "float"
          },
          "default": 0,
          "title": "Crop Duration",
          "description": "Duration in seconds to crop from the selected side before extending from that side."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The URL of the audio file to alter. Must be a valid publicly accessible URL."
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Seed",
          "description": "The seed to use for generation. Will pick a random seed if not provided. Repeating a request with identical parameters (must use lyrics and tags, not prompt) and the same seed will generate the same song."
        },
        {
          "name": "extend_duration",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Extend Duration",
          "description": "Duration in seconds to extend the song. If not provided, will attempt to automatically determine."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Stable Audio 25 Audio To Audio",
      "description": "Stable Audio 2.5 transforms and modifies audio with AI-powered processing and effects.\n    audio, transformation, stable-audio, 2.5, audio-to-audio\n\n    Use cases:\n    - Transform audio characteristics\n    - Apply AI-powered audio effects\n    - Modify audio properties\n    - Generate audio variations\n    - Create processed audio",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.StableAudio25AudioToAudio",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to guide the audio generation"
        },
        {
          "name": "strength",
          "type": {
            "type": "float"
          },
          "default": 0.8,
          "title": "Strength",
          "description": "Sometimes referred to as denoising, this parameter controls how much influence the `audio_url` parameter has on the generated audio. A value of 0 would yield audio that is identical to the input. A value of 1 would be as if you passed in no audio at all."
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The audio clip to transform"
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of steps to denoise the audio for"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt)."
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed"
        },
        {
          "name": "total_seconds",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Total Seconds",
          "description": "The duration of the audio clip to generate. If not provided, it will be set to the duration of the input audio."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio",
        "prompt"
      ]
    },
    {
      "title": "Stable Audio 25 Inpaint",
      "description": "Generate high quality music and sound effects using Stable Audio 2.5 from StabilityAI\n    audio, processing, audio-to-audio, transformation\n\n    Use cases:\n    - Audio enhancement and processing\n    - Voice transformation\n    - Audio style transfer\n    - Sound quality improvement\n    - Audio effect application",
      "namespace": "fal.audio_to_audio",
      "node_type": "fal.audio_to_audio.StableAudio25Inpaint",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "The prompt to guide the audio generation"
        },
        {
          "name": "guidance_scale",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Guidance Scale",
          "description": "How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt)."
        },
        {
          "name": "mask_end",
          "type": {
            "type": "int"
          },
          "default": 190,
          "title": "Mask End",
          "description": "The end point of the audio mask"
        },
        {
          "name": "sync_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Sync Mode",
          "description": "If `True`, the media will be returned as a data URI and the output data won't be available in the request history."
        },
        {
          "name": "audio_url",
          "type": {
            "type": "audio"
          },
          "default": {
            "type": "audio",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null
          },
          "title": "Audio Url",
          "description": "The audio clip to inpaint"
        },
        {
          "name": "seed",
          "type": {
            "type": "int"
          },
          "default": 0,
          "title": "Seed"
        },
        {
          "name": "seconds_total",
          "type": {
            "type": "int"
          },
          "default": 190,
          "title": "Seconds Total",
          "description": "The duration of the audio clip to generate. If not provided, it will be set to the duration of the input audio."
        },
        {
          "name": "num_inference_steps",
          "type": {
            "type": "int"
          },
          "default": 8,
          "title": "Num Inference Steps",
          "description": "The number of steps to denoise the audio for"
        },
        {
          "name": "mask_start",
          "type": {
            "type": "int"
          },
          "default": 30,
          "title": "Mask Start",
          "description": "The start point of the audio mask"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "audio"
      ]
    },
    {
      "title": "Half Moon Ai Ai Detector Detect Text",
      "description": "AI Detector (Text) is an advanced AI service that analyzes a passage and returns a verdict on whether it was likely written by AI.\n    text, processing, transformation, nlp\n\n    Use cases:\n    - Text transformation\n    - Content analysis\n    - Text classification\n    - Language processing\n    - Content detection",
      "namespace": "fal.text_to_text",
      "node_type": "fal.text_to_text.HalfMoonAiAiDetectorDetectText",
      "properties": [
        {
          "name": "text",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text",
          "description": "Text content to analyze for AI generation."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "text"
      ]
    },
    {
      "title": "Kling Video Video To Audio",
      "description": "Generate audio from input videos using Kling\n    audio, extraction, video-to-audio, processing\n\n    Use cases:\n    - Audio extraction from video\n    - Sound separation\n    - Video audio analysis\n    - Music extraction\n    - Sound effect isolation",
      "namespace": "fal.video_to_audio",
      "node_type": "fal.video_to_audio.KlingVideoVideoToAudio",
      "properties": [
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "The video URL to extract audio from. Only .mp4/.mov formats are supported. File size does not exceed 100MB. Video duration between 3.0s and 20.0s."
        },
        {
          "name": "asmr_mode",
          "type": {
            "type": "bool"
          },
          "default": false,
          "title": "Asmr Mode",
          "description": "Enable ASMR mode. This mode enhances detailed sound effects and is suitable for highly immersive content scenarios."
        },
        {
          "name": "background_music_prompt",
          "type": {
            "type": "str"
          },
          "default": "intense car race",
          "title": "Background Music Prompt",
          "description": "Background music prompt. Cannot exceed 200 characters."
        },
        {
          "name": "sound_effect_prompt",
          "type": {
            "type": "str"
          },
          "default": "Car tires screech as they accelerate in a drag race",
          "title": "Sound Effect Prompt",
          "description": "Sound effect prompt. Cannot exceed 200 characters."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "video"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "video_url",
        "asmr_mode",
        "background_music_prompt",
        "sound_effect_prompt"
      ]
    },
    {
      "title": "Mirelo Ai Sfx V15 Video To Audio",
      "description": "Generate synced sounds for any video, and return the new sound track (like MMAudio)\n    audio, extraction, video-to-audio, processing\n\n    Use cases:\n    - Audio extraction from video\n    - Sound separation\n    - Video audio analysis\n    - Music extraction\n    - Sound effect isolation",
      "namespace": "fal.video_to_audio",
      "node_type": "fal.video_to_audio.MireloAiSfxV15VideoToAudio",
      "properties": [
        {
          "name": "num_samples",
          "type": {
            "type": "str"
          },
          "default": 2,
          "title": "Num Samples",
          "description": "The number of samples to generate from the model"
        },
        {
          "name": "duration",
          "type": {
            "type": "str"
          },
          "default": 10,
          "title": "Duration",
          "description": "The duration of the generated audio in seconds"
        },
        {
          "name": "start_offset",
          "type": {
            "type": "str"
          },
          "default": 0,
          "title": "Start Offset",
          "description": "The start offset in seconds to start the audio generation from"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "A video url that can accessed from the API to process and add sound effects"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": 8069,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used"
        },
        {
          "name": "text_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Prompt",
          "description": "Additional description to guide the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "num_samples",
        "duration",
        "start_offset",
        "video_url",
        "seed"
      ]
    },
    {
      "title": "Mirelo Ai Sfx V1 Video To Audio",
      "description": "Generate synced sounds for any video, and return the new sound track (like MMAudio)\n    audio, extraction, video-to-audio, processing\n\n    Use cases:\n    - Audio extraction from video\n    - Sound separation\n    - Video audio analysis\n    - Music extraction\n    - Sound effect isolation",
      "namespace": "fal.video_to_audio",
      "node_type": "fal.video_to_audio.MireloAiSfxV1VideoToAudio",
      "properties": [
        {
          "name": "num_samples",
          "type": {
            "type": "str"
          },
          "default": 2,
          "title": "Num Samples",
          "description": "The number of samples to generate from the model"
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "A video url that can accessed from the API to process and add sound effects"
        },
        {
          "name": "duration",
          "type": {
            "type": "str"
          },
          "default": 10,
          "title": "Duration",
          "description": "The duration of the generated audio in seconds"
        },
        {
          "name": "seed",
          "type": {
            "type": "str"
          },
          "default": 2105,
          "title": "Seed",
          "description": "The seed to use for the generation. If not provided, a random seed will be used"
        },
        {
          "name": "text_prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Text Prompt",
          "description": "Additional description to guide the model"
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "audio"
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "num_samples",
        "video_url",
        "duration",
        "seed",
        "text_prompt"
      ]
    },
    {
      "title": "Sam Audio Visual Separate",
      "description": "Audio separation with SAM Audio. Isolate any sound using natural language\u2014professional-grade audio editing made simple for creators, researchers, and accessibility applications.\n    audio, extraction, video-to-audio, processing\n\n    Use cases:\n    - Audio extraction from video\n    - Sound separation\n    - Video audio analysis\n    - Music extraction\n    - Sound effect isolation",
      "namespace": "fal.video_to_audio",
      "node_type": "fal.video_to_audio.SamAudioVisualSeparate",
      "properties": [
        {
          "name": "prompt",
          "type": {
            "type": "str"
          },
          "default": "",
          "title": "Prompt",
          "description": "Text prompt to assist with separation. Use natural language to describe the target sound."
        },
        {
          "name": "video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Video Url",
          "description": "URL of the video file to process (MP4, MOV, etc.)"
        },
        {
          "name": "acceleration",
          "type": {
            "type": "enum",
            "values": [
              "fast",
              "balanced",
              "quality"
            ],
            "type_name": "nodetool.nodes.fal.video_to_audio.SamAudioVisualSeparate.Acceleration"
          },
          "default": "balanced",
          "title": "Acceleration",
          "description": "The acceleration level to use."
        },
        {
          "name": "mask_video_url",
          "type": {
            "type": "video"
          },
          "default": {
            "type": "video",
            "uri": "",
            "asset_id": null,
            "data": null,
            "metadata": null,
            "duration": null,
            "format": null
          },
          "title": "Mask Video Url",
          "description": "URL of the mask video (binary mask indicating target object). Black=target, White=background."
        },
        {
          "name": "output_format",
          "type": {
            "type": "enum",
            "values": [
              "wav",
              "mp3"
            ],
            "type_name": "nodetool.nodes.fal.video_to_audio.SamAudioVisualSeparate.OutputFormat"
          },
          "default": "wav",
          "title": "Output Format",
          "description": "Output audio format."
        },
        {
          "name": "reranking_candidates",
          "type": {
            "type": "int"
          },
          "default": 1,
          "title": "Reranking Candidates",
          "description": "Number of candidates to generate and rank. Higher improves quality but increases latency and cost."
        }
      ],
      "outputs": [
        {
          "type": {
            "type": "dict",
            "type_args": [
              {
                "type": "str"
              },
              {
                "type": "any"
              }
            ]
          },
          "name": "output"
        }
      ],
      "basic_fields": [
        "prompt",
        "video_url",
        "acceleration",
        "mask_video_url",
        "output_format"
      ]
    }
  ]
}