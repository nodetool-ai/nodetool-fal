# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class AIFaceSwapVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    AI Face Swap replaces faces in videos with target faces while preserving expressions and movements.
    video, face-swap, deepfake, face-replacement, video-to-video

    Use cases:
    - Replace faces in video content
    - Create personalized video content
    - Swap actors in video scenes
    - Generate face replacement effects
    - Create video with different faces
    """

    enable_occlusion_prevention: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable occlusion prevention for handling faces covered by hands/objects. Warning: Enabling this runs an occlusion-aware model which costs 2x more.",
    )
    source_face: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Source face image. Allowed items: bmp, jpeg, png, tiff, webp",
    )
    target_video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Target video URL (max 25 minutes, will be truncated if longer; FPS capped at 25). Allowed items: avi, m4v, mkv, mp4, mpeg, mov, mxf, webm, wmv",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.AIFaceSwapVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class AMTInterpolation(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    AMT (Any-to-Many Temporal) Interpolation creates smooth transitions between video frames.
    video, interpolation, frame-generation, amt, video-to-video

    Use cases:
    - Increase video frame rate smoothly
    - Create slow-motion effects
    - Smooth out choppy video
    - Generate intermediate frames
    - Enhance video playback quality
    """

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the video to be processed",
    )
    low_latency_mode: bool | OutputHandle[bool] = connect_field(
        default=False, description="Enables low latency mode"
    )
    recursive_interpolation_passes: int | OutputHandle[int] = connect_field(
        default=2, description="Number of recursive interpolation passes"
    )
    output_fps: int | OutputHandle[int] = connect_field(
        default=24, description="Output frames per second"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.AMTInterpolation

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class AnimateDiffTurboVideoToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    AnimateDiff Turbo re-animates videos quickly with reduced generation time.
    video, style-transfer, animatediff, turbo, fast, video-to-video

    Use cases:
    - Quickly restyle videos
    - Rapid video transformations
    - Fast video effect application
    - Efficient video processing
    - Real-time video styling
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The prompt to use for generating the image. Be as descriptive as possible for best results.",
    )
    first_n_seconds: int | OutputHandle[int] = connect_field(
        default=3, description="The first N number of seconds of video to animate."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the video.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of frames per second to extract from the video."
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.7, description="The strength of the input video in the final output."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=2,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=12,
        description="The number of inference steps to perform. 4-12 is recommended for turbo mode.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="(bad quality, worst quality:1.2), ugly faces, bad anime",
        description="The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.AnimateDiffTurboVideoToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class AnimateDiffVideoToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    AnimateDiff re-animates videos with new styles and effects using diffusion models.
    video, style-transfer, animatediff, re-animation, video-to-video

    Use cases:
    - Restyle existing videos
    - Apply artistic effects to videos
    - Transform video aesthetics
    - Create stylized video versions
    - Generate video variations
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The prompt to use for generating the image. Be as descriptive as possible for best results.",
    )
    first_n_seconds: int | OutputHandle[int] = connect_field(
        default=3, description="The first N number of seconds of video to animate."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the video.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of frames per second to extract from the video."
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.7, description="The strength of the input video in the final output."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="The number of inference steps to perform."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="(bad quality, worst quality:1.2), ugly faces, bad anime",
        description="The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution).",
    )
    motions: list[str] | OutputHandle[list[str]] = connect_field(
        default=[], description="The motions to apply to the video."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.AnimateDiffVideoToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class AutoCaption(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Auto Caption automatically generates and adds captions to videos with speech recognition.
    video, captions, subtitles, speech-to-text, video-to-video

    Use cases:
    - Add subtitles to videos automatically
    - Generate captions for accessibility
    - Create multilingual subtitles
    - Transcribe video speech
    - Add text overlays to videos
    """

    txt_font: str | OutputHandle[str] = connect_field(
        default="Standard",
        description="Font for generated captions. Choose one in 'Arial','Standard','Garamond', 'Times New Roman','Georgia', or pass a url to a .ttf file",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the .mp4 video with audio. Only videos of size <100MB are allowed.",
    )
    top_align: str | OutputHandle[str] = connect_field(
        default="center",
        description="Top-to-bottom alignment of the text. Can be a string ('top', 'center', 'bottom') or a float (0.0-1.0)",
    )
    txt_color: str | OutputHandle[str] = connect_field(
        default="white",
        description="Colour of the text. Can be a RGB tuple, a color name, or an hexadecimal notation.",
    )
    stroke_width: int | OutputHandle[int] = connect_field(
        default=1, description="Width of the text strokes in pixels"
    )
    refresh_interval: float | OutputHandle[float] = connect_field(
        default=1.5,
        description="Number of seconds the captions should stay on screen. A higher number will also result in more text being displayed at once.",
    )
    font_size: int | OutputHandle[int] = connect_field(
        default=24, description="Size of text in generated captions."
    )
    left_align: str | OutputHandle[str] = connect_field(
        default="center",
        description="Left-to-right alignment of the text. Can be a string ('left', 'center', 'right') or a float (0.0-1.0)",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.AutoCaption

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class BenV2Video(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Ben v2 Video enhances and processes video content with advanced AI techniques.
    video, enhancement, processing, ben, video-to-video

    Use cases:
    - Enhance video quality
    - Process video content
    - Improve video clarity
    - Apply video enhancements
    - Optimize video output
    """

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of video to be used for background removal.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for reproducible generation."
    )
    background_color: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="Optional RGB values (0-255) for the background color. If not provided, the background will be transparent. For ex: [0, 0, 0]",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.BenV2Video

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class BiRefNetV2Video(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    BiRefNet v2 Video performs background removal from videos with high accuracy.
    video, background-removal, segmentation, birefnet, video-to-video

    Use cases:
    - Remove backgrounds from videos
    - Create transparent video backgrounds
    - Isolate video subjects
    - Generate video mattes
    - Prepare videos for compositing
    """

    BiRefNetV2VideoVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.BiRefNetV2VideoVideoWriteMode
    )
    BiRefNetV2VideoVideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.BiRefNetV2VideoVideoOutputType
    )
    BiRefNetV2VideoOperatingResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.BiRefNetV2VideoOperatingResolution
    )
    BiRefNetV2VideoVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.BiRefNetV2VideoVideoQuality
    )
    BiRefNetV2VideoModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.BiRefNetV2VideoModel
    )

    video_write_mode: (
        nodetool.nodes.fal.video_to_video.BiRefNetV2VideoVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.BiRefNetV2VideoVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the video to remove background from",
    )
    video_output_type: (
        nodetool.nodes.fal.video_to_video.BiRefNetV2VideoVideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.BiRefNetV2VideoVideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    operating_resolution: (
        nodetool.nodes.fal.video_to_video.BiRefNetV2VideoOperatingResolution
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.BiRefNetV2VideoOperatingResolution.VALUE_1024X1024,
        description="The resolution to operate on. The higher the resolution, the more accurate the output will be for high res input images. The '2304x2304' option is only available for the 'General Use (Dynamic)' model.",
    )
    video_quality: nodetool.nodes.fal.video_to_video.BiRefNetV2VideoVideoQuality = (
        Field(
            default=nodetool.nodes.fal.video_to_video.BiRefNetV2VideoVideoQuality.HIGH,
            description="The quality of the generated video.",
        )
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    model: nodetool.nodes.fal.video_to_video.BiRefNetV2VideoModel = Field(
        default=nodetool.nodes.fal.video_to_video.BiRefNetV2VideoModel.GENERAL_USE_LIGHT,
        description="Model to use for background removal. The 'General Use (Light)' model is the original model used in the BiRefNet repository. The 'General Use (Light 2K)' model is the original model used in the BiRefNet repository but trained with 2K images. The 'General Use (Heavy)' model is a slower but more accurate model. The 'Matting' model is a model trained specifically for matting images. The 'Portrait' model is a model trained specifically for portrait images. The 'General Use (Dynamic)' model supports dynamic resolutions from 256x256 to 2304x2304. The 'General Use (Light)' model is recommended for most use cases. The corresponding models are as follows: - 'General Use (Light)': BiRefNet - 'General Use (Light 2K)': BiRefNet_lite-2K - 'General Use (Heavy)': BiRefNet_lite - 'Matting': BiRefNet-matting - 'Portrait': BiRefNet-portrait - 'General Use (Dynamic)': BiRefNet_dynamic",
    )
    output_mask: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to output the mask used to remove the background",
    )
    refine_foreground: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to refine the foreground using the estimated mask",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.BiRefNetV2Video

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class BriaVideoEraseKeypoints(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Video
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    BriaVideoEraseKeypointsOutputContainerAndCodec: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.BriaVideoEraseKeypointsOutputContainerAndCodec
    )

    preserve_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, audio will be preserved in the output video.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Input video to erase object from. duration must be less than 5s.",
    )
    output_container_and_codec: (
        nodetool.nodes.fal.video_to_video.BriaVideoEraseKeypointsOutputContainerAndCodec
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.BriaVideoEraseKeypointsOutputContainerAndCodec.MP4_H264,
        description="Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.",
    )
    keypoints: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="Input keypoints [x,y] to erase or keep from the video. Format like so: {'x':100, 'y':100, 'type':'positive/negative'}",
    )
    auto_trim: bool | OutputHandle[bool] = connect_field(
        default=True, description="auto trim the video, to working duration ( 5s )"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.BriaVideoEraseKeypoints

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class BriaVideoEraseMask(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Video
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    BriaVideoEraseMaskOutputContainerAndCodec: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.BriaVideoEraseMaskOutputContainerAndCodec
    )

    preserve_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, audio will be preserved in the output video.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Input video to erase object from. duration must be less than 5s.",
    )
    output_container_and_codec: (
        nodetool.nodes.fal.video_to_video.BriaVideoEraseMaskOutputContainerAndCodec
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.BriaVideoEraseMaskOutputContainerAndCodec.MP4_H264,
        description="Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.",
    )
    mask_video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Input video to mask erase object from. duration must be less than 5s.",
    )
    auto_trim: bool | OutputHandle[bool] = connect_field(
        default=True, description="auto trim the video, to working duration ( 5s )"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.BriaVideoEraseMask

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class BriaVideoErasePrompt(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Video
    video, editing, video-to-video, vid2vid, professional

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    BriaVideoErasePromptOutputContainerAndCodec: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.BriaVideoErasePromptOutputContainerAndCodec
    )

    preserve_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, audio will be preserved in the output video.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Input video to erase object from. duration must be less than 5s.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Input prompt to detect object to erase"
    )
    output_container_and_codec: (
        nodetool.nodes.fal.video_to_video.BriaVideoErasePromptOutputContainerAndCodec
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.BriaVideoErasePromptOutputContainerAndCodec.MP4_H264,
        description="Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.",
    )
    auto_trim: bool | OutputHandle[bool] = connect_field(
        default=True, description="auto trim the video, to working duration ( 5s )"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.BriaVideoErasePrompt

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class BriaVideoEraserKeypoints(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Bria Video Eraser removes objects from videos using keypoint-based selection.
    video, object-removal, eraser, keypoints, bria, video-to-video

    Use cases:
    - Remove objects using keypoint selection
    - Erase specific areas from videos
    - Targeted video content removal
    - Precision video editing
    - Remove elements with point markers
    """

    BriaVideoEraserKeypointsOutputContainerAndCodec: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.BriaVideoEraserKeypointsOutputContainerAndCodec
    )

    preserve_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, audio will be preserved in the output video.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Input video to erase object from. duration must be less than 5s.",
    )
    output_container_and_codec: (
        nodetool.nodes.fal.video_to_video.BriaVideoEraserKeypointsOutputContainerAndCodec
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.BriaVideoEraserKeypointsOutputContainerAndCodec.MP4_H264,
        description="Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.",
    )
    keypoints: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="Input keypoints [x,y] to erase or keep from the video. Format like so: {'x':100, 'y':100, 'type':'positive/negative'}",
    )
    auto_trim: bool | OutputHandle[bool] = connect_field(
        default=True, description="auto trim the video, to working duration ( 5s )"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.BriaVideoEraserKeypoints

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class BriaVideoEraserMask(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Bria Video Eraser removes objects from videos using mask-based selection.
    video, object-removal, eraser, inpainting, bria, video-to-video

    Use cases:
    - Remove unwanted objects from videos
    - Erase people or items from footage
    - Clean up video backgrounds
    - Remove watermarks from videos
    - Edit video content seamlessly
    """

    BriaVideoEraserMaskOutputContainerAndCodec: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.BriaVideoEraserMaskOutputContainerAndCodec
    )

    preserve_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, audio will be preserved in the output video.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Input video to erase object from. duration must be less than 5s.",
    )
    output_container_and_codec: (
        nodetool.nodes.fal.video_to_video.BriaVideoEraserMaskOutputContainerAndCodec
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.BriaVideoEraserMaskOutputContainerAndCodec.MP4_H264,
        description="Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.",
    )
    mask_video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Input video to mask erase object from. duration must be less than 5s.",
    )
    auto_trim: bool | OutputHandle[bool] = connect_field(
        default=True, description="auto trim the video, to working duration ( 5s )"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.BriaVideoEraserMask

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class BriaVideoEraserPrompt(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Bria Video Eraser removes objects from videos using text prompt descriptions.
    video, object-removal, eraser, prompt, bria, video-to-video

    Use cases:
    - Remove objects by describing them
    - Text-based video editing
    - Natural language video cleanup
    - Prompt-driven object removal
    - Semantic video editing
    """

    BriaVideoEraserPromptOutputContainerAndCodec: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.BriaVideoEraserPromptOutputContainerAndCodec
    )

    preserve_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, audio will be preserved in the output video.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Input video to erase object from. duration must be less than 5s.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Input prompt to detect object to erase"
    )
    output_container_and_codec: (
        nodetool.nodes.fal.video_to_video.BriaVideoEraserPromptOutputContainerAndCodec
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.BriaVideoEraserPromptOutputContainerAndCodec.MP4_H264,
        description="Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, gif, mov_h264, mov_h265, mov_proresks, mkv_h264, mkv_h265, mkv_vp9, mkv_mpeg4.",
    )
    auto_trim: bool | OutputHandle[bool] = connect_field(
        default=True, description="auto trim the video, to working duration ( 5s )"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.BriaVideoEraserPrompt

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class BriaVideoIncreaseResolution(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Upscale videos up to 8K output resolution. Trained on fully licensed and commercially safe data.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    BriaVideoIncreaseResolutionOutputContainerAndCodec: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.BriaVideoIncreaseResolutionOutputContainerAndCodec
    )
    BriaVideoIncreaseResolutionDesiredIncrease: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.BriaVideoIncreaseResolutionDesiredIncrease
    )

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Input video to increase resolution. Size should be less than 7680,4320 and duration less than 30s.",
    )
    output_container_and_codec: (
        nodetool.nodes.fal.video_to_video.BriaVideoIncreaseResolutionOutputContainerAndCodec
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.BriaVideoIncreaseResolutionOutputContainerAndCodec.WEBM_VP9,
        description="Output container and codec. Options: mp4_h265, mp4_h264, webm_vp9, mov_h265, mov_proresks, mkv_h265, mkv_h264, mkv_vp9, gif.",
    )
    desired_increase: (
        nodetool.nodes.fal.video_to_video.BriaVideoIncreaseResolutionDesiredIncrease
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.BriaVideoIncreaseResolutionDesiredIncrease.VALUE_2,
        description="desired_increase factor. Options: 2x, 4x.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.BriaVideoIncreaseResolution

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class BytedanceUpscalerUpscaleVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Bytedance Upscaler
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    BytedanceUpscalerUpscaleVideoTargetFps: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.BytedanceUpscalerUpscaleVideoTargetFps
    )
    BytedanceUpscalerUpscaleVideoTargetResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.BytedanceUpscalerUpscaleVideoTargetResolution
    )

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to upscale.",
    )
    target_fps: (
        nodetool.nodes.fal.video_to_video.BytedanceUpscalerUpscaleVideoTargetFps
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.BytedanceUpscalerUpscaleVideoTargetFps.VALUE_30FPS,
        description="The target FPS of the video to upscale.",
    )
    target_resolution: (
        nodetool.nodes.fal.video_to_video.BytedanceUpscalerUpscaleVideoTargetResolution
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.BytedanceUpscalerUpscaleVideoTargetResolution.VALUE_1080P,
        description="The target resolution of the video to upscale.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.BytedanceUpscalerUpscaleVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class CCSR(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

    CCSR (Controllable Color Style Restoration) restores and enhances video colors.
    video, color-restoration, enhancement, ccsr, video-to-video

    Use cases:
    - Restore video colors
    - Enhance video color quality
    - Fix color issues in videos
    - Improve video color grading
    - Restore faded video footage
    """

    CCSRColorFixType: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.CCSRColorFixType
    )
    CCSRTileDiffusion: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.CCSRTileDiffusion
    )

    color_fix_type: nodetool.nodes.fal.video_to_video.CCSRColorFixType = Field(
        default=nodetool.nodes.fal.video_to_video.CCSRColorFixType.ADAIN,
        description="Type of color correction for samples.",
    )
    tile_diffusion_size: int | OutputHandle[int] = connect_field(
        default=1024, description="Size of patch."
    )
    tile_vae_decoder_size: int | OutputHandle[int] = connect_field(
        default=226, description="Size of VAE patch."
    )
    tile_vae_encoder_size: int | OutputHandle[int] = connect_field(
        default=1024, description="Size of latent image"
    )
    t_min: float | OutputHandle[float] = connect_field(
        default=0.3333, description="The starting point of uniform sampling strategy."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL or data URI of the image to upscale.",
    )
    tile_diffusion_stride: int | OutputHandle[int] = connect_field(
        default=512, description="Stride of sliding patch."
    )
    tile_vae: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If specified, a patch-based sampling strategy will be used for VAE decoding.",
    )
    scale: float | OutputHandle[float] = connect_field(
        default=2,
        description="The scale of the output image. The higher the scale, the bigger the output image will be.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for reproducibility. Different seeds will make slightly different results.",
    )
    t_max: float | OutputHandle[float] = connect_field(
        default=0.6667, description="The ending point of uniform sampling strategy."
    )
    steps: int | OutputHandle[int] = connect_field(
        default=50,
        description="The number of steps to run the model for. The higher the number the better the quality and longer it will take to generate.",
    )
    tile_diffusion: nodetool.nodes.fal.video_to_video.CCSRTileDiffusion = Field(
        default=nodetool.nodes.fal.video_to_video.CCSRTileDiffusion.NONE,
        description="If specified, a patch-based sampling strategy will be used for sampling.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.CCSR

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class CassetteaiVideoSoundEffectsGenerator(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Add sound effects to your videos
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="A video file to analyze & re-sound with generated SFX.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.CassetteaiVideoSoundEffectsGenerator

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class ClarityaiCrystalVideoUpscaler(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Crystal Upscaler [Video]
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the input video.",
    )
    scale_factor: float | OutputHandle[float] = connect_field(
        default=2,
        description="Scale factor. The scale factor must be chosen such that the upscaled video does not exceed 5K resolution.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.ClarityaiCrystalVideoUpscaler

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class CogVideoX5BVideoToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    CogVideoX-5B transforms existing videos with new styles and effects.
    video, transformation, cogvideo, style-transfer, video-to-video

    Use cases:
    - Transform video styles
    - Apply effects to existing videos
    - Restyle video content
    - Generate video variations
    - Create artistic video versions
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The video to generate the video from.",
    )
    use_rife: bool | OutputHandle[bool] = connect_field(
        default=True, description="Use RIFE for video interpolation"
    )
    loras: list[types.LoraWeight] | OutputHandle[list[types.LoraWeight]] = (
        connect_field(
            default=[],
            description="The LoRAs to use for the image generation. We currently support one lora.",
        )
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="", description="The size of the generated video."
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="The strength to use for Video to Video. 1.0 completely remakes the video while 0.0 preserves the original.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related video to show you.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="The number of inference steps to perform."
    )
    export_fps: int | OutputHandle[int] = connect_field(
        default=16, description="The target FPS of the video"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="The negative prompt to generate video from"
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.CogVideoX5BVideoToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Controlnext(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Animate a reference image with a driving video using ControlNeXt.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    controlnext_cond_scale: float | OutputHandle[float] = connect_field(
        default=1, description="Condition scale for ControlNeXt."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=7, description="Frames per second for the output video."
    )
    max_frame_num: int | OutputHandle[int] = connect_field(
        default=240, description="Maximum number of frames to process."
    )
    width: int | OutputHandle[int] = connect_field(
        default=576, description="Width of the output video."
    )
    overlap: int | OutputHandle[int] = connect_field(
        default=6, description="Number of overlapping frames between batches."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3, description="Guidance scale for the diffusion process."
    )
    batch_frames: int | OutputHandle[int] = connect_field(
        default=24, description="Number of frames to process in each batch."
    )
    height: int | OutputHandle[int] = connect_field(
        default=1024, description="Height of the output video."
    )
    sample_stride: int | OutputHandle[int] = connect_field(
        default=2, description="Stride for sampling frames from the input video."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the reference image.",
    )
    decode_chunk_size: int | OutputHandle[int] = connect_field(
        default=2, description="Chunk size for decoding frames."
    )
    motion_bucket_id: float | OutputHandle[float] = connect_field(
        default=127, description="Motion bucket ID for the pipeline."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="Number of inference steps."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Controlnext

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class DecartLucyEditDev(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Edit outfits, objects, faces, or restyle your video - all with maximum detail retention.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If set to true, the function will wait for the video to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the video directly in the response without going through the CDN.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the video to edit",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text description of the desired video content"
    )
    enhance_prompt: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enhance the prompt for better results."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.DecartLucyEditDev

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class DecartLucyEditFast(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Lucy Edit [Fast]
    video, editing, video-to-video, vid2vid, fast

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, the function will wait for the video to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the video directly in the response without going through the CDN.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the video to edit",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text description of the desired video content"
    )
    enhance_prompt: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enhance the prompt for better results."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.DecartLucyEditFast

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class DecartLucyEditPro(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Edit outfits, objects, faces, or restyle your video - all with maximum detail retention.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    DecartLucyEditProResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.DecartLucyEditProResolution
    )

    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If set to true, the function will wait for the video to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the video directly in the response without going through the CDN.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the video to edit",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text description of the desired video content"
    )
    resolution: nodetool.nodes.fal.video_to_video.DecartLucyEditProResolution = Field(
        default=nodetool.nodes.fal.video_to_video.DecartLucyEditProResolution.VALUE_720P,
        description="Resolution of the generated video",
    )
    enhance_prompt: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enhance the prompt for better results."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.DecartLucyEditPro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class DecartLucyRestyle(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Lucy Restyle
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    DecartLucyRestyleResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.DecartLucyRestyleResolution
    )

    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, the function will wait for the video to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the video directly in the response without going through the CDN.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the video to edit",
    )
    resolution: nodetool.nodes.fal.video_to_video.DecartLucyRestyleResolution = Field(
        default=nodetool.nodes.fal.video_to_video.DecartLucyRestyleResolution.VALUE_720P,
        description="Resolution of the generated video",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text description of the desired video content"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for video generation"
    )
    enhance_prompt: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enhance the prompt for better results."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.DecartLucyRestyle

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Dubbing(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    This endpoint delivers seamlessly localized videos by generating lip-synced dubs in multiple languages, ensuring natural and immersive multilingual experiences
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    DubbingTargetLanguage: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.DubbingTargetLanguage
    )

    do_lipsync: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to lip sync the audio to the video"
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Input video URL to be dubbed.",
    )
    target_language: nodetool.nodes.fal.video_to_video.DubbingTargetLanguage = Field(
        default=nodetool.nodes.fal.video_to_video.DubbingTargetLanguage.HINDI,
        description="Target language to dub the video to",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Dubbing

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class DwposeVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Predict poses from videos.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    DwposeVideoDrawMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.DwposeVideoDrawMode
    )

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of video to be used for pose estimation",
    )
    draw_mode: nodetool.nodes.fal.video_to_video.DwposeVideoDrawMode = Field(
        default=nodetool.nodes.fal.video_to_video.DwposeVideoDrawMode.BODY_POSE,
        description="Mode of drawing the pose on the video. Options are: 'full-pose', 'body-pose', 'face-pose', 'hand-pose', 'face-hand-mask', 'face-mask', 'hand-mask'.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.DwposeVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Editto(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Editto
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    EdittoSampler: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.EdittoSampler
    )
    EdittoVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.EdittoVideoWriteMode
    )
    EdittoResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.EdittoResolution
    )
    EdittoAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.EdittoAspectRatio
    )
    EdittoVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.EdittoVideoQuality
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source video file. Required for inpainting.",
    )
    acceleration: str | OutputHandle[str] = connect_field(
        default="regular",
        description="Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.",
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Number of frames to interpolate between the original frames. A value of 0 means no interpolation.",
    )
    temporal_downsample_factor: int | OutputHandle[int] = connect_field(
        default=0,
        description="Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.",
    )
    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    frames_per_second: str | OutputHandle[str] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.",
    )
    match_input_num_frames: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 241 (inclusive).",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    sampler: nodetool.nodes.fal.video_to_video.EdittoSampler = Field(
        default=nodetool.nodes.fal.video_to_video.EdittoSampler.UNIPC,
        description="Sampler to use for video generation.",
    )
    video_write_mode: nodetool.nodes.fal.video_to_video.EdittoVideoWriteMode = Field(
        default=nodetool.nodes.fal.video_to_video.EdittoVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    resolution: nodetool.nodes.fal.video_to_video.EdittoResolution = Field(
        default=nodetool.nodes.fal.video_to_video.EdittoResolution.AUTO,
        description="Resolution of the generated video.",
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.EdittoAspectRatio = Field(
        default=nodetool.nodes.fal.video_to_video.EdittoAspectRatio.AUTO,
        description="Aspect ratio of the generated video.",
    )
    return_frames_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, also return a ZIP file containing all generated frames.",
    )
    video_quality: nodetool.nodes.fal.video_to_video.EdittoVideoQuality = Field(
        default=nodetool.nodes.fal.video_to_video.EdittoVideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    match_input_frames_per_second: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    enable_auto_downsample: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Editto

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class FfmpegApiCompose(
    SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]
):
    """

    Compose videos from multiple media sources using FFmpeg API.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    tracks: list[types.Track] | OutputHandle[list[types.Track]] = connect_field(
        default=[], description="List of tracks to be combined into the final media"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.FfmpegApiCompose

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class FfmpegApiMergeAudioVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Merge videos with standalone audio files or audio from video files.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    start_offset: float | OutputHandle[float] = connect_field(
        default=0,
        description="Offset in seconds for when the audio should start relative to the video",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the video file to use as the video track",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the audio file to use as the audio track",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.FfmpegApiMergeAudioVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class FfmpegApiMergeVideos(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Use ffmpeg capabilities to merge 2 or more videos.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    resolution: str | OutputHandle[str] = connect_field(
        default="",
        description="Resolution of the final video. Width and height must be between 512 and 2048.",
    )
    videos: list[str] | OutputHandle[list[str]] = connect_field(
        default=[], description="List of video URLs to merge in order"
    )
    target_fps: str | OutputHandle[str] = connect_field(
        default="",
        description="Target FPS for the output video. If not provided, uses the lowest FPS from input videos.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.FfmpegApiMergeVideos

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class FilmVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Interpolate videos with FILM - Frame Interpolation for Large Motion
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    FilmVideoVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.FilmVideoVideoWriteMode
    )
    FilmVideoVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.FilmVideoVideoQuality
    )

    video_write_mode: nodetool.nodes.fal.video_to_video.FilmVideoVideoWriteMode = Field(
        default=nodetool.nodes.fal.video_to_video.FilmVideoVideoWriteMode.BALANCED,
        description="The write mode of the output video. Only applicable if output_type is 'video'.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to use for interpolation.",
    )
    use_calculated_fps: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used.",
    )
    loop: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=8,
        description="Frames per second for the output video. Only applicable if use_calculated_fps is False.",
    )
    video_quality: nodetool.nodes.fal.video_to_video.FilmVideoVideoQuality = Field(
        default=nodetool.nodes.fal.video_to_video.FilmVideoVideoQuality.HIGH,
        description="The quality of the output video. Only applicable if output_type is 'video'.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    use_scene_detection: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=1,
        description="The number of frames to generate between the input video frames.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.FilmVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class FlashvsrUpscaleVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Flashvsr
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    FlashvsrUpscaleVideoAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideoAcceleration
    )
    FlashvsrUpscaleVideoOutputWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideoOutputWriteMode
    )
    FlashvsrUpscaleVideoOutputFormat: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideoOutputFormat
    )
    FlashvsrUpscaleVideoOutputQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideoOutputQuality
    )

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The input video to be upscaled",
    )
    acceleration: nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideoAcceleration = (
        Field(
            default=nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideoAcceleration.REGULAR,
            description="Acceleration mode for VAE decoding. Options: regular (best quality), high (balanced), full (fastest). More accerleation means longer duration videos can be processed too.",
        )
    )
    quality: int | OutputHandle[int] = connect_field(
        default=70,
        description="Quality level for tile blending (0-100). Controls overlap between tiles to prevent grid artifacts. Higher values provide better quality with more overlap. Recommended: 70-85 for high-res videos, 50-70 for faster processing.",
    )
    output_write_mode: (
        nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideoOutputWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideoOutputWriteMode.BALANCED,
        description="The write mode of the output video.",
    )
    output_format: (
        nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideoOutputFormat
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideoOutputFormat.X264_MP4,
        description="The format of the output video.",
    )
    color_fix: bool | OutputHandle[bool] = connect_field(
        default=True, description="Color correction enabled."
    )
    preserve_audio: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Copy the original audio tracks into the upscaled video using FFmpeg when possible.",
    )
    output_quality: (
        nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideoOutputQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideoOutputQuality.HIGH,
        description="The quality of the output video.",
    )
    upscale_factor: float | OutputHandle[float] = connect_field(
        default=2, description="Upscaling factor to be used."
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned inline and not stored in history.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The random seed used for the generation process."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.FlashvsrUpscaleVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class HunyuanVideoFoley(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Use the capabilities of the hunyuan foley model to bring life to your videos by adding sound effect to them.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to generate audio for.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=4.5, description="Guidance scale for audio generation."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="Number of inference steps for generation."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for reproducible generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="noisy, harsh",
        description="Negative prompt to avoid certain audio characteristics.",
    )
    text_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text description of the desired audio (optional)."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.HunyuanVideoFoley

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class HunyuanVideoLoraVideoToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Hunyuan Video is an Open video generation model with high visual quality, motion diversity, text-video alignment, and generation stability. Use this endpoint to generate videos from videos.
    video, editing, video-to-video, vid2vid, lora

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    HunyuanVideoLoraVideoToVideoAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.HunyuanVideoLoraVideoToVideoAspectRatio
    )
    HunyuanVideoLoraVideoToVideoResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.HunyuanVideoLoraVideoToVideoResolution
    )
    HunyuanVideoLoraVideoToVideoNumFrames: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.HunyuanVideoLoraVideoToVideoNumFrames
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.HunyuanVideoLoraVideoToVideoAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.HunyuanVideoLoraVideoToVideoAspectRatio.RATIO_16_9,
        description="The aspect ratio of the video to generate.",
    )
    resolution: (
        nodetool.nodes.fal.video_to_video.HunyuanVideoLoraVideoToVideoResolution
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.HunyuanVideoLoraVideoToVideoResolution.VALUE_720P,
        description="The resolution of the video to generate.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the video",
    )
    loras: list[types.LoraWeight] | OutputHandle[list[types.LoraWeight]] = (
        connect_field(
            default=[],
            description="The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image.",
        )
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.75, description="Strength of video-to-video"
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed to use for generating the video."
    )
    num_frames: (
        nodetool.nodes.fal.video_to_video.HunyuanVideoLoraVideoToVideoNumFrames
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.HunyuanVideoLoraVideoToVideoNumFrames(
            129
        ),
        description="The number of frames to generate.",
    )
    pro_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.HunyuanVideoLoraVideoToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class HunyuanVideoToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Hunyuan Video transforms existing videos with advanced AI-powered effects.
    video, transformation, hunyuan, video-to-video

    Use cases:
    - Transform video content
    - Apply AI effects to videos
    - Restyle existing footage
    - Generate video variations
    - Create enhanced video versions
    """

    HunyuanVideoToVideoAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.HunyuanVideoToVideoAspectRatio
    )
    HunyuanVideoToVideoResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.HunyuanVideoToVideoResolution
    )
    HunyuanVideoToVideoNumFrames: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.HunyuanVideoToVideoNumFrames
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.HunyuanVideoToVideoAspectRatio = (
        Field(
            default=nodetool.nodes.fal.video_to_video.HunyuanVideoToVideoAspectRatio.RATIO_16_9,
            description="The aspect ratio of the video to generate.",
        )
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the video input.",
    )
    resolution: nodetool.nodes.fal.video_to_video.HunyuanVideoToVideoResolution = Field(
        default=nodetool.nodes.fal.video_to_video.HunyuanVideoToVideoResolution.VALUE_720P,
        description="The resolution of the video to generate.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.85, description="Strength for Video-to-Video"
    )
    loras: list[types.LoraWeight] | OutputHandle[list[types.LoraWeight]] = (
        connect_field(
            default=[],
            description="The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image.",
        )
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    num_frames: nodetool.nodes.fal.video_to_video.HunyuanVideoToVideoNumFrames = Field(
        default=nodetool.nodes.fal.video_to_video.HunyuanVideoToVideoNumFrames(129),
        description="The number of frames to generate.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default="", description="The seed to use for generating the video."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="The number of inference steps to run. Lower gets faster results, higher gets better results.",
    )
    pro_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.HunyuanVideoToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Infinitalk(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Infinitalk model generates a talking avatar video from an image and audio file. The avatar lip-syncs to the provided audio with natural facial expressions.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    InfinitalkResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.InfinitalkResolution
    )
    InfinitalkAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.InfinitalkAcceleration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    resolution: nodetool.nodes.fal.video_to_video.InfinitalkResolution = Field(
        default=nodetool.nodes.fal.video_to_video.InfinitalkResolution.VALUE_480P,
        description="Resolution of the video to generate. Must be either 480p or 720p.",
    )
    acceleration: nodetool.nodes.fal.video_to_video.InfinitalkAcceleration = Field(
        default=nodetool.nodes.fal.video_to_video.InfinitalkAcceleration.REGULAR,
        description="The acceleration level to use for generation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the audio file.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=145,
        description="Number of frames to generate. Must be between 41 to 721.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=42,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Infinitalk

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class InfinitalkVideoToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Infinitalk
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    InfinitalkVideoToVideoResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.InfinitalkVideoToVideoResolution
    )
    InfinitalkVideoToVideoAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.InfinitalkVideoToVideoAcceleration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    resolution: nodetool.nodes.fal.video_to_video.InfinitalkVideoToVideoResolution = (
        Field(
            default=nodetool.nodes.fal.video_to_video.InfinitalkVideoToVideoResolution.VALUE_480P,
            description="Resolution of the video to generate. Must be either 480p or 720p.",
        )
    )
    acceleration: (
        nodetool.nodes.fal.video_to_video.InfinitalkVideoToVideoAcceleration
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.InfinitalkVideoToVideoAcceleration.REGULAR,
        description="The acceleration level to use for generation.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the audio file.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=145,
        description="Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=42,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.InfinitalkVideoToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO1StandardVideoToVideoEdit(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling O1 Edit Video [Standard]
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB. Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s",
    )
    elements: (
        list[types.OmniVideoElementInput]
        | OutputHandle[list[types.OmniVideoElementInput]]
    ) = connect_field(
        default=[],
        description="Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.",
    )
    images: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = connect_field(
        default=[],
        description="Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.",
    )
    keep_audio: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to keep the original audio from the video."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.KlingVideoO1StandardVideoToVideoEdit

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO1StandardVideoToVideoReference(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling O1 Reference Video to Video [Standard]
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    KlingVideoO1StandardVideoToVideoReferenceAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.KlingVideoO1StandardVideoToVideoReferenceAspectRatio
    )
    KlingVideoO1StandardVideoToVideoReferenceDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.KlingVideoO1StandardVideoToVideoReferenceDuration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.KlingVideoO1StandardVideoToVideoReferenceAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.KlingVideoO1StandardVideoToVideoReferenceAspectRatio.AUTO,
        description="The aspect ratio of the generated video frame. If 'auto', the aspect ratio will be determined automatically based on the input video, and the closest aspect ratio to the input video will be used.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB. Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s",
    )
    duration: (
        nodetool.nodes.fal.video_to_video.KlingVideoO1StandardVideoToVideoReferenceDuration
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.KlingVideoO1StandardVideoToVideoReferenceDuration.VALUE_5,
        description="Video duration in seconds.",
    )
    keep_audio: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to keep the original audio from the video."
    )
    elements: (
        list[types.OmniVideoElementInput]
        | OutputHandle[list[types.OmniVideoElementInput]]
    ) = connect_field(
        default=[],
        description="Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.",
    )
    images: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = connect_field(
        default=[],
        description="Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return (
            nodetool.nodes.fal.video_to_video.KlingVideoO1StandardVideoToVideoReference
        )

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO1VideoToVideoEdit(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling O1 Edit Video [Pro]
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB. Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s",
    )
    elements: (
        list[types.OmniVideoElementInput]
        | OutputHandle[list[types.OmniVideoElementInput]]
    ) = connect_field(
        default=[],
        description="Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.",
    )
    images: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = connect_field(
        default=[],
        description="Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.",
    )
    keep_audio: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to keep the original audio from the video."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.KlingVideoO1VideoToVideoEdit

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO1VideoToVideoReference(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling O1 Reference Video to Video [Pro]
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    KlingVideoO1VideoToVideoReferenceAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.KlingVideoO1VideoToVideoReferenceAspectRatio
    )
    KlingVideoO1VideoToVideoReferenceDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.KlingVideoO1VideoToVideoReferenceDuration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Use @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.KlingVideoO1VideoToVideoReferenceAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.KlingVideoO1VideoToVideoReferenceAspectRatio.AUTO,
        description="The aspect ratio of the generated video frame. If 'auto', the aspect ratio will be determined automatically based on the input video, and the closest aspect ratio to the input video will be used.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Reference video URL. Only .mp4/.mov formats supported, 3-10 seconds duration, 720-2160px resolution, max 200MB. Max file size: 200.0MB, Min width: 720px, Min height: 720px, Max width: 2160px, Max height: 2160px, Min duration: 3.0s, Max duration: 10.05s, Min FPS: 24.0, Max FPS: 60.0, Timeout: 30.0s",
    )
    duration: (
        nodetool.nodes.fal.video_to_video.KlingVideoO1VideoToVideoReferenceDuration
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.KlingVideoO1VideoToVideoReferenceDuration.VALUE_5,
        description="Video duration in seconds.",
    )
    keep_audio: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to keep the original audio from the video."
    )
    elements: (
        list[types.OmniVideoElementInput]
        | OutputHandle[list[types.OmniVideoElementInput]]
    ) = connect_field(
        default=[],
        description="Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2, etc. Maximum 4 total (elements + reference images) when using video.",
    )
    images: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = connect_field(
        default=[],
        description="Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.KlingVideoO1VideoToVideoReference

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO3ProVideoToVideoEdit(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling O3 Edit Video [Pro]
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    KlingVideoO3ProVideoToVideoEditShotType: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.KlingVideoO3ProVideoToVideoEditShotType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for video generation. Reference video as @Video1.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Reference video URL. Only .mp4/.mov formats, 3-10s duration, 720-2160px resolution, max 200MB.",
    )
    elements: (
        list[types.KlingV3ImageElementInput]
        | OutputHandle[list[types.KlingV3ImageElementInput]]
    ) = connect_field(
        default=[],
        description="Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.",
    )
    images: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = connect_field(
        default=[],
        description="Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.",
    )
    keep_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to keep the original audio from the reference video.",
    )
    shot_type: (
        nodetool.nodes.fal.video_to_video.KlingVideoO3ProVideoToVideoEditShotType
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.KlingVideoO3ProVideoToVideoEditShotType.CUSTOMIZE,
        description="The type of multi-shot video generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.KlingVideoO3ProVideoToVideoEdit

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO3ProVideoToVideoReference(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling O3 Reference Video to Video [Pro]
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    KlingVideoO3ProVideoToVideoReferenceAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.KlingVideoO3ProVideoToVideoReferenceAspectRatio
    )
    KlingVideoO3ProVideoToVideoReferenceShotType: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.KlingVideoO3ProVideoToVideoReferenceShotType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for video generation. Reference video as @Video1.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.KlingVideoO3ProVideoToVideoReferenceAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.KlingVideoO3ProVideoToVideoReferenceAspectRatio.AUTO,
        description="Aspect ratio.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Reference video URL. Only .mp4/.mov formats, 3-10s duration, 720-2160px resolution, max 200MB.",
    )
    duration: (
        nodetool.nodes.fal.video_to_video.KlingVideoO3ProVideoToVideoReferenceDuration
        | OutputHandle[
            nodetool.nodes.fal.video_to_video.KlingVideoO3ProVideoToVideoReferenceDuration
        ]
        | None
    ) = connect_field(
        default=None,
        description="Video duration in seconds (3-15s for reference video).",
    )
    keep_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to keep the original audio from the reference video.",
    )
    shot_type: (
        nodetool.nodes.fal.video_to_video.KlingVideoO3ProVideoToVideoReferenceShotType
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.KlingVideoO3ProVideoToVideoReferenceShotType.CUSTOMIZE,
        description="The type of multi-shot video generation.",
    )
    elements: (
        list[types.KlingV3ImageElementInput]
        | OutputHandle[list[types.KlingV3ImageElementInput]]
    ) = connect_field(
        default=[],
        description="Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.",
    )
    images: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = connect_field(
        default=[],
        description="Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.KlingVideoO3ProVideoToVideoReference

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO3StandardVideoToVideoEdit(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling O3 Edit Video [Standard]
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    KlingVideoO3StandardVideoToVideoEditShotType: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.KlingVideoO3StandardVideoToVideoEditShotType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for video generation. Reference video as @Video1.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Reference video URL. Only .mp4/.mov formats, 3-10s duration, 720-2160px resolution, max 200MB.",
    )
    elements: (
        list[types.KlingV3ImageElementInput]
        | OutputHandle[list[types.KlingV3ImageElementInput]]
    ) = connect_field(
        default=[],
        description="Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.",
    )
    images: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = connect_field(
        default=[],
        description="Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.",
    )
    keep_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to keep the original audio from the reference video.",
    )
    shot_type: (
        nodetool.nodes.fal.video_to_video.KlingVideoO3StandardVideoToVideoEditShotType
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.KlingVideoO3StandardVideoToVideoEditShotType.CUSTOMIZE,
        description="The type of multi-shot video generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.KlingVideoO3StandardVideoToVideoEdit

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO3StandardVideoToVideoReference(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling O3 Reference Video to Video [Standard]
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    KlingVideoO3StandardVideoToVideoReferenceAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.KlingVideoO3StandardVideoToVideoReferenceAspectRatio
    )
    KlingVideoO3StandardVideoToVideoReferenceShotType: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.KlingVideoO3StandardVideoToVideoReferenceShotType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for video generation. Reference video as @Video1.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.KlingVideoO3StandardVideoToVideoReferenceAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.KlingVideoO3StandardVideoToVideoReferenceAspectRatio.AUTO,
        description="Aspect ratio.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Reference video URL. Only .mp4/.mov formats, 3-10s duration, 720-2160px resolution, max 200MB.",
    )
    duration: (
        nodetool.nodes.fal.video_to_video.KlingVideoO3StandardVideoToVideoReferenceDuration
        | OutputHandle[
            nodetool.nodes.fal.video_to_video.KlingVideoO3StandardVideoToVideoReferenceDuration
        ]
        | None
    ) = connect_field(
        default=None,
        description="Video duration in seconds (3-15s for reference video).",
    )
    keep_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to keep the original audio from the reference video.",
    )
    shot_type: (
        nodetool.nodes.fal.video_to_video.KlingVideoO3StandardVideoToVideoReferenceShotType
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.KlingVideoO3StandardVideoToVideoReferenceShotType.CUSTOMIZE,
        description="The type of multi-shot video generation.",
    )
    elements: (
        list[types.KlingV3ImageElementInput]
        | OutputHandle[list[types.KlingV3ImageElementInput]]
    ) = connect_field(
        default=[],
        description="Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.",
    )
    images: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = connect_field(
        default=[],
        description="Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return (
            nodetool.nodes.fal.video_to_video.KlingVideoO3StandardVideoToVideoReference
        )

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV26ProMotionControl(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video v2.6 Motion Control [Pro]
    video, editing, video-to-video, vid2vid, professional

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    KlingVideoV26ProMotionControlCharacterOrientation: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.KlingVideoV26ProMotionControlCharacterOrientation
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    video: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Reference video URL. The character actions in the generated video will be consistent with this reference video. Should contain a realistic style character with entire body or upper body visible, including head, without obstruction. Duration limit depends on character_orientation: 10s max for 'image', 30s max for 'video'.",
    )
    character_orientation: (
        nodetool.nodes.fal.video_to_video.KlingVideoV26ProMotionControlCharacterOrientation
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.KlingVideoV26ProMotionControlCharacterOrientation(
            ""
        ),
        description="Controls whether the output character's orientation matches the reference image or video. 'video': orientation matches reference video - better for complex motions (max 30s). 'image': orientation matches reference image - better for following camera movements (max 10s).",
    )
    keep_original_sound: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to keep the original sound from the reference video.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Reference image URL. The characters, backgrounds, and other elements in the generated video are based on this reference image. Characters should have clear body proportions, avoid occlusion, and occupy more than 5% of the image area.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.KlingVideoV26ProMotionControl

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV26StandardMotionControl(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video v2.6 Motion Control [Standard]
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    KlingVideoV26StandardMotionControlCharacterOrientation: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.KlingVideoV26StandardMotionControlCharacterOrientation
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    video: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Reference video URL. The character actions in the generated video will be consistent with this reference video. Should contain a realistic style character with entire body or upper body visible, including head, without obstruction. Duration limit depends on character_orientation: 10s max for 'image', 30s max for 'video'.",
    )
    character_orientation: (
        nodetool.nodes.fal.video_to_video.KlingVideoV26StandardMotionControlCharacterOrientation
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.KlingVideoV26StandardMotionControlCharacterOrientation(
            ""
        ),
        description="Controls whether the output character's orientation matches the reference image or video. 'video': orientation matches reference video - better for complex motions (max 30s). 'image': orientation matches reference image - better for following camera movements (max 10s).",
    )
    keep_original_sound: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to keep the original sound from the reference video.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Reference image URL. The characters, backgrounds, and other elements in the generated video are based on this reference image. Characters should have clear body proportions, avoid occlusion, and occupy more than 5% of the image area.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.KlingVideoV26StandardMotionControl

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class KreaWan14BVideoToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Krea Wan 14B
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Prompt for the video-to-video generation."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video. Currently, only outputs of 16:9 aspect ratio and 480p resolution are supported. Video duration should be less than 1000 frames at 16fps, and output frames will be 6 plus a multiple of 12, for example 18, 30, 42, etc.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.85,
        description="Denoising strength for the video-to-video generation. 0.0 preserves the original, 1.0 completely remakes the video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default="", description="Seed for the video-to-video generation."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.KreaWan14BVideoToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Latentsync(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    LatentSync is a video-to-video model that generates lip sync animations from audio using advanced algorithms for high-quality synchronization.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to generate the lip sync for.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=1, description="Guidance scale for the model inference"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for generation. If None, a random seed will be used.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the audio to generate the lip sync for.",
    )
    loop_mode: (
        nodetool.nodes.fal.video_to_video.LatentsyncLoopMode
        | OutputHandle[nodetool.nodes.fal.video_to_video.LatentsyncLoopMode]
        | None
    ) = connect_field(
        default=None,
        description="Video loop mode when audio is longer than video. Options: pingpong, loop",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Latentsync

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class LightxRecamera(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Lightx
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    LightxRecameraCamera: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LightxRecameraCamera
    )
    LightxRecameraMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LightxRecameraMode
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Optional text prompt. If omitted, Light-X will auto-caption the video.",
    )
    trajectory: str | OutputHandle[str] = connect_field(
        default="",
        description="Camera trajectory parameters (required for recamera mode).",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video.",
    )
    camera: nodetool.nodes.fal.video_to_video.LightxRecameraCamera = Field(
        default=nodetool.nodes.fal.video_to_video.LightxRecameraCamera.TRAJ,
        description="Camera control mode.",
    )
    target_pose: list[float] | OutputHandle[list[float]] = connect_field(
        default=[],
        description="Target camera pose [theta, phi, radius, x, y] (required when camera='target').",
    )
    mode: nodetool.nodes.fal.video_to_video.LightxRecameraMode = Field(
        default=nodetool.nodes.fal.video_to_video.LightxRecameraMode.GRADUAL,
        description="Camera motion mode.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.LightxRecamera

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class LightxRelight(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Lightx
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    LightxRelightRelitCondType: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LightxRelightRelitCondType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Optional text prompt. If omitted, Light-X will auto-caption the video.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video.",
    )
    relight_parameters: str | OutputHandle[str] = connect_field(
        default="",
        description="Relighting parameters (required for relight_condition_type='ic'). Not used for 'bg' (which expects a background image URL instead).",
    )
    ref_id: int | OutputHandle[int] = connect_field(
        default=0,
        description="Frame index to use as referencen to relight the video with reference.",
    )
    relit_cond_img: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of conditioning image. Required for relight_condition_type='ref'/'hdr'. Also required for relight_condition_type='bg' (background image).",
    )
    relit_cond_type: nodetool.nodes.fal.video_to_video.LightxRelightRelitCondType = (
        Field(
            default=nodetool.nodes.fal.video_to_video.LightxRelightRelitCondType.IC,
            description="Relight condition type.",
        )
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.LightxRelight

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Ltx219BDistilledExtendVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LTX-2 19B Distilled
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Ltx219BDistilledExtendVideoAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoAcceleration
    )
    Ltx219BDistilledExtendVideoCameraLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoCameraLora
    )
    Ltx219BDistilledExtendVideoExtendDirection: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoExtendDirection
    )
    Ltx219BDistilledExtendVideoVideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoVideoOutputType
    )
    Ltx219BDistilledExtendVideoVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoVideoWriteMode
    )
    Ltx219BDistilledExtendVideoVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoVideoQuality
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to extend.",
    )
    acceleration: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoAcceleration
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoAcceleration.NONE,
        description="The acceleration level to use.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    use_multiscale: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.",
    )
    fps: float | OutputHandle[float] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    camera_lora: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoCameraLora
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoCameraLora.NONE,
        description="The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="auto", description="The size of the generated video."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    camera_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as the end of the extended video.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
        description="The negative prompt to generate the video from.",
    )
    extend_direction: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoExtendDirection
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoExtendDirection.FORWARD,
        description="Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning.",
    )
    video_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="Video conditioning strength. Lower values represent more freedom given to the model to change the video content.",
    )
    video_output_type: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoVideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoVideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )
    num_context_frames: int | OutputHandle[int] = connect_field(
        default=25,
        description="The number of frames to use as context for the extension.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoVideoQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoVideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    end_image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the end image to use for the video generation.",
    )
    match_input_fps: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="When true, match the output FPS to the input video's FPS instead of using the default target FPS.",
    )
    audio_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Ltx219BDistilledExtendVideoLora(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LTX-2 19B Distilled
    video, editing, video-to-video, vid2vid, lora

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Ltx219BDistilledExtendVideoLoraAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraAcceleration
    )
    Ltx219BDistilledExtendVideoLoraCameraLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraCameraLora
    )
    Ltx219BDistilledExtendVideoLoraExtendDirection: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraExtendDirection
    )
    Ltx219BDistilledExtendVideoLoraVideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraVideoOutputType
    )
    Ltx219BDistilledExtendVideoLoraVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraVideoWriteMode
    )
    Ltx219BDistilledExtendVideoLoraVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraVideoQuality
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to extend.",
    )
    acceleration: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraAcceleration
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraAcceleration.NONE,
        description="The acceleration level to use.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    use_multiscale: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.",
    )
    fps: float | OutputHandle[float] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    loras: list[types.LoRAInput] | OutputHandle[list[types.LoRAInput]] = connect_field(
        default=[], description="The LoRAs to use for the generation."
    )
    camera_lora: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraCameraLora
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraCameraLora.NONE,
        description="The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="auto", description="The size of the generated video."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    camera_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as the end of the extended video.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
        description="The negative prompt to generate the video from.",
    )
    extend_direction: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraExtendDirection
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraExtendDirection.FORWARD,
        description="Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning.",
    )
    video_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="Video conditioning strength. Lower values represent more freedom given to the model to change the video content.",
    )
    video_output_type: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraVideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraVideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )
    num_context_frames: int | OutputHandle[int] = connect_field(
        default=25,
        description="The number of frames to use as context for the extension.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraVideoQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLoraVideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    end_image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the end image to use for the video generation.",
    )
    match_input_fps: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="When true, match the output FPS to the input video's FPS instead of using the default target FPS.",
    )
    audio_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Ltx219BDistilledExtendVideoLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Ltx219BDistilledVideoToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LTX-2 19B Distilled
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Ltx219BDistilledVideoToVideoVideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoVideoOutputType
    )
    Ltx219BDistilledVideoToVideoVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoVideoQuality
    )
    Ltx219BDistilledVideoToVideoAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoAcceleration
    )
    Ltx219BDistilledVideoToVideoCameraLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoCameraLora
    )
    Ltx219BDistilledVideoToVideoPreprocessor: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoPreprocessor
    )
    Ltx219BDistilledVideoToVideoVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoVideoWriteMode
    )
    Ltx219BDistilledVideoToVideoIcLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoIcLora
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to generate the video from.",
    )
    use_multiscale: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.",
    )
    ic_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="auto", description="The size of the generated video."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as the end of the video.",
    )
    camera_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    video_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="Video conditioning strength. Lower values represent more freedom given to the model to change the video content.",
    )
    video_output_type: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoVideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoVideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="An optional URL of an image to use as the first frame of the video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoVideoQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoVideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    match_video_length: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.",
    )
    acceleration: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoAcceleration
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoAcceleration.NONE,
        description="The acceleration level to use.",
    )
    fps: float | OutputHandle[float] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    camera_lora: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoCameraLora
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoCameraLora.NONE,
        description="The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the image to use for the video generation.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
        description="The negative prompt to generate the video from.",
    )
    preprocessor: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoPreprocessor
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoPreprocessor.NONE,
        description="The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    ic_lora: nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoIcLora = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoIcLora.MATCH_PREPROCESSOR,
            description="The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)",
        )
    )
    audio: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.",
    )
    audio_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.",
    )
    end_image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the end image to use for the video generation.",
    )
    match_input_fps: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="When true, match the output FPS to the input video's FPS instead of using the default target FPS.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Ltx219BDistilledVideoToVideoLora(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LTX-2 19B Distilled
    video, editing, video-to-video, vid2vid, lora

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Ltx219BDistilledVideoToVideoLoraVideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraVideoOutputType
    )
    Ltx219BDistilledVideoToVideoLoraVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraVideoQuality
    )
    Ltx219BDistilledVideoToVideoLoraAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraAcceleration
    )
    Ltx219BDistilledVideoToVideoLoraCameraLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraCameraLora
    )
    Ltx219BDistilledVideoToVideoLoraPreprocessor: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraPreprocessor
    )
    Ltx219BDistilledVideoToVideoLoraVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraVideoWriteMode
    )
    Ltx219BDistilledVideoToVideoLoraIcLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraIcLora
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to generate the video from.",
    )
    use_multiscale: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.",
    )
    ic_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    loras: list[types.LoRAInput] | OutputHandle[list[types.LoRAInput]] = connect_field(
        default=[], description="The LoRAs to use for the generation."
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="auto", description="The size of the generated video."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as the end of the video.",
    )
    camera_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    video_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="Video conditioning strength. Lower values represent more freedom given to the model to change the video content.",
    )
    video_output_type: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraVideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraVideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="An optional URL of an image to use as the first frame of the video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraVideoQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraVideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    match_video_length: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.",
    )
    acceleration: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraAcceleration
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraAcceleration.NONE,
        description="The acceleration level to use.",
    )
    fps: float | OutputHandle[float] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    camera_lora: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraCameraLora
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraCameraLora.NONE,
        description="The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the image to use for the video generation.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
        description="The negative prompt to generate the video from.",
    )
    preprocessor: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraPreprocessor
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraPreprocessor.NONE,
        description="The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    ic_lora: (
        nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraIcLora
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLoraIcLora.MATCH_PREPROCESSOR,
        description="The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)",
    )
    audio: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.",
    )
    audio_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.",
    )
    end_image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the end image to use for the video generation.",
    )
    match_input_fps: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="When true, match the output FPS to the input video's FPS instead of using the default target FPS.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Ltx219BDistilledVideoToVideoLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Ltx219BExtendVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LTX-2 19B
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Ltx219BExtendVideoAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoAcceleration
    )
    Ltx219BExtendVideoCameraLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoCameraLora
    )
    Ltx219BExtendVideoVideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoVideoOutputType
    )
    Ltx219BExtendVideoVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoVideoWriteMode
    )
    Ltx219BExtendVideoExtendDirection: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoExtendDirection
    )
    Ltx219BExtendVideoVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoVideoQuality
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to extend.",
    )
    acceleration: nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoAcceleration = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoAcceleration.REGULAR,
            description="The acceleration level to use.",
        )
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    audio_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.",
    )
    use_multiscale: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.",
    )
    fps: float | OutputHandle[float] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    camera_lora: nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoCameraLora = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoCameraLora.NONE,
        description="The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="auto", description="The size of the generated video."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3, description="The guidance scale to use."
    )
    camera_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as the end of the extended video.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
        description="The negative prompt to generate the video from.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    video_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="Video conditioning strength. Lower values represent more freedom given to the model to change the video content.",
    )
    video_output_type: (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoVideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoVideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    extend_direction: (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoExtendDirection
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoExtendDirection.FORWARD,
        description="Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )
    num_context_frames: int | OutputHandle[int] = connect_field(
        default=25,
        description="The number of frames to use as context for the extension.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoVideoQuality = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoVideoQuality.HIGH,
            description="The quality of the generated video.",
        )
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    end_image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the end image to use for the video generation.",
    )
    match_input_fps: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="When true, match the output FPS to the input video's FPS instead of using the default target FPS.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40, description="The number of inference steps to use."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Ltx219BExtendVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Ltx219BExtendVideoLora(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LTX-2 19B
    video, editing, video-to-video, vid2vid, lora

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Ltx219BExtendVideoLoraVideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraVideoOutputType
    )
    Ltx219BExtendVideoLoraVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraVideoQuality
    )
    Ltx219BExtendVideoLoraAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraAcceleration
    )
    Ltx219BExtendVideoLoraCameraLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraCameraLora
    )
    Ltx219BExtendVideoLoraExtendDirection: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraExtendDirection
    )
    Ltx219BExtendVideoLoraVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraVideoWriteMode
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to extend.",
    )
    use_multiscale: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    loras: list[types.LoRAInput] | OutputHandle[list[types.LoRAInput]] = connect_field(
        default=[], description="The LoRAs to use for the generation."
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="auto", description="The size of the generated video."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3, description="The guidance scale to use."
    )
    camera_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as the end of the extended video.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )
    video_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="Video conditioning strength. Lower values represent more freedom given to the model to change the video content.",
    )
    video_output_type: (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraVideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraVideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraVideoQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraVideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    acceleration: (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraAcceleration
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraAcceleration.REGULAR,
        description="The acceleration level to use.",
    )
    fps: float | OutputHandle[float] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    camera_lora: nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraCameraLora = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraCameraLora.NONE,
            description="The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
        )
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    extend_direction: (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraExtendDirection
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraExtendDirection.FORWARD,
        description="Direction to extend the video. 'forward' extends from the end of the video, 'backward' extends from the beginning.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
        description="The negative prompt to generate the video from.",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLoraVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    num_context_frames: int | OutputHandle[int] = connect_field(
        default=25,
        description="The number of frames to use as context for the extension.",
    )
    audio_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.",
    )
    end_image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the end image to use for the video generation.",
    )
    match_input_fps: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="When true, match the output FPS to the input video's FPS instead of using the default target FPS.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40, description="The number of inference steps to use."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Ltx219BExtendVideoLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Ltx219BVideoToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LTX-2 19B
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Ltx219BVideoToVideoVideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoVideoOutputType
    )
    Ltx219BVideoToVideoVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoVideoQuality
    )
    Ltx219BVideoToVideoAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoAcceleration
    )
    Ltx219BVideoToVideoCameraLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoCameraLora
    )
    Ltx219BVideoToVideoPreprocessor: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoPreprocessor
    )
    Ltx219BVideoToVideoVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoVideoWriteMode
    )
    Ltx219BVideoToVideoIcLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoIcLora
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to generate the video from.",
    )
    use_multiscale: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.",
    )
    ic_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="auto", description="The size of the generated video."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3, description="The guidance scale to use."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as the end of the video.",
    )
    camera_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    video_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="Video conditioning strength. Lower values represent more freedom given to the model to change the video content.",
    )
    video_output_type: (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoVideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoVideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="An optional URL of an image to use as the first frame of the video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoVideoQuality = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoVideoQuality.HIGH,
            description="The quality of the generated video.",
        )
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    match_video_length: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.",
    )
    acceleration: nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoAcceleration = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoAcceleration.REGULAR,
            description="The acceleration level to use.",
        )
    )
    fps: float | OutputHandle[float] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    camera_lora: nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoCameraLora = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoCameraLora.NONE,
            description="The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
        )
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the image to use for the video generation.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
        description="The negative prompt to generate the video from.",
    )
    preprocessor: nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoPreprocessor = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoPreprocessor.NONE,
            description="The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.",
        )
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    ic_lora: nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoIcLora = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoIcLora.MATCH_PREPROCESSOR,
        description="The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)",
    )
    audio: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40, description="The number of inference steps to use."
    )
    end_image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the end image to use for the video generation.",
    )
    audio_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.",
    )
    match_input_fps: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="When true, match the output FPS to the input video's FPS instead of using the default target FPS.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Ltx219BVideoToVideoLora(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LTX-2 19B
    video, editing, video-to-video, vid2vid, lora

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Ltx219BVideoToVideoLoraVideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraVideoOutputType
    )
    Ltx219BVideoToVideoLoraVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraVideoQuality
    )
    Ltx219BVideoToVideoLoraAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraAcceleration
    )
    Ltx219BVideoToVideoLoraCameraLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraCameraLora
    )
    Ltx219BVideoToVideoLoraPreprocessor: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraPreprocessor
    )
    Ltx219BVideoToVideoLoraVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraVideoWriteMode
    )
    Ltx219BVideoToVideoLoraIcLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraIcLora
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to generate the video from.",
    )
    use_multiscale: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.",
    )
    ic_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the IC-LoRA to use. This allows you to control the strength of the IC-LoRA.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    loras: list[types.LoRAInput] | OutputHandle[list[types.LoRAInput]] = connect_field(
        default=[], description="The LoRAs to use for the generation."
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="auto", description="The size of the generated video."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3, description="The guidance scale to use."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as the end of the video.",
    )
    camera_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    video_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="Video conditioning strength. Lower values represent more freedom given to the model to change the video content.",
    )
    video_output_type: (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraVideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraVideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="An optional URL of an image to use as the first frame of the video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraVideoQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraVideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    match_video_length: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="When enabled, the number of frames will be calculated based on the video duration and FPS. When disabled, use the specified num_frames.",
    )
    acceleration: (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraAcceleration
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraAcceleration.REGULAR,
        description="The acceleration level to use.",
    )
    fps: float | OutputHandle[float] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    camera_lora: nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraCameraLora = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraCameraLora.NONE,
            description="The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
        )
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the image to use for the video generation.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
        description="The negative prompt to generate the video from.",
    )
    preprocessor: (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraPreprocessor
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraPreprocessor.NONE,
        description="The preprocessor to use for the video. When a preprocessor is used and `ic_lora_type` is set to `match_preprocessor`, the IC-LoRA will be loaded based on the preprocessor type.",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    ic_lora: nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraIcLora = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLoraIcLora.MATCH_PREPROCESSOR,
        description="The type of IC-LoRA to load. In-Context LoRA weights are used to condition the video based on edge, depth, or pose videos. Only change this from `match_preprocessor` if your videos are already preprocessed (or you are using the detailer.)",
    )
    audio: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="An optional URL of an audio to use as the audio for the video. If not provided, any audio present in the input video will be used.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40, description="The number of inference steps to use."
    )
    end_image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the end image to use for the video generation.",
    )
    audio_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="Audio conditioning strength. Lower values represent more freedom given to the model to change the audio content.",
    )
    match_input_fps: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="When true, match the output FPS to the input video's FPS instead of using the default target FPS.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Ltx219BVideoToVideoLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Ltx2RetakeVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    LTX Video 2.0 Retake
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Ltx2RetakeVideoRetakeMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltx2RetakeVideoRetakeMode
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to retake the video with"
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to retake",
    )
    start_time: float | OutputHandle[float] = connect_field(
        default=0, description="The start time of the video to retake in seconds"
    )
    duration: float | OutputHandle[float] = connect_field(
        default=5, description="The duration of the video to retake in seconds"
    )
    retake_mode: nodetool.nodes.fal.video_to_video.Ltx2RetakeVideoRetakeMode = Field(
        default=nodetool.nodes.fal.video_to_video.Ltx2RetakeVideoRetakeMode.REPLACE_AUDIO_AND_VIDEO,
        description="The retake mode to use for the retake",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Ltx2RetakeVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class LtxVideo13bDevExtend(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Extend videos using LTX Video-0.9.7 13B and custom LoRA
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    LtxVideo13bDevExtendAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LtxVideo13bDevExtendAspectRatio
    )
    LtxVideo13bDevExtendResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LtxVideo13bDevExtendResolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide generation"
    )
    second_pass_skip_initial_steps: int | OutputHandle[int] = connect_field(
        default=17,
        description="The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.",
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.",
    )
    enable_detail_pass_autoregression: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use autoregression in the detail pass. If True, the model will use the previous frame as input for the next frame in the detail pass.",
    )
    frame_rate: int | OutputHandle[int] = connect_field(
        default=24, description="The frame rate of the video."
    )
    temporal_adain_factor: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution.",
    )
    first_pass_number_of_steps: str | OutputHandle[str] = connect_field(
        default="",
        description="Number of inference steps during the first pass. Deprecated. Use `first_pass_num_inference_steps` instead.",
    )
    number_of_frames: str | OutputHandle[str] = connect_field(
        default="",
        description="The number of frames in the video. Deprecated. Use `num_frames` instead.",
    )
    loras: list[types.LoRAWeight] | OutputHandle[list[types.LoRAWeight]] = (
        connect_field(default=[], description="LoRA weights to use for generation")
    )
    second_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="Number of inference steps during the second pass."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames in the video."
    )
    guidance_scale: str | OutputHandle[str] = connect_field(
        default="", description="Deprecated, not used."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Video to be extended.",
    )
    start_frame_num: int | OutputHandle[int] = connect_field(
        default=0,
        description="Frame number of the video from which the conditioning starts. Must be a multiple of 8.",
    )
    limit_num_frames: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.",
    )
    resample_fps: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.0,
        description="Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.",
    )
    target_fps: int | OutputHandle[int] = connect_field(
        default=0,
        description="Target FPS to resample the video to. Only relevant if `resample_fps` is True.",
    )
    max_num_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Maximum number of frames to use from the video. If None, all frames will be used.",
    )
    conditioning_type: str | OutputHandle[str] = connect_field(
        default="",
        description="Type of conditioning this video provides. This is relevant to ensure in-context LoRA weights are applied correctly, as well as selecting the correct preprocessing pipeline, when enabled.",
    )
    preprocess: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to preprocess the video. If True, the video will be preprocessed to match the conditioning type. This is a no-op for RGB conditioning.",
    )
    enable_detail_pass: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.",
    )
    tone_map_compression_ratio: float | OutputHandle[float] = connect_field(
        default=0,
        description="The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default="", description="Random seed for generation"
    )
    detail_pass_skip_initial_steps: int | OutputHandle[int] = connect_field(
        default=7,
        description="The number of inference steps to skip in the initial steps of the detail pass. By skipping some steps at the beginning, the detail pass can focus on smaller details instead of larger changes.",
    )
    second_pass_number_of_steps: str | OutputHandle[str] = connect_field(
        default="",
        description="Number of inference steps during the second pass. Deprecated. Use `second_pass_num_inference_steps` instead.",
    )
    detail_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the detail pass."
    )
    detail_pass_noise_scale: float | OutputHandle[float] = connect_field(
        default=0.125,
        description="The noise scale for the detail pass. This controls the amount of noise added to the generated video during the detail pass. A value of 0.0 means no noise, while a value of 1.0 means maximum noise.",
    )
    first_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="Number of inference steps during the first pass."
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to expand the prompt using a language model.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, inconsistent motion, blurry, jittery, distorted",
        description="Negative prompt for generation",
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.LtxVideo13bDevExtendAspectRatio = (
        Field(
            default=nodetool.nodes.fal.video_to_video.LtxVideo13bDevExtendAspectRatio.AUTO,
            description="The aspect ratio of the video.",
        )
    )
    resolution: nodetool.nodes.fal.video_to_video.LtxVideo13bDevExtendResolution = (
        Field(
            default=nodetool.nodes.fal.video_to_video.LtxVideo13bDevExtendResolution.VALUE_720P,
            description="Resolution of the generated video.",
        )
    )
    constant_rate_factor: int | OutputHandle[int] = connect_field(
        default=29,
        description="The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.",
    )
    first_pass_skip_final_steps: str | OutputHandle[str] = connect_field(
        default="", description="Deprecated. No longer used."
    )
    num_inference_steps: str | OutputHandle[str] = connect_field(
        default="",
        description="Number of inference steps. Deprecated. Use `first_pass_num_inference_steps` instead.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.LtxVideo13bDevExtend

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class LtxVideo13bDevMulticonditioning(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate videos from prompts, images, and videos using LTX Video-0.9.7 13B and custom LoRA
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    LtxVideo13bDevMulticonditioningAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LtxVideo13bDevMulticonditioningAspectRatio
    )
    LtxVideo13bDevMulticonditioningResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LtxVideo13bDevMulticonditioningResolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide generation"
    )
    second_pass_skip_initial_steps: int | OutputHandle[int] = connect_field(
        default=17,
        description="The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.",
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to reverse the video."
    )
    enable_detail_pass_autoregression: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use autoregression in the detail pass. If True, the model will use the previous frame as input for the next frame in the detail pass.",
    )
    frame_rate: int | OutputHandle[int] = connect_field(
        default=24, description="The frame rate of the video."
    )
    temporal_adain_factor: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution.",
    )
    first_pass_number_of_steps: str | OutputHandle[str] = connect_field(
        default="",
        description="Number of inference steps during the first pass. Deprecated. Use `first_pass_num_inference_steps` instead.",
    )
    number_of_frames: str | OutputHandle[str] = connect_field(
        default="",
        description="The number of frames in the video. Deprecated. Use `num_frames` instead.",
    )
    loras: list[types.LoRAWeight] | OutputHandle[list[types.LoRAWeight]] = (
        connect_field(default=[], description="LoRA weights to use for generation")
    )
    images: (
        list[types.ImageConditioningInput]
        | OutputHandle[list[types.ImageConditioningInput]]
    ) = connect_field(default=[], description="URL of images to use as conditioning")
    second_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="Number of inference steps during the second pass."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames in the video."
    )
    guidance_scale: str | OutputHandle[str] = connect_field(
        default="", description="Deprecated, not used."
    )
    enable_detail_pass: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.",
    )
    tone_map_compression_ratio: float | OutputHandle[float] = connect_field(
        default=0,
        description="The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default="", description="Random seed for generation"
    )
    detail_pass_skip_initial_steps: int | OutputHandle[int] = connect_field(
        default=7,
        description="The number of inference steps to skip in the initial steps of the detail pass. By skipping some steps at the beginning, the detail pass can focus on smaller details instead of larger changes.",
    )
    second_pass_number_of_steps: str | OutputHandle[str] = connect_field(
        default="",
        description="Number of inference steps during the second pass. Deprecated. Use `second_pass_num_inference_steps` instead.",
    )
    detail_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the detail pass."
    )
    detail_pass_noise_scale: float | OutputHandle[float] = connect_field(
        default=0.125,
        description="The noise scale for the detail pass. This controls the amount of noise added to the generated video during the detail pass. A value of 0.0 means no noise, while a value of 1.0 means maximum noise.",
    )
    first_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="Number of inference steps during the first pass."
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to expand the prompt using a language model.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, inconsistent motion, blurry, jittery, distorted",
        description="Negative prompt for generation",
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.LtxVideo13bDevMulticonditioningAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.LtxVideo13bDevMulticonditioningAspectRatio.AUTO,
        description="The aspect ratio of the video.",
    )
    resolution: (
        nodetool.nodes.fal.video_to_video.LtxVideo13bDevMulticonditioningResolution
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.LtxVideo13bDevMulticonditioningResolution.VALUE_720P,
        description="Resolution of the generated video.",
    )
    videos: (
        list[types.VideoConditioningInput]
        | OutputHandle[list[types.VideoConditioningInput]]
    ) = connect_field(default=[], description="Videos to use as conditioning")
    constant_rate_factor: int | OutputHandle[int] = connect_field(
        default=29,
        description="The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.",
    )
    first_pass_skip_final_steps: str | OutputHandle[str] = connect_field(
        default="", description="Deprecated. No longer used."
    )
    num_inference_steps: str | OutputHandle[str] = connect_field(
        default="",
        description="Number of inference steps. Deprecated. Use `first_pass_num_inference_steps` instead.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.LtxVideo13bDevMulticonditioning

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class LtxVideo13bDistilledExtend(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Extend videos using LTX Video-0.9.7 13B Distilled and custom LoRA
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    LtxVideo13bDistilledExtendResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledExtendResolution
    )
    LtxVideo13bDistilledExtendAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledExtendAspectRatio
    )

    second_pass_skip_initial_steps: int | OutputHandle[int] = connect_field(
        default=5,
        description="The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.",
    )
    first_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the first pass."
    )
    frame_rate: int | OutputHandle[int] = connect_field(
        default=30, description="The frame rate of the video."
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide generation"
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to expand the prompt using a language model.",
    )
    loras: list[types.LoRAWeight] | OutputHandle[list[types.LoRAWeight]] = (
        connect_field(default=[], description="LoRA weights to use for generation")
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames in the video."
    )
    second_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the second pass."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Video to be extended.",
    )
    start_frame_num: int | OutputHandle[int] = connect_field(
        default=0,
        description="Frame number of the video from which the conditioning starts. Must be a multiple of 8.",
    )
    limit_num_frames: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.",
    )
    resample_fps: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.0,
        description="Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.",
    )
    target_fps: int | OutputHandle[int] = connect_field(
        default=0,
        description="Target FPS to resample the video to. Only relevant if `resample_fps` is True.",
    )
    max_num_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Maximum number of frames to use from the video. If None, all frames will be used.",
    )
    conditioning_type: str | OutputHandle[str] = connect_field(
        default="",
        description="Type of conditioning this video provides. This is relevant to ensure in-context LoRA weights are applied correctly, as well as selecting the correct preprocessing pipeline, when enabled.",
    )
    preprocess: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to preprocess the video. If True, the video will be preprocessed to match the conditioning type. This is a no-op for RGB conditioning.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, inconsistent motion, blurry, jittery, distorted",
        description="Negative prompt for generation",
    )
    resolution: (
        nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledExtendResolution
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledExtendResolution.VALUE_720P,
        description="Resolution of the generated video (480p or 720p).",
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledExtendAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledExtendAspectRatio.AUTO,
        description="The aspect ratio of the video.",
    )
    constant_rate_factor: int | OutputHandle[int] = connect_field(
        default=35,
        description="The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.",
    )
    first_pass_skip_final_steps: int | OutputHandle[int] = connect_field(
        default=1,
        description="Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledExtend

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class LtxVideo13bDistilledMulticonditioning(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate videos from prompts, images, and videos using LTX Video-0.9.7 13B Distilled and custom LoRA
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    LtxVideo13bDistilledMulticonditioningResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledMulticonditioningResolution
    )
    LtxVideo13bDistilledMulticonditioningAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledMulticonditioningAspectRatio
    )

    second_pass_skip_initial_steps: int | OutputHandle[int] = connect_field(
        default=5,
        description="The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.",
    )
    first_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the first pass."
    )
    frame_rate: int | OutputHandle[int] = connect_field(
        default=30, description="The frame rate of the video."
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to reverse the video."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide generation"
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to expand the prompt using a language model.",
    )
    loras: list[types.LoRAWeight] | OutputHandle[list[types.LoRAWeight]] = (
        connect_field(default=[], description="LoRA weights to use for generation")
    )
    images: (
        list[types.ImageConditioningInput]
        | OutputHandle[list[types.ImageConditioningInput]]
    ) = connect_field(default=[], description="URL of images to use as conditioning")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames in the video."
    )
    second_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the second pass."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, inconsistent motion, blurry, jittery, distorted",
        description="Negative prompt for generation",
    )
    resolution: (
        nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledMulticonditioningResolution
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledMulticonditioningResolution.VALUE_720P,
        description="Resolution of the generated video (480p or 720p).",
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledMulticonditioningAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledMulticonditioningAspectRatio.AUTO,
        description="The aspect ratio of the video.",
    )
    constant_rate_factor: int | OutputHandle[int] = connect_field(
        default=35,
        description="The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.",
    )
    videos: (
        list[types.VideoConditioningInput]
        | OutputHandle[list[types.VideoConditioningInput]]
    ) = connect_field(default=[], description="Videos to use as conditioning")
    first_pass_skip_final_steps: int | OutputHandle[int] = connect_field(
        default=1,
        description="Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.LtxVideo13bDistilledMulticonditioning

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class LtxVideoLoraMulticonditioning(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate videos from prompts, images, and videos using LTX Video-0.9.7 and custom LoRA
    video, editing, video-to-video, vid2vid, lora

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    LtxVideoLoraMulticonditioningAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LtxVideoLoraMulticonditioningAspectRatio
    )
    LtxVideoLoraMulticonditioningResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LtxVideoLoraMulticonditioningResolution
    )

    number_of_steps: int | OutputHandle[int] = connect_field(
        default=30, description="The number of inference steps to use."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to reverse the video."
    )
    frame_rate: int | OutputHandle[int] = connect_field(
        default=25, description="The frame rate of the video."
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to expand the prompt using the LLM."
    )
    number_of_frames: int | OutputHandle[int] = connect_field(
        default=89, description="The number of frames in the video."
    )
    loras: list[types.LoRAWeight] | OutputHandle[list[types.LoRAWeight]] = (
        connect_field(default=[], description="The LoRA weights to use for generation.")
    )
    images: list[types.ImageCondition] | OutputHandle[list[types.ImageCondition]] = (
        connect_field(
            default=[], description="The image conditions to use for generation."
        )
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, low quality, low resolution, inconsistent motion, jittery, distorted",
        description="The negative prompt to use.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.LtxVideoLoraMulticonditioningAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.LtxVideoLoraMulticonditioningAspectRatio.AUTO,
        description="The aspect ratio of the video.",
    )
    resolution: (
        nodetool.nodes.fal.video_to_video.LtxVideoLoraMulticonditioningResolution
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.LtxVideoLoraMulticonditioningResolution.VALUE_720P,
        description="The resolution of the video.",
    )
    videos: list[types.VideoCondition] | OutputHandle[list[types.VideoCondition]] = (
        connect_field(
            default=[], description="The video conditions to use for generation."
        )
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed to use for generation."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.LtxVideoLoraMulticonditioning

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class LtxVideoV095Extend(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate videos from prompts and videos using LTX Video-0.9.5
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    LtxVideoV095ExtendResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LtxVideoV095ExtendResolution
    )
    LtxVideoV095ExtendAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LtxVideoV095ExtendAspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide generation"
    )
    resolution: nodetool.nodes.fal.video_to_video.LtxVideoV095ExtendResolution = Field(
        default=nodetool.nodes.fal.video_to_video.LtxVideoV095ExtendResolution.VALUE_720P,
        description="Resolution of the generated video (480p or 720p).",
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.LtxVideoV095ExtendAspectRatio = (
        Field(
            default=nodetool.nodes.fal.video_to_video.LtxVideoV095ExtendAspectRatio.RATIO_16_9,
            description="Aspect ratio of the generated video (16:9 or 9:16).",
        )
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to expand the prompt using the model's own capabilities.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40, description="Number of inference steps"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, inconsistent motion, blurry, jittery, distorted",
        description="Negative prompt for generation",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Video to be extended.",
    )
    start_frame_num: int | OutputHandle[int] = connect_field(
        default=0,
        description="Frame number of the video from which the conditioning starts. Must be a multiple of 8.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.LtxVideoV095Extend

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class LtxVideoV095Multiconditioning(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate videos from prompts,images, and videos using LTX Video-0.9.5
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    LtxVideoV095MulticonditioningResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LtxVideoV095MulticonditioningResolution
    )
    LtxVideoV095MulticonditioningAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LtxVideoV095MulticonditioningAspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide generation"
    )
    resolution: (
        nodetool.nodes.fal.video_to_video.LtxVideoV095MulticonditioningResolution
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.LtxVideoV095MulticonditioningResolution.VALUE_720P,
        description="Resolution of the generated video (480p or 720p).",
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.LtxVideoV095MulticonditioningAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.LtxVideoV095MulticonditioningAspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video (16:9 or 9:16).",
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to expand the prompt using the model's own capabilities.",
    )
    images: (
        list[types.ImageConditioningInput]
        | OutputHandle[list[types.ImageConditioningInput]]
    ) = connect_field(default=[], description="URL of images to use as conditioning")
    videos: (
        list[types.VideoConditioningInput]
        | OutputHandle[list[types.VideoConditioningInput]]
    ) = connect_field(default=[], description="Videos to use as conditioning")
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40, description="Number of inference steps"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, inconsistent motion, blurry, jittery, distorted",
        description="Negative prompt for generation",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.LtxVideoV095Multiconditioning

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Ltxv13b098DistilledExtend(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Extend videos using LTX Video-0.9.8 13B Distilled and custom LoRA
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Ltxv13b098DistilledExtendResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledExtendResolution
    )
    Ltxv13b098DistilledExtendAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledExtendAspectRatio
    )

    second_pass_skip_initial_steps: int | OutputHandle[int] = connect_field(
        default=5,
        description="The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.",
    )
    first_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the first pass."
    )
    frame_rate: int | OutputHandle[int] = connect_field(
        default=24, description="The frame rate of the video."
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to reverse the video. This is useful for tasks where the video conditioning should be applied in reverse order.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide generation"
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to expand the prompt using a language model.",
    )
    temporal_adain_factor: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution.",
    )
    loras: list[types.LoRAWeight] | OutputHandle[list[types.LoRAWeight]] = (
        connect_field(default=[], description="LoRA weights to use for generation")
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames in the video."
    )
    second_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the second pass."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, inconsistent motion, blurry, jittery, distorted",
        description="Negative prompt for generation",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Video to be extended.",
    )
    start_frame_num: int | OutputHandle[int] = connect_field(
        default=0,
        description="Frame number of the video from which the conditioning starts. Must be a multiple of 8.",
    )
    limit_num_frames: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to limit the number of frames used from the video. If True, the `max_num_frames` parameter will be used to limit the number of frames.",
    )
    resample_fps: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to resample the video to a specific FPS. If True, the `target_fps` parameter will be used to resample the video.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.0,
        description="Strength of the conditioning. 0.0 means no conditioning, 1.0 means full conditioning.",
    )
    target_fps: int | OutputHandle[int] = connect_field(
        default=0,
        description="Target FPS to resample the video to. Only relevant if `resample_fps` is True.",
    )
    max_num_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Maximum number of frames to use from the video. If None, all frames will be used.",
    )
    enable_detail_pass: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.",
    )
    resolution: (
        nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledExtendResolution
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledExtendResolution.VALUE_720P,
        description="Resolution of the generated video.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledExtendAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledExtendAspectRatio.AUTO,
        description="The aspect ratio of the video.",
    )
    tone_map_compression_ratio: float | OutputHandle[float] = connect_field(
        default=0,
        description="The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.",
    )
    constant_rate_factor: int | OutputHandle[int] = connect_field(
        default=29,
        description="The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledExtend

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Ltxv13b098DistilledMulticonditioning(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate long videos from prompts, images, and videos using LTX Video-0.9.8 13B Distilled and custom LoRA
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Ltxv13b098DistilledMulticonditioningResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledMulticonditioningResolution
    )
    Ltxv13b098DistilledMulticonditioningAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledMulticonditioningAspectRatio
    )

    second_pass_skip_initial_steps: int | OutputHandle[int] = connect_field(
        default=5,
        description="The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.",
    )
    first_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the first pass."
    )
    frame_rate: int | OutputHandle[int] = connect_field(
        default=24, description="The frame rate of the video."
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to reverse the video."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide generation"
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to expand the prompt using a language model.",
    )
    temporal_adain_factor: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution.",
    )
    loras: list[types.LoRAWeight] | OutputHandle[list[types.LoRAWeight]] = (
        connect_field(default=[], description="LoRA weights to use for generation")
    )
    images: (
        list[types.ImageConditioningInput]
        | OutputHandle[list[types.ImageConditioningInput]]
    ) = connect_field(default=[], description="URL of images to use as conditioning")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames in the video."
    )
    second_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the second pass."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, inconsistent motion, blurry, jittery, distorted",
        description="Negative prompt for generation",
    )
    enable_detail_pass: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.",
    )
    resolution: (
        nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledMulticonditioningResolution
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledMulticonditioningResolution.VALUE_720P,
        description="Resolution of the generated video.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledMulticonditioningAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledMulticonditioningAspectRatio.AUTO,
        description="The aspect ratio of the video.",
    )
    tone_map_compression_ratio: float | OutputHandle[float] = connect_field(
        default=0,
        description="The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.",
    )
    videos: (
        list[types.VideoConditioningInput]
        | OutputHandle[list[types.VideoConditioningInput]]
    ) = connect_field(default=[], description="Videos to use as conditioning")
    constant_rate_factor: int | OutputHandle[int] = connect_field(
        default=29,
        description="The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Ltxv13b098DistilledMulticonditioning

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class LumaDreamMachineRay2FlashModify(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Ray2 Flash Modify is a video generative model capable of restyling or retexturing the entire shot, from turning live-action into CG or stylized animation, to changing wardrobe, props, or the overall aesthetic and swap environments or time periods, giving you control over background, location, or even weather.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    LumaDreamMachineRay2FlashModifyMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2FlashModifyMode
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Instruction for modifying the video"
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video to modify",
    )
    mode: nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2FlashModifyMode = Field(
        default=nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2FlashModifyMode.FLEX_1,
        description="Amount of modification to apply to the video, adhere_1 is the least amount of modification, reimagine_3 is the most",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Optional URL of the first frame image for modification",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2FlashModify

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class LumaDreamMachineRay2FlashReframe(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Adjust and enhance videos with Ray-2 Reframe. This advanced tool seamlessly reframes videos to your desired aspect ratio, intelligently inpainting missing regions to ensure realistic visuals and coherent motion, delivering exceptional quality and creative flexibility.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    LumaDreamMachineRay2FlashReframeAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2FlashReframeAspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Optional prompt for reframing"
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2FlashReframeAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2FlashReframeAspectRatio(
            ""
        ),
        description="The aspect ratio of the reframed video",
    )
    y_start: int | OutputHandle[int] = connect_field(
        default=0, description="Start Y coordinate for reframing"
    )
    x_end: int | OutputHandle[int] = connect_field(
        default=0, description="End X coordinate for reframing"
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video to reframe",
    )
    y_end: int | OutputHandle[int] = connect_field(
        default=0, description="End Y coordinate for reframing"
    )
    x_start: int | OutputHandle[int] = connect_field(
        default=0, description="Start X coordinate for reframing"
    )
    grid_position_y: int | OutputHandle[int] = connect_field(
        default=0, description="Y position of the grid for reframing"
    )
    grid_position_x: int | OutputHandle[int] = connect_field(
        default=0, description="X position of the grid for reframing"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Optional URL of the first frame image for reframing",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2FlashReframe

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class LumaDreamMachineRay2Modify(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Ray2 Modify is a video generative model capable of restyling or retexturing the entire shot, from turning live-action into CG or stylized animation, to changing wardrobe, props, or the overall aesthetic and swap environments or time periods, giving you control over background, location, or even weather.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    LumaDreamMachineRay2ModifyMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2ModifyMode
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Instruction for modifying the video"
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video to modify",
    )
    mode: nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2ModifyMode = Field(
        default=nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2ModifyMode.FLEX_1,
        description="Amount of modification to apply to the video, adhere_1 is the least amount of modification, reimagine_3 is the most",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Optional URL of the first frame image for modification",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2Modify

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class LumaDreamMachineRay2Reframe(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Adjust and enhance videos with Ray-2 Reframe. This advanced tool seamlessly reframes videos to your desired aspect ratio, intelligently inpainting missing regions to ensure realistic visuals and coherent motion, delivering exceptional quality and creative flexibility.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    LumaDreamMachineRay2ReframeAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2ReframeAspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Optional prompt for reframing"
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2ReframeAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2ReframeAspectRatio(
            ""
        ),
        description="The aspect ratio of the reframed video",
    )
    y_start: int | OutputHandle[int] = connect_field(
        default=0, description="Start Y coordinate for reframing"
    )
    x_end: int | OutputHandle[int] = connect_field(
        default=0, description="End X coordinate for reframing"
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video to reframe",
    )
    y_end: int | OutputHandle[int] = connect_field(
        default=0, description="End Y coordinate for reframing"
    )
    x_start: int | OutputHandle[int] = connect_field(
        default=0, description="Start X coordinate for reframing"
    )
    grid_position_y: int | OutputHandle[int] = connect_field(
        default=0, description="Y position of the grid for reframing"
    )
    grid_position_x: int | OutputHandle[int] = connect_field(
        default=0, description="X position of the grid for reframing"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Optional URL of the first frame image for reframing",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.LumaDreamMachineRay2Reframe

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class MireloAiSfxV15VideoToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Mirelo SFX V1.5
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    num_samples: str | OutputHandle[str] = connect_field(
        default=2, description="The number of samples to generate from the model"
    )
    duration: str | OutputHandle[str] = connect_field(
        default=10, description="The duration of the generated audio in seconds"
    )
    start_offset: str | OutputHandle[str] = connect_field(
        default=0,
        description="The start offset in seconds to start the audio generation from",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="A video url that can accessed from the API to process and add sound effects",
    )
    seed: str | OutputHandle[str] = connect_field(
        default=8069,
        description="The seed to use for the generation. If not provided, a random seed will be used",
    )
    text_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Additional description to guide the model"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.MireloAiSfxV15VideoToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class MireloAiSfxV1VideoToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate synced sounds for any video, and return it with its new sound track (like MMAudio)
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    num_samples: str | OutputHandle[str] = connect_field(
        default=2, description="The number of samples to generate from the model"
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="A video url that can accessed from the API to process and add sound effects",
    )
    duration: str | OutputHandle[str] = connect_field(
        default=10, description="The duration of the generated audio in seconds"
    )
    seed: str | OutputHandle[str] = connect_field(
        default=2105,
        description="The seed to use for the generation. If not provided, a random seed will be used",
    )
    text_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Additional description to guide the model"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.MireloAiSfxV1VideoToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class MoonvalleyMareyMotionTransfer(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Pull motion from a reference video and apply it to new subjects or scenes.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate a video from"
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to use as the control video.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default=-1,
        description="Seed for random number generation. Use -1 for random seed each run.",
    )
    reference_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Optional reference image URL to use for pose control or as a starting frame",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="<synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts",
        description="Negative prompt used to guide the model away from undesirable features.",
    )
    first_frame_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Optional first frame image URL to use as the first frame of the generated video",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.MoonvalleyMareyMotionTransfer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class MoonvalleyMareyPoseTransfer(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Ideal for matching human movement. Your input video determines human poses, gestures, and body movements that will appear in the generated video.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate a video from"
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to use as the control video.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default=-1,
        description="Seed for random number generation. Use -1 for random seed each run.",
    )
    reference_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Optional reference image URL to use for pose control or as a starting frame",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="<synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts",
        description="Negative prompt used to guide the model away from undesirable features.",
    )
    first_frame_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Optional first frame image URL to use as the first frame of the generated video",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.MoonvalleyMareyPoseTransfer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class OneToAllAnimation13B(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    One To All Animation
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    OneToAllAnimation13BResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.OneToAllAnimation13BResolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    resolution: nodetool.nodes.fal.video_to_video.OneToAllAnimation13BResolution = (
        Field(
            default=nodetool.nodes.fal.video_to_video.OneToAllAnimation13BResolution.VALUE_480P,
            description="The resolution of the video to generate.",
        )
    )
    image_guidance_scale: float | OutputHandle[float] = connect_field(
        default=2,
        description="The image guidance scale to use for the video generation.",
    )
    pose_guidance_scale: float | OutputHandle[float] = connect_field(
        default=1.5,
        description="The pose guidance scale to use for the video generation.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to use as a reference for the video generation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as a reference for the video generation.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="The number of inference steps to use for the video generation.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="The negative prompt to generate the video from."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.OneToAllAnimation13B

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class OneToAllAnimation14B(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    One To All Animation
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    OneToAllAnimation14BResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.OneToAllAnimation14BResolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    resolution: nodetool.nodes.fal.video_to_video.OneToAllAnimation14BResolution = (
        Field(
            default=nodetool.nodes.fal.video_to_video.OneToAllAnimation14BResolution.VALUE_480P,
            description="The resolution of the video to generate.",
        )
    )
    image_guidance_scale: float | OutputHandle[float] = connect_field(
        default=2,
        description="The image guidance scale to use for the video generation.",
    )
    pose_guidance_scale: float | OutputHandle[float] = connect_field(
        default=1.5,
        description="The pose guidance scale to use for the video generation.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to use as a reference for the video generation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as a reference for the video generation.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="The number of inference steps to use for the video generation.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="The negative prompt to generate the video from."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.OneToAllAnimation14B

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class PikaV2Pikadditions(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Pikadditions is a powerful video-to-video AI model that allows you to add anyone or anything to any video with seamless integration.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt describing what to add"
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video",
    )
    seed: str | OutputHandle[str] = connect_field(
        default="", description="The seed for the random number generator"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to guide the model"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to add",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.PikaV2Pikadditions

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseExtend(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    PixVerse Extend model is a video extending tool for your videos using with high-quality video extending techniques
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    PixverseExtendResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.PixverseExtendResolution
    )
    PixverseExtendDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.PixverseExtendDuration
    )
    PixverseExtendModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.PixverseExtendModel
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Prompt describing how to extend the video"
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video to extend",
    )
    resolution: nodetool.nodes.fal.video_to_video.PixverseExtendResolution = Field(
        default=nodetool.nodes.fal.video_to_video.PixverseExtendResolution.VALUE_720P,
        description="The resolution of the generated video",
    )
    style: (
        nodetool.nodes.fal.video_to_video.PixverseExtendStyle
        | OutputHandle[nodetool.nodes.fal.video_to_video.PixverseExtendStyle]
        | None
    ) = connect_field(default=None, description="The style of the extended video")
    duration: nodetool.nodes.fal.video_to_video.PixverseExtendDuration = Field(
        default=nodetool.nodes.fal.video_to_video.PixverseExtendDuration.VALUE_5,
        description="The duration of the generated video in seconds. 1080p videos are limited to 5 seconds",
    )
    model: nodetool.nodes.fal.video_to_video.PixverseExtendModel = Field(
        default=nodetool.nodes.fal.video_to_video.PixverseExtendModel.V4_5,
        description="The model version to use for generation",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.PixverseExtend

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseExtendFast(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    PixVerse Extend model is a video extending tool for your videos using with high-quality video extending techniques
    video, editing, video-to-video, vid2vid, fast

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    PixverseExtendFastResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.PixverseExtendFastResolution
    )
    PixverseExtendFastModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.PixverseExtendFastModel
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Prompt describing how to extend the video"
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video to extend",
    )
    resolution: nodetool.nodes.fal.video_to_video.PixverseExtendFastResolution = Field(
        default=nodetool.nodes.fal.video_to_video.PixverseExtendFastResolution.VALUE_720P,
        description="The resolution of the generated video. Fast mode doesn't support 1080p",
    )
    style: (
        nodetool.nodes.fal.video_to_video.PixverseExtendFastStyle
        | OutputHandle[nodetool.nodes.fal.video_to_video.PixverseExtendFastStyle]
        | None
    ) = connect_field(default=None, description="The style of the extended video")
    model: nodetool.nodes.fal.video_to_video.PixverseExtendFastModel = Field(
        default=nodetool.nodes.fal.video_to_video.PixverseExtendFastModel.V4_5,
        description="The model version to use for generation",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.PixverseExtendFast

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseLipsync(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate realistic lipsync animations from audio using advanced algorithms for high-quality synchronization with PixVerse Lipsync model
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    PixverseLipsyncVoiceId: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.PixverseLipsyncVoiceId
    )

    text: str | OutputHandle[str] = connect_field(
        default="", description="Text content for TTS when audio_url is not provided"
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video",
    )
    voice_id: nodetool.nodes.fal.video_to_video.PixverseLipsyncVoiceId = Field(
        default=nodetool.nodes.fal.video_to_video.PixverseLipsyncVoiceId.AUTO,
        description="Voice to use for TTS when audio_url is not provided",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input audio. If not provided, TTS will be used.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.PixverseLipsync

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseSoundEffects(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Add immersive sound effects and background music to your videos using PixVerse sound effects  generation
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Description of the sound effect to generate. If empty, a random sound effect will be generated",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video to add sound effects to",
    )
    original_sound_switch: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to keep the original audio from the video"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.PixverseSoundEffects

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class RifeVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Interpolate videos with RIFE - Real-Time Intermediate Flow Estimation
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to use for interpolation.",
    )
    use_scene_detection: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If True, the input video will be split into scenes before interpolation. This removes smear frames between scenes, but can result in false positives if the scene detection is not accurate. If False, the entire video will be treated as a single scene.",
    )
    loop: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If True, the final frame will be looped back to the first frame to create a seamless loop. If False, the final frame will not loop back.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=1,
        description="The number of frames to generate between the input video frames.",
    )
    use_calculated_fps: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If True, the function will use the calculated FPS of the input video multiplied by the number of frames to determine the output FPS. If False, the passed FPS will be used.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=8,
        description="Frames per second for the output video. Only applicable if use_calculated_fps is False.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.RifeVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Sam2Video(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    SAM 2 is a model for segmenting images and videos in real-time.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to be segmented.",
    )
    prompts: list[types.PointPrompt] | OutputHandle[list[types.PointPrompt]] = (
        connect_field(default=[], description="List of prompts to segment the video")
    )
    boundingbox_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Return per-frame bounding box overlays as a zip archive.",
    )
    box_prompts: list[types.BoxPrompt] | OutputHandle[list[types.BoxPrompt]] = (
        connect_field(default=[], description="Coordinates for boxes")
    )
    apply_mask: bool | OutputHandle[bool] = connect_field(
        default=False, description="Apply the mask on the video."
    )
    mask_url: str | OutputHandle[str] = connect_field(
        default="", description="The URL of the mask to be applied initially."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Sam2Video

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Sam3Video(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Sam 3
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth').",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to be segmented.",
    )
    detection_threshold: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="Detection confidence threshold (0.0-1.0). Lower = more detections but less precise.",
    )
    box_prompts: list[types.BoxPromptBase] | OutputHandle[list[types.BoxPromptBase]] = (
        connect_field(
            default=[],
            description="List of box prompt coordinates (x_min, y_min, x_max, y_max).",
        )
    )
    point_prompts: (
        list[types.PointPromptBase] | OutputHandle[list[types.PointPromptBase]]
    ) = connect_field(default=[], description="List of point prompts")
    apply_mask: bool | OutputHandle[bool] = connect_field(
        default=True, description="Apply the mask on the video."
    )
    text_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="[DEPRECATED] Use 'prompt' instead. Kept for backward compatibility.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Sam3Video

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Sam3VideoRle(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Sam 3
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Sam3VideoRleRleReturnMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Sam3VideoRleRleReturnMode
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for segmentation. Use commas to track multiple objects (e.g., 'person, cloth').",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to be segmented.",
    )
    detection_threshold: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="Detection confidence threshold (0.0-1.0). Lower = more detections but less precise. Defaults: 0.5 for existing, 0.7 for new objects. Try 0.2-0.3 if text prompts fail.",
    )
    rle_return_mode: nodetool.nodes.fal.video_to_video.Sam3VideoRleRleReturnMode = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Sam3VideoRleRleReturnMode.LIST,
            description="Mode of returning rles.",
        )
    )
    mask_url: str | OutputHandle[str] = connect_field(
        default="", description="The URL of the mask to be applied initially."
    )
    box_prompts: list[types.BoxPrompt] | OutputHandle[list[types.BoxPrompt]] = (
        connect_field(
            default=[], description="List of box prompts with optional frame_index."
        )
    )
    point_prompts: list[types.PointPrompt] | OutputHandle[list[types.PointPrompt]] = (
        connect_field(
            default=[], description="List of point prompts with frame indices."
        )
    )
    boundingbox_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Return per-frame bounding box overlays as a zip archive.",
    )
    frame_index: int | OutputHandle[int] = connect_field(
        default=0,
        description="Frame index used for initial interaction when mask_url is provided.",
    )
    return_rle: bool | OutputHandle[bool] = connect_field(
        default=False, description="Return the Run Length Encoding of the mask."
    )
    apply_mask: bool | OutputHandle[bool] = connect_field(
        default=False, description="Apply the mask on the video."
    )
    text_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="[DEPRECATED] Use 'prompt' instead. Kept for backward compatibility.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Sam3VideoRle

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Scail(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Scail
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    ScailResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.ScailResolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to guide video generation."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to use as a reference for the video generation.",
    )
    resolution: nodetool.nodes.fal.video_to_video.ScailResolution = Field(
        default=nodetool.nodes.fal.video_to_video.ScailResolution.VALUE_512P,
        description="Output resolution. Outputs 896x512 (landscape) or 512x896 (portrait) based on the input image aspect ratio.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28,
        description="The number of inference steps to use for the video generation.",
    )
    multi_character: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable multi-character mode. Use when driving video has multiple people.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as a reference for the video generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Scail

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class SeedvrUpscaleVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    SeedVR2
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    SeedvrUpscaleVideoUpscaleMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideoUpscaleMode
    )
    SeedvrUpscaleVideoOutputFormat: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideoOutputFormat
    )
    SeedvrUpscaleVideoOutputWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideoOutputWriteMode
    )
    SeedvrUpscaleVideoTargetResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideoTargetResolution
    )
    SeedvrUpscaleVideoOutputQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideoOutputQuality
    )

    upscale_mode: nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideoUpscaleMode = (
        Field(
            default=nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideoUpscaleMode.FACTOR,
            description="The mode to use for the upscale. If 'target', the upscale factor will be calculated based on the target resolution. If 'factor', the upscale factor will be used directly.",
        )
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The input video to be processed",
    )
    noise_scale: float | OutputHandle[float] = connect_field(
        default=0.1, description="The noise scale to use for the generation process."
    )
    output_format: nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideoOutputFormat = (
        Field(
            default=nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideoOutputFormat.X264_MP4,
            description="The format of the output video.",
        )
    )
    output_write_mode: (
        nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideoOutputWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideoOutputWriteMode.BALANCED,
        description="The write mode of the output video.",
    )
    target_resolution: (
        nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideoTargetResolution
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideoTargetResolution.VALUE_1080P,
        description="The target resolution to upscale to when `upscale_mode` is `target`.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    output_quality: (
        nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideoOutputQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideoOutputQuality.HIGH,
        description="The quality of the output video.",
    )
    upscale_factor: float | OutputHandle[float] = connect_field(
        default=2,
        description="Upscaling factor to be used. Will multiply the dimensions with this factor when `upscale_mode` is `factor`.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default="", description="The random seed used for the generation process."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.SeedvrUpscaleVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Sora2VideoToVideoRemix(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Sora 2
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Updated text prompt that directs the remix generation"
    )
    video_id: str | OutputHandle[str] = connect_field(
        default="",
        description="The video_id from a previous Sora 2 generation. Note: You can only remix videos that were generated by Sora (via text-to-video or image-to-video endpoints), not arbitrary uploaded videos.",
    )
    delete_video: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to delete the video after generation for privacy reasons. If True, the video cannot be used for remixing and will be permanently deleted.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Sora2VideoToVideoRemix

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class SteadyDancer(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Steady Dancer
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    SteadyDancerAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.SteadyDancerAcceleration
    )
    SteadyDancerAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.SteadyDancerAspectRatio
    )
    SteadyDancerResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.SteadyDancerResolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="A person dancing with smooth and natural movements.",
        description="Text prompt describing the desired animation.",
    )
    video: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the driving pose video. The motion from this video will be transferred to the reference image.",
    )
    acceleration: nodetool.nodes.fal.video_to_video.SteadyDancerAcceleration = Field(
        default=nodetool.nodes.fal.video_to_video.SteadyDancerAcceleration.AGGRESSIVE,
        description="Acceleration levels.",
    )
    pose_guidance_scale: float | OutputHandle[float] = connect_field(
        default=1, description="Pose guidance scale for pose control strength."
    )
    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    pose_guidance_end: float | OutputHandle[float] = connect_field(
        default=0.4,
        description="End ratio for pose guidance. Controls when pose guidance ends.",
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=0,
        description="Frames per second of the generated video. Must be between 5 to 24. If not specified, uses the FPS from the input video.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=1, description="Classifier-free guidance scale for prompt adherence."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Number of frames to generate. If not specified, uses the frame count from the input video (capped at 241). Will be adjusted to nearest valid value (must satisfy 4k+1 pattern).",
    )
    use_turbo: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized (num_inference_steps=6, guidance_scale=1.0) and uses the LightX2V distillation LoRA.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurred, distorted face, bad anatomy, extra limbs, poorly drawn hands, poorly drawn feet, disfigured, out of frame, duplicate, watermark, signature, text",
        description="Negative prompt for video generation.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.SteadyDancerAspectRatio = Field(
        default=nodetool.nodes.fal.video_to_video.SteadyDancerAspectRatio.AUTO,
        description="Aspect ratio of the generated video. If 'auto', will be determined from the reference image.",
    )
    pose_guidance_start: float | OutputHandle[float] = connect_field(
        default=0.1,
        description="Start ratio for pose guidance. Controls when pose guidance begins.",
    )
    resolution: nodetool.nodes.fal.video_to_video.SteadyDancerResolution = Field(
        default=nodetool.nodes.fal.video_to_video.SteadyDancerResolution.VALUE_576P,
        description="Resolution of the generated video. 576p is default, 720p for higher quality. 480p is lower quality.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the reference image to animate. This is the person/character whose appearance will be preserved.",
    )
    preserve_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If enabled, copies audio from the input driving video to the output video.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=6,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.SteadyDancer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class SyncLipsync(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate realistic lipsync animations from audio using advanced algorithms for high-quality synchronization.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    SyncLipsyncSyncMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.SyncLipsyncSyncMode
    )
    SyncLipsyncModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.SyncLipsyncModel
    )

    sync_mode: nodetool.nodes.fal.video_to_video.SyncLipsyncSyncMode = Field(
        default=nodetool.nodes.fal.video_to_video.SyncLipsyncSyncMode.CUT_OFF,
        description="Lipsync mode when audio and video durations are out of sync.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video",
    )
    model: nodetool.nodes.fal.video_to_video.SyncLipsyncModel = Field(
        default=nodetool.nodes.fal.video_to_video.SyncLipsyncModel.LIPSYNC_1_9_0_BETA,
        description="The model to use for lipsyncing",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input audio",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.SyncLipsync

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class SyncLipsyncReact1(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Sync React-1
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    SyncLipsyncReact1ModelMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.SyncLipsyncReact1ModelMode
    )
    SyncLipsyncReact1LipsyncMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.SyncLipsyncReact1LipsyncMode
    )
    SyncLipsyncReact1Emotion: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.SyncLipsyncReact1Emotion
    )

    model_mode: nodetool.nodes.fal.video_to_video.SyncLipsyncReact1ModelMode = Field(
        default=nodetool.nodes.fal.video_to_video.SyncLipsyncReact1ModelMode.FACE,
        description="Controls the edit region and movement scope for the model. Available options: - `lips`: Only lipsync using react-1 (minimal facial changes). - `face`: Lipsync + facial expressions without head movements. - `head`: Lipsync + facial expressions + natural talking head movements.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the input video. Must be **15 seconds or shorter**.",
    )
    lipsync_mode: nodetool.nodes.fal.video_to_video.SyncLipsyncReact1LipsyncMode = (
        Field(
            default=nodetool.nodes.fal.video_to_video.SyncLipsyncReact1LipsyncMode.BOUNCE,
            description="Lipsync mode when audio and video durations are out of sync.",
        )
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL to the input audio. Must be **15 seconds or shorter**.",
    )
    temperature: float | OutputHandle[float] = connect_field(
        default=0.5, description="Controls the expresiveness of the lipsync."
    )
    emotion: nodetool.nodes.fal.video_to_video.SyncLipsyncReact1Emotion = Field(
        default=nodetool.nodes.fal.video_to_video.SyncLipsyncReact1Emotion(""),
        description="Emotion prompt for the generation. Currently supports single-word emotions only.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.SyncLipsyncReact1

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class SyncLipsyncV2(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate realistic lipsync animations from audio using advanced algorithms for high-quality synchronization with Sync Lipsync 2.0 model
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    SyncLipsyncV2SyncMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.SyncLipsyncV2SyncMode
    )
    SyncLipsyncV2Model: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.SyncLipsyncV2Model
    )

    sync_mode: nodetool.nodes.fal.video_to_video.SyncLipsyncV2SyncMode = Field(
        default=nodetool.nodes.fal.video_to_video.SyncLipsyncV2SyncMode.CUT_OFF,
        description="Lipsync mode when audio and video durations are out of sync.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video",
    )
    model: nodetool.nodes.fal.video_to_video.SyncLipsyncV2Model = Field(
        default=nodetool.nodes.fal.video_to_video.SyncLipsyncV2Model.LIPSYNC_2,
        description="The model to use for lipsyncing. `lipsync-2-pro` will cost roughly 1.67 times as much as `lipsync-2` for the same duration.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input audio",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.SyncLipsyncV2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class SyncLipsyncV2Pro(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate high-quality realistic lipsync animations from audio while preserving unique details like natural teeth and unique facial features using the state-of-the-art Sync Lipsync 2 Pro model.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    SyncLipsyncV2ProSyncMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.SyncLipsyncV2ProSyncMode
    )

    sync_mode: nodetool.nodes.fal.video_to_video.SyncLipsyncV2ProSyncMode = Field(
        default=nodetool.nodes.fal.video_to_video.SyncLipsyncV2ProSyncMode.CUT_OFF,
        description="Lipsync mode when audio and video durations are out of sync.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input audio",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.SyncLipsyncV2Pro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Thinksound(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate realistic audio for a video with an optional text prompt and combine
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="A prompt to guide the audio generation. If not provided, it will be extracted from the video.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to generate the audio for.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=24, description="The number of inference steps for audio generation."
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="The classifier-free guidance scale for audio generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Thinksound

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class ThinksoundAudio(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """

    Generate realistic audio from a video with an optional text prompt
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="A prompt to guide the audio generation. If not provided, it will be extracted from the video.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to generate the audio for.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=24, description="The number of inference steps for audio generation."
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="The classifier-free guidance scale for audio generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.ThinksoundAudio

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class TopazUpscaleVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Professional-grade video upscaling using Topaz technology. Enhance your videos with high-quality upscaling.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    H264_output: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to use H264 codec for output video. Default is H265.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the video to upscale",
    )
    upscale_factor: float | OutputHandle[float] = connect_field(
        default=2,
        description="Factor to upscale the video by (e.g. 2.0 doubles width and height)",
    )
    target_fps: int | OutputHandle[int] = connect_field(
        default=0,
        description="Target FPS for frame interpolation. If set, frame interpolation will be enabled.",
    )
    focus: float | OutputHandle[float] = connect_field(
        default=0.2, description="Focus/sharpening level (0.0-1.0, default 0.2)"
    )
    sharpen: float | OutputHandle[float] = connect_field(
        default=0.6, description="Sharpening level (0.0-1.0, default 0.6)"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.TopazUpscaleVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class VeedLipsync(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate realistic lipsync from any audio using VEED's latest model
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description=None,
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description=None,
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.VeedLipsync

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class VeedVideoBackgroundRemoval(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Video Background Removal
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    VeedVideoBackgroundRemovalOutputCodec: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemovalOutputCodec
    )

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description=None,
    )
    subject_is_person: bool | OutputHandle[bool] = connect_field(
        default=True, description="Set to False if the subject is not a person."
    )
    output_codec: (
        nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemovalOutputCodec
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemovalOutputCodec.VP9,
        description="Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.",
    )
    refine_foreground_edges: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Improves the quality of the extracted object's edges.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemoval

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class VeedVideoBackgroundRemovalFast(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Video Background Removal
    video, editing, video-to-video, vid2vid, fast

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    VeedVideoBackgroundRemovalFastOutputCodec: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemovalFastOutputCodec
    )

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description=None,
    )
    subject_is_person: bool | OutputHandle[bool] = connect_field(
        default=True, description="Set to False if the subject is not a person."
    )
    output_codec: (
        nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemovalFastOutputCodec
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemovalFastOutputCodec.VP9,
        description="Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.",
    )
    refine_foreground_edges: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Improves the quality of the extracted object's edges.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemovalFast

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class VeedVideoBackgroundRemovalGreenScreen(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Video Background Removal
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    VeedVideoBackgroundRemovalGreenScreenOutputCodec: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemovalGreenScreenOutputCodec
    )

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description=None,
    )
    output_codec: (
        nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemovalGreenScreenOutputCodec
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemovalGreenScreenOutputCodec.VP9,
        description="Single VP9 video with alpha channel or two videos (rgb and alpha) in H264 format. H264 is recommended for better RGB quality.",
    )
    spill_suppression_strength: str | OutputHandle[str] = connect_field(
        default=0.8,
        description="Increase the value if green spots remain in the video, decrease if color changes are noticed on the extracted subject.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.VeedVideoBackgroundRemovalGreenScreen

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Veo31ExtendVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Veo 3.1
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Veo31ExtendVideoDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Veo31ExtendVideoDuration
    )
    Veo31ExtendVideoAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Veo31ExtendVideoAspectRatio
    )
    Veo31ExtendVideoResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Veo31ExtendVideoResolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt describing how the video should be extended",
    )
    duration: nodetool.nodes.fal.video_to_video.Veo31ExtendVideoDuration = Field(
        default=nodetool.nodes.fal.video_to_video.Veo31ExtendVideoDuration.VALUE_7S,
        description="The duration of the generated video.",
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.Veo31ExtendVideoAspectRatio = Field(
        default=nodetool.nodes.fal.video_to_video.Veo31ExtendVideoAspectRatio.AUTO,
        description="The aspect ratio of the generated video.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    auto_fix: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio.",
    )
    resolution: nodetool.nodes.fal.video_to_video.Veo31ExtendVideoResolution = Field(
        default=nodetool.nodes.fal.video_to_video.Veo31ExtendVideoResolution.VALUE_720P,
        description="The resolution of the generated video.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A negative prompt to guide the video generation."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Veo31ExtendVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Veo31FastExtendVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Veo 3.1 Fast
    video, editing, video-to-video, vid2vid, fast

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Veo31FastExtendVideoDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Veo31FastExtendVideoDuration
    )
    Veo31FastExtendVideoAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Veo31FastExtendVideoAspectRatio
    )
    Veo31FastExtendVideoResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Veo31FastExtendVideoResolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt describing how the video should be extended",
    )
    duration: nodetool.nodes.fal.video_to_video.Veo31FastExtendVideoDuration = Field(
        default=nodetool.nodes.fal.video_to_video.Veo31FastExtendVideoDuration.VALUE_7S,
        description="The duration of the generated video.",
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.Veo31FastExtendVideoAspectRatio = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Veo31FastExtendVideoAspectRatio.AUTO,
            description="The aspect ratio of the generated video.",
        )
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    auto_fix: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the video to extend. The video should be 720p or 1080p resolution in 16:9 or 9:16 aspect ratio.",
    )
    resolution: nodetool.nodes.fal.video_to_video.Veo31FastExtendVideoResolution = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Veo31FastExtendVideoResolution.VALUE_720P,
            description="The resolution of the generated video.",
        )
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A negative prompt to guide the video generation."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Veo31FastExtendVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class VideoAsPrompt(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Video As Prompt
    video, editing, video-to-video, vid2vid, professional

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    VideoAsPromptAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.VideoAsPromptAspectRatio
    )
    VideoAsPromptResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.VideoAsPromptResolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate an image from."
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.VideoAsPromptAspectRatio = Field(
        default=nodetool.nodes.fal.video_to_video.VideoAsPromptAspectRatio.RATIO_9_16,
        description="Aspect ratio of the generated video.",
    )
    resolution: nodetool.nodes.fal.video_to_video.VideoAsPromptResolution = Field(
        default=nodetool.nodes.fal.video_to_video.VideoAsPromptResolution.VALUE_480P,
        description="Resolution of the generated video.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="reference video to generate effect video from.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image to generate the effect video for.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second for the output video. Only applicable if output_type is 'video'.",
    )
    video_description: str | OutputHandle[str] = connect_field(
        default="", description="A brief description of the input video content."
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="Random seed for reproducible generation. If set none, a random seed will be used.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5, description="Guidance scale for generation."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="If set to true, the safety checker will be enabled."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=49, description="The number of frames to generate."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.VideoAsPrompt

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class VideoUpscaler(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Video Upscaler enhances video resolution and quality using AI.
    video, upscaling, enhancement, resolution, video-to-video

    Use cases:
    - Upscale low resolution videos
    - Enhance video quality
    - Increase video resolution
    - Improve video clarity
    - Restore old video footage
    """

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to upscale",
    )
    scale: float | OutputHandle[float] = connect_field(
        default=2, description="The scale factor"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.VideoUpscaler

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Wan22VaceFunA14bDepth(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    VACE Fun for Wan 2.2 A14B from Alibaba-PAI
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Wan22VaceFunA14bDepthTransparencyMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthTransparencyMode
    )
    Wan22VaceFunA14bDepthSampler: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthSampler
    )
    Wan22VaceFunA14bDepthVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthVideoQuality
    )
    Wan22VaceFunA14bDepthInterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthInterpolatorModel
    )
    Wan22VaceFunA14bDepthAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthAcceleration
    )
    Wan22VaceFunA14bDepthVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthVideoWriteMode
    )
    Wan22VaceFunA14bDepthResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthResolution
    )
    Wan22VaceFunA14bDepthAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthAspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source video file. Required for depth task.",
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Number of frames to interpolate between the original frames. A value of 0 means no interpolation.",
    )
    temporal_downsample_factor: int | OutputHandle[int] = connect_field(
        default=0,
        description="Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.",
    )
    first_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the first frame of the video. If provided, the model will use this frame as a reference.",
    )
    ref_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="URLs to source reference image. If provided, the model will use this image as reference.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 241 (inclusive).",
    )
    auto_downsample_min_fps: float | OutputHandle[float] = connect_field(
        default=15,
        description="The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.",
    )
    transparency_mode: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthTransparencyMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthTransparencyMode.CONTENT_AWARE,
        description="The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.",
    )
    sampler: nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthSampler = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthSampler.UNIPC,
        description="Sampler to use for video generation.",
    )
    video_quality: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthVideoQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthVideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthInterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthInterpolatorModel.FILM,
        description="The model to use for frame interpolation. Options are 'rife' or 'film'.",
    )
    enable_auto_downsample: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.",
    )
    preprocess: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to preprocess the input video."
    )
    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    acceleration: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthAcceleration
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthAcceleration.REGULAR,
        description="Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.",
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.",
    )
    match_input_num_frames: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    resolution: nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthResolution = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthResolution.AUTO,
            description="Resolution of the generated video.",
        )
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthAspectRatio = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepthAspectRatio.AUTO,
            description="Aspect ratio of the generated video.",
        )
    )
    return_frames_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, also return a ZIP file containing all generated frames.",
    )
    match_input_frames_per_second: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    last_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the last frame of the video. If provided, the model will use this frame as a reference.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bDepth

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Wan22VaceFunA14bInpainting(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    VACE Fun for Wan 2.2 A14B from Alibaba-PAI
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Wan22VaceFunA14bInpaintingTransparencyMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingTransparencyMode
    )
    Wan22VaceFunA14bInpaintingSampler: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingSampler
    )
    Wan22VaceFunA14bInpaintingVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingVideoQuality
    )
    Wan22VaceFunA14bInpaintingInterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingInterpolatorModel
    )
    Wan22VaceFunA14bInpaintingAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingAcceleration
    )
    Wan22VaceFunA14bInpaintingVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingVideoWriteMode
    )
    Wan22VaceFunA14bInpaintingResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingResolution
    )
    Wan22VaceFunA14bInpaintingAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingAspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source video file. Required for inpainting.",
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Number of frames to interpolate between the original frames. A value of 0 means no interpolation.",
    )
    temporal_downsample_factor: int | OutputHandle[int] = connect_field(
        default=0,
        description="Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.",
    )
    first_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the first frame of the video. If provided, the model will use this frame as a reference.",
    )
    ref_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="Urls to source reference image. If provided, the model will use this image as reference.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 241 (inclusive).",
    )
    auto_downsample_min_fps: float | OutputHandle[float] = connect_field(
        default=15,
        description="The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.",
    )
    transparency_mode: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingTransparencyMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingTransparencyMode.CONTENT_AWARE,
        description="The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.",
    )
    sampler: nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingSampler = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingSampler.UNIPC,
            description="Sampler to use for video generation.",
        )
    )
    video_quality: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingVideoQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingVideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    mask_video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source mask file. Required for inpainting.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingInterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingInterpolatorModel.FILM,
        description="The model to use for frame interpolation. Options are 'rife' or 'film'.",
    )
    preprocess: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to preprocess the input video."
    )
    enable_auto_downsample: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.",
    )
    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    acceleration: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingAcceleration
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingAcceleration.REGULAR,
        description="Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.",
    )
    mask_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video using salient mask tracking. Will be ignored if mask_video_url is provided.",
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.",
    )
    match_input_num_frames: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    resolution: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingResolution
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingResolution.AUTO,
        description="Resolution of the generated video.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpaintingAspectRatio.AUTO,
        description="Aspect ratio of the generated video.",
    )
    return_frames_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, also return a ZIP file containing all generated frames.",
    )
    match_input_frames_per_second: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    last_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the last frame of the video. If provided, the model will use this frame as a reference.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bInpainting

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Wan22VaceFunA14bOutpainting(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    VACE Fun for Wan 2.2 A14B from Alibaba-PAI
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Wan22VaceFunA14bOutpaintingSampler: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingSampler
    )
    Wan22VaceFunA14bOutpaintingTransparencyMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingTransparencyMode
    )
    Wan22VaceFunA14bOutpaintingVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingVideoQuality
    )
    Wan22VaceFunA14bOutpaintingInterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingInterpolatorModel
    )
    Wan22VaceFunA14bOutpaintingAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingAcceleration
    )
    Wan22VaceFunA14bOutpaintingVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingVideoWriteMode
    )
    Wan22VaceFunA14bOutpaintingResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingResolution
    )
    Wan22VaceFunA14bOutpaintingAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingAspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source video file. Required for outpainting.",
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Number of frames to interpolate between the original frames. A value of 0 means no interpolation.",
    )
    temporal_downsample_factor: int | OutputHandle[int] = connect_field(
        default=0,
        description="Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.",
    )
    first_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the first frame of the video. If provided, the model will use this frame as a reference.",
    )
    ref_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="URLs to source reference image. If provided, the model will use this image as reference.",
    )
    expand_ratio: float | OutputHandle[float] = connect_field(
        default=0.25,
        description="Amount of expansion. This is a float value between 0 and 1, where 0.25 adds 25% to the original video size on the specified sides.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 241 (inclusive).",
    )
    auto_downsample_min_fps: float | OutputHandle[float] = connect_field(
        default=15,
        description="The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.",
    )
    expand_bottom: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to expand the video to the bottom."
    )
    sampler: nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingSampler = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingSampler.UNIPC,
            description="Sampler to use for video generation.",
        )
    )
    transparency_mode: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingTransparencyMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingTransparencyMode.CONTENT_AWARE,
        description="The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.",
    )
    video_quality: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingVideoQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingVideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingInterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingInterpolatorModel.FILM,
        description="The model to use for frame interpolation. Options are 'rife' or 'film'.",
    )
    enable_auto_downsample: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.",
    )
    expand_top: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to expand the video to the top."
    )
    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    expand_left: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to expand the video to the left."
    )
    acceleration: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingAcceleration
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingAcceleration.REGULAR,
        description="Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.",
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.",
    )
    match_input_num_frames: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    resolution: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingResolution
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingResolution.AUTO,
        description="Resolution of the generated video.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpaintingAspectRatio.AUTO,
        description="Aspect ratio of the generated video.",
    )
    expand_right: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to expand the video to the right."
    )
    return_frames_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, also return a ZIP file containing all generated frames.",
    )
    match_input_frames_per_second: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    last_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the last frame of the video. If provided, the model will use this frame as a reference.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bOutpainting

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Wan22VaceFunA14bPose(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    VACE Fun for Wan 2.2 A14B from Alibaba-PAI
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Wan22VaceFunA14bPoseTransparencyMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseTransparencyMode
    )
    Wan22VaceFunA14bPoseSampler: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseSampler
    )
    Wan22VaceFunA14bPoseVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseVideoQuality
    )
    Wan22VaceFunA14bPoseInterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseInterpolatorModel
    )
    Wan22VaceFunA14bPoseAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseAcceleration
    )
    Wan22VaceFunA14bPoseVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseVideoWriteMode
    )
    Wan22VaceFunA14bPoseResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseResolution
    )
    Wan22VaceFunA14bPoseAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseAspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt to guide video generation. For pose task, the prompt should describe the desired pose and action of the subject in the video.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source video file. Required for pose task.",
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Number of frames to interpolate between the original frames. A value of 0 means no interpolation.",
    )
    temporal_downsample_factor: int | OutputHandle[int] = connect_field(
        default=0,
        description="Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.",
    )
    first_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the first frame of the video. If provided, the model will use this frame as a reference.",
    )
    ref_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="URLs to source reference image. If provided, the model will use this image as reference.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 241 (inclusive).",
    )
    auto_downsample_min_fps: float | OutputHandle[float] = connect_field(
        default=15,
        description="The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.",
    )
    transparency_mode: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseTransparencyMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseTransparencyMode.CONTENT_AWARE,
        description="The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.",
    )
    sampler: nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseSampler = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseSampler.UNIPC,
        description="Sampler to use for video generation.",
    )
    video_quality: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseVideoQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseVideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseInterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseInterpolatorModel.FILM,
        description="The model to use for frame interpolation. Options are 'rife' or 'film'.",
    )
    enable_auto_downsample: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.",
    )
    preprocess: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to preprocess the input video."
    )
    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    acceleration: nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseAcceleration = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseAcceleration.REGULAR,
            description="Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.",
        )
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.",
    )
    match_input_num_frames: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    resolution: nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseResolution = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseResolution.AUTO,
            description="Resolution of the generated video.",
        )
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseAspectRatio = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPoseAspectRatio.AUTO,
            description="Aspect ratio of the generated video.",
        )
    )
    return_frames_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, also return a ZIP file containing all generated frames.",
    )
    match_input_frames_per_second: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    last_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the last frame of the video. If provided, the model will use this frame as a reference.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bPose

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class Wan22VaceFunA14bReframe(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    VACE Fun for Wan 2.2 A14B from Alibaba-PAI
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    Wan22VaceFunA14bReframeTransparencyMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeTransparencyMode
    )
    Wan22VaceFunA14bReframeSampler: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeSampler
    )
    Wan22VaceFunA14bReframeVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeVideoQuality
    )
    Wan22VaceFunA14bReframeInterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeInterpolatorModel
    )
    Wan22VaceFunA14bReframeAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeAcceleration
    )
    Wan22VaceFunA14bReframeVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeVideoWriteMode
    )
    Wan22VaceFunA14bReframeResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeResolution
    )
    Wan22VaceFunA14bReframeAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeAspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt to guide video generation. Optional for reframing.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source video file. This video will be used as a reference for the reframe task.",
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Number of frames to interpolate between the original frames. A value of 0 means no interpolation.",
    )
    temporal_downsample_factor: int | OutputHandle[int] = connect_field(
        default=0,
        description="Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.",
    )
    first_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the first frame of the video. If provided, the model will use this frame as a reference.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 241 (inclusive).",
    )
    auto_downsample_min_fps: float | OutputHandle[float] = connect_field(
        default=15,
        description="The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.",
    )
    transparency_mode: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeTransparencyMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeTransparencyMode.CONTENT_AWARE,
        description="The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.",
    )
    sampler: nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeSampler = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeSampler.UNIPC,
        description="Sampler to use for video generation.",
    )
    trim_borders: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to trim borders from the video."
    )
    video_quality: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeVideoQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeVideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeInterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeInterpolatorModel.FILM,
        description="The model to use for frame interpolation. Options are 'rife' or 'film'.",
    )
    enable_auto_downsample: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.",
    )
    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    acceleration: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeAcceleration
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeAcceleration.REGULAR,
        description="Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.",
    )
    zoom_factor: float | OutputHandle[float] = connect_field(
        default=0,
        description="Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.",
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.",
    )
    match_input_num_frames: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    resolution: nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeResolution = (
        Field(
            default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeResolution.AUTO,
            description="Resolution of the generated video.",
        )
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframeAspectRatio.AUTO,
        description="Aspect ratio of the generated video.",
    )
    return_frames_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, also return a ZIP file containing all generated frames.",
    )
    match_input_frames_per_second: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    last_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the last frame of the video. If provided, the model will use this frame as a reference.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.Wan22VaceFunA14bReframe

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class WanFunControl(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate pose or depth controlled video using Alibaba-PAI's Wan 2.2 Fun
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    WanFunControlPreprocessType: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanFunControlPreprocessType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video."
    )
    shift: float | OutputHandle[float] = connect_field(
        default=5, description="The shift for the scheduler."
    )
    preprocess_video: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to preprocess the video. If True, the video will be preprocessed to depth or pose.",
    )
    reference_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the reference image to use as a reference for the video generation.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=16,
        description="The fps to generate. Only used when match_input_fps is False.",
    )
    match_input_num_frames: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to match the number of frames in the input video.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=6, description="The guidance scale."
    )
    preprocess_type: nodetool.nodes.fal.video_to_video.WanFunControlPreprocessType = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanFunControlPreprocessType.DEPTH,
            description="The type of preprocess to apply to the video. Only used when preprocess_video is True.",
        )
    )
    control_video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the control video to use as a reference for the video generation.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="The negative prompt to generate the video."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="The number of frames to generate. Only used when match_input_num_frames is False.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=27, description="The number of inference steps."
    )
    match_input_fps: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to match the fps in the input video."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.WanFunControl

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class WanV2214bAnimateMove(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan-Animate is a video model that generates high-fidelity character videos by replicating the expressions and movements of characters from reference videos.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    WanV2214bAnimateMoveResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanV2214bAnimateMoveResolution
    )
    WanV2214bAnimateMoveVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanV2214bAnimateMoveVideoWriteMode
    )
    WanV2214bAnimateMoveVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanV2214bAnimateMoveVideoQuality
    )

    shift: float | OutputHandle[float] = connect_field(
        default=5,
        description="Shift value for the video. Must be between 1.0 and 10.0.",
    )
    resolution: nodetool.nodes.fal.video_to_video.WanV2214bAnimateMoveResolution = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanV2214bAnimateMoveResolution.VALUE_480P,
            description="Resolution of the generated video (480p, 580p, or 720p).",
        )
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video.",
    )
    return_frames_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, also return a ZIP archive containing per-frame images generated on GPU (lossless).",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.WanV2214bAnimateMoveVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanV2214bAnimateMoveVideoWriteMode.BALANCED,
        description="The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.",
    )
    enable_output_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, output video will be checked for safety after generation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.",
    )
    video_quality: (
        nodetool.nodes.fal.video_to_video.WanV2214bAnimateMoveVideoQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanV2214bAnimateMoveVideoQuality.HIGH,
        description="The quality of the output video. Higher quality means better visual quality but larger file size.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=20,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    use_turbo: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized for best results.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, input data will be checked for safety before processing.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.WanV2214bAnimateMove

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class WanV2214bAnimateReplace(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan-Animate Replace is a model that can integrate animated characters into reference videos, replacing the original character while preserving the scenes lighting and color tone for seamless environmental integration.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    WanV2214bAnimateReplaceResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanV2214bAnimateReplaceResolution
    )
    WanV2214bAnimateReplaceVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanV2214bAnimateReplaceVideoWriteMode
    )
    WanV2214bAnimateReplaceVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanV2214bAnimateReplaceVideoQuality
    )

    shift: float | OutputHandle[float] = connect_field(
        default=5,
        description="Shift value for the video. Must be between 1.0 and 10.0.",
    )
    resolution: nodetool.nodes.fal.video_to_video.WanV2214bAnimateReplaceResolution = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanV2214bAnimateReplaceResolution.VALUE_480P,
            description="Resolution of the generated video (480p, 580p, or 720p).",
        )
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video.",
    )
    return_frames_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, also return a ZIP archive containing per-frame images generated on GPU (lossless).",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.WanV2214bAnimateReplaceVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanV2214bAnimateReplaceVideoWriteMode.BALANCED,
        description="The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.",
    )
    enable_output_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, output video will be checked for safety after generation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.",
    )
    video_quality: (
        nodetool.nodes.fal.video_to_video.WanV2214bAnimateReplaceVideoQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanV2214bAnimateReplaceVideoQuality.HIGH,
        description="The quality of the output video. Higher quality means better visual quality but larger file size.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=20,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    use_turbo: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, applies quality enhancement for faster generation with improved quality. When enabled, parameters are automatically optimized for best results.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, input data will be checked for safety before processing.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.WanV2214bAnimateReplace

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class WanV22A14bVideoToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan-2.2 video-to-video is a video model that generates high-quality videos with high visual quality and motion diversity from text prompts and source videos.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    WanV22A14bVideoToVideoAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoAcceleration
    )
    WanV22A14bVideoToVideoVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoVideoWriteMode
    )
    WanV22A14bVideoToVideoAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoAspectRatio
    )
    WanV22A14bVideoToVideoResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoResolution
    )
    WanV22A14bVideoToVideoVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoVideoQuality
    )
    WanV22A14bVideoToVideoInterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoInterpolatorModel
    )

    shift: float | OutputHandle[float] = connect_field(
        default=5,
        description="Shift value for the video. Must be between 1.0 and 10.0.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video.",
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=1,
        description="Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.",
    )
    acceleration: (
        nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoAcceleration
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoAcceleration.REGULAR,
        description="Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    resample_fps: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the video will be resampled to the passed frames per second. If false, the video will not be resampled.",
    )
    frames_per_second: str | OutputHandle[str] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5,
        description="Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 17 to 161 (inclusive).",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, input data will be checked for safety before processing.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt for video generation."
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoVideoWriteMode.BALANCED,
        description="The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoAspectRatio.AUTO,
        description="Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input video.",
    )
    resolution: nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoResolution = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoResolution.VALUE_720P,
            description="Resolution of the generated video (480p, 580p, or 720p).",
        )
    )
    enable_output_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, output video will be checked for safety after generation.",
    )
    guidance_scale_2: float | OutputHandle[float] = connect_field(
        default=4,
        description="Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.",
    )
    video_quality: (
        nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoVideoQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoVideoQuality.HIGH,
        description="The quality of the output video. Higher quality means better visual quality but larger file size.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.9,
        description="Strength of the video transformation. A value of 1.0 means the output will be completely based on the prompt, while a value of 0.0 means the output will be identical to the input video.",
    )
    adjust_fps_for_interpolation: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoInterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideoInterpolatorModel.FILM,
        description="The model to use for frame interpolation. If None, no interpolation is applied.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=27,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.WanV22A14bVideoToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class WanV26ReferenceToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan v2.6 Reference to Video
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    WanV26ReferenceToVideoAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanV26ReferenceToVideoAspectRatio
    )
    WanV26ReferenceToVideoResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanV26ReferenceToVideoResolution
    )
    WanV26ReferenceToVideoDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanV26ReferenceToVideoDuration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Use @Video1, @Video2, @Video3 to reference subjects from your videos. Works for people, animals, or objects. For multi-shot prompts: '[0-3s] Shot 1. [3-6s] Shot 2.' Max 800 characters.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.WanV26ReferenceToVideoAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanV26ReferenceToVideoAspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video.",
    )
    videos: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="Reference videos for subject consistency (1-3 videos). Videos' FPS must be at least 16 FPS.Reference in prompt as @Video1, @Video2, @Video3. Works for people, animals, or objects.",
    )
    resolution: nodetool.nodes.fal.video_to_video.WanV26ReferenceToVideoResolution = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanV26ReferenceToVideoResolution.VALUE_1080P,
            description="Video resolution tier. R2V only supports 720p and 1080p (no 480p).",
        )
    )
    duration: nodetool.nodes.fal.video_to_video.WanV26ReferenceToVideoDuration = Field(
        default=nodetool.nodes.fal.video_to_video.WanV26ReferenceToVideoDuration.VALUE_5,
        description="Duration of the generated video in seconds. R2V supports only 5 or 10 seconds (no 15s).",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="If set to true, the safety checker will be enabled."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    multi_shots: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="When true (default), enables intelligent multi-shot segmentation for coherent narrative videos with multiple shots. When false, generates single continuous shot. Only active when enable_prompt_expansion is True.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Negative prompt to describe content to avoid. Max 500 characters.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt rewriting using LLM."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.WanV26ReferenceToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class WanVace(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Vace a video generation model that uses a source image, mask, and video to create prompted videos with controllable sources.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    WanVaceTask: typing.ClassVar[type] = nodetool.nodes.fal.video_to_video.WanVaceTask
    WanVaceAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVaceAspectRatio
    )
    WanVaceResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVaceResolution
    )

    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source video file. If provided, the model will use this video as a reference.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    ref_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="Urls to source reference image. If provided, the model will use this image as reference.",
    )
    task: nodetool.nodes.fal.video_to_video.WanVaceTask = Field(
        default=nodetool.nodes.fal.video_to_video.WanVaceTask.DEPTH,
        description="Task type for the model.",
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 24.",
    )
    mask_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.WanVaceAspectRatio = Field(
        default=nodetool.nodes.fal.video_to_video.WanVaceAspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video (16:9 or 9:16).",
    )
    resolution: nodetool.nodes.fal.video_to_video.WanVaceResolution = Field(
        default=nodetool.nodes.fal.video_to_video.WanVaceResolution.VALUE_720P,
        description="Resolution of the generated video (480p,580p, or 720p).",
    )
    mask_video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source mask file. If provided, the model will use this mask as a reference.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    preprocess: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to preprocess the input video."
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.WanVace

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class WanVace13b(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Vace a video generation model that uses a source image, mask, and video to create prompted videos with controllable sources.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    WanVace13bTask: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace13bTask
    )
    WanVace13bResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace13bResolution
    )
    WanVace13bAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace13bAspectRatio
    )

    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source video file. If provided, the model will use this video as a reference.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    mask_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.",
    )
    task: nodetool.nodes.fal.video_to_video.WanVace13bTask = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace13bTask.DEPTH,
        description="Task type for the model.",
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 24.",
    )
    ref_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="Urls to source reference image. If provided, the model will use this image as reference.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 100 (inclusive). Works only with only reference images as input if source video or mask video is provided output len would be same as source up to 241 frames",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    resolution: nodetool.nodes.fal.video_to_video.WanVace13bResolution = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace13bResolution.VALUE_720P,
        description="Resolution of the generated video (480p,580p, or 720p).",
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.WanVace13bAspectRatio = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace13bAspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video (16:9 or 9:16).",
    )
    mask_video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source mask file. If provided, the model will use this mask as a reference.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    preprocess: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to preprocess the input video."
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.WanVace13b

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class WanVace14b(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    VACE is a video generation model that uses a source image, mask, and video to create prompted videos with controllable sources.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    WanVace14bTransparencyMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bTransparencyMode
    )
    WanVace14bSampler: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bSampler
    )
    WanVace14bVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bVideoQuality
    )
    WanVace14bInterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bInterpolatorModel
    )
    WanVace14bTask: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bTask
    )
    WanVace14bVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bVideoWriteMode
    )
    WanVace14bResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bResolution
    )
    WanVace14bAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bAspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source video file. If provided, the model will use this video as a reference.",
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Number of frames to interpolate between the original frames. A value of 0 means no interpolation.",
    )
    temporal_downsample_factor: int | OutputHandle[int] = connect_field(
        default=0,
        description="Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.",
    )
    first_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the first frame of the video. If provided, the model will use this frame as a reference.",
    )
    ref_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="URLs to source reference image. If provided, the model will use this image as reference.",
    )
    transparency_mode: nodetool.nodes.fal.video_to_video.WanVace14bTransparencyMode = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanVace14bTransparencyMode.CONTENT_AWARE,
            description="The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.",
        )
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 241 (inclusive).",
    )
    auto_downsample_min_fps: float | OutputHandle[float] = connect_field(
        default=15,
        description="The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.",
    )
    sampler: nodetool.nodes.fal.video_to_video.WanVace14bSampler = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bSampler.UNIPC,
        description="Sampler to use for video generation.",
    )
    video_quality: nodetool.nodes.fal.video_to_video.WanVace14bVideoQuality = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bVideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    mask_video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source mask file. If provided, the model will use this mask as a reference.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.video_to_video.WanVace14bInterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bInterpolatorModel.FILM,
        description="The model to use for frame interpolation. Options are 'rife' or 'film'.",
    )
    enable_auto_downsample: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.",
    )
    preprocess: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to preprocess the input video."
    )
    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    acceleration: str | OutputHandle[str] = connect_field(
        default="regular",
        description="Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.",
    )
    mask_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video. If provided mask video url will be ignored.",
    )
    task: nodetool.nodes.fal.video_to_video.WanVace14bTask = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bTask.DEPTH,
        description="Task type for the model.",
    )
    match_input_num_frames: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.",
    )
    frames_per_second: str | OutputHandle[str] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    video_write_mode: nodetool.nodes.fal.video_to_video.WanVace14bVideoWriteMode = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanVace14bVideoWriteMode.BALANCED,
            description="The write mode of the generated video.",
        )
    )
    return_frames_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, also return a ZIP file containing all generated frames.",
    )
    resolution: nodetool.nodes.fal.video_to_video.WanVace14bResolution = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bResolution.AUTO,
        description="Resolution of the generated video.",
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.WanVace14bAspectRatio = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bAspectRatio.AUTO,
        description="Aspect ratio of the generated video.",
    )
    match_input_frames_per_second: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    last_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the last frame of the video. If provided, the model will use this frame as a reference.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.WanVace14b

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class WanVace14bDepth(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    VACE is a video generation model that uses a source image, mask, and video to create prompted videos with controllable sources.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    WanVace14bDepthTransparencyMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bDepthTransparencyMode
    )
    WanVace14bDepthSampler: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bDepthSampler
    )
    WanVace14bDepthVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bDepthVideoQuality
    )
    WanVace14bDepthInterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bDepthInterpolatorModel
    )
    WanVace14bDepthVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bDepthVideoWriteMode
    )
    WanVace14bDepthResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bDepthResolution
    )
    WanVace14bDepthAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bDepthAspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source video file. Required for depth task.",
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Number of frames to interpolate between the original frames. A value of 0 means no interpolation.",
    )
    temporal_downsample_factor: int | OutputHandle[int] = connect_field(
        default=0,
        description="Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.",
    )
    first_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the first frame of the video. If provided, the model will use this frame as a reference.",
    )
    ref_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="URLs to source reference image. If provided, the model will use this image as reference.",
    )
    transparency_mode: (
        nodetool.nodes.fal.video_to_video.WanVace14bDepthTransparencyMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bDepthTransparencyMode.CONTENT_AWARE,
        description="The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 241 (inclusive).",
    )
    auto_downsample_min_fps: float | OutputHandle[float] = connect_field(
        default=15,
        description="The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.",
    )
    sampler: nodetool.nodes.fal.video_to_video.WanVace14bDepthSampler = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bDepthSampler.UNIPC,
        description="Sampler to use for video generation.",
    )
    video_quality: nodetool.nodes.fal.video_to_video.WanVace14bDepthVideoQuality = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanVace14bDepthVideoQuality.HIGH,
            description="The quality of the generated video.",
        )
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.video_to_video.WanVace14bDepthInterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bDepthInterpolatorModel.FILM,
        description="The model to use for frame interpolation. Options are 'rife' or 'film'.",
    )
    enable_auto_downsample: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.",
    )
    preprocess: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to preprocess the input video."
    )
    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    acceleration: str | OutputHandle[str] = connect_field(
        default="regular",
        description="Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.",
    )
    match_input_num_frames: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.",
    )
    frames_per_second: str | OutputHandle[str] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.WanVace14bDepthVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bDepthVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    return_frames_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, also return a ZIP file containing all generated frames.",
    )
    resolution: nodetool.nodes.fal.video_to_video.WanVace14bDepthResolution = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bDepthResolution.AUTO,
        description="Resolution of the generated video.",
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.WanVace14bDepthAspectRatio = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bDepthAspectRatio.AUTO,
        description="Aspect ratio of the generated video.",
    )
    match_input_frames_per_second: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    last_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the last frame of the video. If provided, the model will use this frame as a reference.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.WanVace14bDepth

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class WanVace14bInpainting(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    VACE is a video generation model that uses a source image, mask, and video to create prompted videos with controllable sources.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    WanVace14bInpaintingTransparencyMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bInpaintingTransparencyMode
    )
    WanVace14bInpaintingSampler: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bInpaintingSampler
    )
    WanVace14bInpaintingVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bInpaintingVideoQuality
    )
    WanVace14bInpaintingInterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bInpaintingInterpolatorModel
    )
    WanVace14bInpaintingVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bInpaintingVideoWriteMode
    )
    WanVace14bInpaintingResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bInpaintingResolution
    )
    WanVace14bInpaintingAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bInpaintingAspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source video file. Required for inpainting.",
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Number of frames to interpolate between the original frames. A value of 0 means no interpolation.",
    )
    temporal_downsample_factor: int | OutputHandle[int] = connect_field(
        default=0,
        description="Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.",
    )
    first_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the first frame of the video. If provided, the model will use this frame as a reference.",
    )
    ref_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="Urls to source reference image. If provided, the model will use this image as reference.",
    )
    transparency_mode: (
        nodetool.nodes.fal.video_to_video.WanVace14bInpaintingTransparencyMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bInpaintingTransparencyMode.CONTENT_AWARE,
        description="The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 241 (inclusive).",
    )
    auto_downsample_min_fps: float | OutputHandle[float] = connect_field(
        default=15,
        description="The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.",
    )
    sampler: nodetool.nodes.fal.video_to_video.WanVace14bInpaintingSampler = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bInpaintingSampler.UNIPC,
        description="Sampler to use for video generation.",
    )
    video_quality: (
        nodetool.nodes.fal.video_to_video.WanVace14bInpaintingVideoQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bInpaintingVideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    mask_video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source mask file. Required for inpainting.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.video_to_video.WanVace14bInpaintingInterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bInpaintingInterpolatorModel.FILM,
        description="The model to use for frame interpolation. Options are 'rife' or 'film'.",
    )
    enable_auto_downsample: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    preprocess: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to preprocess the input video."
    )
    acceleration: str | OutputHandle[str] = connect_field(
        default="regular",
        description="Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.",
    )
    mask_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL to the guiding mask file. If provided, the model will use this mask as a reference to create masked video using salient mask tracking. Will be ignored if mask_video_url is provided.",
    )
    match_input_num_frames: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.",
    )
    frames_per_second: str | OutputHandle[str] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.WanVace14bInpaintingVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bInpaintingVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    return_frames_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, also return a ZIP file containing all generated frames.",
    )
    resolution: nodetool.nodes.fal.video_to_video.WanVace14bInpaintingResolution = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanVace14bInpaintingResolution.AUTO,
            description="Resolution of the generated video.",
        )
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.WanVace14bInpaintingAspectRatio = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanVace14bInpaintingAspectRatio.AUTO,
            description="Aspect ratio of the generated video.",
        )
    )
    match_input_frames_per_second: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    last_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the last frame of the video. If provided, the model will use this frame as a reference.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.WanVace14bInpainting

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class WanVace14bOutpainting(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    VACE is a video generation model that uses a source image, mask, and video to create prompted videos with controllable sources.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    WanVace14bOutpaintingTransparencyMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingTransparencyMode
    )
    WanVace14bOutpaintingSampler: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingSampler
    )
    WanVace14bOutpaintingVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingVideoQuality
    )
    WanVace14bOutpaintingInterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingInterpolatorModel
    )
    WanVace14bOutpaintingVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingVideoWriteMode
    )
    WanVace14bOutpaintingResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingResolution
    )
    WanVace14bOutpaintingAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingAspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source video file. Required for outpainting.",
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Number of frames to interpolate between the original frames. A value of 0 means no interpolation.",
    )
    temporal_downsample_factor: int | OutputHandle[int] = connect_field(
        default=0,
        description="Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.",
    )
    first_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the first frame of the video. If provided, the model will use this frame as a reference.",
    )
    ref_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="URLs to source reference image. If provided, the model will use this image as reference.",
    )
    expand_ratio: float | OutputHandle[float] = connect_field(
        default=0.25,
        description="Amount of expansion. This is a float value between 0 and 1, where 0.25 adds 25% to the original video size on the specified sides.",
    )
    transparency_mode: (
        nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingTransparencyMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingTransparencyMode.CONTENT_AWARE,
        description="The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 241 (inclusive).",
    )
    auto_downsample_min_fps: float | OutputHandle[float] = connect_field(
        default=15,
        description="The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.",
    )
    expand_bottom: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to expand the video to the bottom."
    )
    sampler: nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingSampler = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingSampler.UNIPC,
        description="Sampler to use for video generation.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.",
    )
    video_quality: (
        nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingVideoQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingVideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingInterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingInterpolatorModel.FILM,
        description="The model to use for frame interpolation. Options are 'rife' or 'film'.",
    )
    enable_auto_downsample: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.",
    )
    expand_top: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to expand the video to the top."
    )
    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    expand_left: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to expand the video to the left."
    )
    acceleration: str | OutputHandle[str] = connect_field(
        default="regular",
        description="Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.",
    )
    match_input_num_frames: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.",
    )
    frames_per_second: str | OutputHandle[str] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    return_frames_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, also return a ZIP file containing all generated frames.",
    )
    resolution: nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingResolution = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingResolution.AUTO,
            description="Resolution of the generated video.",
        )
    )
    expand_right: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to expand the video to the right."
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingAspectRatio = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanVace14bOutpaintingAspectRatio.AUTO,
            description="Aspect ratio of the generated video.",
        )
    )
    match_input_frames_per_second: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    last_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the last frame of the video. If provided, the model will use this frame as a reference.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.WanVace14bOutpainting

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class WanVace14bPose(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    VACE is a video generation model that uses a source image, mask, and video to create prompted videos with controllable sources.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    WanVace14bPoseTransparencyMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bPoseTransparencyMode
    )
    WanVace14bPoseSampler: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bPoseSampler
    )
    WanVace14bPoseVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bPoseVideoQuality
    )
    WanVace14bPoseInterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bPoseInterpolatorModel
    )
    WanVace14bPoseVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bPoseVideoWriteMode
    )
    WanVace14bPoseResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bPoseResolution
    )
    WanVace14bPoseAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bPoseAspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt to guide video generation. For pose task, the prompt should describe the desired pose and action of the subject in the video.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source video file. Required for pose task.",
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Number of frames to interpolate between the original frames. A value of 0 means no interpolation.",
    )
    temporal_downsample_factor: int | OutputHandle[int] = connect_field(
        default=0,
        description="Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.",
    )
    first_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the first frame of the video. If provided, the model will use this frame as a reference.",
    )
    ref_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="URLs to source reference image. If provided, the model will use this image as reference.",
    )
    transparency_mode: (
        nodetool.nodes.fal.video_to_video.WanVace14bPoseTransparencyMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bPoseTransparencyMode.CONTENT_AWARE,
        description="The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 241 (inclusive).",
    )
    auto_downsample_min_fps: float | OutputHandle[float] = connect_field(
        default=15,
        description="The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.",
    )
    sampler: nodetool.nodes.fal.video_to_video.WanVace14bPoseSampler = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bPoseSampler.UNIPC,
        description="Sampler to use for video generation.",
    )
    video_quality: nodetool.nodes.fal.video_to_video.WanVace14bPoseVideoQuality = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bPoseVideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.video_to_video.WanVace14bPoseInterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bPoseInterpolatorModel.FILM,
        description="The model to use for frame interpolation. Options are 'rife' or 'film'.",
    )
    enable_auto_downsample: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.",
    )
    preprocess: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to preprocess the input video."
    )
    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    acceleration: str | OutputHandle[str] = connect_field(
        default="regular",
        description="Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.",
    )
    match_input_num_frames: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.",
    )
    frames_per_second: str | OutputHandle[str] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    video_write_mode: nodetool.nodes.fal.video_to_video.WanVace14bPoseVideoWriteMode = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanVace14bPoseVideoWriteMode.BALANCED,
            description="The write mode of the generated video.",
        )
    )
    return_frames_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, also return a ZIP file containing all generated frames.",
    )
    resolution: nodetool.nodes.fal.video_to_video.WanVace14bPoseResolution = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bPoseResolution.AUTO,
        description="Resolution of the generated video.",
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.WanVace14bPoseAspectRatio = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bPoseAspectRatio.AUTO,
        description="Aspect ratio of the generated video.",
    )
    match_input_frames_per_second: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    last_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the last frame of the video. If provided, the model will use this frame as a reference.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.WanVace14bPose

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class WanVace14bReframe(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    VACE is a video generation model that uses a source image, mask, and video to create prompted videos with controllable sources.
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    WanVace14bReframeTransparencyMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bReframeTransparencyMode
    )
    WanVace14bReframeSampler: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bReframeSampler
    )
    WanVace14bReframeVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bReframeVideoQuality
    )
    WanVace14bReframeInterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bReframeInterpolatorModel
    )
    WanVace14bReframeVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bReframeVideoWriteMode
    )
    WanVace14bReframeResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bReframeResolution
    )
    WanVace14bReframeAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVace14bReframeAspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt to guide video generation. Optional for reframing.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source video file. This video will be used as a reference for the reframe task.",
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Number of frames to interpolate between the original frames. A value of 0 means no interpolation.",
    )
    temporal_downsample_factor: int | OutputHandle[int] = connect_field(
        default=0,
        description="Temporal downsample factor for the video. This is an integer value that determines how many frames to skip in the video. A value of 0 means no downsampling. For each downsample factor, one upsample factor will automatically be applied.",
    )
    first_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the first frame of the video. If provided, the model will use this frame as a reference.",
    )
    transparency_mode: (
        nodetool.nodes.fal.video_to_video.WanVace14bReframeTransparencyMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bReframeTransparencyMode.CONTENT_AWARE,
        description="The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 241 (inclusive).",
    )
    trim_borders: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to trim borders from the video."
    )
    auto_downsample_min_fps: float | OutputHandle[float] = connect_field(
        default=15,
        description="The minimum frames per second to downsample the video to. This is used to help determine the auto downsample factor to try and find the lowest detail-preserving downsample factor. The default value is appropriate for most videos, if you are using a video with very fast motion, you may need to increase this value. If your video has a very low amount of motion, you could decrease this value to allow for higher downsampling and thus longer sequences.",
    )
    sampler: nodetool.nodes.fal.video_to_video.WanVace14bReframeSampler = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bReframeSampler.UNIPC,
        description="Sampler to use for video generation.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.",
    )
    video_quality: nodetool.nodes.fal.video_to_video.WanVace14bReframeVideoQuality = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanVace14bReframeVideoQuality.HIGH,
            description="The quality of the generated video.",
        )
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.video_to_video.WanVace14bReframeInterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bReframeInterpolatorModel.FILM,
        description="The model to use for frame interpolation. Options are 'rife' or 'film'.",
    )
    enable_auto_downsample: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the model will automatically temporally downsample the video to an appropriate frame length for the model, then will interpolate it back to the original frame length.",
    )
    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    acceleration: str | OutputHandle[str] = connect_field(
        default="regular",
        description="Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.",
    )
    zoom_factor: float | OutputHandle[float] = connect_field(
        default=0,
        description="Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.",
    )
    match_input_num_frames: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, the number of frames in the generated video will match the number of frames in the input video. If false, the number of frames will be determined by the num_frames parameter.",
    )
    frames_per_second: str | OutputHandle[str] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 30. Ignored if match_input_frames_per_second is true.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.WanVace14bReframeVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bReframeVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    return_frames_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, also return a ZIP file containing all generated frames.",
    )
    resolution: nodetool.nodes.fal.video_to_video.WanVace14bReframeResolution = Field(
        default=nodetool.nodes.fal.video_to_video.WanVace14bReframeResolution.AUTO,
        description="Resolution of the generated video.",
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.WanVace14bReframeAspectRatio = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanVace14bReframeAspectRatio.AUTO,
            description="Aspect ratio of the generated video.",
        )
    )
    match_input_frames_per_second: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, the frames per second of the generated video will match the input video. If false, the frames per second will be determined by the frames_per_second parameter.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    last_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the last frame of the video. If provided, the model will use this frame as a reference.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.WanVace14bReframe

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class WanVaceAppsLongReframe(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan 2.1 VACE Long Reframe
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    WanVaceAppsLongReframeAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeAcceleration
    )
    WanVaceAppsLongReframeSampler: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeSampler
    )
    WanVaceAppsLongReframeVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeVideoWriteMode
    )
    WanVaceAppsLongReframeAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeAspectRatio
    )
    WanVaceAppsLongReframeResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeResolution
    )
    WanVaceAppsLongReframeTransparencyMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeTransparencyMode
    )
    WanVaceAppsLongReframeVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeVideoQuality
    )
    WanVaceAppsLongReframeInterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeInterpolatorModel
    )

    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL to the source video file. This video will be used as a reference for the reframe task.",
    )
    acceleration: (
        nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeAcceleration
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeAcceleration.REGULAR,
        description="Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.",
    )
    paste_back: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to paste back the reframed scene to the original video.",
    )
    zoom_factor: float | OutputHandle[float] = connect_field(
        default=0,
        description="Zoom factor for the video. When this value is greater than 0, the video will be zoomed in by this factor (in relation to the canvas size,) cutting off the edges of the video. A value of 0 means no zoom.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt to guide video generation. Optional for reframing.",
    )
    scene_threshold: float | OutputHandle[float] = connect_field(
        default=30,
        description="Threshold for scene detection sensitivity (0-100). Lower values detect more scenes.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="Guidance scale for classifier-free guidance. Higher values encourage the model to generate images closely related to the text prompt.",
    )
    auto_downsample_min_fps: float | OutputHandle[float] = connect_field(
        default=6, description="Minimum FPS for auto downsample."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="letterboxing, borders, black bars, bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    sampler: nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeSampler = Field(
        default=nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeSampler.UNIPC,
        description="Sampler to use for video generation.",
    )
    video_write_mode: (
        nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeVideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeVideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeAspectRatio.AUTO,
        description="Aspect ratio of the generated video.",
    )
    resolution: nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeResolution = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeResolution.AUTO,
            description="Resolution of the generated video.",
        )
    )
    transparency_mode: (
        nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeTransparencyMode
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeTransparencyMode.CONTENT_AWARE,
        description="The transparency mode to apply to the first and last frames. This controls how the transparent areas of the first and last frames are filled.",
    )
    return_frames_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, also return a ZIP file containing all generated frames.",
    )
    trim_borders: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to trim borders from the video."
    )
    video_quality: (
        nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeVideoQuality
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeVideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    interpolator_model: (
        nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeInterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframeInterpolatorModel.FILM,
        description="The model to use for frame interpolation. Options are 'rife' or 'film'.",
    )
    enable_auto_downsample: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable auto downsample."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.WanVaceAppsLongReframe

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class WanVaceAppsVideoEdit(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan VACE Video Edit
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    WanVaceAppsVideoEditResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEditResolution
    )
    WanVaceAppsVideoEditAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEditAcceleration
    )
    WanVaceAppsVideoEditAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEditAspectRatio
    )
    WanVaceAppsVideoEditVideoType: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEditVideoType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Prompt to edit the video."
    )
    resolution: nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEditResolution = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEditResolution.AUTO,
            description="Resolution of the edited video.",
        )
    )
    acceleration: nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEditAcceleration = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEditAcceleration.REGULAR,
            description="Acceleration to use for inference. Options are 'none' or 'regular'. Accelerated inference will very slightly affect output, but will be significantly faster.",
        )
    )
    return_frames_zip: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to include a ZIP archive containing all generated frames.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the input video.",
    )
    aspect_ratio: nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEditAspectRatio = (
        Field(
            default=nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEditAspectRatio.AUTO,
            description="Aspect ratio of the edited video.",
        )
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    video_type: nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEditVideoType = Field(
        default=nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEditVideoType.AUTO,
        description="The type of video you're editing. Use 'general' for most videos, and 'human' for videos emphasizing human subjects and motions. The default value 'auto' means the model will guess based on the first frame of the video.",
    )
    images: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = connect_field(
        default=[],
        description="URLs of the input images to use as a reference for the generation.",
    )
    enable_auto_downsample: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to enable automatic downsampling. If your video has a high frame rate or is long, enabling longer sequences to be generated. The video will be interpolated back to the original frame rate after generation.",
    )
    auto_downsample_min_fps: float | OutputHandle[float] = connect_field(
        default=15,
        description="The minimum frames per second to downsample the video to.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.WanVaceAppsVideoEdit

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class WanVisionEnhancer(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan Vision Enhancer
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    WanVisionEnhancerTargetResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WanVisionEnhancerTargetResolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Optional prompt to prepend to the VLM-generated description. Leave empty to use only the auto-generated description from the video.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to enhance with Wan Video. Maximum 200MB file size. Videos longer than 500 frames will have only the first 500 frames processed (~8-21 seconds depending on fps).",
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="Random seed for reproducibility. If not provided, a random seed will be used.",
    )
    target_resolution: (
        nodetool.nodes.fal.video_to_video.WanVisionEnhancerTargetResolution
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WanVisionEnhancerTargetResolution.VALUE_720P,
        description="Target output resolution for the enhanced video. 720p (native, fast) or 1080p (upscaled, slower). Processing is always done at 720p, then upscaled if 1080p selected.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="oversaturated, overexposed, static, blurry details, subtitles, stylized, artwork, painting, still frame, overall gray, worst quality, low quality, JPEG artifacts, ugly, mutated, extra fingers, poorly drawn hands, poorly drawn face, deformed, disfigured, malformed limbs, fused fingers, static motion, cluttered background, three legs, crowded background, walking backwards",
        description="Negative prompt to avoid unwanted features.",
    )
    creativity: int | OutputHandle[int] = connect_field(
        default=1,
        description="Controls how much the model enhances/changes the video. 0 = Minimal change (preserves original), 1 = Subtle enhancement (default), 2 = Medium enhancement, 3 = Strong enhancement, 4 = Maximum enhancement.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.WanVisionEnhancer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.video_to_video
from nodetool.workflows.base_node import BaseNode


class WorkflowUtilitiesAutoSubtitle(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Workflow Utilities
    video, editing, video-to-video, vid2vid

    Use cases:
    - Video style transfer
    - Video enhancement and restoration
    - Automated video editing
    - Special effects generation
    - Content repurposing
    """

    WorkflowUtilitiesAutoSubtitleFontWeight: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitleFontWeight
    )
    WorkflowUtilitiesAutoSubtitleFontColor: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitleFontColor
    )
    WorkflowUtilitiesAutoSubtitleStrokeColor: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitleStrokeColor
    )
    WorkflowUtilitiesAutoSubtitleHighlightColor: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitleHighlightColor
    )
    WorkflowUtilitiesAutoSubtitlePosition: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitlePosition
    )
    WorkflowUtilitiesAutoSubtitleBackgroundColor: typing.ClassVar[type] = (
        nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitleBackgroundColor
    )

    font_weight: (
        nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitleFontWeight
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitleFontWeight.BOLD,
        description="Font weight (TikTok style typically uses bold or black)",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the video file to add automatic subtitles to Max file size: 95.4MB, Timeout: 30.0s",
    )
    stroke_width: int | OutputHandle[int] = connect_field(
        default=3, description="Text stroke/outline width in pixels (0 for no stroke)"
    )
    font_color: (
        nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitleFontColor
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitleFontColor.WHITE,
        description="Subtitle text color for non-active words",
    )
    font_size: int | OutputHandle[int] = connect_field(
        default=100,
        description="Font size for subtitles (TikTok style uses larger text)",
    )
    language: str | OutputHandle[str] = connect_field(
        default="en",
        description="Language code for transcription (e.g., 'en', 'es', 'fr', 'de', 'it', 'pt', 'nl', 'ja', 'zh', 'ko') or 3-letter ISO code (e.g., 'eng', 'spa', 'fra')",
    )
    y_offset: int | OutputHandle[int] = connect_field(
        default=75,
        description="Vertical offset in pixels (positive = move down, negative = move up)",
    )
    background_opacity: float | OutputHandle[float] = connect_field(
        default=0,
        description="Background opacity (0.0 = fully transparent, 1.0 = fully opaque)",
    )
    stroke_color: (
        nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitleStrokeColor
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitleStrokeColor.BLACK,
        description="Text stroke/outline color",
    )
    highlight_color: (
        nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitleHighlightColor
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitleHighlightColor.PURPLE,
        description="Color for the currently speaking word (karaoke-style highlight)",
    )
    enable_animation: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Enable animation effects for subtitles (bounce style entrance)",
    )
    font_name: str | OutputHandle[str] = connect_field(
        default="Montserrat",
        description="Any Google Font name from fonts.google.com (e.g., 'Montserrat', 'Poppins', 'BBH Sans Hegarty')",
    )
    position: (
        nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitlePosition
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitlePosition.BOTTOM,
        description="Vertical position of subtitles",
    )
    words_per_subtitle: int | OutputHandle[int] = connect_field(
        default=3,
        description="Maximum number of words per subtitle segment. Use 1 for single-word display, 2-3 for short phrases, or 8-12 for full sentences.",
    )
    background_color: (
        nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitleBackgroundColor
    ) = Field(
        default=nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitleBackgroundColor.NONE,
        description="Background color behind text ('none' or 'transparent' for no background)",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.video_to_video.WorkflowUtilitiesAutoSubtitle

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
