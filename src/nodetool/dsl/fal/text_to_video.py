# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.fal.image_to_video


class KlingTextToVideoV2(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate videos directly from text prompts using Kling Video V2 Master.
    video, generation, animation, text-to-video, kling-v2

    Use cases:
    - Visualize scripts or storyboards
    - Produce short promotional videos from text
    - Create animated social media content
    - Generate concept previews for film ideas
    - Produce text-driven motion graphics
    """

    KlingDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingDuration
    )
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.image_to_video.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    duration: nodetool.nodes.fal.image_to_video.KlingDuration = Field(
        default=nodetool.nodes.fal.image_to_video.KlingDuration.FIVE_SECONDS,
        description="The duration of the generated video",
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingTextToVideoV2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.fal.image_to_video


class KlingVideoV2(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate videos from images using Kling Video V2 Master. Create smooth and realistic animations from a single frame.
    video, generation, animation, img2vid, kling-v2

    Use cases:
    - Convert artwork into animated clips
    - Produce dynamic marketing visuals
    - Generate motion graphics from static scenes
    - Create short cinematic sequences
    - Enhance presentations with video content
    """

    KlingDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingDuration
    )
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.image_to_video.AspectRatio

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to transform into a video",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="A description of the desired video motion and style"
    )
    duration: nodetool.nodes.fal.image_to_video.KlingDuration = Field(
        default=nodetool.nodes.fal.image_to_video.KlingDuration.FIVE_SECONDS,
        description="The duration of the generated video",
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoV2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseEffects(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Apply text-driven effects to a video with Pixverse 4.5.
        video, effects, pixverse, text-guided

        Use cases:
        - Stylize existing footage
        - Add visual effects via text
        - Enhance marketing videos
        - Create experimental clips
        - Transform user content
    """

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The source video",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text describing the effect"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Optional seed for deterministic output"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseEffects

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Animate an image into a video using Pixverse 4.5.
        video, generation, pixverse, image-to-video

        Use cases:
        - Bring photos to life
        - Create moving artwork
        - Generate short clips from images
        - Produce social media animations
        - Experiment with visual storytelling
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The source image",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Optional style or motion prompt"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Optional seed for deterministic output"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from text prompts with Pixverse 4.5 API.
        video, generation, pixverse, text-to-video

        Use cases:
        - Create animated scenes from text
        - Generate marketing clips
        - Produce dynamic social posts
        - Prototype video ideas
        - Explore creative storytelling
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the video"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Optional seed for deterministic output"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseTextToVideoFast(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos quickly from text prompts with Pixverse 4.5 Fast.
        video, generation, pixverse, text-to-video, fast

        Use cases:
        - Rapid video prototyping
        - Generate quick social posts
        - Produce short marketing clips
        - Test creative ideas fast
        - Create video drafts
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the video"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Optional seed for deterministic output"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseTextToVideoFast

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseTransition(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Apply Pixverse transitions between images.
        video, generation, transition, pixverse

        Use cases:
        - Blend between two images
        - Create animated transitions
        - Generate morphing effects
        - Produce smooth scene changes
        - Experiment with visual flows
    """

    start_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The starting image",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The ending image",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Optional seed for deterministic output"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseTransition

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class Veo3(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate high-quality videos from text prompts with Google's Veo 3 model.
    video, generation, text-to-video, prompt, audio

    Use cases:
    - Produce short cinematic clips from descriptions
    - Create social media videos
    - Generate visual storyboards
    - Experiment with video concepts
    - Produce marketing content
    """

    Veo3AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Veo3AspectRatio
    )
    Veo3Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Veo3Duration

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt describing the video you want to generate",
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.Veo3AspectRatio = Field(
        default=nodetool.nodes.fal.text_to_video.Veo3AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video. If it is not set to 16:9, the video will be outpainted with Luma Ray 2 Reframe functionality.",
    )
    duration: nodetool.nodes.fal.text_to_video.Veo3Duration = Field(
        default=nodetool.nodes.fal.text_to_video.Veo3Duration.EIGHT_SECONDS,
        description="The duration of the generated video in seconds",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to generate audio for the video. If false, %33 less credits will be used.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="A seed to use for the video generation"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A negative prompt to guide the video generation"
    )
    enhance_prompt: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enhance the video generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Veo3

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanFlf2V(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate video loops from text prompts using WAN-FLF2V.
    video, generation, wan, text-to-video

    Use cases:
    - Generate looping videos from descriptions
    - Produce motion graphics from prompts
    - Create abstract video ideas
    - Develop creative transitions
    - Experiment with AI-generated motion
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Randomization seed for reproducible results"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanFlf2V

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanProImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Convert an image into a short video clip using Wan Pro.
    video, generation, wan, professional, image-to-video

    Use cases:
    - Create dynamic videos from product photos
    - Generate animations from static artwork
    - Produce short promotional clips
    - Transform images into motion graphics
    - Experiment with visual storytelling
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The input image to animate",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Optional prompt describing the desired motion"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Randomization seed for reproducible results"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanProImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanProTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate a short video clip from a text prompt using Wan Pro.
    video, generation, wan, professional, text-to-video

    Use cases:
    - Create animated scenes from descriptions
    - Generate short creative videos
    - Produce promotional content
    - Visualize storyboards
    - Experiment with narrative ideas
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Randomization seed for reproducible results"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanT2V(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate videos from text using the WAN-T2V model.
    video, generation, wan, text-to-video

    Use cases:
    - Produce creative videos from prompts
    - Experiment with motion concepts
    - Generate quick animated drafts
    - Visualize ideas for stories
    - Create short social media clips
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Randomization seed for reproducible results"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanT2V

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanV2_1_13BTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Create videos from text using WAN v2.1 1.3B, an open-source text-to-video model.
    video, generation, wan, text-to-video

    Use cases:
    - Produce short clips from prompts
    - Generate concept videos
    - Create quick visualizations
    - Iterate on storytelling ideas
    - Experiment with AI video synthesis
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Randomization seed for reproducible results"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanV2_1_13BTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
