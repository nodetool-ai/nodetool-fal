# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class AnimateDiffSparseCtrlLCM(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    AnimateDiff SparseCtrl LCM animates drawings with latent consistency models for fast generation.
    video, generation, animatediff, sparsectrl, lcm, animation, text-to-video

    Use cases:
    - Animate hand-drawn sketches
    - Bring drawings to life
    - Create animated illustrations
    - Generate animations from concept art
    - Produce animation from sparse frames
    """

    ControlnetType: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.AnimateDiffSparseCtrlLCM.ControlnetType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The prompt to use for generating the image. Be as descriptive as possible for best results.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.",
    )
    controlnet_type: (
        nodetool.nodes.fal.text_to_video.AnimateDiffSparseCtrlLCM.ControlnetType
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.AnimateDiffSparseCtrlLCM.ControlnetType.SCRIBBLE,
        description="The type of controlnet to use for generating the video. The controlnet determines how the video will be animated.",
    )
    keyframe_2_index: int | OutputHandle[int] = connect_field(
        default=0,
        description="The frame index of the third keyframe to use for the generation.",
    )
    keyframe_0_index: int | OutputHandle[int] = connect_field(
        default=0,
        description="The frame index of the first keyframe to use for the generation.",
    )
    keyframe_1_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the second keyframe to use for the generation.",
    )
    keyframe_1_index: int | OutputHandle[int] = connect_field(
        default=0,
        description="The frame index of the second keyframe to use for the generation.",
    )
    guidance_scale: int | OutputHandle[int] = connect_field(
        default=1,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=4,
        description="Increasing the amount of steps tells Stable Diffusion that it should take more steps to generate your final result which can increase the amount of detail in your image.",
    )
    keyframe_2_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the third keyframe to use for the generation.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The negative prompt to use. Use it to specify what you don't want.",
    )
    keyframe_0_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the first keyframe to use for the generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.AnimateDiffSparseCtrlLCM

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class AnimateDiffTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    AnimateDiff generates smooth animations from text prompts using diffusion models.
    video, generation, animatediff, animation, text-to-video, txt2vid

    Use cases:
    - Animate ideas from text descriptions
    - Create animated content quickly
    - Generate motion graphics from prompts
    - Produce animated concept art
    - Create video loops and sequences
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The prompt to use for generating the video. Be as descriptive as possible for best results.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=16, description="The number of frames to generate for the video."
    )
    fps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of frames per second to extract from the video."
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="square", description="The size of the video to generate."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=25, description="The number of inference steps to perform."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="(bad quality, worst quality:1.2), ugly faces, bad anime",
        description="The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution).",
    )
    motions: list[str] | OutputHandle[list[str]] = connect_field(
        default=[], description="The motions to apply to the video."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.AnimateDiffTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class AnimateDiffTurboTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    AnimateDiff Turbo generates animations at lightning speed with reduced steps.
    video, generation, animatediff, turbo, fast, text-to-video, txt2vid

    Use cases:
    - Rapidly prototype video animations
    - Create quick video previews
    - Generate animations with minimal latency
    - Iterate on video concepts quickly
    - Produce real-time animation effects
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The prompt to use for generating the video. Be as descriptive as possible for best results.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of frames per second to extract from the video."
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="square", description="The size of the video to generate."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=2,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8,
        description="The number of inference steps to perform. 4-12 is recommended for turbo mode.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="(bad quality, worst quality:1.2), ugly faces, bad anime",
        description="The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution).",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=16, description="The number of frames to generate for the video."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.AnimateDiffTurboTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class ArgilAvatarsTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Argil Avatars creates realistic talking avatar videos from text descriptions.
    video, generation, avatar, talking-head, argil, text-to-video

    Use cases:
    - Generate avatar spokesperson videos
    - Create virtual presenter content
    - Produce automated video announcements
    - Generate character-based narratives
    - Create social media avatar videos
    """

    Voice: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo.Voice
    )
    Avatar: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo.Avatar
    )

    text: str | OutputHandle[str] = connect_field(default="", description=None)
    voice: nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo.Voice = Field(
        default=nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo.Voice(""),
        description=None,
    )
    remove_background: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enabling the remove background feature will result in a 50% increase in the price.",
    )
    avatar: nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo.Avatar = Field(
        default=nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo.Avatar(""),
        description=None,
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class BytedanceSeedanceV1LiteTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Seedance 1.0 Lite
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1LiteTextToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1LiteTextToVideo.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1LiteTextToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt used to generate the video"
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1LiteTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1LiteTextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: (
        nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1LiteTextToVideo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1LiteTextToVideo.Resolution.VALUE_720P,
        description="Video resolution - 480p for faster generation, 720p for higher quality",
    )
    duration: (
        nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1LiteTextToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1LiteTextToVideo.Duration.VALUE_5,
        description="Duration of the video in seconds",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="If set to true, the safety checker will be enabled."
    )
    camera_fixed: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to fix the camera position"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed to control video generation. Use -1 for random.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1LiteTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class BytedanceSeedanceV1ProTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Seedance 1.0 Pro, a high quality video generation model developed by Bytedance.
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1ProTextToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1ProTextToVideo.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1ProTextToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt used to generate the video"
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1ProTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1ProTextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: (
        nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1ProTextToVideo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1ProTextToVideo.Resolution.VALUE_1080P,
        description="Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality",
    )
    duration: (
        nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1ProTextToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1ProTextToVideo.Duration.VALUE_5,
        description="Duration of the video in seconds",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="If set to true, the safety checker will be enabled."
    )
    camera_fixed: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to fix the camera position"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed to control video generation. Use -1 for random.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.BytedanceSeedanceV1ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class CogVideoX5B(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    CogVideoX-5B is a powerful open-source text-to-video generation model with 5 billion parameters.
    video, generation, cogvideo, text-to-video, txt2vid

    Use cases:
    - Generate detailed videos from text prompts
    - Create animated storytelling content
    - Produce concept videos for pitches
    - Generate video storyboards
    - Create educational demonstrations
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    use_rife: bool | OutputHandle[bool] = connect_field(
        default=True, description="Use RIFE for video interpolation"
    )
    loras: list[types.LoraWeight] | OutputHandle[list[types.LoraWeight]] = (
        connect_field(
            default=[],
            description="The LoRAs to use for the image generation. We currently support one lora.",
        )
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="", description="The size of the generated video."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related video to show you.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="The number of inference steps to perform."
    )
    export_fps: int | OutputHandle[int] = connect_field(
        default=16, description="The target FPS of the video"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="The negative prompt to generate video from"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.CogVideoX5B

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class FastSvdLcmTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate short video clips from your images using SVD v1.1 at Lightning Speed
    video, generation, text-to-video, txt2vid, fast

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The prompt to use as a starting point for the generation.",
    )
    cond_aug: float | OutputHandle[float] = connect_field(
        default=0.02,
        description="The conditoning augmentation determines the amount of noise that will be added to the conditioning frame. The higher the number, the more noise there will be, and the less the video will look like the initial image. Increase it for more motion.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=10,
        description="The FPS of the generated video. The higher the number, the faster the video will play. Total video length is 25 frames.",
    )
    motion_bucket_id: int | OutputHandle[int] = connect_field(
        default=127,
        description="The motion bucket id determines the motion of the generated video. The higher the number, the more motion there will be.",
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="landscape_16_9", description="The size of the generated video."
    )
    steps: int | OutputHandle[int] = connect_field(
        default=4,
        description="The number of steps to run the model for. The higher the number the better the quality and longer it will take to generate.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.FastSvdLcmTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class FastSvdTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate short video clips from your prompts using SVD v1.1
    video, generation, text-to-video, txt2vid, fast

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    DeepCache: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.FastSvdTextToVideo.DeepCache
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The prompt to use as a starting point for the generation.",
    )
    cond_aug: float | OutputHandle[float] = connect_field(
        default=0.02,
        description="The conditoning augmentation determines the amount of noise that will be added to the conditioning frame. The higher the number, the more noise there will be, and the less the video will look like the initial image. Increase it for more motion.",
    )
    deep_cache: nodetool.nodes.fal.text_to_video.FastSvdTextToVideo.DeepCache = Field(
        default=nodetool.nodes.fal.text_to_video.FastSvdTextToVideo.DeepCache.NONE,
        description="Enabling [DeepCache](https://github.com/horseee/DeepCache) will make the execution faster, but might sometimes degrade overall quality. The higher the setting, the faster the execution will be, but the more quality might be lost.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=10,
        description="The FPS of the generated video. The higher the number, the faster the video will play. Total video length is 25 frames.",
    )
    motion_bucket_id: int | OutputHandle[int] = connect_field(
        default=127,
        description="The motion bucket id determines the motion of the generated video. The higher the number, the more motion there will be.",
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="landscape_16_9", description="The size of the generated video."
    )
    steps: int | OutputHandle[int] = connect_field(
        default=20,
        description="The number of steps to run the model for. The higher the number the better the quality and longer it will take to generate.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="unrealistic, saturated, high contrast, big nose, painting, drawing, sketch, cartoon, anime, manga, render, CG, 3d, watermark, signature, label",
        description="The negative prompt to use as a starting point for the generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.FastSvdTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class HunyuanVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Hunyuan Video is Tencent's advanced text-to-video model for high-quality video generation.
    video, generation, hunyuan, text-to-video, txt2vid

    Use cases:
    - Generate cinematic videos from text descriptions
    - Create marketing videos from product descriptions
    - Produce educational video content
    - Generate creative video concepts
    - Create animated scenes from stories
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.HunyuanVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.HunyuanVideo.Resolution
    )
    NumFrames: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.HunyuanVideo.NumFrames
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.HunyuanVideo.AspectRatio = Field(
        default=nodetool.nodes.fal.text_to_video.HunyuanVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the video to generate.",
    )
    resolution: nodetool.nodes.fal.text_to_video.HunyuanVideo.Resolution = Field(
        default=nodetool.nodes.fal.text_to_video.HunyuanVideo.Resolution.VALUE_720P,
        description="The resolution of the video to generate.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="The number of inference steps to run. Lower gets faster results, higher gets better results.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed to use for generating the video."
    )
    num_frames: nodetool.nodes.fal.text_to_video.HunyuanVideo.NumFrames = Field(
        default=nodetool.nodes.fal.text_to_video.HunyuanVideo.NumFrames(129),
        description="The number of frames to generate.",
    )
    pro_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.HunyuanVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class HunyuanVideoLora(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Hunyuan Video is an Open video generation model with high visual quality, motion diversity, text-video alignment, and generation stability
    video, generation, text-to-video, txt2vid, lora

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.HunyuanVideoLora.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.HunyuanVideoLora.Resolution
    )
    NumFrames: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.HunyuanVideoLora.NumFrames
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.HunyuanVideoLora.AspectRatio = Field(
        default=nodetool.nodes.fal.text_to_video.HunyuanVideoLora.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the video to generate.",
    )
    resolution: nodetool.nodes.fal.text_to_video.HunyuanVideoLora.Resolution = Field(
        default=nodetool.nodes.fal.text_to_video.HunyuanVideoLora.Resolution.VALUE_720P,
        description="The resolution of the video to generate.",
    )
    loras: list[types.LoraWeight] | OutputHandle[list[types.LoraWeight]] = (
        connect_field(
            default=[],
            description="The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image.",
        )
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed to use for generating the video."
    )
    num_frames: nodetool.nodes.fal.text_to_video.HunyuanVideoLora.NumFrames = Field(
        default=nodetool.nodes.fal.text_to_video.HunyuanVideoLora.NumFrames(129),
        description="The number of frames to generate.",
    )
    pro_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.HunyuanVideoLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class HunyuanVideoV15TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Hunyuan Video V1.5
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.HunyuanVideoV15TextToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.HunyuanVideoV15TextToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video."
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.HunyuanVideoV15TextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.HunyuanVideoV15TextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the video.",
    )
    resolution: (
        nodetool.nodes.fal.text_to_video.HunyuanVideoV15TextToVideo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.HunyuanVideoV15TextToVideo.Resolution.VALUE_480P,
        description="The resolution of the video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Enable prompt expansion to enhance the input prompt."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for reproducibility."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28, description="The number of inference steps."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="The negative prompt to guide what not to generate."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.HunyuanVideoV15TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class InfinitalkSingleText(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Infinitalk
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Resolution
    )
    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Acceleration
    )
    Voice: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Voice
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    resolution: nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Resolution = (
        Field(
            default=nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Resolution.VALUE_480P,
            description="Resolution of the video to generate. Must be either 480p or 720p.",
        )
    )
    acceleration: nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Acceleration = (
        Field(
            default=nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Acceleration.REGULAR,
            description="The acceleration level to use for generation.",
        )
    )
    text_input: str | OutputHandle[str] = connect_field(
        default="", description="The text input to guide video generation."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.",
    )
    voice: nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Voice = Field(
        default=nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Voice(""),
        description="The voice to use for speech generation",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=145,
        description="Number of frames to generate. Must be between 41 to 721.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=42,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.InfinitalkSingleText

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class InfinityStarTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Infinity Star
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.InfinityStarTextToVideo.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for generating the video"
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.InfinityStarTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.InfinityStarTextToVideo.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated output",
    )
    enhance_prompt: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use an LLM to enhance the prompt."
    )
    use_apg: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use APG"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Guidance scale for generation"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="Number of inference steps"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. Leave empty for random generation.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to guide what to avoid in generation"
    )
    tau_video: float | OutputHandle[float] = connect_field(
        default=0.4, description="Tau value for video scale"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.InfinityStarTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class Kandinsky5ProTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kandinsky5 Pro
    video, generation, text-to-video, txt2vid, professional

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Resolution
    )
    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Acceleration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.AspectRatio
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    resolution: nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Resolution.VALUE_512P,
            description="Video resolution: 512p or 1024p.",
        )
    )
    acceleration: (
        nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Acceleration.REGULAR,
        description="Acceleration level for faster generation.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.AspectRatio.RATIO_3_2,
        description="Aspect ratio of the generated video. One of (3:2, 1:1, 2:3).",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28, description="The number of inference steps."
    )
    duration: nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Duration = (
        Field(
            default=nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Duration.VALUE_5S,
            description="The length of the video to generate (5s or 10s)",
        )
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class Kandinsky5TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kandinsky5
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    resolution: nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.Resolution.VALUE_768X512,
            description="Resolution of the generated video in W:H format. Will be calculated based on the aspect ratio(768x512, 512x512, 512x768).",
        )
    )
    duration: nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.Duration = Field(
        default=nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.Duration.VALUE_5S,
        description="The length of the video to generate (5s or 10s)",
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.AspectRatio.RATIO_3_2,
            description="Aspect ratio of the generated video. One of (3:2, 1:1, 2:3).",
        )
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="The number of inference steps."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class Kandinsky5TextToVideoDistill(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kandinsky5
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    duration: nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.Duration = (
        Field(
            default=nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.Duration.VALUE_5S,
            description="The length of the video to generate (5s or 10s)",
        )
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.AspectRatio.RATIO_3_2,
        description="Aspect ratio of the generated video. One of (3:2, 1:1, 2:3).",
    )
    resolution: (
        nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.Resolution
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.Resolution.VALUE_768X512,
        description="Resolution of the generated video in W:H format. Will be calculated based on the aspect ratio(768x512, 512x512, 512x768).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoLipsyncAudioToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling LipSync is an audio-to-video model that generates realistic lip movements from audio input.
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to generate the lip sync for. Supports .mp4/.mov, ≤100MB, 2–10s, 720p/1080p only, width/height 720–1920px.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the audio to generate the lip sync for. Minimum duration is 2s and maximum duration is 60s. Maximum file size is 5MB.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoLipsyncAudioToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoLipsyncTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling LipSync is a text-to-video model that generates realistic lip movements from text input.
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    VoiceId: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoLipsyncTextToVideo.VoiceId
    )
    VoiceLanguage: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoLipsyncTextToVideo.VoiceLanguage
    )

    text: str | OutputHandle[str] = connect_field(
        default="",
        description="Text content for lip-sync video generation. Max 120 characters.",
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the video to generate the lip sync for. Supports .mp4/.mov, ≤100MB, 2-60s, 720p/1080p only, width/height 720–1920px. If validation fails, an error is returned.",
    )
    voice_id: nodetool.nodes.fal.text_to_video.KlingVideoLipsyncTextToVideo.VoiceId = (
        Field(
            default=nodetool.nodes.fal.text_to_video.KlingVideoLipsyncTextToVideo.VoiceId(
                ""
            ),
            description="Voice ID to use for speech synthesis",
        )
    )
    voice_language: (
        nodetool.nodes.fal.text_to_video.KlingVideoLipsyncTextToVideo.VoiceLanguage
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoLipsyncTextToVideo.VoiceLanguage.EN,
        description="The voice language corresponding to the Voice ID",
    )
    voice_speed: float | OutputHandle[float] = connect_field(
        default=1, description="Speech rate for Text to Video generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoLipsyncTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO3ProTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video O3 Pro generates professional quality videos from text prompts with enhanced fidelity.
    video, generation, kling, o3, pro, text-to-video, txt2vid

    Use cases:
    - Create professional-grade videos from detailed prompts
    - Generate cinematic video content with precise motion
    - Produce high-fidelity advertising videos
    - Create premium animated content from scripts
    - Generate top-tier video for film and media
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoO3ProTextToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoO3ProTextToVideo.AspectRatio
    )
    ShotType: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoO3ProTextToVideo.ShotType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for video generation. Required unless multi_prompt is provided.",
    )
    duration: nodetool.nodes.fal.text_to_video.KlingVideoO3ProTextToVideo.Duration = (
        Field(
            default=nodetool.nodes.fal.text_to_video.KlingVideoO3ProTextToVideo.Duration.VALUE_5,
            description="Video duration in seconds (3-15s).",
        )
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.KlingVideoO3ProTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoO3ProTextToVideo.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video.",
    )
    multi_prompt: (
        list[types.KlingV3MultiPromptElement]
        | OutputHandle[list[types.KlingV3MultiPromptElement]]
    ) = connect_field(
        default=[], description="List of prompts for multi-shot video generation."
    )
    voice_ids: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to generate native audio for the video."
    )
    shot_type: nodetool.nodes.fal.text_to_video.KlingVideoO3ProTextToVideo.ShotType = (
        Field(
            default=nodetool.nodes.fal.text_to_video.KlingVideoO3ProTextToVideo.ShotType.CUSTOMIZE,
            description="The type of multi-shot video generation.",
        )
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoO3ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO3StandardTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video O3 Standard generates videos from text prompts with balanced quality and speed.
    video, generation, kling, o3, standard, text-to-video, txt2vid

    Use cases:
    - Generate cinematic videos from text descriptions
    - Create marketing videos from product descriptions
    - Produce educational video content from scripts
    - Generate social media video content
    - Create animated scenes from text prompts
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoO3StandardTextToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoO3StandardTextToVideo.AspectRatio
    )
    ShotType: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoO3StandardTextToVideo.ShotType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for video generation. Required unless multi_prompt is provided.",
    )
    duration: (
        nodetool.nodes.fal.text_to_video.KlingVideoO3StandardTextToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoO3StandardTextToVideo.Duration.VALUE_5,
        description="Video duration in seconds (3-15s).",
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.KlingVideoO3StandardTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoO3StandardTextToVideo.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video.",
    )
    multi_prompt: (
        list[types.KlingV3MultiPromptElement]
        | OutputHandle[list[types.KlingV3MultiPromptElement]]
    ) = connect_field(
        default=[], description="List of prompts for multi-shot video generation."
    )
    voice_ids: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to generate native audio for the video."
    )
    shot_type: (
        nodetool.nodes.fal.text_to_video.KlingVideoO3StandardTextToVideo.ShotType
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoO3StandardTextToVideo.ShotType.CUSTOMIZE,
        description="The type of multi-shot video generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoO3StandardTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV15ProEffects(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips from your prompts using Kling 1.5 (pro)
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV15ProEffects.Duration
    )
    EffectScene: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV15ProEffects.EffectScene
    )

    duration: nodetool.nodes.fal.text_to_video.KlingVideoV15ProEffects.Duration = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV15ProEffects.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    effect_scene: (
        nodetool.nodes.fal.text_to_video.KlingVideoV15ProEffects.EffectScene
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV15ProEffects.EffectScene(
            ""
        ),
        description="The effect scene to use for the video generation",
    )
    input_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="URL of images to be used for hug, kiss or heart_gesture video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoV15ProEffects

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV15ProTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips from your prompts using Kling 1.5 (pro)
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV15ProTextToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV15ProTextToVideo.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    duration: nodetool.nodes.fal.text_to_video.KlingVideoV15ProTextToVideo.Duration = (
        Field(
            default=nodetool.nodes.fal.text_to_video.KlingVideoV15ProTextToVideo.Duration.VALUE_5,
            description="The duration of the generated video in seconds",
        )
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.KlingVideoV15ProTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV15ProTextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoV15ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV16ProEffects(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips from your prompts using Kling 1.6 (pro)
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV16ProEffects.Duration
    )
    EffectScene: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV16ProEffects.EffectScene
    )

    duration: nodetool.nodes.fal.text_to_video.KlingVideoV16ProEffects.Duration = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV16ProEffects.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    effect_scene: (
        nodetool.nodes.fal.text_to_video.KlingVideoV16ProEffects.EffectScene
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV16ProEffects.EffectScene(
            ""
        ),
        description="The effect scene to use for the video generation",
    )
    input_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="URL of images to be used for hug, kiss or heart_gesture video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoV16ProEffects

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV16ProTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips from your prompts using Kling 1.6 (pro)
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV16ProTextToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV16ProTextToVideo.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    duration: nodetool.nodes.fal.text_to_video.KlingVideoV16ProTextToVideo.Duration = (
        Field(
            default=nodetool.nodes.fal.text_to_video.KlingVideoV16ProTextToVideo.Duration.VALUE_5,
            description="The duration of the generated video in seconds",
        )
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.KlingVideoV16ProTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV16ProTextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoV16ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV16StandardEffects(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips from your prompts using Kling 1.6 (std)
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV16StandardEffects.Duration
    )
    EffectScene: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV16StandardEffects.EffectScene
    )

    duration: nodetool.nodes.fal.text_to_video.KlingVideoV16StandardEffects.Duration = (
        Field(
            default=nodetool.nodes.fal.text_to_video.KlingVideoV16StandardEffects.Duration.VALUE_5,
            description="The duration of the generated video in seconds",
        )
    )
    effect_scene: (
        nodetool.nodes.fal.text_to_video.KlingVideoV16StandardEffects.EffectScene
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV16StandardEffects.EffectScene(
            ""
        ),
        description="The effect scene to use for the video generation",
    )
    input_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="URL of images to be used for hug, kiss or heart_gesture video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoV16StandardEffects

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV16StandardTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips from your prompts using Kling 1.6 (std)
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV16StandardTextToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV16StandardTextToVideo.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    duration: (
        nodetool.nodes.fal.text_to_video.KlingVideoV16StandardTextToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV16StandardTextToVideo.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.KlingVideoV16StandardTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV16StandardTextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoV16StandardTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV1StandardEffects(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips from your prompts using Kling 1.0
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV1StandardEffects.Duration
    )
    EffectScene: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV1StandardEffects.EffectScene
    )

    duration: nodetool.nodes.fal.text_to_video.KlingVideoV1StandardEffects.Duration = (
        Field(
            default=nodetool.nodes.fal.text_to_video.KlingVideoV1StandardEffects.Duration.VALUE_5,
            description="The duration of the generated video in seconds",
        )
    )
    effect_scene: (
        nodetool.nodes.fal.text_to_video.KlingVideoV1StandardEffects.EffectScene
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV1StandardEffects.EffectScene(
            ""
        ),
        description="The effect scene to use for the video generation",
    )
    input_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="URL of images to be used for hug, kiss or heart_gesture video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoV1StandardEffects

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV1StandardTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video v1 Standard generates videos from text with balanced quality and speed.
    video, generation, kling, text-to-video, txt2vid

    Use cases:
    - Generate standard quality videos
    - Create video content efficiently
    - Produce videos for web use
    - Generate video previews
    - Create video concepts
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    duration: (
        nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    advanced_camera_control: str | OutputHandle[str] = connect_field(
        default="", description="Advanced Camera control parameters"
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )
    camera_control: (
        nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.CameraControl
        | OutputHandle[
            nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.CameraControl
        ]
        | None
    ) = connect_field(default=None, description="Camera control parameters")

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV21MasterTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling 2.1 Master: The premium endpoint for Kling 2.1, designed for top-tier text-to-video generation with unparalleled motion fluidity, cinematic visuals, and exceptional prompt precision.
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV21MasterTextToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV21MasterTextToVideo.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    duration: (
        nodetool.nodes.fal.text_to_video.KlingVideoV21MasterTextToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV21MasterTextToVideo.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.KlingVideoV21MasterTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV21MasterTextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoV21MasterTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV26ProTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video v2.6 Text to Video
    video, generation, text-to-video, txt2vid, professional

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV26ProTextToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV26ProTextToVideo.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    duration: nodetool.nodes.fal.text_to_video.KlingVideoV26ProTextToVideo.Duration = (
        Field(
            default=nodetool.nodes.fal.text_to_video.KlingVideoV26ProTextToVideo.Duration.VALUE_5,
            description="The duration of the generated video in seconds",
        )
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.KlingVideoV26ProTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV26ProTextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoV26ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV3ProTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video V3 Pro generates professional quality videos from text prompts with enhanced visual fidelity using the latest V3 model.
    video, generation, kling, v3, pro, text-to-video, txt2vid

    Use cases:
    - Create professional-grade videos from detailed prompts
    - Generate cinematic video content with precise motion
    - Produce high-fidelity advertising videos
    - Create premium animated content from scripts
    - Generate top-tier video for film and media
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV3ProTextToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV3ProTextToVideo.AspectRatio
    )
    ShotType: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV3ProTextToVideo.ShotType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.",
    )
    voice_ids: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice",
    )
    duration: nodetool.nodes.fal.text_to_video.KlingVideoV3ProTextToVideo.Duration = (
        Field(
            default=nodetool.nodes.fal.text_to_video.KlingVideoV3ProTextToVideo.Duration.VALUE_5,
            description="The duration of the generated video in seconds",
        )
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.KlingVideoV3ProTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV3ProTextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )
    multi_prompt: (
        list[types.KlingV3MultiPromptElement]
        | OutputHandle[list[types.KlingV3MultiPromptElement]]
    ) = connect_field(
        default=[],
        description="List of prompts for multi-shot video generation. If provided, overrides the single prompt and divides the video into multiple shots with specified prompts and durations.",
    )
    shot_type: nodetool.nodes.fal.text_to_video.KlingVideoV3ProTextToVideo.ShotType = (
        Field(
            default=nodetool.nodes.fal.text_to_video.KlingVideoV3ProTextToVideo.ShotType.CUSTOMIZE,
            description="The type of multi-shot video generation",
        )
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoV3ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV3StandardTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video V3 Standard generates videos from text prompts with balanced quality and speed using the latest V3 model.
    video, generation, kling, v3, standard, text-to-video, txt2vid

    Use cases:
    - Generate cinematic videos from text descriptions
    - Create marketing videos from product descriptions
    - Produce educational video content from scripts
    - Generate social media video content
    - Create animated scenes from text prompts
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV3StandardTextToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV3StandardTextToVideo.AspectRatio
    )
    ShotType: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.KlingVideoV3StandardTextToVideo.ShotType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.",
    )
    voice_ids: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice",
    )
    duration: (
        nodetool.nodes.fal.text_to_video.KlingVideoV3StandardTextToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV3StandardTextToVideo.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.KlingVideoV3StandardTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV3StandardTextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )
    multi_prompt: (
        list[types.KlingV3MultiPromptElement]
        | OutputHandle[list[types.KlingV3MultiPromptElement]]
    ) = connect_field(
        default=[],
        description="List of prompts for multi-shot video generation. If provided, overrides the single prompt and divides the video into multiple shots with specified prompts and durations.",
    )
    shot_type: (
        nodetool.nodes.fal.text_to_video.KlingVideoV3StandardTextToVideo.ShotType
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.KlingVideoV3StandardTextToVideo.ShotType.CUSTOMIZE,
        description="The type of multi-shot video generation",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoV3StandardTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class KreaWan14BTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Krea Wan 14b- Text to Video
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Prompt for the video-to-video generation."
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=78,
        description="Number of frames to generate. Must be a multiple of 12 plus 6, for example 6, 18, 30, 42, etc.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default="", description="Seed for the video-to-video generation."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KreaWan14BTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class LTXVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    LTX Video generates high-quality videos from text prompts with advanced temporal consistency.
    video, generation, ltx, text-to-video, txt2vid

    Use cases:
    - Generate temporally consistent videos
    - Create smooth video sequences
    - Produce high-quality video content
    - Generate professional video clips
    - Create cinematic video scenes
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3, description="The guidance scale to use."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed to use for random number generation."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="The number of inference steps to take."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly",
        description="The negative prompt to generate the video from.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LTXVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class LongcatVideoDistilledTextToVideo480P(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LongCat Video Distilled
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoWriteMode
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.AspectRatio
    )
    VideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoOutputType
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoQuality
    )

    video_write_mode: (
        nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to guide the video generation."
    )
    video_output_type: (
        nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=15, description="The frame rate of the generated video."
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: (
        nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable safety checker."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=162, description="The number of frames to generate."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=12, description="The number of inference steps to use."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class LongcatVideoDistilledTextToVideo720P(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LongCat Video Distilled
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoWriteMode
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.AspectRatio
    )
    VideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoOutputType
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoQuality
    )

    video_write_mode: (
        nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to guide the video generation."
    )
    video_output_type: (
        nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=30, description="The frame rate of the generated video."
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    num_refine_inference_steps: int | OutputHandle[int] = connect_field(
        default=12, description="The number of inference steps to use for refinement."
    )
    video_quality: (
        nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable safety checker."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=162, description="The number of frames to generate."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=12, description="The number of inference steps to use."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class LongcatVideoTextToVideo480P(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LongCat Video
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.Acceleration
    )
    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoWriteMode
    )
    VideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoOutputType
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.AspectRatio
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoQuality
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to guide the video generation."
    )
    acceleration: (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.Acceleration.REGULAR,
        description="The acceleration level to use for the video generation.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=15, description="The frame rate of the generated video."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=4, description="The guidance scale to use for the video generation."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=162, description="The number of frames to generate."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable safety checker."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
        description="The negative prompt to use for the video generation.",
    )
    video_write_mode: (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    video_output_type: (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40,
        description="The number of inference steps to use for the video generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class LongcatVideoTextToVideo720P(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LongCat Video
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.Acceleration
    )
    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoWriteMode
    )
    VideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoOutputType
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.AspectRatio
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoQuality
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to guide the video generation."
    )
    acceleration: (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.Acceleration.REGULAR,
        description="The acceleration level to use for the video generation.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=30, description="The frame rate of the generated video."
    )
    num_refine_inference_steps: int | OutputHandle[int] = connect_field(
        default=40, description="The number of inference steps to use for refinement."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=4, description="The guidance scale to use for the video generation."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=162, description="The number of frames to generate."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable safety checker."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
        description="The negative prompt to use for the video generation.",
    )
    video_write_mode: (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    video_output_type: (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: (
        nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40,
        description="The number of inference steps to use for the video generation.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class Ltx219BDistilledTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LTX-2 19B Distilled
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.Acceleration
    )
    CameraLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.CameraLora
    )
    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoWriteMode
    )
    VideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoOutputType
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoQuality
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    use_multiscale: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.",
    )
    acceleration: (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.Acceleration.NONE,
        description="The acceleration level to use.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    fps: float | OutputHandle[float] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="landscape_4_3", description="The size of the generated video."
    )
    camera_lora: (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.CameraLora
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.CameraLora.NONE,
        description="The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    camera_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
        description="The negative prompt to generate the video from.",
    )
    video_write_mode: (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    video_output_type: (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class Ltx219BDistilledTextToVideoLora(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LTX-2 19B Distilled
    video, generation, text-to-video, txt2vid, lora

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.Acceleration
    )
    CameraLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.CameraLora
    )
    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoWriteMode
    )
    VideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoOutputType
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoQuality
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    use_multiscale: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.",
    )
    acceleration: (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.Acceleration.NONE,
        description="The acceleration level to use.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    fps: float | OutputHandle[float] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    loras: list[types.LoRAInput] | OutputHandle[list[types.LoRAInput]] = connect_field(
        default=[], description="The LoRAs to use for the generation."
    )
    camera_lora: (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.CameraLora
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.CameraLora.NONE,
        description="The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="landscape_4_3", description="The size of the generated video."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    camera_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
        description="The negative prompt to generate the video from.",
    )
    video_write_mode: (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    video_output_type: (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: (
        nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class Ltx219BTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LTX-2 19B
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.Acceleration
    )
    CameraLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.CameraLora
    )
    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoWriteMode
    )
    VideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoOutputType
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoQuality
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    use_multiscale: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.",
    )
    acceleration: nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.Acceleration = (
        Field(
            default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.Acceleration.REGULAR,
            description="The acceleration level to use.",
        )
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    fps: float | OutputHandle[float] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    camera_lora: nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.CameraLora = Field(
        default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.CameraLora.NONE,
        description="The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="landscape_4_3", description="The size of the generated video."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3, description="The guidance scale to use."
    )
    camera_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
        description="The negative prompt to generate the video from.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )
    video_write_mode: (
        nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    video_output_type: (
        nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoQuality = (
        Field(
            default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoQuality.HIGH,
            description="The quality of the generated video.",
        )
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40, description="The number of inference steps to use."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class Ltx219BTextToVideoLora(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LTX-2 19B
    video, generation, text-to-video, txt2vid, lora

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.Acceleration
    )
    CameraLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.CameraLora
    )
    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoWriteMode
    )
    VideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoOutputType
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoQuality
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    use_multiscale: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.",
    )
    acceleration: (
        nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.Acceleration.REGULAR,
        description="The acceleration level to use.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    fps: float | OutputHandle[float] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    loras: list[types.LoRAInput] | OutputHandle[list[types.LoRAInput]] = connect_field(
        default=[], description="The LoRAs to use for the generation."
    )
    camera_lora: nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.CameraLora = (
        Field(
            default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.CameraLora.NONE,
            description="The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
        )
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="landscape_4_3", description="The size of the generated video."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3, description="The guidance scale to use."
    )
    camera_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
        description="The negative prompt to generate the video from.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )
    video_write_mode: (
        nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    video_output_type: (
        nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: (
        nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40, description="The number of inference steps to use."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class LtxVideo13bDev(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate videos from prompts using LTX Video-0.9.7 13B and custom LoRA
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LtxVideo13bDev.Resolution
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LtxVideo13bDev.AspectRatio
    )

    second_pass_skip_initial_steps: int | OutputHandle[int] = connect_field(
        default=17,
        description="The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.",
    )
    first_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="Number of inference steps during the first pass."
    )
    frame_rate: int | OutputHandle[int] = connect_field(
        default=30, description="The frame rate of the video."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide generation"
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to reverse the video."
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to expand the prompt using a language model.",
    )
    loras: list[types.LoRAWeight] | OutputHandle[list[types.LoRAWeight]] = (
        connect_field(default=[], description="LoRA weights to use for generation")
    )
    second_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="Number of inference steps during the second pass."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames in the video."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, inconsistent motion, blurry, jittery, distorted",
        description="Negative prompt for generation",
    )
    resolution: nodetool.nodes.fal.text_to_video.LtxVideo13bDev.Resolution = Field(
        default=nodetool.nodes.fal.text_to_video.LtxVideo13bDev.Resolution.VALUE_720P,
        description="Resolution of the generated video (480p or 720p).",
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.LtxVideo13bDev.AspectRatio = Field(
        default=nodetool.nodes.fal.text_to_video.LtxVideo13bDev.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video (16:9, 1:1 or 9:16).",
    )
    first_pass_skip_final_steps: int | OutputHandle[int] = connect_field(
        default=3,
        description="Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LtxVideo13bDev

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class LtxVideo13bDistilled(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate videos from prompts using LTX Video-0.9.7 13B Distilled and custom LoRA
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LtxVideo13bDistilled.Resolution
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LtxVideo13bDistilled.AspectRatio
    )

    second_pass_skip_initial_steps: int | OutputHandle[int] = connect_field(
        default=5,
        description="The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.",
    )
    first_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the first pass."
    )
    frame_rate: int | OutputHandle[int] = connect_field(
        default=30, description="The frame rate of the video."
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to reverse the video."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide generation"
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to expand the prompt using a language model.",
    )
    loras: list[types.LoRAWeight] | OutputHandle[list[types.LoRAWeight]] = (
        connect_field(default=[], description="LoRA weights to use for generation")
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames in the video."
    )
    second_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the second pass."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, inconsistent motion, blurry, jittery, distorted",
        description="Negative prompt for generation",
    )
    resolution: nodetool.nodes.fal.text_to_video.LtxVideo13bDistilled.Resolution = (
        Field(
            default=nodetool.nodes.fal.text_to_video.LtxVideo13bDistilled.Resolution.VALUE_720P,
            description="Resolution of the generated video (480p or 720p).",
        )
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.LtxVideo13bDistilled.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.text_to_video.LtxVideo13bDistilled.AspectRatio.RATIO_16_9,
            description="Aspect ratio of the generated video (16:9, 1:1 or 9:16).",
        )
    )
    first_pass_skip_final_steps: int | OutputHandle[int] = connect_field(
        default=1,
        description="Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LtxVideo13bDistilled

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class LtxVideoV095(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate videos from prompts using LTX Video-0.9.5
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LtxVideoV095.Resolution
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LtxVideoV095.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide generation"
    )
    resolution: nodetool.nodes.fal.text_to_video.LtxVideoV095.Resolution = Field(
        default=nodetool.nodes.fal.text_to_video.LtxVideoV095.Resolution.VALUE_720P,
        description="Resolution of the generated video (480p or 720p).",
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.LtxVideoV095.AspectRatio = Field(
        default=nodetool.nodes.fal.text_to_video.LtxVideoV095.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video (16:9 or 9:16).",
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to expand the prompt using the model's own capabilities.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40, description="Number of inference steps"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, inconsistent motion, blurry, jittery, distorted",
        description="Negative prompt for generation",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LtxVideoV095

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class Ltxv13b098Distilled(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate long videos from prompts using LTX Video-0.9.8 13B Distilled and custom LoRA
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltxv13b098Distilled.Resolution
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Ltxv13b098Distilled.AspectRatio
    )

    second_pass_skip_initial_steps: int | OutputHandle[int] = connect_field(
        default=5,
        description="The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.",
    )
    first_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the first pass."
    )
    frame_rate: int | OutputHandle[int] = connect_field(
        default=24, description="The frame rate of the video."
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to reverse the video."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide generation"
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to expand the prompt using a language model.",
    )
    temporal_adain_factor: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution.",
    )
    loras: list[types.LoRAWeight] | OutputHandle[list[types.LoRAWeight]] = (
        connect_field(default=[], description="LoRA weights to use for generation")
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames in the video."
    )
    second_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the second pass."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, inconsistent motion, blurry, jittery, distorted",
        description="Negative prompt for generation",
    )
    enable_detail_pass: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.",
    )
    resolution: nodetool.nodes.fal.text_to_video.Ltxv13b098Distilled.Resolution = Field(
        default=nodetool.nodes.fal.text_to_video.Ltxv13b098Distilled.Resolution.VALUE_720P,
        description="Resolution of the generated video.",
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.Ltxv13b098Distilled.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.text_to_video.Ltxv13b098Distilled.AspectRatio.RATIO_16_9,
            description="Aspect ratio of the generated video.",
        )
    )
    tone_map_compression_ratio: float | OutputHandle[float] = connect_field(
        default=0,
        description="The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Ltxv13b098Distilled

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class LumaDreamMachineRay2(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Ray2 is a large-scale video generative model capable of creating realistic visuals with natural, coherent motion.
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2.AspectRatio.RATIO_16_9,
            description="The aspect ratio of the generated video",
        )
    )
    resolution: nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2.Resolution = (
        Field(
            default=nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2.Resolution.VALUE_540P,
            description="The resolution of the generated video (720p costs 2x more, 1080p costs 4x more)",
        )
    )
    loop: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether the video should loop (end of video is blended with the beginning)",
    )
    duration: nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2.Duration = Field(
        default=nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2.Duration.VALUE_5S,
        description="The duration of the generated video (9s costs 2x more)",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class LumaDreamMachineRay2Flash(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Ray2 Flash is a fast video generative model capable of creating realistic visuals with natural, coherent motion.
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2Flash.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2Flash.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2Flash.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2Flash.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2Flash.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: (
        nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2Flash.Resolution
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2Flash.Resolution.VALUE_540P,
        description="The resolution of the generated video (720p costs 2x more, 1080p costs 4x more)",
    )
    loop: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether the video should loop (end of video is blended with the beginning)",
    )
    duration: nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2Flash.Duration = (
        Field(
            default=nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2Flash.Duration.VALUE_5S,
            description="The duration of the generated video (9s costs 2x more)",
        )
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LumaDreamMachineRay2Flash

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class LumaDreamMachineTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Luma Dream Machine generates creative videos from text with dreamlike aesthetics.
    video, generation, luma, dream-machine, text-to-video, txt2vid

    Use cases:
    - Generate dreamlike video content
    - Create surreal video sequences
    - Produce artistic video interpretations
    - Generate creative video concepts
    - Create imaginative video art
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LumaDreamMachineTextToVideo.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.LumaDreamMachineTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.LumaDreamMachineTextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    loop: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether the video should loop (end of video is blended with the beginning)",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LumaDreamMachineTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class LumaPhoton(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Luma Photon generates photorealistic videos from text with high visual fidelity.
    video, generation, luma, photon, photorealistic, text-to-video

    Use cases:
    - Generate photorealistic video content
    - Create realistic video simulations
    - Produce lifelike video scenes
    - Generate high-fidelity video outputs
    - Create realistic visual content
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.LumaPhoton.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: nodetool.nodes.fal.text_to_video.LumaPhoton.AspectRatio = Field(
        default=nodetool.nodes.fal.text_to_video.LumaPhoton.AspectRatio.RATIO_1_1,
        description="The aspect ratio of the generated video",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LumaPhoton

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class MinimaxHailuo02ProTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    MiniMax Hailuo-02 Text To Video API (Pro, 1080p): Advanced video generation model with 1080p resolution
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the model's prompt optimizer"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.MinimaxHailuo02ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class MinimaxHailuo23ProTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    MiniMax Hailuo 2.3 [Pro] (Text to Video)
    video, generation, text-to-video, txt2vid, professional

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for video generation"
    )
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the model's prompt optimizer"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.MinimaxHailuo23ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class MinimaxHailuo23StandardTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    MiniMax Hailuo 2.3 [Standard] (Text to Video)
    video, generation, text-to-video, txt2vid, professional

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.MinimaxHailuo23StandardTextToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    duration: (
        nodetool.nodes.fal.text_to_video.MinimaxHailuo23StandardTextToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.MinimaxHailuo23StandardTextToVideo.Duration.VALUE_6,
        description="The duration of the video in seconds.",
    )
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the model's prompt optimizer"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.MinimaxHailuo23StandardTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class MinimaxVideo01(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate video clips from your prompts using MiniMax model
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the model's prompt optimizer"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.MinimaxVideo01

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class MinimaxVideo01Director(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips more accurately with respect to natural language descriptions and using camera movement instructions for shot control.
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for video generation. Camera movement instructions can be added using square brackets (e.g. [Pan left] or [Zoom in]). You can use up to 3 combined movements per prompt. Supported movements: Truck left/right, Pan left/right, Push in/Pull out, Pedestal up/down, Tilt up/down, Zoom in/out, Shake, Tracking shot, Static shot. For example: [Truck left, Pan right, Zoom in]. For a more detailed guide, refer https://sixth-switch-2ac.notion.site/T2V-01-Director-Model-Tutorial-with-camera-movement-1886c20a98eb80f395b8e05291ad8645",
    )
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the model's prompt optimizer"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.MinimaxVideo01Director

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class MinimaxVideo01Live(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips from your prompts using MiniMax model
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the model's prompt optimizer"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.MinimaxVideo01Live

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class MochiV1(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Mochi v1 generates creative videos from text with unique artistic style.
    video, generation, mochi, artistic, text-to-video, txt2vid

    Use cases:
    - Generate artistic video content
    - Create stylized animations
    - Produce creative video art
    - Generate experimental videos
    - Create unique visual content
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate a video from."
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed to use for generating the video."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="The negative prompt for the video."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.MochiV1

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class MoonvalleyMareyT2V(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Marey Realism V1.5
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V.Duration
    )
    Dimensions: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V.Dimensions
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate a video from"
    )
    duration: nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V.Duration = Field(
        default=nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V.Duration.VALUE_5S,
        description="The duration of the generated video.",
    )
    dimensions: nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V.Dimensions = Field(
        default=nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V.Dimensions.VALUE_1920X1080,
        description="The dimensions of the generated video in width x height format.",
    )
    guidance_scale: str | OutputHandle[str] = connect_field(
        default="",
        description="Controls how strongly the generation is guided by the prompt (0-20). Higher values follow the prompt more closely.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default=-1,
        description="Seed for random number generation. Use -1 for random seed each run.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="<synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts",
        description="Negative prompt used to guide the model away from undesirable features.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class Ovi(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Ovi Text to Video
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ovi.Resolution

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    resolution: nodetool.nodes.fal.text_to_video.Ovi.Resolution = Field(
        default=nodetool.nodes.fal.text_to_video.Ovi.Resolution.VALUE_992X512,
        description="Resolution of the generated video in W:H format. One of (512x992, 992x512, 960x512, 512x960, 720x720, or 448x1120).",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="The number of inference steps."
    )
    audio_negative_prompt: str | OutputHandle[str] = connect_field(
        default="robotic, muffled, echo, distorted",
        description="Negative prompt for audio generation.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="jitter, bad hands, blur, distortion",
        description="Negative prompt for video generation.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Ovi

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PikaV21TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Start with a simple text input to create dynamic generations that defy expectations. Anything you dream can come to life with sharp details, impressive character control and cinematic camera moves.
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PikaV21TextToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PikaV21TextToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: nodetool.nodes.fal.text_to_video.PikaV21TextToVideo.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.text_to_video.PikaV21TextToVideo.AspectRatio.RATIO_16_9,
            description="The aspect ratio of the generated video",
        )
    )
    resolution: nodetool.nodes.fal.text_to_video.PikaV21TextToVideo.Resolution = Field(
        default=nodetool.nodes.fal.text_to_video.PikaV21TextToVideo.Resolution.VALUE_720P,
        description="The resolution of the generated video",
    )
    duration: int | OutputHandle[int] = connect_field(
        default=5, description="The duration of the generated video in seconds"
    )
    seed: str | OutputHandle[str] = connect_field(
        default="", description="The seed for the random number generator"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A negative prompt to guide the model"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PikaV21TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PikaV2TurboTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Pika v2 Turbo creates videos from a text prompt with high quality output.
    video, generation, text-to-video, txt2vid, fast

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PikaV2TurboTextToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PikaV2TurboTextToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.PikaV2TurboTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.PikaV2TurboTextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: nodetool.nodes.fal.text_to_video.PikaV2TurboTextToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.text_to_video.PikaV2TurboTextToVideo.Resolution.VALUE_720P,
            description="The resolution of the generated video",
        )
    )
    duration: int | OutputHandle[int] = connect_field(
        default=5, description="The duration of the generated video in seconds"
    )
    seed: str | OutputHandle[str] = connect_field(
        default="", description="The seed for the random number generator"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A negative prompt to guide the model"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PikaV2TurboTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV35TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate high quality video clips from text prompts using PixVerse v3.5
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV35TextToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV35TextToVideo.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV35TextToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.PixverseV35TextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.PixverseV35TextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: nodetool.nodes.fal.text_to_video.PixverseV35TextToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.text_to_video.PixverseV35TextToVideo.Resolution.VALUE_720P,
            description="The resolution of the generated video",
        )
    )
    style: (
        nodetool.nodes.fal.text_to_video.PixverseV35TextToVideo.Style
        | OutputHandle[nodetool.nodes.fal.text_to_video.PixverseV35TextToVideo.Style]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    duration: nodetool.nodes.fal.text_to_video.PixverseV35TextToVideo.Duration = Field(
        default=nodetool.nodes.fal.text_to_video.PixverseV35TextToVideo.Duration.VALUE_5,
        description="The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseV35TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV35TextToVideoFast(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate high quality video clips quickly from text prompts using PixVerse v3.5 Fast
    video, generation, text-to-video, txt2vid, fast

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV35TextToVideoFast.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV35TextToVideoFast.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.PixverseV35TextToVideoFast.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.PixverseV35TextToVideoFast.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: (
        nodetool.nodes.fal.text_to_video.PixverseV35TextToVideoFast.Resolution
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.PixverseV35TextToVideoFast.Resolution.VALUE_720P,
        description="The resolution of the generated video",
    )
    style: (
        nodetool.nodes.fal.text_to_video.PixverseV35TextToVideoFast.Style
        | OutputHandle[
            nodetool.nodes.fal.text_to_video.PixverseV35TextToVideoFast.Style
        ]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseV35TextToVideoFast

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV45TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate high quality video clips from text and image prompts using PixVerse v4.5
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV45TextToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV45TextToVideo.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV45TextToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.PixverseV45TextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.PixverseV45TextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: nodetool.nodes.fal.text_to_video.PixverseV45TextToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.text_to_video.PixverseV45TextToVideo.Resolution.VALUE_720P,
            description="The resolution of the generated video",
        )
    )
    style: (
        nodetool.nodes.fal.text_to_video.PixverseV45TextToVideo.Style
        | OutputHandle[nodetool.nodes.fal.text_to_video.PixverseV45TextToVideo.Style]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    duration: nodetool.nodes.fal.text_to_video.PixverseV45TextToVideo.Duration = Field(
        default=nodetool.nodes.fal.text_to_video.PixverseV45TextToVideo.Duration.VALUE_5,
        description="The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseV45TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV45TextToVideoFast(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate high quality and fast video clips from text and image prompts using PixVerse v4.5 fast
    video, generation, text-to-video, txt2vid, fast

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV45TextToVideoFast.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV45TextToVideoFast.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.PixverseV45TextToVideoFast.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.PixverseV45TextToVideoFast.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: (
        nodetool.nodes.fal.text_to_video.PixverseV45TextToVideoFast.Resolution
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.PixverseV45TextToVideoFast.Resolution.VALUE_720P,
        description="The resolution of the generated video",
    )
    style: (
        nodetool.nodes.fal.text_to_video.PixverseV45TextToVideoFast.Style
        | OutputHandle[
            nodetool.nodes.fal.text_to_video.PixverseV45TextToVideoFast.Style
        ]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseV45TextToVideoFast

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV4TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate high quality video clips from text and image prompts using PixVerse v4
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV4TextToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV4TextToVideo.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV4TextToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: nodetool.nodes.fal.text_to_video.PixverseV4TextToVideo.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.text_to_video.PixverseV4TextToVideo.AspectRatio.RATIO_16_9,
            description="The aspect ratio of the generated video",
        )
    )
    resolution: nodetool.nodes.fal.text_to_video.PixverseV4TextToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.text_to_video.PixverseV4TextToVideo.Resolution.VALUE_720P,
            description="The resolution of the generated video",
        )
    )
    style: (
        nodetool.nodes.fal.text_to_video.PixverseV4TextToVideo.Style
        | OutputHandle[nodetool.nodes.fal.text_to_video.PixverseV4TextToVideo.Style]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    duration: nodetool.nodes.fal.text_to_video.PixverseV4TextToVideo.Duration = Field(
        default=nodetool.nodes.fal.text_to_video.PixverseV4TextToVideo.Duration.VALUE_5,
        description="The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseV4TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV4TextToVideoFast(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate high quality and fast video clips from text and image prompts using PixVerse v4 fast
    video, generation, text-to-video, txt2vid, fast

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV4TextToVideoFast.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV4TextToVideoFast.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.PixverseV4TextToVideoFast.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.PixverseV4TextToVideoFast.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: (
        nodetool.nodes.fal.text_to_video.PixverseV4TextToVideoFast.Resolution
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.PixverseV4TextToVideoFast.Resolution.VALUE_720P,
        description="The resolution of the generated video",
    )
    style: (
        nodetool.nodes.fal.text_to_video.PixverseV4TextToVideoFast.Style
        | OutputHandle[nodetool.nodes.fal.text_to_video.PixverseV4TextToVideoFast.Style]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseV4TextToVideoFast

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV55TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Pixverse
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.Resolution.VALUE_720P,
            description="The resolution of the generated video",
        )
    )
    style: (
        nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.Style
        | OutputHandle[nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.Style]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    thinking_type: (
        nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.ThinkingType
        | OutputHandle[
            nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.ThinkingType
        ]
        | None
    ) = connect_field(
        default=None,
        description="Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision",
    )
    generate_multi_clip_switch: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable multi-clip generation with dynamic camera changes",
    )
    duration: nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.Duration = Field(
        default=nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo.Duration.VALUE_5,
        description="The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds",
    )
    generate_audio_switch: bool | OutputHandle[bool] = connect_field(
        default=False, description="Enable audio generation (BGM, SFX, dialogue)"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseV55TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV56TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Pixverse
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.Resolution.VALUE_720P,
            description="The resolution of the generated video",
        )
    )
    style: (
        nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.Style
        | OutputHandle[nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.Style]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    thinking_type: (
        nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.ThinkingType
        | OutputHandle[
            nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.ThinkingType
        ]
        | None
    ) = connect_field(
        default=None,
        description="Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision",
    )
    duration: nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.Duration = Field(
        default=nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo.Duration.VALUE_5,
        description="The duration of the generated video in seconds. 1080p videos are limited to 5 or 8 seconds",
    )
    generate_audio_switch: bool | OutputHandle[bool] = connect_field(
        default=False, description="Enable audio generation (BGM, SFX, dialogue)"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseV56TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV5TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Pixverse
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.AspectRatio.RATIO_16_9,
            description="The aspect ratio of the generated video",
        )
    )
    resolution: nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Resolution.VALUE_720P,
            description="The resolution of the generated video",
        )
    )
    style: (
        nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Style
        | OutputHandle[nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Style]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    duration: nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Duration = Field(
        default=nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Duration.VALUE_5,
        description="The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class SanaVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Sana Video
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.SanaVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt describing the video to generate"
    )
    resolution: nodetool.nodes.fal.text_to_video.SanaVideo.Resolution = Field(
        default=nodetool.nodes.fal.text_to_video.SanaVideo.Resolution.VALUE_480P,
        description="The resolution of the output video",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=16, description="Frames per second for the output video"
    )
    motion_score: int | OutputHandle[int] = connect_field(
        default=30, description="Motion intensity score (higher = more motion)"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=6,
        description="Guidance scale for generation (higher = more prompt adherence)",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28, description="Number of denoising steps"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducible generation. If not provided, a random seed will be used.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="A chaotic sequence with misshapen, deformed limbs in heavy motion blur, sudden disappearance, jump cuts, jerky movements, rapid shot changes, frames out of sync, inconsistent character shapes, temporal artifacts, jitter, and ghosting effects, creating a disorienting visual experience.",
        description="The negative prompt describing what to avoid in the generation",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81, description="Number of frames to generate"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.SanaVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class SeeDanceV15ProTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    SeeDance v1.5 Pro from ByteDance generates high-quality dance videos from text prompts.
    video, generation, dance, seedance, bytedance, text-to-video

    Use cases:
    - Generate dance choreography videos
    - Create dance performance visualizations
    - Produce music video concepts
    - Generate dance training content
    - Create dance animation prototypes
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.AspectRatio
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.Duration
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt used to generate the video"
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    duration: nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.Duration = (
        Field(
            default=nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.Duration.VALUE_5,
            description="Duration of the video in seconds",
        )
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video"
    )
    resolution: (
        nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.Resolution.VALUE_720P,
        description="Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="If set to true, the safety checker will be enabled."
    )
    camera_fixed: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to fix the camera position"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed to control video generation. Use -1 for random.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class SeeDanceV1ProFastTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    SeeDance v1 Pro Fast generates dance videos quickly from text with reduced generation time.
    video, generation, dance, seedance, fast, bytedance, text-to-video

    Use cases:
    - Rapidly prototype dance videos
    - Create quick dance previews
    - Generate dance concepts efficiently
    - Iterate on choreography ideas
    - Produce dance storyboards
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt used to generate the video"
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: (
        nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.Resolution.VALUE_1080P,
        description="Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality",
    )
    duration: nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.Duration = (
        Field(
            default=nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.Duration.VALUE_5,
            description="Duration of the video in seconds",
        )
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="If set to true, the safety checker will be enabled."
    )
    camera_fixed: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to fix the camera position"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed to control video generation. Use -1 for random.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class StableVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Stable Video generates consistent and stable video sequences from text prompts.
    video, generation, stable, text-to-video, txt2vid

    Use cases:
    - Generate stable video sequences
    - Create consistent video content
    - Produce reliable video outputs
    - Generate predictable video scenes
    - Create controlled video generation
    """

    motion_bucket_id: int | OutputHandle[int] = connect_field(
        default=127,
        description="The motion bucket id determines the motion of the generated video. The higher the number, the more motion there will be.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    cond_aug: float | OutputHandle[float] = connect_field(
        default=0.02,
        description="The conditoning augmentation determines the amount of noise that will be added to the conditioning frame. The higher the number, the more noise there will be, and the less the video will look like the initial image. Increase it for more motion.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as a starting point for the generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.StableVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class T2VTurbo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    T2V Turbo generates videos from text at high speed with optimized performance.
    video, generation, turbo, fast, text-to-video, txt2vid

    Use cases:
    - Generate videos with minimal latency
    - Create rapid video prototypes
    - Produce quick video previews
    - Generate real-time video content
    - Create efficient video workflows
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate images from"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="The guidance scale"
    )
    seed: str | OutputHandle[str] = connect_field(
        default="", description="The seed to use for the random number generator"
    )
    export_fps: int | OutputHandle[int] = connect_field(
        default=8, description="The FPS of the exported video"
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=16, description="The number of frames to generate"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=4, description="The number of steps to sample"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.T2VTurbo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class Transpixar(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

    Transform text into stunning videos with TransPixar - an AI model that generates both RGB footage and alpha channels, enabling seamless compositing and creative video effects.
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related video to show you.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=24, description="The number of inference steps to perform."
    )
    export_fps: int | OutputHandle[int] = connect_field(
        default=8, description="The target FPS of the video"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="The negative prompt to generate video from"
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Transpixar

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class VeedAvatarsTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    VEED Avatars generates talking avatar videos from text using realistic AI-powered characters.
    video, generation, avatar, talking-head, veed, text-to-video

    Use cases:
    - Create talking avatar presentations
    - Generate spokesperson videos
    - Produce educational talking head videos
    - Create personalized video messages
    - Generate multilingual avatar content
    """

    AvatarId: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.VeedAvatarsTextToVideo.AvatarId
    )

    text: str | OutputHandle[str] = connect_field(default="", description=None)
    avatar_id: nodetool.nodes.fal.text_to_video.VeedAvatarsTextToVideo.AvatarId = Field(
        default=nodetool.nodes.fal.text_to_video.VeedAvatarsTextToVideo.AvatarId(""),
        description="The avatar to use for the video",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.VeedAvatarsTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class VeedFabric10Text(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    VEED Fabric 1.0 generates video content from text using advanced video synthesis.
    video, generation, fabric, veed, text-to-video, txt2vid

    Use cases:
    - Generate marketing videos from text
    - Create explainer video content
    - Produce video ads from copy
    - Generate social media videos
    - Create branded video content
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.VeedFabric10Text.Resolution
    )

    text: str | OutputHandle[str] = connect_field(default="", description=None)
    resolution: nodetool.nodes.fal.text_to_video.VeedFabric10Text.Resolution = Field(
        default=nodetool.nodes.fal.text_to_video.VeedFabric10Text.Resolution(""),
        description="Resolution",
    )
    voice_description: str | OutputHandle[str] = connect_field(
        default="",
        description="Optional additional voice description. The primary voice description is auto-generated from the image. You can use simple descriptors like 'British accent' or 'Confident' or provide a detailed description like 'Confident male voice, mid-20s, with notes of...'",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description=None,
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.VeedFabric10Text

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class Veo2(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Veo 2 creates videos with realistic motion and high quality output. Explore different styles and find your own with extensive camera controls.
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Veo2.Duration
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Veo2.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt describing the video you want to generate",
    )
    duration: nodetool.nodes.fal.text_to_video.Veo2.Duration = Field(
        default=nodetool.nodes.fal.text_to_video.Veo2.Duration.VALUE_5S,
        description="The duration of the generated video in seconds",
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.Veo2.AspectRatio = Field(
        default=nodetool.nodes.fal.text_to_video.Veo2.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="A seed to use for the video generation"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A negative prompt to guide the video generation"
    )
    enhance_prompt: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enhance the video generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Veo2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class Veo31(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Veo 3.1
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Veo31.Duration
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Veo31.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Veo31.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt describing the video you want to generate",
    )
    duration: nodetool.nodes.fal.text_to_video.Veo31.Duration = Field(
        default=nodetool.nodes.fal.text_to_video.Veo31.Duration.VALUE_8S,
        description="The duration of the generated video.",
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.Veo31.AspectRatio = Field(
        default=nodetool.nodes.fal.text_to_video.Veo31.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    auto_fix: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.",
    )
    resolution: nodetool.nodes.fal.text_to_video.Veo31.Resolution = Field(
        default=nodetool.nodes.fal.text_to_video.Veo31.Resolution.VALUE_720P,
        description="The resolution of the generated video.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A negative prompt to guide the video generation."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Veo31

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class Veo31Fast(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Veo 3.1 Fast
    video, generation, text-to-video, txt2vid, fast

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Veo31Fast.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Veo31Fast.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Veo31Fast.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt describing the video you want to generate",
    )
    duration: nodetool.nodes.fal.text_to_video.Veo31Fast.Duration = Field(
        default=nodetool.nodes.fal.text_to_video.Veo31Fast.Duration.VALUE_8S,
        description="The duration of the generated video.",
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.Veo31Fast.AspectRatio = Field(
        default=nodetool.nodes.fal.text_to_video.Veo31Fast.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    auto_fix: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.",
    )
    resolution: nodetool.nodes.fal.text_to_video.Veo31Fast.Resolution = Field(
        default=nodetool.nodes.fal.text_to_video.Veo31Fast.Resolution.VALUE_720P,
        description="The resolution of the generated video.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A negative prompt to guide the video generation."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Veo31Fast

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class ViduQ1TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Vidu Q1 Text to Video generates high-quality 1080p videos with exceptional visual quality and motion diversity
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.ViduQ1TextToVideo.AspectRatio
    )
    Style: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.ViduQ1TextToVideo.Style
    )
    MovementAmplitude: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.ViduQ1TextToVideo.MovementAmplitude
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for video generation, max 1500 characters"
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.ViduQ1TextToVideo.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.text_to_video.ViduQ1TextToVideo.AspectRatio.RATIO_16_9,
            description="The aspect ratio of the output video",
        )
    )
    style: nodetool.nodes.fal.text_to_video.ViduQ1TextToVideo.Style = Field(
        default=nodetool.nodes.fal.text_to_video.ViduQ1TextToVideo.Style.GENERAL,
        description="The style of output video",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for the random number generator"
    )
    movement_amplitude: (
        nodetool.nodes.fal.text_to_video.ViduQ1TextToVideo.MovementAmplitude
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.ViduQ1TextToVideo.MovementAmplitude.AUTO,
        description="The movement amplitude of objects in the frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.ViduQ1TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class Wan25PreviewTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan 2.5 Text to Video
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.AspectRatio
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.Duration
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt for video generation. Supports Chinese and English, max 800 characters.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    duration: nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.Duration = Field(
        default=nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.Duration.VALUE_5,
        description="Duration of the generated video in seconds. Choose between 5 or 10 seconds.",
    )
    resolution: nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.Resolution.VALUE_1080P,
            description="Video resolution tier",
        )
    )
    audio: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the audio to use as the background music. Must be publicly accessible. Limit handling: If the audio duration exceeds the duration value (5 or 10 seconds), the audio is truncated to the first 5 or 10 seconds, and the rest is discarded. If the audio is shorter than the video, the remaining part of the video will be silent. For example, if the audio is 3 seconds long and the video duration is 5 seconds, the first 3 seconds of the output video will have sound, and the last 2 seconds will be silent. - Format: WAV, MP3. - Duration: 3 to 30 s. - File size: Up to 15 MB.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="If set to true, the safety checker will be enabled."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Negative prompt to describe content to avoid. Max 500 characters.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanAlpha(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Wan Alpha
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Sampler: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.WanAlpha.Sampler
    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanAlpha.VideoWriteMode
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanAlpha.Resolution
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanAlpha.AspectRatio
    )
    VideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanAlpha.VideoOutputType
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanAlpha.VideoQuality
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to guide the video generation."
    )
    shift: float | OutputHandle[float] = connect_field(
        default=10.5, description="The shift of the generated video."
    )
    mask_clamp_upper: float | OutputHandle[float] = connect_field(
        default=0.75, description="The upper bound of the mask clamping."
    )
    fps: int | OutputHandle[int] = connect_field(
        default=16, description="The frame rate of the generated video."
    )
    mask_clamp_lower: float | OutputHandle[float] = connect_field(
        default=0.1, description="The lower bound of the mask clamping."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81, description="The number of frames to generate."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable safety checker."
    )
    mask_binarization_threshold: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="The threshold for mask binarization. When binarize_mask is True, this threshold will be used to binarize the mask. This will also be used for transparency when the output type is `.webm`.",
    )
    sampler: nodetool.nodes.fal.text_to_video.WanAlpha.Sampler = Field(
        default=nodetool.nodes.fal.text_to_video.WanAlpha.Sampler.EULER,
        description="The sampler to use.",
    )
    video_write_mode: nodetool.nodes.fal.text_to_video.WanAlpha.VideoWriteMode = Field(
        default=nodetool.nodes.fal.text_to_video.WanAlpha.VideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    resolution: nodetool.nodes.fal.text_to_video.WanAlpha.Resolution = Field(
        default=nodetool.nodes.fal.text_to_video.WanAlpha.Resolution.VALUE_480P,
        description="The resolution of the generated video.",
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.WanAlpha.AspectRatio = Field(
        default=nodetool.nodes.fal.text_to_video.WanAlpha.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video.",
    )
    video_output_type: nodetool.nodes.fal.text_to_video.WanAlpha.VideoOutputType = (
        Field(
            default=nodetool.nodes.fal.text_to_video.WanAlpha.VideoOutputType.VP9_WEBM,
            description="The output type of the generated video.",
        )
    )
    binarize_mask: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to binarize the mask."
    )
    video_quality: nodetool.nodes.fal.text_to_video.WanAlpha.VideoQuality = Field(
        default=nodetool.nodes.fal.text_to_video.WanAlpha.VideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="The number of inference steps to use."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanAlpha

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanProTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan-2.1 Pro is a premium text-to-video model that generates high-quality 1080p videos at 30fps with up to 6 seconds duration, delivering exceptional visual quality and motion diversity from text prompts
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video"
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker"
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanT2v(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Wan-2.1 is a text-to-video model that generates high-quality videos with high visual quality and motion diversity from text prompts
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanT2v.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanT2v.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.WanT2v.AspectRatio = Field(
        default=nodetool.nodes.fal.text_to_video.WanT2v.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video (16:9 or 9:16).",
    )
    resolution: nodetool.nodes.fal.text_to_video.WanT2v.Resolution = Field(
        default=nodetool.nodes.fal.text_to_video.WanT2v.Resolution.VALUE_720P,
        description="Resolution of the generated video (480p, 580p, or 720p).",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    turbo_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If true, the video will be generated faster with no noticeable degradation in the visual quality.",
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 24.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 100 (inclusive).",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanT2v

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanT2vLora(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Add custom LoRAs to Wan-2.1 is a text-to-video model that generates high-quality videos with high visual quality and motion diversity from images
    video, generation, text-to-video, txt2vid, lora

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanT2vLora.Resolution
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanT2vLora.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    resolution: nodetool.nodes.fal.text_to_video.WanT2vLora.Resolution = Field(
        default=nodetool.nodes.fal.text_to_video.WanT2vLora.Resolution.VALUE_480P,
        description="Resolution of the generated video (480p,580p, or 720p).",
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False, description="If true, the video will be reversed."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.WanT2vLora.AspectRatio = Field(
        default=nodetool.nodes.fal.text_to_video.WanT2vLora.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video (16:9 or 9:16).",
    )
    loras: list[types.LoraWeight] | OutputHandle[list[types.LoraWeight]] = (
        connect_field(
            default=[], description="LoRA weights to be used in the inference."
        )
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 24.",
    )
    turbo_mode: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, the video will be generated faster with no noticeable degradation in the visual quality.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 100 (inclusive).",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanT2vLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanV225bTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan 2.2's 5B model produces up to 5 seconds of video 720p at 24FPS with fluid motion and powerful prompt understanding
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.VideoWriteMode
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.Resolution
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.VideoQuality
    )
    InterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.InterpolatorModel
    )

    shift: float | OutputHandle[float] = connect_field(
        default=5,
        description="Shift value for the video. Must be between 1.0 and 10.0.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.",
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=24,
        description="Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5,
        description="Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 17 to 161 (inclusive).",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, input data will be checked for safety before processing.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt for video generation."
    )
    video_write_mode: (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.VideoWriteMode.BALANCED,
        description="The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.",
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.AspectRatio.RATIO_16_9,
            description="Aspect ratio of the generated video (16:9 or 9:16).",
        )
    )
    resolution: nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.Resolution = Field(
        default=nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.Resolution.VALUE_720P,
        description="Resolution of the generated video (580p or 720p).",
    )
    enable_output_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, output video will be checked for safety after generation.",
    )
    video_quality: nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.VideoQuality = (
        Field(
            default=nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.VideoQuality.HIGH,
            description="The quality of the output video. Higher quality means better visual quality but larger file size.",
        )
    )
    adjust_fps_for_interpolation: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.InterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV225bTextToVideo.InterpolatorModel.FILM,
        description="The model to use for frame interpolation. If None, no interpolation is applied.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanV225bTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanV225bTextToVideoDistill(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan 2.2's 5B distill model produces up to 5 seconds of video 720p at 24FPS with fluid motion and powerful prompt understanding
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.VideoWriteMode
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.Resolution
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.VideoQuality
    )
    InterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.InterpolatorModel
    )

    shift: float | OutputHandle[float] = connect_field(
        default=5,
        description="Shift value for the video. Must be between 1.0 and 10.0.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.",
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=24,
        description="Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 17 to 161 (inclusive).",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, input data will be checked for safety before processing.",
    )
    video_write_mode: (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.VideoWriteMode.BALANCED,
        description="The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video (16:9 or 9:16).",
    )
    resolution: (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.Resolution
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.Resolution.VALUE_720P,
        description="Resolution of the generated video (580p or 720p).",
    )
    enable_output_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, output video will be checked for safety after generation.",
    )
    video_quality: (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.VideoQuality.HIGH,
        description="The quality of the output video. Higher quality means better visual quality but larger file size.",
    )
    adjust_fps_for_interpolation: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.InterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill.InterpolatorModel.FILM,
        description="The model to use for frame interpolation. If None, no interpolation is applied.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanV225bTextToVideoDistill

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanV225bTextToVideoFastWan(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan 2.2's 5B FastVideo model produces up to 5 seconds of video 720p at 24FPS with fluid motion and powerful prompt understanding
    video, generation, text-to-video, txt2vid, fast

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.VideoWriteMode
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.Resolution
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.VideoQuality
    )
    InterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.InterpolatorModel
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.",
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=24,
        description="Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5,
        description="Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 17 to 161 (inclusive).",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, input data will be checked for safety before processing.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt for video generation."
    )
    video_write_mode: (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.VideoWriteMode.BALANCED,
        description="The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video (16:9 or 9:16).",
    )
    resolution: (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.Resolution
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.Resolution.VALUE_720P,
        description="Resolution of the generated video (580p or 720p).",
    )
    enable_output_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, output video will be checked for safety after generation.",
    )
    video_quality: (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.VideoQuality.HIGH,
        description="The quality of the output video. Higher quality means better visual quality but larger file size.",
    )
    adjust_fps_for_interpolation: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.InterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan.InterpolatorModel.FILM,
        description="The model to use for frame interpolation. If None, no interpolation is applied.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanV225bTextToVideoFastWan

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanV22A14bTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan-2.2 text-to-video is a video model that generates high-quality videos with high visual quality and motion diversity from text prompts.
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.Acceleration
    )
    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.VideoWriteMode
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.Resolution
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.VideoQuality
    )
    InterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.InterpolatorModel
    )

    shift: float | OutputHandle[float] = connect_field(
        default=5,
        description="Shift value for the video. Must be between 1.0 and 10.0.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=1,
        description="Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.",
    )
    acceleration: (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.Acceleration.REGULAR,
        description="Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.",
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5,
        description="Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 17 to 161 (inclusive).",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, input data will be checked for safety before processing.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt for video generation."
    )
    video_write_mode: (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.VideoWriteMode.BALANCED,
        description="The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.",
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.AspectRatio.RATIO_16_9,
            description="Aspect ratio of the generated video (16:9 or 9:16).",
        )
    )
    resolution: nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.Resolution.VALUE_720P,
            description="Resolution of the generated video (480p, 580p, or 720p).",
        )
    )
    enable_output_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, output video will be checked for safety after generation.",
    )
    guidance_scale_2: float | OutputHandle[float] = connect_field(
        default=4,
        description="Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.",
    )
    video_quality: (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.VideoQuality.HIGH,
        description="The quality of the output video. Higher quality means better visual quality but larger file size.",
    )
    adjust_fps_for_interpolation: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.InterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo.InterpolatorModel.FILM,
        description="The model to use for frame interpolation. If None, no interpolation is applied.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=27,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanV22A14bTextToVideoLora(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan-2.2 text-to-video is a video model that generates high-quality videos with high visual quality and motion diversity from text prompts. This endpoint supports LoRAs made for Wan 2.2.
    video, generation, text-to-video, txt2vid, lora

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.Acceleration
    )
    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.VideoWriteMode
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.Resolution
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.VideoQuality
    )
    InterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.InterpolatorModel
    )

    shift: float | OutputHandle[float] = connect_field(
        default=5,
        description="Shift value for the video. Must be between 1.0 and 10.0.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=1,
        description="Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.",
    )
    acceleration: (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.Acceleration.REGULAR,
        description="Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.",
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False, description="If true, the video will be reversed."
    )
    loras: list[types.LoRAWeight] | OutputHandle[list[types.LoRAWeight]] = (
        connect_field(
            default=[], description="LoRA weights to be used in the inference."
        )
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5,
        description="Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 17 to 161 (inclusive).",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, input data will be checked for safety before processing.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt for video generation."
    )
    video_write_mode: (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.VideoWriteMode.BALANCED,
        description="The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video (16:9 or 9:16).",
    )
    resolution: (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.Resolution
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.Resolution.VALUE_720P,
        description="Resolution of the generated video (480p, 580p, or 720p).",
    )
    enable_output_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, output video will be checked for safety after generation.",
    )
    guidance_scale_2: float | OutputHandle[float] = connect_field(
        default=4,
        description="Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.",
    )
    video_quality: (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.VideoQuality.HIGH,
        description="The quality of the output video. Higher quality means better visual quality but larger file size.",
    )
    adjust_fps_for_interpolation: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.InterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora.InterpolatorModel.FILM,
        description="The model to use for frame interpolation. If None, no interpolation is applied.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=27,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanV22A14bTextToVideoTurbo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan-2.2 turbo text-to-video is a video model that generates high-quality videos with high visual quality and motion diversity from text prompts.
    video, generation, text-to-video, txt2vid, fast

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.VideoWriteMode
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.Resolution
    )
    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.Acceleration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.AspectRatio
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.VideoQuality
    )

    video_write_mode: (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.VideoWriteMode.BALANCED,
        description="The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.",
    )
    resolution: (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.Resolution.VALUE_720P,
        description="Resolution of the generated video (480p, 580p, or 720p).",
    )
    acceleration: (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.Acceleration.REGULAR,
        description="Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video (16:9 or 9:16).",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    enable_output_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, output video will be checked for safety after generation.",
    )
    video_quality: (
        nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo.VideoQuality.HIGH,
        description="The quality of the output video. Higher quality means better visual quality but larger file size.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, input data will be checked for safety before processing.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanV22A14bTextToVideoTurbo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanV26TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan v2.6 Text to Video
    video, generation, text-to-video, txt2vid

    Use cases:
    - AI-generated video content
    - Marketing and advertising videos
    - Educational content creation
    - Social media video posts
    - Automated video production
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV26TextToVideo.Duration
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV26TextToVideo.Resolution
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.WanV26TextToVideo.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt for video generation. Supports Chinese and English, max 800 characters. For multi-shot videos, use format: 'Overall description. First shot [0-3s] content. Second shot [3-5s] content.'",
    )
    duration: nodetool.nodes.fal.text_to_video.WanV26TextToVideo.Duration = Field(
        default=nodetool.nodes.fal.text_to_video.WanV26TextToVideo.Duration.VALUE_5,
        description="Duration of the generated video in seconds. Choose between 5, 10, or 15 seconds.",
    )
    resolution: nodetool.nodes.fal.text_to_video.WanV26TextToVideo.Resolution = Field(
        default=nodetool.nodes.fal.text_to_video.WanV26TextToVideo.Resolution.VALUE_1080P,
        description="Video resolution tier. Wan 2.6 T2V only supports 720p and 1080p (no 480p).",
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.WanV26TextToVideo.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.text_to_video.WanV26TextToVideo.AspectRatio.RATIO_16_9,
            description="The aspect ratio of the generated video. Wan 2.6 supports additional ratios.",
        )
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time.",
    )
    audio: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the audio to use as the background music. Must be publicly accessible. Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds), the audio is truncated to the first N seconds, and the rest is discarded. If the audio is shorter than the video, the remaining part of the video will be silent. For example, if the audio is 3 seconds long and the video duration is 5 seconds, the first 3 seconds of the output video will have sound, and the last 2 seconds will be silent. - Format: WAV, MP3. - Duration: 3 to 30 s. - File size: Up to 15 MB.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    multi_shots: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="When true, enables intelligent multi-shot segmentation for coherent narrative videos. Only active when enable_prompt_expansion is True. Set to false for single-shot generation.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Negative prompt to describe content to avoid. Max 500 characters.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="If set to true, the safety checker will be enabled."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanV26TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
