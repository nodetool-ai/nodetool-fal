# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class AnimateDiffSparseCtrlLCM(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        AnimateDiff SparseCtrl LCM animates drawings with latent consistency models for fast generation.
        video, generation, animatediff, sparsectrl, lcm, animation, text-to-video

        Use cases:
        - Animate hand-drawn sketches
        - Bring drawings to life
        - Create animated illustrations
        - Generate animations from concept art
        - Produce animation from sparse frames
    """

    ControlnetType: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.AnimateDiffSparseCtrlLCM.ControlnetType

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to use for generating the image. Be as descriptive as possible for best results.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.')
    controlnet_type: nodetool.nodes.fal.text_to_video.AnimateDiffSparseCtrlLCM.ControlnetType = Field(default=nodetool.nodes.fal.text_to_video.AnimateDiffSparseCtrlLCM.ControlnetType.SCRIBBLE, description='The type of controlnet to use for generating the video. The controlnet determines how the video will be animated.')
    keyframe_2_index: int | OutputHandle[int] = connect_field(default=0, description='The frame index of the third keyframe to use for the generation.')
    keyframe_0_index: int | OutputHandle[int] = connect_field(default=0, description='The frame index of the first keyframe to use for the generation.')
    keyframe_1_image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The URL of the second keyframe to use for the generation.')
    keyframe_1_index: int | OutputHandle[int] = connect_field(default=0, description='The frame index of the second keyframe to use for the generation.')
    guidance_scale: int | OutputHandle[int] = connect_field(default=1, description='The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=4, description='Increasing the amount of steps tells Stable Diffusion that it should take more steps to generate your final result which can increase the amount of detail in your image.')
    keyframe_2_image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The URL of the third keyframe to use for the generation.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description="The negative prompt to use. Use it to specify what you don't want.")
    keyframe_0_image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The URL of the first keyframe to use for the generation.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.AnimateDiffSparseCtrlLCM

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class AnimateDiffTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        AnimateDiff generates smooth animations from text prompts using diffusion models.
        video, generation, animatediff, animation, text-to-video, txt2vid

        Use cases:
        - Animate ideas from text descriptions
        - Create animated content quickly
        - Generate motion graphics from prompts
        - Produce animated concept art
        - Create video loops and sequences
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to use for generating the video. Be as descriptive as possible for best results.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.')
    fps: int | OutputHandle[int] = connect_field(default=8, description='Number of frames per second to extract from the video.')
    video_size: str | OutputHandle[str] = connect_field(default='square', description='The size of the video to generate.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=7.5, description='The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.')
    num_frames: int | OutputHandle[int] = connect_field(default=16, description='The number of frames to generate for the video.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=25, description='The number of inference steps to perform.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='(bad quality, worst quality:1.2), ugly faces, bad anime', description="The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution).")
    motions: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The motions to apply to the video.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.AnimateDiffTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class AnimateDiffTurboTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        AnimateDiff Turbo generates animations at lightning speed with reduced steps.
        video, generation, animatediff, turbo, fast, text-to-video, txt2vid

        Use cases:
        - Rapidly prototype video animations
        - Create quick video previews
        - Generate animations with minimal latency
        - Iterate on video concepts quickly
        - Produce real-time animation effects
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to use for generating the video. Be as descriptive as possible for best results.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.')
    fps: int | OutputHandle[int] = connect_field(default=8, description='Number of frames per second to extract from the video.')
    video_size: str | OutputHandle[str] = connect_field(default='square', description='The size of the video to generate.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=1, description='The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.')
    num_frames: int | OutputHandle[int] = connect_field(default=16, description='The number of frames to generate for the video.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=4, description='The number of inference steps to perform. 4-12 is recommended for turbo mode.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='(bad quality, worst quality:1.2), ugly faces, bad anime', description="The negative prompt to use. Use it to address details that you don't want in the image. This could be colors, objects, scenery and even the small details (e.g. moustache, blurry, low resolution).")
    motions: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The motions to apply to the video.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.AnimateDiffTurboTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class ArgilAvatarsTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Argil Avatars creates realistic talking avatar videos from text descriptions.
        video, generation, avatar, talking-head, argil, text-to-video

        Use cases:
        - Generate avatar spokesperson videos
        - Create virtual presenter content
        - Produce automated video announcements
        - Generate character-based narratives
        - Create social media avatar videos
    """

    Voice: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo.Voice
    Avatar: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo.Avatar

    text: str | OutputHandle[str] = connect_field(default='', description=None)
    voice: nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo.Voice = Field(default=nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo.Voice(''), description=None)
    remove_background: bool | OutputHandle[bool] = connect_field(default=False, description='Enabling the remove background feature will result in a 50% increase in the price.')
    avatar: nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo.Avatar = Field(default=nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo.Avatar(''), description=None)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.ArgilAvatarsTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class CogVideoX5B(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        CogVideoX-5B is a powerful open-source text-to-video generation model with 5 billion parameters.
        video, generation, cogvideo, text-to-video, txt2vid

        Use cases:
        - Generate detailed videos from text prompts
        - Create animated storytelling content
        - Produce concept videos for pitches
        - Generate video storyboards
        - Create educational demonstrations
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate the video from.')
    use_rife: bool | OutputHandle[bool] = connect_field(default=True, description='Use RIFE for video interpolation')
    loras: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The LoRAs to use for the image generation. We currently support one lora.')
    video_size: str | OutputHandle[str] = connect_field(default='', description='The size of the generated video.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=7, description='The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related video to show you.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=50, description='The number of inference steps to perform.')
    export_fps: int | OutputHandle[int] = connect_field(default=16, description='The target FPS of the video')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='The negative prompt to generate video from')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same video every time.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.CogVideoX5B

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class HunyuanVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Hunyuan Video is Tencent's advanced text-to-video model for high-quality video generation.
        video, generation, hunyuan, text-to-video, txt2vid

        Use cases:
        - Generate cinematic videos from text descriptions
        - Create marketing videos from product descriptions
        - Produce educational video content
        - Generate creative video concepts
        - Create animated scenes from stories
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.HunyuanVideo.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.HunyuanVideo.Resolution
    NumFrames: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.HunyuanVideo.NumFrames

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate the video from.')
    aspect_ratio: nodetool.nodes.fal.text_to_video.HunyuanVideo.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.HunyuanVideo.AspectRatio.RATIO_16_9, description='The aspect ratio of the video to generate.')
    resolution: nodetool.nodes.fal.text_to_video.HunyuanVideo.Resolution = Field(default=nodetool.nodes.fal.text_to_video.HunyuanVideo.Resolution.VALUE_720P, description='The resolution of the video to generate.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=False, description='If set to true, the safety checker will be enabled.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=30, description='The number of inference steps to run. Lower gets faster results, higher gets better results.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed to use for generating the video.')
    num_frames: nodetool.nodes.fal.text_to_video.HunyuanVideo.NumFrames = Field(default=nodetool.nodes.fal.text_to_video.HunyuanVideo.NumFrames(129), description='The number of frames to generate.')
    pro_mode: bool | OutputHandle[bool] = connect_field(default=False, description='By default, generations are done with 35 steps. Pro mode does 55 steps which results in higher quality videos but will take more time and cost 2x more billing units.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.HunyuanVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class HunyuanVideoV1_5TextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Hunyuan Video V1.5
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.HunyuanVideoV1_5TextToVideo.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.HunyuanVideoV1_5TextToVideo.Resolution

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate the video.')
    aspect_ratio: nodetool.nodes.fal.text_to_video.HunyuanVideoV1_5TextToVideo.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.HunyuanVideoV1_5TextToVideo.AspectRatio.RATIO_16_9, description='The aspect ratio of the video.')
    resolution: nodetool.nodes.fal.text_to_video.HunyuanVideoV1_5TextToVideo.Resolution = Field(default=nodetool.nodes.fal.text_to_video.HunyuanVideoV1_5TextToVideo.Resolution.VALUE_480P, description='The resolution of the video.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=True, description='Enable prompt expansion to enhance the input prompt.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducibility.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='The negative prompt to guide what not to generate.')
    num_frames: int | OutputHandle[int] = connect_field(default=121, description='The number of frames to generate.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.HunyuanVideoV1_5TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class InfinitalkSingleText(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Infinitalk
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Resolution
    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Acceleration
    Voice: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Voice

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt to guide video generation.')
    resolution: nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Resolution = Field(default=nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Resolution.VALUE_480P, description='Resolution of the video to generate. Must be either 480p or 720p.')
    acceleration: nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Acceleration = Field(default=nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Acceleration.REGULAR, description='The acceleration level to use for generation.')
    text_input: str | OutputHandle[str] = connect_field(default='', description='The text input to guide video generation.')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.')
    voice: nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Voice = Field(default=nodetool.nodes.fal.text_to_video.InfinitalkSingleText.Voice(''), description='The voice to use for speech generation')
    num_frames: int | OutputHandle[int] = connect_field(default=145, description='Number of frames to generate. Must be between 41 to 721.')
    seed: int | OutputHandle[int] = connect_field(default=42, description='Random seed for reproducibility. If None, a random seed is chosen.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.InfinitalkSingleText

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class InfinityStarTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Infinity Star
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.InfinityStarTextToVideo.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(default='', description='Text prompt for generating the video')
    aspect_ratio: nodetool.nodes.fal.text_to_video.InfinityStarTextToVideo.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.InfinityStarTextToVideo.AspectRatio.RATIO_16_9, description='Aspect ratio of the generated output')
    enhance_prompt: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to use an LLM to enhance the prompt.')
    use_apg: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to use APG')
    guidance_scale: float | OutputHandle[float] = connect_field(default=7.5, description='Guidance scale for generation')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=50, description='Number of inference steps')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducibility. Leave empty for random generation.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Negative prompt to guide what to avoid in generation')
    tau_video: float | OutputHandle[float] = connect_field(default=0.4, description='Tau value for video scale')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.InfinityStarTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class Kandinsky5ProTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Kandinsky5 Pro
        video, generation, text-to-video, txt2vid, professional

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Resolution
    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Acceleration
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.AspectRatio
    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Duration

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt to guide video generation.')
    resolution: nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Resolution = Field(default=nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Resolution.VALUE_512P, description='Video resolution: 512p or 1024p.')
    acceleration: nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Acceleration = Field(default=nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Acceleration.REGULAR, description='Acceleration level for faster generation.')
    aspect_ratio: nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.AspectRatio.RATIO_3_2, description='Aspect ratio of the generated video. One of (3:2, 1:1, 2:3).')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps.')
    duration: nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Duration = Field(default=nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo.Duration.VALUE_5S, description='The length of the video to generate (5s or 10s)')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Kandinsky5ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class Kandinsky5TextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Kandinsky5
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.Resolution
    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.Duration
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt to guide video generation.')
    resolution: nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.Resolution = Field(default=nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.Resolution.VALUE_768X512, description='Resolution of the generated video in W:H format. Will be calculated based on the aspect ratio(768x512, 512x512, 512x768).')
    duration: nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.Duration = Field(default=nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.Duration.VALUE_5S, description='The length of the video to generate (5s or 10s)')
    aspect_ratio: nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo.AspectRatio.RATIO_3_2, description='Aspect ratio of the generated video. One of (3:2, 1:1, 2:3).')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=30, description='The number of inference steps.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class Kandinsky5TextToVideoDistill(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Kandinsky5
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.Duration
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.Resolution

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt to guide video generation.')
    duration: nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.Duration = Field(default=nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.Duration.VALUE_5S, description='The length of the video to generate (5s or 10s)')
    aspect_ratio: nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.AspectRatio.RATIO_3_2, description='Aspect ratio of the generated video. One of (3:2, 1:1, 2:3).')
    resolution: nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.Resolution = Field(default=nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill.Resolution.VALUE_768X512, description='Resolution of the generated video in W:H format. Will be calculated based on the aspect ratio(768x512, 512x512, 512x768).')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideoDistill

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class KlingVideoV1StandardTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Kling Video v1 Standard generates videos from text with balanced quality and speed.
        video, generation, kling, text-to-video, txt2vid

        Use cases:
        - Generate standard quality videos
        - Create video content efficiently
        - Produce videos for web use
        - Generate video previews
        - Create video concepts
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.AspectRatio
    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.Duration

    prompt: str | OutputHandle[str] = connect_field(default='', description=None)
    aspect_ratio: nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.AspectRatio.RATIO_16_9, description='The aspect ratio of the generated video frame')
    advanced_camera_control: str | OutputHandle[str] = connect_field(default='', description='Advanced Camera control parameters')
    duration: nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.Duration = Field(default=nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.Duration.VALUE_5, description='The duration of the generated video in seconds')
    camera_control: nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.CameraControl | OutputHandle[nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo.CameraControl] | None = connect_field(default=None, description='Camera control parameters')
    negative_prompt: str | OutputHandle[str] = connect_field(default='blur, distort, and low quality', description=None)
    cfg_scale: float | OutputHandle[float] = connect_field(default=0.5, description='The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoV1StandardTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class KlingVideoV2_6ProTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Kling Video v2.6 Text to Video
        video, generation, text-to-video, txt2vid, professional

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.KlingVideoV2_6ProTextToVideo.AspectRatio
    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.KlingVideoV2_6ProTextToVideo.Duration

    prompt: str | OutputHandle[str] = connect_field(default='', description=None)
    aspect_ratio: nodetool.nodes.fal.text_to_video.KlingVideoV2_6ProTextToVideo.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.KlingVideoV2_6ProTextToVideo.AspectRatio.RATIO_16_9, description='The aspect ratio of the generated video frame')
    duration: nodetool.nodes.fal.text_to_video.KlingVideoV2_6ProTextToVideo.Duration = Field(default=nodetool.nodes.fal.text_to_video.KlingVideoV2_6ProTextToVideo.Duration.VALUE_5, description='The duration of the generated video in seconds')
    generate_audio: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='blur, distort, and low quality', description=None)
    cfg_scale: float | OutputHandle[float] = connect_field(default=0.5, description='The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoV2_6ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class KreaWan14BTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Krea Wan 14b- Text to Video
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='Prompt for the video-to-video generation.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.')
    num_frames: int | OutputHandle[int] = connect_field(default=78, description='Number of frames to generate. Must be a multiple of 12 plus 6, for example 6, 18, 30, 42, etc.')
    seed: str | OutputHandle[str] = connect_field(default='', description='Seed for the video-to-video generation.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KreaWan14BTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class LTXVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        LTX Video generates high-quality videos from text prompts with advanced temporal consistency.
        video, generation, ltx, text-to-video, txt2vid

        Use cases:
        - Generate temporally consistent videos
        - Create smooth video sequences
        - Produce high-quality video content
        - Generate professional video clips
        - Create cinematic video scenes
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate the video from.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=3, description='The guidance scale to use.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed to use for random number generation.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=30, description='The number of inference steps to take.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly', description='The negative prompt to generate the video from.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LTXVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class LongcatVideoDistilledTextToVideo480P(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        LongCat Video Distilled
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    VideoWriteMode: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoWriteMode
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.AspectRatio
    VideoOutputType: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoOutputType
    VideoQuality: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoQuality

    video_write_mode: nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoWriteMode = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoWriteMode.BALANCED, description='The write mode of the generated video.')
    aspect_ratio: nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.AspectRatio.RATIO_16_9, description='The aspect ratio of the generated video.')
    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to guide the video generation.')
    video_output_type: nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoOutputType = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoOutputType.X264__MP4, description='The output type of the generated video.')
    fps: int | OutputHandle[int] = connect_field(default=15, description='The frame rate of the generated video.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    video_quality: nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoQuality = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P.VideoQuality.HIGH, description='The quality of the generated video.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable safety checker.')
    num_frames: int | OutputHandle[int] = connect_field(default=162, description='The number of frames to generate.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=12, description='The number of inference steps to use.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed for the random number generator.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enable prompt expansion.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo480P

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class LongcatVideoDistilledTextToVideo720P(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        LongCat Video Distilled
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    VideoWriteMode: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoWriteMode
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.AspectRatio
    VideoOutputType: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoOutputType
    VideoQuality: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoQuality

    video_write_mode: nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoWriteMode = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoWriteMode.BALANCED, description='The write mode of the generated video.')
    aspect_ratio: nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.AspectRatio.RATIO_16_9, description='The aspect ratio of the generated video.')
    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to guide the video generation.')
    video_output_type: nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoOutputType = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoOutputType.X264__MP4, description='The output type of the generated video.')
    fps: int | OutputHandle[int] = connect_field(default=30, description='The frame rate of the generated video.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    num_refine_inference_steps: int | OutputHandle[int] = connect_field(default=12, description='The number of inference steps to use for refinement.')
    video_quality: nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoQuality = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P.VideoQuality.HIGH, description='The quality of the generated video.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable safety checker.')
    num_frames: int | OutputHandle[int] = connect_field(default=162, description='The number of frames to generate.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=12, description='The number of inference steps to use.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed for the random number generator.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enable prompt expansion.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LongcatVideoDistilledTextToVideo720P

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class LongcatVideoTextToVideo480P(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        LongCat Video
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.Acceleration
    VideoWriteMode: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoWriteMode
    VideoOutputType: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoOutputType
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.AspectRatio
    VideoQuality: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoQuality

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to guide the video generation.')
    acceleration: nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.Acceleration = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.Acceleration.REGULAR, description='The acceleration level to use for the video generation.')
    fps: int | OutputHandle[int] = connect_field(default=15, description='The frame rate of the generated video.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=4, description='The guidance scale to use for the video generation.')
    num_frames: int | OutputHandle[int] = connect_field(default=162, description='The number of frames to generate.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable safety checker.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards', description='The negative prompt to use for the video generation.')
    video_write_mode: nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoWriteMode = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoWriteMode.BALANCED, description='The write mode of the generated video.')
    video_output_type: nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoOutputType = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoOutputType.X264__MP4, description='The output type of the generated video.')
    aspect_ratio: nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.AspectRatio.RATIO_16_9, description='The aspect ratio of the generated video.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    video_quality: nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoQuality = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P.VideoQuality.HIGH, description='The quality of the generated video.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enable prompt expansion.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed for the random number generator.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=40, description='The number of inference steps to use for the video generation.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo480P

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class LongcatVideoTextToVideo720P(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        LongCat Video
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.Acceleration
    VideoWriteMode: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoWriteMode
    VideoOutputType: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoOutputType
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.AspectRatio
    VideoQuality: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoQuality

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to guide the video generation.')
    acceleration: nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.Acceleration = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.Acceleration.REGULAR, description='The acceleration level to use for the video generation.')
    fps: int | OutputHandle[int] = connect_field(default=30, description='The frame rate of the generated video.')
    num_refine_inference_steps: int | OutputHandle[int] = connect_field(default=40, description='The number of inference steps to use for refinement.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=4, description='The guidance scale to use for the video generation.')
    num_frames: int | OutputHandle[int] = connect_field(default=162, description='The number of frames to generate.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable safety checker.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards', description='The negative prompt to use for the video generation.')
    video_write_mode: nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoWriteMode = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoWriteMode.BALANCED, description='The write mode of the generated video.')
    video_output_type: nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoOutputType = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoOutputType.X264__MP4, description='The output type of the generated video.')
    aspect_ratio: nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.AspectRatio.RATIO_16_9, description='The aspect ratio of the generated video.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    video_quality: nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoQuality = Field(default=nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P.VideoQuality.HIGH, description='The quality of the generated video.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enable prompt expansion.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=40, description='The number of inference steps to use for the video generation.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed for the random number generator.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LongcatVideoTextToVideo720P

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class Ltx219BDistilledTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        LTX-2 19B Distilled
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.Acceleration
    CameraLora: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.CameraLora
    VideoWriteMode: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoWriteMode
    VideoOutputType: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoOutputType
    VideoQuality: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoQuality

    use_multiscale: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.')
    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate the video from.')
    acceleration: nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.Acceleration = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.Acceleration.NONE, description='The acceleration level to use.')
    generate_audio: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to generate audio for the video.')
    fps: float | OutputHandle[float] = connect_field(default=25, description='The frames per second of the generated video.')
    camera_lora: nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.CameraLora = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.CameraLora.NONE, description='The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.')
    video_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated video.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the safety checker.')
    camera_lora_scale: float | OutputHandle[float] = connect_field(default=1, description='The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.')
    num_frames: int | OutputHandle[int] = connect_field(default=121, description='The number of frames to generate.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.', description='The negative prompt to generate the video from.')
    video_write_mode: nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoWriteMode = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoWriteMode.BALANCED, description='The write mode of the generated video.')
    video_output_type: nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoOutputType = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoOutputType.X264__MP4, description='The output type of the generated video.')
    video_quality: nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoQuality = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo.VideoQuality.HIGH, description='The quality of the generated video.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable prompt expansion.')
    seed: str | OutputHandle[str] = connect_field(default='', description='The seed for the random number generator.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class Ltx219BDistilledTextToVideoLora(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        LTX-2 19B Distilled
        video, generation, text-to-video, txt2vid, lora

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.Acceleration
    CameraLora: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.CameraLora
    VideoWriteMode: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoWriteMode
    VideoOutputType: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoOutputType
    VideoQuality: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoQuality

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate the video from.')
    use_multiscale: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.')
    acceleration: nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.Acceleration = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.Acceleration.NONE, description='The acceleration level to use.')
    generate_audio: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to generate audio for the video.')
    fps: float | OutputHandle[float] = connect_field(default=25, description='The frames per second of the generated video.')
    loras: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The LoRAs to use for the generation.')
    camera_lora: nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.CameraLora = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.CameraLora.NONE, description='The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.')
    video_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated video.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the safety checker.')
    camera_lora_scale: float | OutputHandle[float] = connect_field(default=1, description='The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.')
    num_frames: int | OutputHandle[int] = connect_field(default=121, description='The number of frames to generate.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.', description='The negative prompt to generate the video from.')
    video_write_mode: nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoWriteMode = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoWriteMode.BALANCED, description='The write mode of the generated video.')
    video_output_type: nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoOutputType = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoOutputType.X264__MP4, description='The output type of the generated video.')
    video_quality: nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoQuality = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora.VideoQuality.HIGH, description='The quality of the generated video.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable prompt expansion.')
    seed: str | OutputHandle[str] = connect_field(default='', description='The seed for the random number generator.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Ltx219BDistilledTextToVideoLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class Ltx219BTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        LTX-2 19B
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.Acceleration
    CameraLora: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.CameraLora
    VideoWriteMode: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoWriteMode
    VideoOutputType: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoOutputType
    VideoQuality: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoQuality

    use_multiscale: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.')
    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate the video from.')
    acceleration: nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.Acceleration = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.Acceleration.REGULAR, description='The acceleration level to use.')
    generate_audio: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to generate audio for the video.')
    fps: float | OutputHandle[float] = connect_field(default=25, description='The frames per second of the generated video.')
    camera_lora: nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.CameraLora = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.CameraLora.NONE, description='The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.')
    video_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated video.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the safety checker.')
    camera_lora_scale: float | OutputHandle[float] = connect_field(default=1, description='The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=3, description='The guidance scale to use.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.', description='The negative prompt to generate the video from.')
    num_frames: int | OutputHandle[int] = connect_field(default=121, description='The number of frames to generate.')
    video_write_mode: nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoWriteMode = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoWriteMode.BALANCED, description='The write mode of the generated video.')
    video_output_type: nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoOutputType = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoOutputType.X264__MP4, description='The output type of the generated video.')
    video_quality: nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoQuality = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo.VideoQuality.HIGH, description='The quality of the generated video.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable prompt expansion.')
    seed: str | OutputHandle[str] = connect_field(default='', description='The seed for the random number generator.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=40, description='The number of inference steps to use.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Ltx219BTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class Ltx219BTextToVideoLora(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        LTX-2 19B
        video, generation, text-to-video, txt2vid, lora

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.Acceleration
    CameraLora: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.CameraLora
    VideoWriteMode: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoWriteMode
    VideoOutputType: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoOutputType
    VideoQuality: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoQuality

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate the video from.')
    use_multiscale: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.')
    acceleration: nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.Acceleration = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.Acceleration.REGULAR, description='The acceleration level to use.')
    generate_audio: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to generate audio for the video.')
    fps: float | OutputHandle[float] = connect_field(default=25, description='The frames per second of the generated video.')
    loras: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The LoRAs to use for the generation.')
    camera_lora: nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.CameraLora = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.CameraLora.NONE, description='The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.')
    video_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated video.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=3, description='The guidance scale to use.')
    camera_lora_scale: float | OutputHandle[float] = connect_field(default=1, description='The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the safety checker.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.', description='The negative prompt to generate the video from.')
    num_frames: int | OutputHandle[int] = connect_field(default=121, description='The number of frames to generate.')
    video_write_mode: nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoWriteMode = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoWriteMode.BALANCED, description='The write mode of the generated video.')
    video_output_type: nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoOutputType = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoOutputType.X264__MP4, description='The output type of the generated video.')
    video_quality: nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoQuality = Field(default=nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora.VideoQuality.HIGH, description='The quality of the generated video.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable prompt expansion.')
    seed: str | OutputHandle[str] = connect_field(default='', description='The seed for the random number generator.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=40, description='The number of inference steps to use.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Ltx219BTextToVideoLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class LumaDreamMachineTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Luma Dream Machine generates creative videos from text with dreamlike aesthetics.
        video, generation, luma, dream-machine, text-to-video, txt2vid

        Use cases:
        - Generate dreamlike video content
        - Create surreal video sequences
        - Produce artistic video interpretations
        - Generate creative video concepts
        - Create imaginative video art
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LumaDreamMachineTextToVideo.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(default='', description=None)
    aspect_ratio: nodetool.nodes.fal.text_to_video.LumaDreamMachineTextToVideo.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.LumaDreamMachineTextToVideo.AspectRatio.RATIO_16_9, description='The aspect ratio of the generated video')
    loop: bool | OutputHandle[bool] = connect_field(default=False, description='Whether the video should loop (end of video is blended with the beginning)')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LumaDreamMachineTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class LumaPhoton(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Luma Photon generates photorealistic videos from text with high visual fidelity.
        video, generation, luma, photon, photorealistic, text-to-video

        Use cases:
        - Generate photorealistic video content
        - Create realistic video simulations
        - Produce lifelike video scenes
        - Generate high-fidelity video outputs
        - Create realistic visual content
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.LumaPhoton.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(default='', description=None)
    aspect_ratio: nodetool.nodes.fal.text_to_video.LumaPhoton.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.LumaPhoton.AspectRatio.RATIO_1_1, description='The aspect ratio of the generated video')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LumaPhoton

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class MinimaxHailuo2_3ProTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        MiniMax Hailuo 2.3 [Pro] (Text to Video)
        video, generation, text-to-video, txt2vid, professional

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    prompt_optimizer: bool | OutputHandle[bool] = connect_field(default=True, description="Whether to use the model's prompt optimizer")
    prompt: str | OutputHandle[str] = connect_field(default='', description='Text prompt for video generation')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.MinimaxHailuo2_3ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class MinimaxHailuo2_3StandardTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        MiniMax Hailuo 2.3 [Standard] (Text to Video)
        video, generation, text-to-video, txt2vid, professional

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.MinimaxHailuo2_3StandardTextToVideo.Duration

    prompt_optimizer: bool | OutputHandle[bool] = connect_field(default=True, description="Whether to use the model's prompt optimizer")
    duration: nodetool.nodes.fal.text_to_video.MinimaxHailuo2_3StandardTextToVideo.Duration = Field(default=nodetool.nodes.fal.text_to_video.MinimaxHailuo2_3StandardTextToVideo.Duration.VALUE_6, description='The duration of the video in seconds.')
    prompt: str | OutputHandle[str] = connect_field(default='', description=None)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.MinimaxHailuo2_3StandardTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class MochiV1(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Mochi v1 generates creative videos from text with unique artistic style.
        video, generation, mochi, artistic, text-to-video, txt2vid

        Use cases:
        - Generate artistic video content
        - Create stylized animations
        - Produce creative video art
        - Generate experimental videos
        - Create unique visual content
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate a video from.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable prompt expansion.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed to use for generating the video.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='The negative prompt for the video.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.MochiV1

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class MoonvalleyMareyT2V(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Marey Realism V1.5
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V.Duration
    Dimensions: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V.Dimensions

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate a video from')
    duration: nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V.Duration = Field(default=nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V.Duration.VALUE_5S, description='The duration of the generated video.')
    dimensions: nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V.Dimensions = Field(default=nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V.Dimensions.VALUE_1920X1080, description='The dimensions of the generated video in width x height format.')
    guidance_scale: str | OutputHandle[str] = connect_field(default='', description='Controls how strongly the generation is guided by the prompt (0-20). Higher values follow the prompt more closely.')
    seed: str | OutputHandle[str] = connect_field(default=-1, description='Seed for random number generation. Use -1 for random seed each run.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='<synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts', description='Negative prompt used to guide the model away from undesirable features.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.MoonvalleyMareyT2V

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class Ovi(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Ovi Text to Video
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Ovi.Resolution

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt to guide video generation.')
    resolution: nodetool.nodes.fal.text_to_video.Ovi.Resolution = Field(default=nodetool.nodes.fal.text_to_video.Ovi.Resolution.VALUE_992X512, description='Resolution of the generated video in W:H format. One of (512x992, 992x512, 960x512, 512x960, 720x720, or 448x1120).')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=30, description='The number of inference steps.')
    audio_negative_prompt: str | OutputHandle[str] = connect_field(default='robotic, muffled, echo, distorted', description='Negative prompt for audio generation.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='jitter, bad hands, blur, distortion', description='Negative prompt for video generation.')
    seed: str | OutputHandle[str] = connect_field(default='', description='Random seed for reproducibility. If None, a random seed is chosen.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Ovi

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class PixverseV5TextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Pixverse
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Resolution
    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Duration

    prompt: str | OutputHandle[str] = connect_field(default='', description=None)
    aspect_ratio: nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.AspectRatio.RATIO_16_9, description='The aspect ratio of the generated video')
    resolution: nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Resolution = Field(default=nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Resolution.VALUE_720P, description='The resolution of the generated video')
    style: nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Style | OutputHandle[nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Style] | None = connect_field(default=None, description='The style of the generated video')
    duration: nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Duration = Field(default=nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo.Duration.VALUE_5, description='The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same video every time.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Negative prompt to be used for the generation')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseV5TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class PixverseV5_5TextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Pixverse
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.Resolution
    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.Duration

    prompt: str | OutputHandle[str] = connect_field(default='', description=None)
    aspect_ratio: nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.AspectRatio.RATIO_16_9, description='The aspect ratio of the generated video')
    resolution: nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.Resolution = Field(default=nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.Resolution.VALUE_720P, description='The resolution of the generated video')
    style: nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.Style | OutputHandle[nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.Style] | None = connect_field(default=None, description='The style of the generated video')
    thinking_type: nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.ThinkingType | OutputHandle[nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.ThinkingType] | None = connect_field(default=None, description="Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision")
    generate_multi_clip_switch: bool | OutputHandle[bool] = connect_field(default=False, description='Enable multi-clip generation with dynamic camera changes')
    duration: nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.Duration = Field(default=nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo.Duration.VALUE_5, description='The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds')
    generate_audio_switch: bool | OutputHandle[bool] = connect_field(default=False, description='Enable audio generation (BGM, SFX, dialogue)')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same video every time.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Negative prompt to be used for the generation')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseV5_5TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class PixverseV5_6TextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Pixverse
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.Resolution
    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.Duration

    prompt: str | OutputHandle[str] = connect_field(default='', description=None)
    aspect_ratio: nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.AspectRatio.RATIO_16_9, description='The aspect ratio of the generated video')
    resolution: nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.Resolution = Field(default=nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.Resolution.VALUE_720P, description='The resolution of the generated video')
    style: nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.Style | OutputHandle[nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.Style] | None = connect_field(default=None, description='The style of the generated video')
    thinking_type: nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.ThinkingType | OutputHandle[nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.ThinkingType] | None = connect_field(default=None, description="Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision")
    duration: nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.Duration = Field(default=nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo.Duration.VALUE_5, description='The duration of the generated video in seconds. 1080p videos are limited to 5 or 8 seconds')
    generate_audio_switch: bool | OutputHandle[bool] = connect_field(default=False, description='Enable audio generation (BGM, SFX, dialogue)')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same video every time.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Negative prompt to be used for the generation')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseV5_6TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class SanaVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Sana Video
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.SanaVideo.Resolution

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt describing the video to generate')
    resolution: nodetool.nodes.fal.text_to_video.SanaVideo.Resolution = Field(default=nodetool.nodes.fal.text_to_video.SanaVideo.Resolution.VALUE_480P, description='The resolution of the output video')
    fps: int | OutputHandle[int] = connect_field(default=16, description='Frames per second for the output video')
    motion_score: int | OutputHandle[int] = connect_field(default=30, description='Motion intensity score (higher = more motion)')
    guidance_scale: float | OutputHandle[float] = connect_field(default=6, description='Guidance scale for generation (higher = more prompt adherence)')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='Number of denoising steps')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducible generation. If not provided, a random seed will be used.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='A chaotic sequence with misshapen, deformed limbs in heavy motion blur, sudden disappearance, jump cuts, jerky movements, rapid shot changes, frames out of sync, inconsistent character shapes, temporal artifacts, jitter, and ghosting effects, creating a disorienting visual experience.', description='The negative prompt describing what to avoid in the generation')
    num_frames: int | OutputHandle[int] = connect_field(default=81, description='Number of frames to generate')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.SanaVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class SeeDanceV15ProTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        SeeDance v1.5 Pro from ByteDance generates high-quality dance videos from text prompts.
        video, generation, dance, seedance, bytedance, text-to-video

        Use cases:
        - Generate dance choreography videos
        - Create dance performance visualizations
        - Produce music video concepts
        - Generate dance training content
        - Create dance animation prototypes
    """

    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.Resolution
    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.Duration
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt used to generate the video')
    resolution: nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.Resolution = Field(default=nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.Resolution.VALUE_720P, description='Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality')
    duration: nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.Duration = Field(default=nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.Duration.VALUE_5, description='Duration of the video in seconds')
    generate_audio: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to generate audio for the video')
    aspect_ratio: nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo.AspectRatio.RATIO_16_9, description='The aspect ratio of the generated video')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    camera_fixed: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to fix the camera position')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed to control video generation. Use -1 for random.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.SeeDanceV15ProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class SeeDanceV1ProFastTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        SeeDance v1 Pro Fast generates dance videos quickly from text with reduced generation time.
        video, generation, dance, seedance, fast, bytedance, text-to-video

        Use cases:
        - Rapidly prototype dance videos
        - Create quick dance previews
        - Generate dance concepts efficiently
        - Iterate on choreography ideas
        - Produce dance storyboards
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.AspectRatio
    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.Duration
    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.Resolution

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt used to generate the video')
    aspect_ratio: nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.AspectRatio.RATIO_16_9, description='The aspect ratio of the generated video')
    duration: nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.Duration = Field(default=nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.Duration.VALUE_5, description='Duration of the video in seconds')
    resolution: nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.Resolution = Field(default=nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo.Resolution.VALUE_1080P, description='Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    camera_fixed: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to fix the camera position')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed to control video generation. Use -1 for random.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.SeeDanceV1ProFastTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class StableVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Stable Video generates consistent and stable video sequences from text prompts.
        video, generation, stable, text-to-video, txt2vid

        Use cases:
        - Generate stable video sequences
        - Create consistent video content
        - Produce reliable video outputs
        - Generate predictable video scenes
        - Create controlled video generation
    """

    motion_bucket_id: int | OutputHandle[int] = connect_field(default=127, description='The motion bucket id determines the motion of the generated video. The higher the number, the more motion there will be.')
    fps: int | OutputHandle[int] = connect_field(default=25, description='The frames per second of the generated video.')
    cond_aug: float | OutputHandle[float] = connect_field(default=0.02, description='The conditoning augmentation determines the amount of noise that will be added to the conditioning frame. The higher the number, the more noise there will be, and the less the video will look like the initial image. Increase it for more motion.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The URL of the image to use as a starting point for the generation.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.StableVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class T2VTurbo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        T2V Turbo generates videos from text at high speed with optimized performance.
        video, generation, turbo, fast, text-to-video, txt2vid

        Use cases:
        - Generate videos with minimal latency
        - Create rapid video prototypes
        - Produce quick video previews
        - Generate real-time video content
        - Create efficient video workflows
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate images from')
    guidance_scale: float | OutputHandle[float] = connect_field(default=7.5, description='The guidance scale')
    seed: str | OutputHandle[str] = connect_field(default='', description='The seed to use for the random number generator')
    export_fps: int | OutputHandle[int] = connect_field(default=8, description='The FPS of the exported video')
    num_frames: int | OutputHandle[int] = connect_field(default=16, description='The number of frames to generate')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=4, description='The number of steps to sample')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.T2VTurbo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class VeedAvatarsTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        VEED Avatars generates talking avatar videos from text using realistic AI-powered characters.
        video, generation, avatar, talking-head, veed, text-to-video

        Use cases:
        - Create talking avatar presentations
        - Generate spokesperson videos
        - Produce educational talking head videos
        - Create personalized video messages
        - Generate multilingual avatar content
    """

    AvatarId: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.VeedAvatarsTextToVideo.AvatarId

    text: str | OutputHandle[str] = connect_field(default='', description=None)
    avatar_id: nodetool.nodes.fal.text_to_video.VeedAvatarsTextToVideo.AvatarId = Field(default=nodetool.nodes.fal.text_to_video.VeedAvatarsTextToVideo.AvatarId(''), description='The avatar to use for the video')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.VeedAvatarsTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class VeedFabric10Text(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        VEED Fabric 1.0 generates video content from text using advanced video synthesis.
        video, generation, fabric, veed, text-to-video, txt2vid

        Use cases:
        - Generate marketing videos from text
        - Create explainer video content
        - Produce video ads from copy
        - Generate social media videos
        - Create branded video content
    """

    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.VeedFabric10Text.Resolution

    text: str | OutputHandle[str] = connect_field(default='', description=None)
    resolution: nodetool.nodes.fal.text_to_video.VeedFabric10Text.Resolution = Field(default=nodetool.nodes.fal.text_to_video.VeedFabric10Text.Resolution(''), description='Resolution')
    voice_description: str | OutputHandle[str] = connect_field(default='', description="Optional additional voice description. The primary voice description is auto-generated from the image. You can use simple descriptors like 'British accent' or 'Confident' or provide a detailed description like 'Confident male voice, mid-20s, with notes of...'")
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description=None)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.VeedFabric10Text

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class Veo3_1(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Veo 3.1
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Veo3_1.Duration
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Veo3_1.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Veo3_1.Resolution

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt describing the video you want to generate')
    duration: nodetool.nodes.fal.text_to_video.Veo3_1.Duration = Field(default=nodetool.nodes.fal.text_to_video.Veo3_1.Duration.VALUE_8S, description='The duration of the generated video.')
    aspect_ratio: nodetool.nodes.fal.text_to_video.Veo3_1.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.Veo3_1.AspectRatio.RATIO_16_9, description='Aspect ratio of the generated video')
    generate_audio: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to generate audio for the video.')
    auto_fix: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.')
    resolution: nodetool.nodes.fal.text_to_video.Veo3_1.Resolution = Field(default=nodetool.nodes.fal.text_to_video.Veo3_1.Resolution.VALUE_720P, description='The resolution of the generated video.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed for the random number generator.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='A negative prompt to guide the video generation.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Veo3_1

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class Veo3_1Fast(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Veo 3.1 Fast
        video, generation, text-to-video, txt2vid, fast

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Veo3_1Fast.Duration
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Veo3_1Fast.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Veo3_1Fast.Resolution

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt describing the video you want to generate')
    duration: nodetool.nodes.fal.text_to_video.Veo3_1Fast.Duration = Field(default=nodetool.nodes.fal.text_to_video.Veo3_1Fast.Duration.VALUE_8S, description='The duration of the generated video.')
    aspect_ratio: nodetool.nodes.fal.text_to_video.Veo3_1Fast.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.Veo3_1Fast.AspectRatio.RATIO_16_9, description='Aspect ratio of the generated video')
    generate_audio: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to generate audio for the video.')
    auto_fix: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.')
    resolution: nodetool.nodes.fal.text_to_video.Veo3_1Fast.Resolution = Field(default=nodetool.nodes.fal.text_to_video.Veo3_1Fast.Resolution.VALUE_720P, description='The resolution of the generated video.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed for the random number generator.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='A negative prompt to guide the video generation.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Veo3_1Fast

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class Wan25PreviewTextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Wan 2.5 Text to Video
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.Duration
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.AspectRatio
    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.Resolution

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt for video generation. Supports Chinese and English, max 800 characters.')
    duration: nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.Duration = Field(default=nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.Duration.VALUE_5, description='Duration of the generated video in seconds. Choose between 5 or 10 seconds.')
    aspect_ratio: nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.AspectRatio.RATIO_16_9, description='The aspect ratio of the generated video')
    resolution: nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.Resolution = Field(default=nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo.Resolution.VALUE_1080P, description='Video resolution tier')
    audio_url: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(default=types.VideoRef(type='video', uri='', asset_id=None, data=None, metadata=None, duration=None, format=None), description='URL of the audio to use as the background music. Must be publicly accessible. Limit handling: If the audio duration exceeds the duration value (5 or 10 seconds), the audio is truncated to the first 5 or 10 seconds, and the rest is discarded. If the audio is shorter than the video, the remaining part of the video will be silent. For example, if the audio is 3 seconds long and the video duration is 5 seconds, the first 3 seconds of the output video will have sound, and the last 2 seconds will be silent. - Format: WAV, MP3. - Duration: 3 to 30 s. - File size: Up to 15 MB.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducibility. If None, a random seed is chosen.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Negative prompt to describe content to avoid. Max 500 characters.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Wan25PreviewTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class WanAlpha(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Wan Alpha
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Sampler: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.WanAlpha.Sampler
    VideoWriteMode: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.WanAlpha.VideoWriteMode
    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.WanAlpha.Resolution
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.WanAlpha.AspectRatio
    VideoOutputType: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.WanAlpha.VideoOutputType
    VideoQuality: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.WanAlpha.VideoQuality

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to guide the video generation.')
    shift: float | OutputHandle[float] = connect_field(default=10.5, description='The shift of the generated video.')
    mask_clamp_upper: float | OutputHandle[float] = connect_field(default=0.75, description='The upper bound of the mask clamping.')
    fps: int | OutputHandle[int] = connect_field(default=16, description='The frame rate of the generated video.')
    mask_clamp_lower: float | OutputHandle[float] = connect_field(default=0.1, description='The lower bound of the mask clamping.')
    num_frames: int | OutputHandle[int] = connect_field(default=81, description='The number of frames to generate.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable safety checker.')
    mask_binarization_threshold: float | OutputHandle[float] = connect_field(default=0.8, description='The threshold for mask binarization. When binarize_mask is True, this threshold will be used to binarize the mask. This will also be used for transparency when the output type is `.webm`.')
    sampler: nodetool.nodes.fal.text_to_video.WanAlpha.Sampler = Field(default=nodetool.nodes.fal.text_to_video.WanAlpha.Sampler.EULER, description='The sampler to use.')
    video_write_mode: nodetool.nodes.fal.text_to_video.WanAlpha.VideoWriteMode = Field(default=nodetool.nodes.fal.text_to_video.WanAlpha.VideoWriteMode.BALANCED, description='The write mode of the generated video.')
    resolution: nodetool.nodes.fal.text_to_video.WanAlpha.Resolution = Field(default=nodetool.nodes.fal.text_to_video.WanAlpha.Resolution.VALUE_480P, description='The resolution of the generated video.')
    aspect_ratio: nodetool.nodes.fal.text_to_video.WanAlpha.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.WanAlpha.AspectRatio.RATIO_16_9, description='The aspect ratio of the generated video.')
    video_output_type: nodetool.nodes.fal.text_to_video.WanAlpha.VideoOutputType = Field(default=nodetool.nodes.fal.text_to_video.WanAlpha.VideoOutputType.VP9__WEBM, description='The output type of the generated video.')
    binarize_mask: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to binarize the mask.')
    video_quality: nodetool.nodes.fal.text_to_video.WanAlpha.VideoQuality = Field(default=nodetool.nodes.fal.text_to_video.WanAlpha.VideoQuality.HIGH, description='The quality of the generated video.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enable prompt expansion.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=8, description='The number of inference steps to use.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed for the random number generator.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanAlpha

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode

class WanV2_6TextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Wan v2.6 Text to Video
        video, generation, text-to-video, txt2vid

        Use cases:
        - AI-generated video content
        - Marketing and advertising videos
        - Educational content creation
        - Social media video posts
        - Automated video production
    """

    Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.WanV2_6TextToVideo.Duration
    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.WanV2_6TextToVideo.Resolution
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.WanV2_6TextToVideo.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(default='', description="The text prompt for video generation. Supports Chinese and English, max 800 characters. For multi-shot videos, use format: 'Overall description. First shot [0-3s] content. Second shot [3-5s] content.'")
    duration: nodetool.nodes.fal.text_to_video.WanV2_6TextToVideo.Duration = Field(default=nodetool.nodes.fal.text_to_video.WanV2_6TextToVideo.Duration.VALUE_5, description='Duration of the generated video in seconds. Choose between 5, 10, or 15 seconds.')
    resolution: nodetool.nodes.fal.text_to_video.WanV2_6TextToVideo.Resolution = Field(default=nodetool.nodes.fal.text_to_video.WanV2_6TextToVideo.Resolution.VALUE_1080P, description='Video resolution tier. Wan 2.6 T2V only supports 720p and 1080p (no 480p).')
    aspect_ratio: nodetool.nodes.fal.text_to_video.WanV2_6TextToVideo.AspectRatio = Field(default=nodetool.nodes.fal.text_to_video.WanV2_6TextToVideo.AspectRatio.RATIO_16_9, description='The aspect ratio of the generated video. Wan 2.6 supports additional ratios.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time.')
    audio_url: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(default=types.VideoRef(type='video', uri='', asset_id=None, data=None, metadata=None, duration=None, format=None), description='URL of the audio to use as the background music. Must be publicly accessible. Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds), the audio is truncated to the first N seconds, and the rest is discarded. If the audio is shorter than the video, the remaining part of the video will be silent. For example, if the audio is 3 seconds long and the video duration is 5 seconds, the first 3 seconds of the output video will have sound, and the last 2 seconds will be silent. - Format: WAV, MP3. - Duration: 3 to 30 s. - File size: Up to 15 MB.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducibility. If None, a random seed is chosen.')
    multi_shots: bool | OutputHandle[bool] = connect_field(default=True, description='When true, enables intelligent multi-shot segmentation for coherent narrative videos. Only active when enable_prompt_expansion is True. Set to false for single-shot generation.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Negative prompt to describe content to avoid. Max 500 characters.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanV2_6TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


