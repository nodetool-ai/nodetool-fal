# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class HunyuanVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Hunyuan Video generates videos from text prompts using Tencent's model.
    video, generation, hunyuan, tencent, text-to-video

    Use cases:
    - Create videos from descriptions
    - Generate animated content
    - Produce motion graphics
    - Create promotional clips
    - Generate concept videos
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="Number of inference steps"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.0, description="How closely to follow the prompt"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for reproducible generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.HunyuanVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class HunyuanVideoV15TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Hunyuan Video V1.5 Text-to-Video with improved quality and motion.
    video, generation, hunyuan, v1.5, text-to-video

    Use cases:
    - Create high-quality video content
    - Generate smooth animations
    - Produce professional videos
    - Create motion graphics
    - Generate video effects
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="Number of inference steps"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.0, description="How closely to follow the prompt"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for reproducible generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.HunyuanVideoV15TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class Kandinsky5TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kandinsky 5 Text-to-Video generates creative videos from text prompts.
    video, generation, kandinsky, text-to-video, artistic

    Use cases:
    - Create artistic video content
    - Generate creative animations
    - Produce stylized videos
    - Create video art
    - Generate experimental content
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="What to avoid in the generated video"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="Number of inference steps"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=4.0, description="How closely to follow the prompt"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for reproducible generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Kandinsky5TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.fal.image_to_video


class KlingTextToVideoV2(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate videos directly from text prompts using Kling Video V2 Master.
    video, generation, animation, text-to-video, kling-v2

    Use cases:
    - Visualize scripts or storyboards
    - Produce short promotional videos from text
    - Create animated social media content
    - Generate concept previews for film ideas
    - Produce text-driven motion graphics
    """

    KlingDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingDuration
    )
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.image_to_video.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    duration: nodetool.nodes.fal.image_to_video.KlingDuration = Field(
        default=nodetool.nodes.fal.image_to_video.KlingDuration.FIVE_SECONDS,
        description="The duration of the generated video",
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingTextToVideoV2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.fal.image_to_video


class KlingVideoV2(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate videos from images using Kling Video V2 Master. Create smooth and realistic animations from a single frame.
    video, generation, animation, img2vid, kling-v2

    Use cases:
    - Convert artwork into animated clips
    - Produce dynamic marketing visuals
    - Generate motion graphics from static scenes
    - Create short cinematic sequences
    - Enhance presentations with video content
    """

    KlingDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingDuration
    )
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.image_to_video.AspectRatio

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to transform into a video",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="A description of the desired video motion and style"
    )
    duration: nodetool.nodes.fal.image_to_video.KlingDuration = Field(
        default=nodetool.nodes.fal.image_to_video.KlingDuration.FIVE_SECONDS,
        description="The duration of the generated video",
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoV2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.fal.image_to_video


class KlingVideoV21TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video V2.1 Master Text-to-Video with enhanced quality and motion.
    video, generation, kling, v2.1, text-to-video

    Use cases:
    - Create professional video content
    - Generate high-quality animations
    - Produce cinematic clips
    - Create promotional videos
    - Generate concept previews
    """

    KlingDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingDuration
    )
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.image_to_video.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    duration: nodetool.nodes.fal.image_to_video.KlingDuration = Field(
        default=nodetool.nodes.fal.image_to_video.KlingDuration.FIVE_SECONDS,
        description="The duration of the generated video",
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.KlingVideoV21TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class LTX2TextToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    LTX 2 Text-to-Video generates videos from text with the LTX model.
    video, generation, ltx, text-to-video

    Use cases:
    - Create videos from descriptions
    - Generate animated content
    - Produce motion graphics
    - Create video clips
    - Generate promotional content
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="What to avoid in the generated video"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="Number of inference steps"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.0, description="How closely to follow the prompt"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for reproducible generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LTX2TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.fal.image_to_video


class LumaRay2FlashTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Luma Ray 2 Flash Text-to-Video is a fast version for quick video generation.
    video, generation, luma, ray2, flash, text-to-video, fast

    Use cases:
    - Quick video prototyping
    - Rapid content creation
    - Fast video iterations
    - Real-time video generation
    - Quick concept tests
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.image_to_video.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for reproducible generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LumaRay2FlashTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.fal.image_to_video


class LumaRay2TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Luma Ray 2 Text-to-Video generates high-quality videos from text prompts.
    video, generation, luma, ray2, text-to-video, txt2vid

    Use cases:
    - Create videos from descriptions
    - Generate cinematic content
    - Produce creative videos
    - Create marketing clips
    - Generate concept videos
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.image_to_video.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    loop: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether the video should loop"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for reproducible generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.LumaRay2TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class MiniMaxHailuo23TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    MiniMax Hailuo 2.3 Standard Text-to-Video with improved quality.
    video, generation, minimax, hailuo, 2.3, text-to-video

    Use cases:
    - Create videos from text
    - Generate smooth animations
    - Produce video content
    - Create motion graphics
    - Generate promotional clips
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the prompt optimizer"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for reproducible generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.MiniMaxHailuo23TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class MochiV1(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Mochi V1 generates creative videos from text prompts with unique style.
    video, generation, mochi, text-to-video, creative

    Use cases:
    - Create creative video content
    - Generate artistic animations
    - Produce stylized videos
    - Create experimental clips
    - Generate unique video effects
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="What to avoid in the generated video"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="Number of inference steps"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=4.5, description="How closely to follow the prompt"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for reproducible generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.MochiV1

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PikaV21TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Pika V2.1 Text-to-Video generates videos from text prompts.
    video, generation, pika, v2.1, text-to-video

    Use cases:
    - Create video content from text
    - Generate animated clips
    - Produce motion graphics
    - Create video effects
    - Generate promotional content
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for reproducible generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PikaV21TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PikaV22TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Pika V2.2 Text-to-Video generates creative videos from text prompts.
    video, generation, pika, v2.2, text-to-video, creative

    Use cases:
    - Create creative video content
    - Generate artistic animations
    - Produce stylized videos
    - Create unique video clips
    - Generate experimental content
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="What to avoid in the generated video"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for reproducible generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PikaV22TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseEffects(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """
    Apply text-driven effects to a video with Pixverse 4.5.
        video, effects, pixverse, text-guided

        Use cases:
        - Stylize existing footage
        - Add visual effects via text
        - Enhance marketing videos
        - Create experimental clips
        - Transform user content
    """

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The source video",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text describing the effect"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Optional seed for deterministic output"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseEffects

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Animate an image into a video using Pixverse 4.5.
        video, generation, pixverse, image-to-video

        Use cases:
        - Bring photos to life
        - Create moving artwork
        - Generate short clips from images
        - Produce social media animations
        - Experiment with visual storytelling
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The source image",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Optional style or motion prompt"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Optional seed for deterministic output"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos from text prompts with Pixverse 4.5 API.
        video, generation, pixverse, text-to-video

        Use cases:
        - Create animated scenes from text
        - Generate marketing clips
        - Produce dynamic social posts
        - Prototype video ideas
        - Explore creative storytelling
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the video"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Optional seed for deterministic output"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseTextToVideoFast(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Generate videos quickly from text prompts with Pixverse 4.5 Fast.
        video, generation, pixverse, text-to-video, fast

        Use cases:
        - Rapid video prototyping
        - Generate quick social posts
        - Produce short marketing clips
        - Test creative ideas fast
        - Create video drafts
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the video"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Optional seed for deterministic output"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseTextToVideoFast

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseTransition(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """
    Apply Pixverse transitions between images.
        video, generation, transition, pixverse

        Use cases:
        - Blend between two images
        - Create animated transitions
        - Generate morphing effects
        - Produce smooth scene changes
        - Experiment with visual flows
    """

    start_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The starting image",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The ending image",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Optional seed for deterministic output"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.PixverseTransition

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.fal.image_to_video


class Sora2TextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    OpenAI Sora 2 Text-to-Video generates high-quality videos from text.
    video, generation, openai, sora, sora2, text-to-video

    Use cases:
    - Create cinematic videos from text
    - Generate realistic motion
    - Produce professional video content
    - Create video narratives
    - Generate concept videos
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.image_to_video.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    duration: int | OutputHandle[int] = connect_field(
        default=5, description="Duration of the video in seconds"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for reproducible generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Sora2TextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class Veo3(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate high-quality videos from text prompts with Google's Veo 3 model.
    video, generation, text-to-video, prompt, audio

    Use cases:
    - Produce short cinematic clips from descriptions
    - Create social media videos
    - Generate visual storyboards
    - Experiment with video concepts
    - Produce marketing content
    """

    Veo3AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_video.Veo3AspectRatio
    )
    Veo3Duration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_video.Veo3Duration

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt describing the video you want to generate",
    )
    aspect_ratio: nodetool.nodes.fal.text_to_video.Veo3AspectRatio = Field(
        default=nodetool.nodes.fal.text_to_video.Veo3AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video. If it is not set to 16:9, the video will be outpainted with Luma Ray 2 Reframe functionality.",
    )
    duration: nodetool.nodes.fal.text_to_video.Veo3Duration = Field(
        default=nodetool.nodes.fal.text_to_video.Veo3Duration.EIGHT_SECONDS,
        description="The duration of the generated video in seconds",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to generate audio for the video. If false, %33 less credits will be used.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="A seed to use for the video generation"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A negative prompt to guide the video generation"
    )
    enhance_prompt: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enhance the video generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.Veo3

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanFlf2V(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate video loops from text prompts using WAN-FLF2V.
    video, generation, wan, text-to-video

    Use cases:
    - Generate looping videos from descriptions
    - Produce motion graphics from prompts
    - Create abstract video ideas
    - Develop creative transitions
    - Experiment with AI-generated motion
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Randomization seed for reproducible results"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanFlf2V

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanProImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Convert an image into a short video clip using Wan Pro.
    video, generation, wan, professional, image-to-video

    Use cases:
    - Create dynamic videos from product photos
    - Generate animations from static artwork
    - Produce short promotional clips
    - Transform images into motion graphics
    - Experiment with visual storytelling
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The input image to animate",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Optional prompt describing the desired motion"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Randomization seed for reproducible results"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanProImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanProTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate a short video clip from a text prompt using Wan Pro.
    video, generation, wan, professional, text-to-video

    Use cases:
    - Create animated scenes from descriptions
    - Generate short creative videos
    - Produce promotional content
    - Visualize storyboards
    - Experiment with narrative ideas
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Randomization seed for reproducible results"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanProTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanT2V(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate videos from text using the WAN-T2V model.
    video, generation, wan, text-to-video

    Use cases:
    - Produce creative videos from prompts
    - Experiment with motion concepts
    - Generate quick animated drafts
    - Visualize ideas for stories
    - Create short social media clips
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Randomization seed for reproducible results"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanT2V

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_video
from nodetool.workflows.base_node import BaseNode


class WanV2_1_13BTextToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Create videos from text using WAN v2.1 1.3B, an open-source text-to-video model.
    video, generation, wan, text-to-video

    Use cases:
    - Produce short clips from prompts
    - Generate concept videos
    - Create quick visualizations
    - Iterate on storytelling ideas
    - Experiment with AI video synthesis
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the desired video"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Randomization seed for reproducible results"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_video.WanV2_1_13BTextToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
