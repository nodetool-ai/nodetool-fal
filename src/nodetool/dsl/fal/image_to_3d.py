# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_3d
from nodetool.workflows.base_node import BaseNode

class BytedanceSeed3DImageTo3D(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Bytedance
        3d, generation, image-to-3d, modeling

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL of the image for the 3D asset generation.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_3d.BytedanceSeed3DImageTo3D

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_3d
from nodetool.workflows.base_node import BaseNode

class Hunyuan3DV3ImageTo3D(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Hunyuan3d V3
        3d, generation, image-to-3d, modeling

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    PolygonType: typing.ClassVar[type] = nodetool.nodes.fal.image_to_3d.Hunyuan3DV3ImageTo3D.PolygonType
    GenerateType: typing.ClassVar[type] = nodetool.nodes.fal.image_to_3d.Hunyuan3DV3ImageTo3D.GenerateType

    input_image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL of image to use while generating the 3D model.')
    polygon_type: nodetool.nodes.fal.image_to_3d.Hunyuan3DV3ImageTo3D.PolygonType = Field(default=nodetool.nodes.fal.image_to_3d.Hunyuan3DV3ImageTo3D.PolygonType.TRIANGLE, description='Polygon type. Only takes effect when GenerateType is LowPoly.')
    face_count: int | OutputHandle[int] = connect_field(default=500000, description='Target face count. Range: 40000-1500000')
    right_image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Optional right view image URL for better 3D reconstruction.')
    back_image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Optional back view image URL for better 3D reconstruction.')
    enable_pbr: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enable PBR material generation. Does not take effect when generate_type is Geometry.')
    generate_type: nodetool.nodes.fal.image_to_3d.Hunyuan3DV3ImageTo3D.GenerateType = Field(default=nodetool.nodes.fal.image_to_3d.Hunyuan3DV3ImageTo3D.GenerateType.NORMAL, description='Generation type. Normal: textured model. LowPoly: polygon reduction. Geometry: white model without texture.')
    left_image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Optional left view image URL for better 3D reconstruction.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_3d.Hunyuan3DV3ImageTo3D

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_3d
from nodetool.workflows.base_node import BaseNode

class Hunyuan3DV3SketchTo3D(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Hunyuan3d V3
        3d, generation, image-to-3d, modeling

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    input_image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL of sketch or line art image to transform into a 3D model. Image resolution must be between 128x128 and 5000x5000 pixels.')
    prompt: str | OutputHandle[str] = connect_field(default='', description='Text prompt describing the 3D content attributes such as color, category, and material.')
    face_count: int | OutputHandle[int] = connect_field(default=500000, description='Target face count. Range: 40000-1500000')
    enable_pbr: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enable PBR material generation.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_3d.Hunyuan3DV3SketchTo3D

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_3d
from nodetool.workflows.base_node import BaseNode

class Hunyuan_WorldImageToWorld(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        Hunyuan World
        3d, generation, image-to-3d, modeling

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    classes: str | OutputHandle[str] = connect_field(default='', description='Classes to use for the world generation.')
    export_drc: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to export DRC (Dynamic Resource Configuration).')
    labels_fg1: str | OutputHandle[str] = connect_field(default='', description='Labels for the first foreground object.')
    labels_fg2: str | OutputHandle[str] = connect_field(default='', description='Labels for the second foreground object.')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The URL of the image to convert to a world.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_3d.Hunyuan_WorldImageToWorld

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_3d
from nodetool.workflows.base_node import BaseNode

class Hyper3DRodinV2(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Hyper3d
        3d, generation, image-to-3d, modeling

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    QualityMeshOption: typing.ClassVar[type] = nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.QualityMeshOption
    GeometryFileFormat: typing.ClassVar[type] = nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.GeometryFileFormat
    Material: typing.ClassVar[type] = nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.Material

    quality_mesh_option: nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.QualityMeshOption = Field(default=nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.QualityMeshOption.VALUE_500K_TRIANGLE, description="Combined quality and mesh type selection. Quad = smooth surfaces, Triangle = detailed geometry. These corresponds to `mesh_mode` (if the option contains 'Triangle', mesh_mode is 'Raw', otherwise 'Quad') and `quality_override` (the numeric part of the option) parameters in Hyper3D API.")
    prompt: str | OutputHandle[str] = connect_field(default='', description='A textual prompt to guide model generation. Optional for Image-to-3D mode - if empty, AI will generate a prompt based on your images.')
    preview_render: bool | OutputHandle[bool] = connect_field(default=False, description='Generate a preview render image of the 3D model along with the model files.')
    bbox_condition: list[int] | OutputHandle[list[int]] = connect_field(default=[], description='An array that specifies the bounding box dimensions [width, height, length].')
    TAPose: bool | OutputHandle[bool] = connect_field(default=False, description='Generate characters in T-pose or A-pose format, making them easier to rig and animate in 3D software.')
    input_image_urls: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='URL of images to use while generating the 3D model. Required for Image-to-3D mode. Up to 5 images allowed.')
    use_original_alpha: bool | OutputHandle[bool] = connect_field(default=False, description='When enabled, preserves the transparency channel from input images during 3D generation.')
    geometry_file_format: nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.GeometryFileFormat = Field(default=nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.GeometryFileFormat.GLB, description='Format of the geometry file. Possible values: glb, usdz, fbx, obj, stl. Default is glb.')
    addons: nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.Addons | OutputHandle[nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.Addons] | None = connect_field(default=None, description='The HighPack option will provide 4K resolution textures instead of the default 1K, as well as models with high-poly. It will cost **triple the billable units**.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Seed value for randomization, ranging from 0 to 65535. Optional.')
    material: nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.Material = Field(default=nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2.Material.ALL, description='Material type. PBR: Physically-based materials with realistic lighting. Shaded: Simple materials with baked lighting. All: Both types included.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_3d.Hyper3DRodinV2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_3d
from nodetool.workflows.base_node import BaseNode

class MeshyV5MultiImageTo3D(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Meshy 5 Multi
        3d, generation, image-to-3d, modeling

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    Topology: typing.ClassVar[type] = nodetool.nodes.fal.image_to_3d.MeshyV5MultiImageTo3D.Topology
    SymmetryMode: typing.ClassVar[type] = nodetool.nodes.fal.image_to_3d.MeshyV5MultiImageTo3D.SymmetryMode

    enable_pbr: bool | OutputHandle[bool] = connect_field(default=False, description='Generate PBR Maps (metallic, roughness, normal) in addition to base color. Requires should_texture to be true.')
    should_texture: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to generate textures. False provides mesh without textures for 5 credits, True adds texture generation for additional 10 credits.')
    target_polycount: int | OutputHandle[int] = connect_field(default=30000, description='Target number of polygons in the generated model')
    is_a_t_pose: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to generate the model in an A/T pose')
    texture_image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='2D image to guide the texturing process. Requires should_texture to be true.')
    topology: nodetool.nodes.fal.image_to_3d.MeshyV5MultiImageTo3D.Topology = Field(default=nodetool.nodes.fal.image_to_3d.MeshyV5MultiImageTo3D.Topology.TRIANGLE, description='Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, input data will be checked for safety before processing.')
    symmetry_mode: nodetool.nodes.fal.image_to_3d.MeshyV5MultiImageTo3D.SymmetryMode = Field(default=nodetool.nodes.fal.image_to_3d.MeshyV5MultiImageTo3D.SymmetryMode.AUTO, description='Controls symmetry behavior during model generation.')
    image_urls: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='1 to 4 images for 3D model creation. All images should depict the same object from different angles. Supports .jpg, .jpeg, .png formats, and AVIF/HEIF which will be automatically converted. If more than 4 images are provided, only the first 4 will be used.')
    texture_prompt: str | OutputHandle[str] = connect_field(default='', description='Text prompt to guide the texturing process. Requires should_texture to be true.')
    should_remesh: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the remesh phase. When false, returns triangular mesh ignoring topology and target_polycount.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_3d.MeshyV5MultiImageTo3D

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_3d
from nodetool.workflows.base_node import BaseNode

class MeshyV6PreviewImageTo3D(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Meshy 6 Preview
        3d, generation, image-to-3d, modeling

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    Topology: typing.ClassVar[type] = nodetool.nodes.fal.image_to_3d.MeshyV6PreviewImageTo3D.Topology
    SymmetryMode: typing.ClassVar[type] = nodetool.nodes.fal.image_to_3d.MeshyV6PreviewImageTo3D.SymmetryMode

    enable_pbr: bool | OutputHandle[bool] = connect_field(default=False, description='Generate PBR Maps (metallic, roughness, normal) in addition to base color')
    is_a_t_pose: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to generate the model in an A/T pose')
    target_polycount: int | OutputHandle[int] = connect_field(default=30000, description='Target number of polygons in the generated model')
    should_texture: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to generate textures')
    texture_image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='2D image to guide the texturing process')
    topology: nodetool.nodes.fal.image_to_3d.MeshyV6PreviewImageTo3D.Topology = Field(default=nodetool.nodes.fal.image_to_3d.MeshyV6PreviewImageTo3D.Topology.TRIANGLE, description='Specify the topology of the generated model. Quad for smooth surfaces, Triangle for detailed geometry.')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Image URL or base64 data URI for 3D model creation. Supports .jpg, .jpeg, and .png formats. Also supports AVIF and HEIF formats which will be automatically converted.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, input data will be checked for safety before processing.')
    symmetry_mode: nodetool.nodes.fal.image_to_3d.MeshyV6PreviewImageTo3D.SymmetryMode = Field(default=nodetool.nodes.fal.image_to_3d.MeshyV6PreviewImageTo3D.SymmetryMode.AUTO, description='Controls symmetry behavior during model generation. Off disables symmetry, Auto determines it automatically, On enforces symmetry.')
    texture_prompt: str | OutputHandle[str] = connect_field(default='', description='Text prompt to guide the texturing process')
    should_remesh: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the remesh phase')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_3d.MeshyV6PreviewImageTo3D

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_3d
from nodetool.workflows.base_node import BaseNode

class Omnipart(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Omnipart
        3d, generation, image-to-3d, modeling

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    input_image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL of image to use while generating the 3D model.')
    parts: str | OutputHandle[str] = connect_field(default='', description="Specify which segments to merge (e.g., '0,1;3,4' merges segments 0&1 together and 3&4 together)")
    seed: int | OutputHandle[int] = connect_field(default=765464, description='The same seed and the same prompt given to the same version of the model will output the same image every time.')
    minimum_segment_size: int | OutputHandle[int] = connect_field(default=2000, description='Minimum segment size (pixels) for the model.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=7.5, description='Guidance scale for the model.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_3d.Omnipart

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_3d
from nodetool.workflows.base_node import BaseNode

class Pshuman(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Pshuman
        3d, generation, image-to-3d, modeling

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    guidance_scale: float | OutputHandle[float] = connect_field(default=4, description='Guidance scale for the diffusion process. Controls how much the output adheres to the generated views.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Seed for reproducibility. If None, a random seed will be used.')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='A direct URL to the input image of a person.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_3d.Pshuman

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_3d
from nodetool.workflows.base_node import BaseNode

class Sam33DBody(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Sam 3
        3d, generation, image-to-3d, modeling

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL of the image containing humans')
    include_3d_keypoints: bool | OutputHandle[bool] = connect_field(default=True, description='Include 3D keypoint markers (spheres) in the GLB mesh for visualization')
    mask_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Optional URL of a binary mask image (white=person, black=background). When provided, skips auto human detection and uses this mask instead. Bbox is auto-computed from the mask.')
    export_meshes: bool | OutputHandle[bool] = connect_field(default=True, description='Export individual mesh files (.ply) per person')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_3d.Sam33DBody

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_3d
from nodetool.workflows.base_node import BaseNode

class Sam33DObjects(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Sam 3
        3d, generation, image-to-3d, modeling

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    prompt: str | OutputHandle[str] = connect_field(default='car', description="Text prompt for auto-segmentation when no masks provided (e.g., 'chair', 'lamp')")
    export_textured_glb: bool | OutputHandle[bool] = connect_field(default=False, description='If True, exports GLB with baked texture and UVs instead of vertex colors.')
    detection_threshold: float | OutputHandle[float] = connect_field(default=0.0, description="Detection confidence threshold (0.1-1.0). Lower = more detections but less precise. If not set, uses the model's default.")
    pointmap_url: str | OutputHandle[str] = connect_field(default='', description='Optional URL to external pointmap/depth data (NPY or NPZ format) for improved 3D reconstruction depth estimation')
    box_prompts: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='Box prompts for auto-segmentation when no masks provided. Multiple boxes supported - each produces a separate object mask for 3D reconstruction.')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL of the image to reconstruct in 3D')
    mask_urls: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='Optional list of mask URLs (one per object). If not provided, use prompt/point_prompts/box_prompts to auto-segment, or entire image will be used.')
    point_prompts: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='Point prompts for auto-segmentation when no masks provided')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducibility')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_3d.Sam33DObjects

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


