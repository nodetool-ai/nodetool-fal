# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode


class BriaBackgroundRemove(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """

    Bria RMBG 2.0 enables seamless removal of backgrounds from images, ideal for professional editing tasks.
    Trained exclusively on licensed data for safe and risk-free commercial use.
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image to remove background from",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.BriaBackgroundRemove

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode


class BriaBackgroundReplace(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """
    Bria Background Replace allows for efficient swapping of backgrounds in images via text prompts or reference image, delivering realistic and polished results. Trained exclusively on licensed data for safe and risk-free commercial use.
        image, background, replacement, swap

        Use cases:
        - Replace image backgrounds seamlessly
        - Create professional photo compositions
        - Generate custom scene settings
        - Produce commercial-ready images
        - Create consistent visual environments
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image to replace background",
    )
    ref_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Reference image for the new background",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Prompt to generate new background"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt for background generation"
    )
    refine_prompt: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to refine the prompt"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same image every time"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.BriaBackgroundReplace

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode


class BriaEraser(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    Bria Eraser enables precise removal of unwanted objects from images while maintaining high-quality outputs. Trained exclusively on licensed data for safe and risk-free commercial use.
    image, removal, cleanup

    Use cases:
    - Remove unwanted objects from images
    - Clean up image imperfections
    - Prepare images for commercial use
    - Remove distracting elements
    - Create clean, professional images
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image to erase from",
    )
    mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The mask for areas to be cleaned",
    )
    mask_type: str | OutputHandle[str] = connect_field(
        default="manual",
        description="Type of mask - 'manual' for user-created or 'automatic' for algorithm-generated",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.BriaEraser

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode


class BriaExpand(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Bria Expand expands images beyond their borders in high quality. Trained exclusively on licensed data for safe and risk-free commercial use.
        image, expansion, outpainting

        Use cases:
        - Extend image boundaries seamlessly
        - Create wider or taller compositions
        - Expand images for different aspect ratios
        - Generate additional scene content
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The input image to expand",
    )
    canvas_width: int | OutputHandle[int] = connect_field(
        default=1200,
        description="The desired width of the final image, after the expansion",
    )
    canvas_height: int | OutputHandle[int] = connect_field(
        default=674,
        description="The desired height of the final image, after the expansion",
    )
    original_image_width: int | OutputHandle[int] = connect_field(
        default=610,
        description="The desired width of the original image, inside the full canvas",
    )
    original_image_height: int | OutputHandle[int] = connect_field(
        default=855,
        description="The desired height of the original image, inside the full canvas",
    )
    original_image_x: int | OutputHandle[int] = connect_field(
        default=301,
        description="The desired x-coordinate of the original image, inside the full canvas",
    )
    original_image_y: int | OutputHandle[int] = connect_field(
        default=-66,
        description="The desired y-coordinate of the original image, inside the full canvas",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text on which you wish to base the image expansion"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="The negative prompt to use when generating images"
    )
    num_images: int | OutputHandle[int] = connect_field(
        default=1, description="Number of images to generate"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same image every time"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.BriaExpand

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode


class BriaGenFill(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Bria GenFill enables high-quality object addition or visual transformation. Trained exclusively on licensed data for safe and risk-free commercial use.
        image, generation, filling, transformation

        Use cases:
        - Add new objects to existing images
        - Transform specific image areas
        - Generate contextual content
        - Create seamless visual additions
        - Produce professional image modifications
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image to erase from",
    )
    mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The mask for areas to be cleaned",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate images"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="The negative prompt to use when generating images"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same image every time"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.BriaGenFill

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode


class BriaProductShot(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Place any product in any scenery with just a prompt or reference image while maintaining high integrity of the product. Trained exclusively on licensed data for safe and risk-free commercial use and optimized for eCommerce.
        image, product, placement, ecommerce

        Use cases:
        - Create professional product photography
        - Generate contextual product shots
        - Place products in custom environments
        - Create eCommerce product listings
        - Generate marketing visuals
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The product image to be placed",
    )
    scene_description: str | OutputHandle[str] = connect_field(
        default="", description="Text description of the new scene/background"
    )
    ref_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Reference image for the new scene/background",
    )
    optimize_description: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to optimize the scene description"
    )
    placement_type: str | OutputHandle[str] = connect_field(
        default="manual_placement",
        description="How to position the product (original, automatic, manual_placement, manual_padding)",
    )
    manual_placement_selection: str | OutputHandle[str] = connect_field(
        default="bottom_center",
        description="Specific placement position when using manual_placement",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.BriaProductShot

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode


class ClarityUpscaler(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Upscale images to improve resolution and sharpness.

        clarity, upscale, enhancement

        Use cases:
        - Increase image resolution for printing
        - Improve clarity of low-quality images
        - Enhance textures and graphics
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image to upscale",
    )
    scale: int | OutputHandle[int] = connect_field(
        default=2, description="Upscaling factor"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.ClarityUpscaler

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.fal.text_to_image


class FluxDevRedux(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    FLUX.1 [dev] Redux is a high-performance endpoint that enables rapid transformation of existing images, delivering high-quality style transfers and image modifications.
    image, transformation, style-transfer, development, flux

    Use cases:
    - Transform images with advanced controls
    - Create customized image variations
    - Apply precise style modifications
    """

    ImageSizePreset: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_image.ImageSizePreset
    )

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The input image to transform",
    )
    image_size: nodetool.nodes.fal.text_to_image.ImageSizePreset = Field(
        default=nodetool.nodes.fal.text_to_image.ImageSizePreset.LANDSCAPE_4_3,
        description="The size of the generated image",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28, description="The number of inference steps to perform"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5, description="How closely the model should stick to your prompt"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same image every time"
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="If true, the safety checker will be enabled"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.FluxDevRedux

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.fal.text_to_image


class FluxLoraCanny(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    FLUX LoRA Canny enables precise control over composition and style through edge detection and LoRA-based guidance mechanisms.
        image, edge, lora, style-transfer, control

        Use cases:
        - Generate stylized images with structural control
        - Create edge-guided artistic transformations
        - Apply custom styles while maintaining composition
        - Produce consistent style variations
    """

    ImageSizePreset: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_image.ImageSizePreset
    )

    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The control image to generate the Canny edge map from",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate an image from"
    )
    image_size: nodetool.nodes.fal.text_to_image.ImageSizePreset = Field(
        default=nodetool.nodes.fal.text_to_image.ImageSizePreset.LANDSCAPE_4_3,
        description="The size of the generated image",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28, description="The number of inference steps to perform"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5, description="How closely the model should stick to your prompt"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same image every time"
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=0.6, description="The strength of the LoRA adaptation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.FluxLoraCanny

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.fal.text_to_image


class FluxLoraDepth(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    FLUX LoRA Depth enables precise control over composition and structure through depth map detection and LoRA-based guidance mechanisms.
        image, depth, lora, structure, control

        Use cases:
        - Generate depth-aware stylized images
        - Create 3D-conscious artistic transformations
        - Maintain spatial relationships with custom styles
        - Produce depth-consistent variations
        - Generate images with controlled perspective
    """

    ImageSizePreset: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_image.ImageSizePreset
    )

    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The control image to generate the depth map from",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate an image from"
    )
    image_size: nodetool.nodes.fal.text_to_image.ImageSizePreset = Field(
        default=nodetool.nodes.fal.text_to_image.ImageSizePreset.LANDSCAPE_4_3,
        description="The size of the generated image",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28, description="The number of inference steps to perform"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5, description="How closely the model should stick to your prompt"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same image every time"
    )
    lora_scale: float | OutputHandle[float] = connect_field(
        default=0.6, description="The strength of the LoRA adaptation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.FluxLoraDepth

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.fal.text_to_image


class FluxProCanny(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    FLUX.1 [pro] Canny enables precise control over composition, style, and structure through advanced edge detection and guidance mechanisms.
        image, edge, composition, style, control

        Use cases:
        - Generate images with precise structural control
        - Create artwork based on edge maps
        - Transform sketches into detailed images
        - Maintain specific compositional elements
        - Generate variations with consistent structure
    """

    ImageSizePreset: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_image.ImageSizePreset
    )

    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The control image to generate the Canny edge map from",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate an image from"
    )
    image_size: nodetool.nodes.fal.text_to_image.ImageSizePreset = Field(
        default=nodetool.nodes.fal.text_to_image.ImageSizePreset.LANDSCAPE_4_3,
        description="The size of the generated image",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28, description="The number of inference steps to perform"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5, description="How closely the model should stick to your prompt"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same image every time"
    )
    safety_tolerance: str | OutputHandle[str] = connect_field(
        default="2", description="Safety tolerance level (1-6, 1 being most strict)"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.FluxProCanny

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.fal.text_to_image


class FluxProDepth(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    FLUX.1 [pro] Depth enables precise control over composition and structure through depth map detection and guidance mechanisms.
        image, depth-map, composition, structure, control

        Use cases:
        - Generate images with controlled depth perception
        - Create 3D-aware image transformations
        - Maintain spatial relationships in generated images
        - Produce images with accurate perspective
        - Generate variations with consistent depth structure
    """

    ImageSizePreset: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_image.ImageSizePreset
    )

    control_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The control image to generate the depth map from",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate an image from"
    )
    image_size: nodetool.nodes.fal.text_to_image.ImageSizePreset = Field(
        default=nodetool.nodes.fal.text_to_image.ImageSizePreset.LANDSCAPE_4_3,
        description="The size of the generated image",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28, description="The number of inference steps to perform"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5, description="How closely the model should stick to your prompt"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same image every time"
    )
    safety_tolerance: str | OutputHandle[str] = connect_field(
        default="2", description="Safety tolerance level (1-6, 1 being most strict)"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.FluxProDepth

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode


class FluxProFill(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    FLUX.1 [pro] Fill is a high-performance endpoint that enables rapid transformation of existing images with inpainting/outpainting capabilities.
        image, inpainting, outpainting, transformation, professional

        Use cases:
        - Fill in missing or masked parts of images
        - Extend images beyond their original boundaries
        - Remove and replace unwanted elements
        - Create seamless image completions
        - Generate context-aware image content
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The input image to transform",
    )
    mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The mask for inpainting",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to fill the masked part of the image"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28, description="The number of inference steps to perform"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same image every time"
    )
    safety_tolerance: str | OutputHandle[str] = connect_field(
        default="2", description="Safety tolerance level (1-6, 1 being most strict)"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.FluxProFill

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.fal.text_to_image


class FluxProRedux(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

    FLUX.1 [pro] Redux is a high-performance endpoint that enables rapid transformation of existing images, delivering high-quality style transfers and image modifications.
    image, transformation, style-transfer, flux

    Use cases:
    - Create professional image transformations
    - Generate style transfers
    """

    ImageSizePreset: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_image.ImageSizePreset
    )

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The input image to transform",
    )
    image_size: nodetool.nodes.fal.text_to_image.ImageSizePreset = Field(
        default=nodetool.nodes.fal.text_to_image.ImageSizePreset.LANDSCAPE_4_3,
        description="The size of the generated image",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28, description="The number of inference steps to perform"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5, description="How closely the model should stick to your prompt"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same image every time"
    )
    safety_tolerance: str | OutputHandle[str] = connect_field(
        default="2", description="Safety tolerance level (1-6, 1 being most strict)"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.FluxProRedux

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.fal.text_to_image


class FluxProUltraRedux(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """

    FLUX1.1 [pro] ultra Redux is a high-performance endpoint that enables rapid transformation of existing images, delivering high-quality style transfers and image modifications with the core FLUX capabilities.
    image, transformation, style-transfer, ultra, professional

    Use cases:
    - Apply precise image modifications
    - Process images with maximum control
    """

    ImageSizePreset: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_image.ImageSizePreset
    )

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The input image to transform",
    )
    image_size: nodetool.nodes.fal.text_to_image.ImageSizePreset = Field(
        default=nodetool.nodes.fal.text_to_image.ImageSizePreset.LANDSCAPE_4_3,
        description="The size of the generated image",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28, description="The number of inference steps to perform"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5, description="How closely the model should stick to your prompt"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same image every time"
    )
    safety_tolerance: str | OutputHandle[str] = connect_field(
        default="2", description="Safety tolerance level (1-6, 1 being most strict)"
    )
    image_prompt_strength: float | OutputHandle[float] = connect_field(
        default=0.1, description="The strength of the image prompt, between 0 and 1"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.FluxProUltraRedux

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode
import nodetool.nodes.fal.text_to_image


class FluxSchnellRedux(
    SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]
):
    """

    FLUX.1 [schnell] Redux is a high-performance endpoint that enables rapid transformation of existing images, delivering high-quality style transfers and image modifications with the core FLUX capabilities.
    image, transformation, style-transfer, fast, flux

    Use cases:
    - Transform images with style transfers
    - Apply artistic modifications to photos
    - Create image variations
    """

    ImageSizePreset: typing.ClassVar[type] = (
        nodetool.nodes.fal.text_to_image.ImageSizePreset
    )

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The input image to transform",
    )
    image_size: nodetool.nodes.fal.text_to_image.ImageSizePreset = Field(
        default=nodetool.nodes.fal.text_to_image.ImageSizePreset.LANDSCAPE_4_3,
        description="The size of the generated image",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=4, description="The number of inference steps to perform"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same image every time"
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="If true, the safety checker will be enabled"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.FluxSchnellRedux

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode


class IdeogramV2Edit(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Transform existing images with Ideogram V2's editing capabilities. Modify, adjust, and refine images while maintaining high fidelity and realistic outputs with precise prompt control.
        image, editing, transformation, fidelity, control

        Use cases:
        - Edit specific parts of images with precision
        - Create targeted image modifications
        - Refine and enhance image details
        - Generate contextual image edits
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to fill the masked part of the image"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to edit",
    )
    mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The mask for editing",
    )
    style: str | OutputHandle[str] = connect_field(
        default="auto",
        description="Style of generated image (auto, general, realistic, design, render_3D, anime)",
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to expand the prompt with MagicPrompt functionality",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same image every time"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.IdeogramV2Edit

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode


class IdeogramV2Remix(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Reimagine existing images with Ideogram V2's remix feature. Create variations and adaptations while preserving core elements and adding new creative directions through prompt guidance.
        image, remix, variation, creativity, adaptation

        Use cases:
        - Create artistic variations of images
        - Generate style-transferred versions
        - Produce creative image adaptations
        - Transform images while preserving key elements
        - Generate alternative interpretations
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to remix the image with"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to remix",
    )
    aspect_ratio: str | OutputHandle[str] = connect_field(
        default="1:1", description="The aspect ratio of the generated image"
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.8, description="Strength of the input image in the remix"
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to expand the prompt with MagicPrompt functionality",
    )
    style: str | OutputHandle[str] = connect_field(
        default="auto",
        description="Style of generated image (auto, general, realistic, design, render_3D, anime)",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same image every time"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.IdeogramV2Remix

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_image
from nodetool.workflows.base_node import BaseNode


class WanEffects(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """
    Apply stylized effects to an image using the WAN Effects model.

        image, transformation, style, filter

        Use cases:
        - Add artistic filters to photos
        - Create stylized social media images
        - Quickly generate meme-style effects
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Input image to apply the effect to",
    )
    effect: str | OutputHandle[str] = connect_field(
        default="", description="Name of the effect to apply"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_image.WanEffects

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
