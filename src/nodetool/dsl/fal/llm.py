# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.llm
from nodetool.workflows.base_node import BaseNode

class OpenRouter(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        OpenRouter provides unified access to any LLM (Large Language Model) through a single API.
        llm, chat, openrouter, multimodel, language-model

        Use cases:
        - Run any LLM through unified interface
        - Switch between models seamlessly
        - Access multiple LLM providers
        - Flexible model selection
        - Unified LLM API access
    """

    model: str | OutputHandle[str] = connect_field(default='', description='Name of the model to use. Charged based on actual token usage.')
    prompt: str | OutputHandle[str] = connect_field(default='', description='Prompt to be used for the chat completion')
    max_tokens: str | OutputHandle[str] = connect_field(default='', description="This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.")
    temperature: float | OutputHandle[float] = connect_field(default=1, description="This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.")
    reasoning: bool | OutputHandle[bool] = connect_field(default=False, description='Should reasoning be the part of the final answer.')
    system_prompt: str | OutputHandle[str] = connect_field(default='', description='System prompt to provide context or instructions to the model')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.llm.OpenRouter

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.llm
from nodetool.workflows.base_node import BaseNode

class OpenRouterChatCompletions(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        OpenRouter Chat Completions provides OpenAI-compatible interface for any LLM.
        llm, chat, openai-compatible, openrouter, chat-completions

        Use cases:
        - OpenAI-compatible LLM access
        - Drop-in replacement for OpenAI API
        - Multi-model chat completions
        - Standardized chat interface
        - Universal LLM chat API
    """

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.llm.OpenRouterChatCompletions

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.llm
from nodetool.workflows.base_node import BaseNode

class Qwen3Guard(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Qwen 3 Guard provides content safety and moderation using Qwen's LLM.
        llm, safety, moderation, qwen, guard

        Use cases:
        - Content safety checking
        - Moderation of text content
        - Safety filtering for outputs
        - Content policy enforcement
        - Text safety analysis
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='The input text to be classified')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.llm.Qwen3Guard

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


