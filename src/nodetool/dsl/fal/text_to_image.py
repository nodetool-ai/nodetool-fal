# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class BriaFiboGenerate(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Fibo
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.BriaFiboGenerate.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(default='', description='Prompt for image generation.')
    aspect_ratio: nodetool.nodes.fal.text_to_image.BriaFiboGenerate.AspectRatio = Field(default=nodetool.nodes.fal.text_to_image.BriaFiboGenerate.AspectRatio.RATIO_1_1, description='Aspect ratio. Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9')
    steps_num: int | OutputHandle[int] = connect_field(default=50, description='Number of inference steps.')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Reference image (file or URL).')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description='If true, returns the image directly in the response (increases latency).')
    guidance_scale: int | OutputHandle[int] = connect_field(default=5, description='Guidance scale for text.')
    seed: int | OutputHandle[int] = connect_field(default=5555, description='Random seed for reproducibility.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Negative prompt for image generation.')
    structured_prompt: str | OutputHandle[str] = connect_field(default='', description='The structured prompt to generate an image from.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.BriaFiboGenerate

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class BriaFiboLiteGenerate(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Fibo Lite
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.BriaFiboLiteGenerate.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(default='', description='Prompt for image generation.')
    steps_num: int | OutputHandle[int] = connect_field(default=8, description='Number of inference steps for Fibo Lite.')
    aspect_ratio: nodetool.nodes.fal.text_to_image.BriaFiboLiteGenerate.AspectRatio = Field(default=nodetool.nodes.fal.text_to_image.BriaFiboLiteGenerate.AspectRatio.RATIO_1_1, description='Aspect ratio. Options: 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Reference image (file or URL).')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description='If true, returns the image directly in the response (increases latency).')
    seed: int | OutputHandle[int] = connect_field(default=5555, description='Random seed for reproducibility.')
    structured_prompt: str | OutputHandle[str] = connect_field(default='', description='The structured prompt to generate an image from.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.BriaFiboLiteGenerate

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class BytedanceDreaminaV3_1TextToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Bytedance
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt used to generate the image')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate')
    image_size: str | OutputHandle[str] = connect_field(default='', description='The size of the generated image. Width and height must be between 512 and 2048.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed to control the stochasticity of image generation.')
    enhance_prompt: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to use an LLM to enhance the prompt')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.BytedanceDreaminaV3_1TextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class BytedanceSeedreamV3TextToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Bytedance
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt used to generate the image')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate')
    image_size: str | OutputHandle[str] = connect_field(default='', description='Use for finer control over the output image size. Will be used over aspect_ratio, if both are provided. Width and height must be between 512 and 2048.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=2.5, description='Controls how closely the output image aligns with the input prompt. Higher values mean stronger prompt correlation.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed to control the stochasticity of image generation.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.BytedanceSeedreamV3TextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class BytedanceSeedreamV45TextToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        ByteDance SeeDream v4.5 generates advanced images from text with cutting-edge AI technology.
        image, generation, bytedance, seedream, v4.5, text-to-image

        Use cases:
        - Generate images with SeeDream v4.5
        - Create cutting-edge visual content
        - Produce advanced AI artwork
        - Generate images with latest tech
        - Create modern AI visuals
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt used to generate the image')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of separate model generations to be run with the prompt.')
    image_size: str | OutputHandle[str] = connect_field(default='', description='The size of the generated image. Width and height must be between 1920 and 4096, or total number of pixels must be between 2560*1440 and 4096*4096.')
    max_images: int | OutputHandle[int] = connect_field(default=1, description='If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed to control the stochasticity of image generation.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.BytedanceSeedreamV45TextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class BytedanceSeedreamV4TextToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Bytedance Seedream v4
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    EnhancePromptMode: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.BytedanceSeedreamV4TextToImage.EnhancePromptMode

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt used to generate the image')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of separate model generations to be run with the prompt.')
    image_size: str | OutputHandle[str] = connect_field(default='', description='The size of the generated image. Total pixels must be between 960x960 and 4096x4096.')
    max_images: int | OutputHandle[int] = connect_field(default=1, description='If set to a number greater than one, enables multi-image generation. The model will potentially return up to `max_images` images every generation, and in total, `num_images` generations will be carried out. In total, the number of images generated will be between `num_images` and `max_images*num_images`.')
    enhance_prompt_mode: nodetool.nodes.fal.text_to_image.BytedanceSeedreamV4TextToImage.EnhancePromptMode = Field(default=nodetool.nodes.fal.text_to_image.BytedanceSeedreamV4TextToImage.EnhancePromptMode.STANDARD, description='The mode to use for enhancing prompt enhancement. Standard mode provides higher quality results but takes longer to generate. Fast mode provides average quality results but takes less time to generate.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed to control the stochasticity of image generation.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.BytedanceSeedreamV4TextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Emu3_5ImageTextToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Emu 3.5 Image
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Emu3_5ImageTextToImage.Resolution
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Emu3_5ImageTextToImage.AspectRatio
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Emu3_5ImageTextToImage.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to create the image.')
    resolution: nodetool.nodes.fal.text_to_image.Emu3_5ImageTextToImage.Resolution = Field(default=nodetool.nodes.fal.text_to_image.Emu3_5ImageTextToImage.Resolution.VALUE_720P, description='The resolution of the output image.')
    aspect_ratio: nodetool.nodes.fal.text_to_image.Emu3_5ImageTextToImage.AspectRatio = Field(default=nodetool.nodes.fal.text_to_image.Emu3_5ImageTextToImage.AspectRatio.RATIO_1_1, description='The aspect ratio of the output image.')
    output_format: nodetool.nodes.fal.text_to_image.Emu3_5ImageTextToImage.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Emu3_5ImageTextToImage.OutputFormat.PNG, description='The format of the output image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to return the image in sync mode.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the safety checker.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed for the inference.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Emu3_5ImageTextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux1Krea(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX.1 Krea [dev]
        flux, generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux1Krea.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux1Krea.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    acceleration: nodetool.nodes.fal.text_to_image.Flux1Krea.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.Flux1Krea.Acceleration.REGULAR, description='The speed of the generation. The higher the speed, the faster the generation.')
    output_format: nodetool.nodes.fal.text_to_image.Flux1Krea.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux1Krea.OutputFormat.JPEG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')
    seed: str | OutputHandle[str] = connect_field(default='', description='The same seed and the same prompt given to the same version of the model will output the same image every time.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=4.5, description='The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux1Krea

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux1Srpo(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX.1 SRPO [dev]
        flux, generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux1Srpo.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux1Srpo.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    acceleration: nodetool.nodes.fal.text_to_image.Flux1Srpo.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.Flux1Srpo.Acceleration.REGULAR, description='The speed of the generation. The higher the speed, the faster the generation.')
    output_format: nodetool.nodes.fal.text_to_image.Flux1Srpo.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux1Srpo.OutputFormat.JPEG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')
    seed: str | OutputHandle[str] = connect_field(default='', description='The same seed and the same prompt given to the same version of the model will output the same image every time.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=4.5, description='The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux1Srpo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux2Flash(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX.2 Flash is an ultra-fast variant of FLUX.2 designed for instant image generation with minimal latency.
        image, generation, flux, ultra-fast, flash, text-to-image, txt2img

        Use cases:
        - Instant preview generation for user interfaces
        - Real-time collaborative design tools
        - Lightning-fast concept exploration
        - High-speed batch processing
        - Interactive gaming and entertainment applications
    """

    ImageSizePreset: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ImageSizePreset
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2Flash.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: nodetool.nodes.fal.text_to_image.ImageSizePreset = Field(default=nodetool.nodes.fal.text_to_image.ImageSizePreset.LANDSCAPE_4_3, description='Size preset for the generated image')
    output_format: nodetool.nodes.fal.text_to_image.Flux2Flash.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux2Flash.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=2.5, description='Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Seed for reproducible results. Use -1 for random')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=False, description='If set to true, the prompt will be expanded for better results.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux2Flash

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux2Flex(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Flux 2 Flex
        flux, generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2Flex.OutputFormat
    SafetyTolerance: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2Flex.SafetyTolerance

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    output_format: nodetool.nodes.fal.text_to_image.Flux2Flex.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux2Flex.OutputFormat.JPEG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    safety_tolerance: nodetool.nodes.fal.text_to_image.Flux2Flex.SafetyTolerance = Field(default=nodetool.nodes.fal.text_to_image.Flux2Flex.SafetyTolerance.VALUE_2, description='The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=True, description="Whether to expand the prompt using the model's own knowledge.")
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the safety checker.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed to use for the generation.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=3.5, description='The guidance scale to use for the generation.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux2Flex

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux2Klein4B(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX-2 Klein 4B generates images with the efficient 4-billion parameter model for balanced quality and speed.
        image, generation, flux-2, klein, 4b, text-to-image

        Use cases:
        - Generate images with 4B model
        - Create balanced quality-speed content
        - Produce efficient visual artwork
        - Generate images with good performance
        - Create optimized visuals
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2Klein4B.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the image to generate.')
    output_format: nodetool.nodes.fal.text_to_image.Flux2Klein4B.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux2Klein4B.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description='If `True`, the media will be returned as a data URI. Output is not stored when this is True.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=4, description='The number of inference steps to perform.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed to use for the generation. If not provided, a random seed will be used.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux2Klein4B

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux2Klein4BBase(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX-2 Klein 4B Base provides foundation model generation with 4-billion parameters.
        image, generation, flux-2, klein, 4b, base

        Use cases:
        - Generate with base 4B model
        - Create foundation quality content
        - Produce standard visual artwork
        - Generate images with base model
        - Create baseline visuals
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2Klein4BBase.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2Klein4BBase.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the image to generate.')
    acceleration: nodetool.nodes.fal.text_to_image.Flux2Klein4BBase.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.Flux2Klein4BBase.Acceleration.REGULAR, description='The acceleration level to use for image generation.')
    output_format: nodetool.nodes.fal.text_to_image.Flux2Klein4BBase.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux2Klein4BBase.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description='If `True`, the media will be returned as a data URI. Output is not stored when this is True.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed to use for the generation. If not provided, a random seed will be used.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Negative prompt for classifier-free guidance. Describes what to avoid in the image.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=5, description='Guidance scale for classifier-free guidance.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux2Klein4BBase

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux2Klein4BBaseLora(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX-2 Klein 4B Base with LoRA enables custom-trained 4B models for specialized generation.
        image, generation, flux-2, klein, 4b, base, lora

        Use cases:
        - Generate with custom 4B base model
        - Create specialized foundation content
        - Produce domain-specific visuals
        - Generate with fine-tuned 4B model
        - Create customized baseline visuals
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2Klein4BBaseLora.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2Klein4BBaseLora.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the image to generate.')
    acceleration: nodetool.nodes.fal.text_to_image.Flux2Klein4BBaseLora.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.Flux2Klein4BBaseLora.Acceleration.REGULAR, description='The acceleration level to use for image generation.')
    output_format: nodetool.nodes.fal.text_to_image.Flux2Klein4BBaseLora.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux2Klein4BBaseLora.OutputFormat.PNG, description='The format of the generated image.')
    loras: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='List of LoRA weights to apply (maximum 3).')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description='If `True`, the media will be returned as a data URI. Output is not stored when this is True.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed to use for the generation. If not provided, a random seed will be used.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Negative prompt for classifier-free guidance. Describes what to avoid in the image.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=5, description='Guidance scale for classifier-free guidance.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux2Klein4BBaseLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux2Klein9B(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX-2 Klein 9B generates high-quality images with the powerful 9-billion parameter model.
        image, generation, flux-2, klein, 9b, text-to-image

        Use cases:
        - Generate high-quality images with 9B model
        - Create superior visual content
        - Produce detailed artwork
        - Generate images with powerful model
        - Create premium quality visuals
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2Klein9B.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the image to generate.')
    output_format: nodetool.nodes.fal.text_to_image.Flux2Klein9B.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux2Klein9B.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description='If `True`, the media will be returned as a data URI. Output is not stored when this is True.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=4, description='The number of inference steps to perform.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed to use for the generation. If not provided, a random seed will be used.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux2Klein9B

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux2Klein9BBase(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX-2 Klein 9B Base provides foundation generation with the full 9-billion parameter model.
        image, generation, flux-2, klein, 9b, base

        Use cases:
        - Generate with base 9B model
        - Create high-quality foundation content
        - Produce superior baseline artwork
        - Generate images with powerful base
        - Create premium baseline visuals
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2Klein9BBase.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2Klein9BBase.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the image to generate.')
    acceleration: nodetool.nodes.fal.text_to_image.Flux2Klein9BBase.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.Flux2Klein9BBase.Acceleration.REGULAR, description='The acceleration level to use for image generation.')
    output_format: nodetool.nodes.fal.text_to_image.Flux2Klein9BBase.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux2Klein9BBase.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description='If `True`, the media will be returned as a data URI. Output is not stored when this is True.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed to use for the generation. If not provided, a random seed will be used.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Negative prompt for classifier-free guidance. Describes what to avoid in the image.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=5, description='Guidance scale for classifier-free guidance.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux2Klein9BBase

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux2Klein9BBaseLora(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX-2 Klein 9B Base with LoRA combines powerful generation with custom-trained models.
        image, generation, flux-2, klein, 9b, base, lora

        Use cases:
        - Generate with custom 9B base model
        - Create specialized high-quality content
        - Produce custom superior visuals
        - Generate with fine-tuned 9B model
        - Create advanced customized visuals
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2Klein9BBaseLora.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2Klein9BBaseLora.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the image to generate.')
    acceleration: nodetool.nodes.fal.text_to_image.Flux2Klein9BBaseLora.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.Flux2Klein9BBaseLora.Acceleration.REGULAR, description='The acceleration level to use for image generation.')
    output_format: nodetool.nodes.fal.text_to_image.Flux2Klein9BBaseLora.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux2Klein9BBaseLora.OutputFormat.PNG, description='The format of the generated image.')
    loras: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='List of LoRA weights to apply (maximum 3).')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description='If `True`, the media will be returned as a data URI. Output is not stored when this is True.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed to use for the generation. If not provided, a random seed will be used.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Negative prompt for classifier-free guidance. Describes what to avoid in the image.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=5, description='Guidance scale for classifier-free guidance.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux2Klein9BBaseLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux2LoraGalleryBallpointPenSketch(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Flux 2 Lora Gallery
        flux, generation, text-to-image, txt2img, ai-art, lora

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2LoraGalleryBallpointPenSketch.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2LoraGalleryBallpointPenSketch.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description="The prompt to generate a ballpoint pen sketch style image. Use 'b4llp01nt' trigger word for best results.")
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    acceleration: nodetool.nodes.fal.text_to_image.Flux2LoraGalleryBallpointPenSketch.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.Flux2LoraGalleryBallpointPenSketch.Acceleration.REGULAR, description="Acceleration level for image generation. 'regular' balances speed and quality.")
    lora_scale: float | OutputHandle[float] = connect_field(default=1, description='The strength of the ballpoint pen sketch effect.')
    output_format: nodetool.nodes.fal.text_to_image.Flux2LoraGalleryBallpointPenSketch.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux2LoraGalleryBallpointPenSketch.OutputFormat.PNG, description='The format of the output image')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and won't be saved in history.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=2.5, description='The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.')
    seed: str | OutputHandle[str] = connect_field(default='', description='Random seed for reproducibility. Same seed with same prompt will produce same result.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the safety checker for the generated image.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=40, description='The number of inference steps to perform.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux2LoraGalleryBallpointPenSketch

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux2LoraGalleryDigitalComicArt(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Flux 2 Lora Gallery
        flux, generation, text-to-image, txt2img, ai-art, lora

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2LoraGalleryDigitalComicArt.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2LoraGalleryDigitalComicArt.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description="The prompt to generate a digital comic art style image. Use 'd1g1t4l' trigger word for best results.")
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    acceleration: nodetool.nodes.fal.text_to_image.Flux2LoraGalleryDigitalComicArt.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.Flux2LoraGalleryDigitalComicArt.Acceleration.REGULAR, description="Acceleration level for image generation. 'regular' balances speed and quality.")
    lora_scale: float | OutputHandle[float] = connect_field(default=1, description='The strength of the digital comic art effect.')
    output_format: nodetool.nodes.fal.text_to_image.Flux2LoraGalleryDigitalComicArt.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux2LoraGalleryDigitalComicArt.OutputFormat.PNG, description='The format of the output image')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and won't be saved in history.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=2.5, description='The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.')
    seed: str | OutputHandle[str] = connect_field(default='', description='Random seed for reproducibility. Same seed with same prompt will produce same result.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the safety checker for the generated image.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=40, description='The number of inference steps to perform.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux2LoraGalleryDigitalComicArt

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux2LoraGalleryHdrStyle(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Flux 2 Lora Gallery
        flux, generation, text-to-image, txt2img, ai-art, lora

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2LoraGalleryHdrStyle.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2LoraGalleryHdrStyle.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description="The prompt to generate an HDR style image. The trigger word 'Hyp3rRe4list1c' will be automatically prepended.")
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    acceleration: nodetool.nodes.fal.text_to_image.Flux2LoraGalleryHdrStyle.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.Flux2LoraGalleryHdrStyle.Acceleration.REGULAR, description="Acceleration level for image generation. 'regular' balances speed and quality.")
    lora_scale: float | OutputHandle[float] = connect_field(default=1, description='The strength of the HDR style effect.')
    output_format: nodetool.nodes.fal.text_to_image.Flux2LoraGalleryHdrStyle.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux2LoraGalleryHdrStyle.OutputFormat.PNG, description='The format of the output image')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and won't be saved in history.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=2.5, description='The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.')
    seed: str | OutputHandle[str] = connect_field(default='', description='Random seed for reproducibility. Same seed with same prompt will produce same result.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the safety checker for the generated image.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=40, description='The number of inference steps to perform.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux2LoraGalleryHdrStyle

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux2LoraGalleryRealism(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Flux 2 Lora Gallery
        flux, generation, text-to-image, txt2img, ai-art, lora

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2LoraGalleryRealism.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2LoraGalleryRealism.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate a realistic image with natural lighting and authentic details.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    acceleration: nodetool.nodes.fal.text_to_image.Flux2LoraGalleryRealism.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.Flux2LoraGalleryRealism.Acceleration.REGULAR, description="Acceleration level for image generation. 'regular' balances speed and quality.")
    lora_scale: float | OutputHandle[float] = connect_field(default=1, description='The strength of the realism effect.')
    output_format: nodetool.nodes.fal.text_to_image.Flux2LoraGalleryRealism.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux2LoraGalleryRealism.OutputFormat.PNG, description='The format of the output image')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and won't be saved in history.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=2.5, description='The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.')
    seed: str | OutputHandle[str] = connect_field(default='', description='Random seed for reproducibility. Same seed with same prompt will produce same result.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the safety checker for the generated image.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=40, description='The number of inference steps to perform.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux2LoraGalleryRealism

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux2LoraGallerySatelliteViewStyle(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Flux 2 Lora Gallery
        flux, generation, text-to-image, txt2img, ai-art, lora

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2LoraGallerySatelliteViewStyle.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2LoraGallerySatelliteViewStyle.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate a satellite/aerial view style image.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    acceleration: nodetool.nodes.fal.text_to_image.Flux2LoraGallerySatelliteViewStyle.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.Flux2LoraGallerySatelliteViewStyle.Acceleration.REGULAR, description="Acceleration level for image generation. 'regular' balances speed and quality.")
    lora_scale: float | OutputHandle[float] = connect_field(default=1, description='The strength of the satellite view style effect.')
    output_format: nodetool.nodes.fal.text_to_image.Flux2LoraGallerySatelliteViewStyle.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux2LoraGallerySatelliteViewStyle.OutputFormat.PNG, description='The format of the output image')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and won't be saved in history.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=2.5, description='The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.')
    seed: str | OutputHandle[str] = connect_field(default='', description='Random seed for reproducibility. Same seed with same prompt will produce same result.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the safety checker for the generated image.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=40, description='The number of inference steps to perform.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux2LoraGallerySatelliteViewStyle

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux2LoraGallerySepiaVintage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Flux 2 Lora Gallery
        flux, generation, text-to-image, txt2img, ai-art, lora

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2LoraGallerySepiaVintage.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2LoraGallerySepiaVintage.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate a sepia vintage photography style image.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    acceleration: nodetool.nodes.fal.text_to_image.Flux2LoraGallerySepiaVintage.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.Flux2LoraGallerySepiaVintage.Acceleration.REGULAR, description="Acceleration level for image generation. 'regular' balances speed and quality.")
    lora_scale: float | OutputHandle[float] = connect_field(default=1, description='The strength of the sepia vintage photography effect.')
    output_format: nodetool.nodes.fal.text_to_image.Flux2LoraGallerySepiaVintage.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux2LoraGallerySepiaVintage.OutputFormat.PNG, description='The format of the output image')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and won't be saved in history.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=2.5, description='The CFG (Classifier Free Guidance) scale. Controls how closely the model follows the prompt.')
    seed: str | OutputHandle[str] = connect_field(default='', description='Random seed for reproducibility. Same seed with same prompt will produce same result.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the safety checker for the generated image.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=40, description='The number of inference steps to perform.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux2LoraGallerySepiaVintage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux2Max(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX-2 Max generates maximum quality images with the most advanced FLUX-2 model for premium results.
        image, generation, flux-2, max, premium, text-to-image

        Use cases:
        - Generate maximum quality images
        - Create premium visual content
        - Produce professional-grade artwork
        - Generate images with best model
        - Create superior quality visuals
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2Max.OutputFormat
    SafetyTolerance: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2Max.SafetyTolerance

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    output_format: nodetool.nodes.fal.text_to_image.Flux2Max.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux2Max.OutputFormat.JPEG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    safety_tolerance: nodetool.nodes.fal.text_to_image.Flux2Max.SafetyTolerance = Field(default=nodetool.nodes.fal.text_to_image.Flux2Max.SafetyTolerance.VALUE_2, description='The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the safety checker.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed to use for the generation.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux2Max

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Flux2Turbo(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX.2 Turbo is a blazing-fast image generation model optimized for speed without sacrificing quality, ideal for real-time applications.
        image, generation, flux, fast, turbo, text-to-image, txt2img

        Use cases:
        - Real-time image generation for interactive apps
        - Rapid prototyping of visual concepts
        - Generate multiple variations instantly
        - Live visual effects and augmented reality
        - High-throughput batch image processing
    """

    ImageSizePreset: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ImageSizePreset
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Flux2Turbo.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate')
    image_size: nodetool.nodes.fal.text_to_image.ImageSizePreset = Field(default=nodetool.nodes.fal.text_to_image.ImageSizePreset.LANDSCAPE_4_3, description='Size preset for the generated image')
    output_format: nodetool.nodes.fal.text_to_image.Flux2Turbo.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Flux2Turbo.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=2.5, description='Guidance Scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Seed for reproducible results. Use -1 for random')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=False, description='If set to true, the prompt will be expanded for better results.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Flux2Turbo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class FluxDev(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX.1 [dev] is a powerful open-weight text-to-image model with 12 billion parameters. Optimized for prompt following and visual quality.
        image, generation, flux, text-to-image, txt2img

        Use cases:
        - Generate high-quality images from text prompts
        - Create detailed illustrations with precise control
        - Produce professional artwork and designs
        - Generate multiple variations from one prompt
        - Create safe-for-work content with built-in safety checker
    """

    ImageSizePreset: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ImageSizePreset
    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxDev.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxDev.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate')
    image_size: nodetool.nodes.fal.text_to_image.ImageSizePreset = Field(default=nodetool.nodes.fal.text_to_image.ImageSizePreset.LANDSCAPE_4_3, description='Size preset for the generated image')
    acceleration: nodetool.nodes.fal.text_to_image.FluxDev.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.FluxDev.Acceleration.NONE, description='The speed of the generation. The higher the speed, the faster the generation.')
    output_format: nodetool.nodes.fal.text_to_image.FluxDev.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.FluxDev.OutputFormat.JPEG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Enable safety checker to filter unsafe content')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Seed for reproducible results. Use -1 for random')
    guidance_scale: float | OutputHandle[float] = connect_field(default=3.5, description='How strictly to follow the prompt. Higher values are more literal')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='Number of denoising steps. More steps typically improve quality')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.FluxDev

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class FluxKontextLoraTextToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Flux Kontext Lora
        flux, generation, text-to-image, txt2img, ai-art, lora

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxKontextLoraTextToImage.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxKontextLoraTextToImage.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate the image with')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    acceleration: nodetool.nodes.fal.text_to_image.FluxKontextLoraTextToImage.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.FluxKontextLoraTextToImage.Acceleration.NONE, description='The speed of the generation. The higher the speed, the faster the generation.')
    output_format: nodetool.nodes.fal.text_to_image.FluxKontextLoraTextToImage.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.FluxKontextLoraTextToImage.OutputFormat.PNG, description='The format of the generated image.')
    loras: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=2.5, description='The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=30, description='The number of inference steps to perform.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same image every time.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.FluxKontextLoraTextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class FluxKrea(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX.1 Krea [dev]
        flux, generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxKrea.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxKrea.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    acceleration: nodetool.nodes.fal.text_to_image.FluxKrea.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.FluxKrea.Acceleration.NONE, description='The speed of the generation. The higher the speed, the faster the generation.')
    output_format: nodetool.nodes.fal.text_to_image.FluxKrea.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.FluxKrea.OutputFormat.JPEG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    seed: str | OutputHandle[str] = connect_field(default='', description='The same seed and the same prompt given to the same version of the model will output the same image every time.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=4.5, description='The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.FluxKrea

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class FluxKreaLora(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX.1 Krea [dev] with LoRAs
        flux, generation, text-to-image, txt2img, ai-art, lora

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxKreaLora.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate. This is always set to 1 for streaming output.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    output_format: nodetool.nodes.fal.text_to_image.FluxKreaLora.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.FluxKreaLora.OutputFormat.JPEG, description='The format of the generated image.')
    loras: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=3.5, description='The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same image every time.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.FluxKreaLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class FluxKreaLoraStream(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Flux Krea Lora
        flux, generation, text-to-image, txt2img, ai-art, lora

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxKreaLoraStream.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate. This is always set to 1 for streaming output.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    output_format: nodetool.nodes.fal.text_to_image.FluxKreaLoraStream.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.FluxKreaLoraStream.OutputFormat.JPEG, description='The format of the generated image.')
    loras: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=3.5, description='The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same image every time.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.FluxKreaLoraStream

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class FluxLora(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX with LoRA support enables fine-tuned image generation using custom LoRA models for specific styles or subjects.
        image, generation, flux, lora, fine-tuning, text-to-image, txt2img

        Use cases:
        - Generate images with custom artistic styles
        - Create consistent characters across images
        - Apply brand-specific visual styles
        - Generate images with specialized subjects
        - Combine multiple LoRA models for unique results
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxLora.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate. This is always set to 1 for streaming output.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='Size preset for the generated image')
    output_format: nodetool.nodes.fal.text_to_image.FluxLora.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.FluxLora.OutputFormat.JPEG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    loras: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='List of LoRA models to apply with their weights')
    guidance_scale: float | OutputHandle[float] = connect_field(default=3.5, description='How strictly to follow the prompt')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='Number of denoising steps')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Seed for reproducible results. Use -1 for random')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Enable safety checker to filter unsafe content')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.FluxLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class FluxProNew(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX.1 Pro New is the latest version of the professional FLUX model with enhanced capabilities and improved output quality.
        image, generation, flux, professional, text-to-image, txt2img

        Use cases:
        - Generate professional-grade marketing visuals
        - Create high-quality product renders
        - Produce detailed architectural visualizations
        - Design premium brand assets
        - Generate photorealistic commercial imagery
    """

    ImageSizePreset: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ImageSizePreset
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxProNew.OutputFormat
    SafetyTolerance: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxProNew.SafetyTolerance

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: nodetool.nodes.fal.text_to_image.ImageSizePreset = Field(default=nodetool.nodes.fal.text_to_image.ImageSizePreset.LANDSCAPE_4_3, description='Size preset for the generated image')
    output_format: nodetool.nodes.fal.text_to_image.FluxProNew.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.FluxProNew.OutputFormat.JPEG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    safety_tolerance: nodetool.nodes.fal.text_to_image.FluxProNew.SafetyTolerance = Field(default=nodetool.nodes.fal.text_to_image.FluxProNew.SafetyTolerance.VALUE_2, description='The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=3.5, description='The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Seed for reproducible results. Use -1 for random')
    enhance_prompt: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enhance the prompt for better results.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.FluxProNew

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class FluxSchnell(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX.1 [schnell] is a fast distilled version of FLUX.1 optimized for speed. Can generate high-quality images in 1-4 steps.
        image, generation, flux, fast, text-to-image, txt2img

        Use cases:
        - Generate images quickly for rapid iteration
        - Create concept art with minimal latency
        - Produce preview images before final generation
        - Generate multiple variations efficiently
        - Real-time image generation applications
    """

    ImageSizePreset: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ImageSizePreset
    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxSchnell.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxSchnell.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate')
    image_size: nodetool.nodes.fal.text_to_image.ImageSizePreset = Field(default=nodetool.nodes.fal.text_to_image.ImageSizePreset.LANDSCAPE_4_3, description='Size preset for the generated image')
    acceleration: nodetool.nodes.fal.text_to_image.FluxSchnell.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.FluxSchnell.Acceleration.NONE, description='The speed of the generation. The higher the speed, the faster the generation.')
    output_format: nodetool.nodes.fal.text_to_image.FluxSchnell.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.FluxSchnell.OutputFormat.JPEG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Enable safety checker to filter unsafe content')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Seed for reproducible results. Use -1 for random')
    guidance_scale: float | OutputHandle[float] = connect_field(default=3.5, description='The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=4, description='Number of denoising steps (1-4 recommended for schnell)')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.FluxSchnell

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class FluxSrpo(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX.1 SRPO [dev]
        flux, generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxSrpo.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxSrpo.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    acceleration: nodetool.nodes.fal.text_to_image.FluxSrpo.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.FluxSrpo.Acceleration.NONE, description='The speed of the generation. The higher the speed, the faster the generation.')
    output_format: nodetool.nodes.fal.text_to_image.FluxSrpo.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.FluxSrpo.OutputFormat.JPEG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    seed: str | OutputHandle[str] = connect_field(default='', description='The same seed and the same prompt given to the same version of the model will output the same image every time.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=4.5, description='The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.FluxSrpo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class FluxV1Pro(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX.1 Pro is a state-of-the-art image generation model with superior prompt following and image quality.
        image, generation, flux, pro, text-to-image, txt2img

        Use cases:
        - Generate professional-grade images for commercial use
        - Create highly detailed artwork with complex prompts
        - Produce marketing materials and brand assets
        - Generate photorealistic images
        - Create custom visual content with precise control
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxV1Pro.OutputFormat
    SafetyTolerance: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxV1Pro.SafetyTolerance

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='Size preset for the generated image')
    output_format: nodetool.nodes.fal.text_to_image.FluxV1Pro.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.FluxV1Pro.OutputFormat.JPEG, description='Output image format (jpeg or png)')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    safety_tolerance: nodetool.nodes.fal.text_to_image.FluxV1Pro.SafetyTolerance = Field(default=nodetool.nodes.fal.text_to_image.FluxV1Pro.SafetyTolerance.VALUE_2, description='Safety checker tolerance level (1-6). Higher is more permissive')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Enable safety checker to filter unsafe content')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Seed for reproducible results. Use -1 for random')
    enhance_prompt: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enhance the prompt for better results.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.FluxV1Pro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class FluxV1ProUltra(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        FLUX.1 Pro Ultra delivers the highest quality image generation with enhanced detail and realism.
        image, generation, flux, pro, ultra, text-to-image, txt2img

        Use cases:
        - Generate ultra-high quality photorealistic images
        - Create professional photography-grade visuals
        - Produce detailed product renders
        - Generate premium marketing materials
        - Create artistic masterpieces with fine details
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxV1ProUltra.OutputFormat
    SafetyTolerance: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.FluxV1ProUltra.SafetyTolerance

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate')
    aspect_ratio: str | OutputHandle[str] = connect_field(default='16:9', description='Aspect ratio for the generated image')
    enhance_prompt: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enhance the prompt for better results.')
    output_format: nodetool.nodes.fal.text_to_image.FluxV1ProUltra.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.FluxV1ProUltra.OutputFormat.JPEG, description='The format of the generated image.')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The image URL to generate an image from.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    safety_tolerance: nodetool.nodes.fal.text_to_image.FluxV1ProUltra.SafetyTolerance = Field(default=nodetool.nodes.fal.text_to_image.FluxV1ProUltra.SafetyTolerance.VALUE_2, description='The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive.')
    image_prompt_strength: float | OutputHandle[float] = connect_field(default=0.1, description='Strength of image prompt influence (0-1)')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Seed for reproducible results. Use -1 for random')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    raw: bool | OutputHandle[bool] = connect_field(default=False, description='Generate less processed, more natural results')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.FluxV1ProUltra

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Gemini25FlashImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Gemini 2.5 Flash Image
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Gemini25FlashImage.AspectRatio
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Gemini25FlashImage.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt to generate an image from.')
    aspect_ratio: nodetool.nodes.fal.text_to_image.Gemini25FlashImage.AspectRatio = Field(default=nodetool.nodes.fal.text_to_image.Gemini25FlashImage.AspectRatio.RATIO_1_1, description='The aspect ratio of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    output_format: nodetool.nodes.fal.text_to_image.Gemini25FlashImage.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Gemini25FlashImage.OutputFormat.PNG, description='The format of the generated image.')
    limit_generations: bool | OutputHandle[bool] = connect_field(default=False, description='Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Gemini25FlashImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Gemini3ProImagePreview(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Gemini 3 Pro Image Preview
        generation, text-to-image, txt2img, ai-art, professional

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Gemini3ProImagePreview.Resolution
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Gemini3ProImagePreview.OutputFormat
    SafetyTolerance: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Gemini3ProImagePreview.SafetyTolerance

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    enable_web_search: bool | OutputHandle[bool] = connect_field(default=False, description='Enable web search for the image generation task. This will allow the model to use the latest information from the web to generate the image.')
    aspect_ratio: str | OutputHandle[str] = connect_field(default='1:1', description='The aspect ratio of the generated image. Use "auto" to let the model decide based on the prompt.')
    resolution: nodetool.nodes.fal.text_to_image.Gemini3ProImagePreview.Resolution = Field(default=nodetool.nodes.fal.text_to_image.Gemini3ProImagePreview.Resolution.VALUE_1K, description='The resolution of the image to generate.')
    output_format: nodetool.nodes.fal.text_to_image.Gemini3ProImagePreview.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Gemini3ProImagePreview.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    safety_tolerance: nodetool.nodes.fal.text_to_image.Gemini3ProImagePreview.SafetyTolerance = Field(default=nodetool.nodes.fal.text_to_image.Gemini3ProImagePreview.SafetyTolerance.VALUE_4, description='The safety tolerance level for content moderation. 1 is the most strict (blocks most content), 6 is the least strict.')
    seed: str | OutputHandle[str] = connect_field(default='', description='The seed for the random number generator.')
    limit_generations: bool | OutputHandle[bool] = connect_field(default=False, description='Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Gemini3ProImagePreview

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class GlmImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        GLM Image generates images from text with advanced AI understanding and quality output.
        image, generation, glm, ai, text-to-image

        Use cases:
        - Generate images with GLM AI
        - Create intelligent visual content
        - Produce AI-powered artwork
        - Generate images with understanding
        - Create smart visuals from text
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.GlmImage.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='Text prompt for image generation.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='square_hd', description='Output image size.')
    output_format: nodetool.nodes.fal.text_to_image.GlmImage.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.GlmImage.OutputFormat.JPEG, description='Output image format.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description='If True, the image will be returned as a base64 data URI instead of a URL.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=1.5, description='Classifier-free guidance scale. Higher values make the model follow the prompt more closely.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducibility. The same seed with the same prompt will produce the same image.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=False, description='If True, the prompt will be enhanced using an LLM for more detailed and higher quality results.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=30, description='Number of diffusion denoising steps. More steps generally produce higher quality images.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Enable NSFW safety checking on the generated images.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.GlmImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class GptImage15(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        GPT Image 1.5 generates images from text with GPT-powered language understanding and visual creation.
        image, generation, gpt, language-ai, text-to-image

        Use cases:
        - Generate images with GPT understanding
        - Create language-aware visual content
        - Produce intelligent artwork
        - Generate images with natural language
        - Create GPT-powered visuals
    """

    ImageSize: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.GptImage15.ImageSize
    Background: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.GptImage15.Background
    Quality: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.GptImage15.Quality
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.GptImage15.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt for image generation')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate')
    image_size: nodetool.nodes.fal.text_to_image.GptImage15.ImageSize = Field(default=nodetool.nodes.fal.text_to_image.GptImage15.ImageSize.VALUE_1024X1024, description='Aspect ratio for the generated image')
    background: nodetool.nodes.fal.text_to_image.GptImage15.Background = Field(default=nodetool.nodes.fal.text_to_image.GptImage15.Background.AUTO, description='Background for the generated image')
    quality: nodetool.nodes.fal.text_to_image.GptImage15.Quality = Field(default=nodetool.nodes.fal.text_to_image.GptImage15.Quality.HIGH, description='Quality for the generated image')
    output_format: nodetool.nodes.fal.text_to_image.GptImage15.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.GptImage15.OutputFormat.PNG, description='Output format for the images')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.GptImage15

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class GptImage1Mini(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        GPT Image 1 Mini
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Background: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.GptImage1Mini.Background
    ImageSize: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.GptImage1Mini.ImageSize
    Quality: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.GptImage1Mini.Quality
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.GptImage1Mini.OutputFormat

    background: nodetool.nodes.fal.text_to_image.GptImage1Mini.Background = Field(default=nodetool.nodes.fal.text_to_image.GptImage1Mini.Background.AUTO, description='Background for the generated image')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate')
    image_size: nodetool.nodes.fal.text_to_image.GptImage1Mini.ImageSize = Field(default=nodetool.nodes.fal.text_to_image.GptImage1Mini.ImageSize.AUTO, description='Aspect ratio for the generated image')
    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt for image generation')
    quality: nodetool.nodes.fal.text_to_image.GptImage1Mini.Quality = Field(default=nodetool.nodes.fal.text_to_image.GptImage1Mini.Quality.AUTO, description='Quality for the generated image')
    output_format: nodetool.nodes.fal.text_to_image.GptImage1Mini.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.GptImage1Mini.OutputFormat.PNG, description='Output format for the images')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.GptImage1Mini

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class HunyuanImageV2_1TextToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Hunyuan Image
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.HunyuanImageV2_1TextToImage.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='square_hd', description='The desired size of the generated image.')
    use_reprompt: bool | OutputHandle[bool] = connect_field(default=True, description='Enable prompt enhancement for potentially better results.')
    use_refiner: bool | OutputHandle[bool] = connect_field(default=False, description='Enable the refiner model for improved image quality.')
    output_format: nodetool.nodes.fal.text_to_image.HunyuanImageV2_1TextToImage.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.HunyuanImageV2_1TextToImage.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=3.5, description='Controls how much the model adheres to the prompt. Higher values mean stricter adherence.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducible results. If None, a random seed is used.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='The negative prompt to guide the image generation away from certain concepts.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='Number of denoising steps.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.HunyuanImageV2_1TextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class HunyuanImageV3InstructTextToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Hunyuan Image v3 Instruct generates high-quality images from text with advanced instruction understanding.
        image, generation, hunyuan, v3, instruct, text-to-image

        Use cases:
        - Generate images with detailed instructions
        - Create artwork with precise text control
        - Produce high-quality visual content
        - Generate images with advanced understanding
        - Create professional visuals from text
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.HunyuanImageV3InstructTextToImage.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='auto', description='The desired size of the generated image. If auto, image size will be determined by the model.')
    output_format: nodetool.nodes.fal.text_to_image.HunyuanImageV3InstructTextToImage.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.HunyuanImageV3InstructTextToImage.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducible results. If None, a random seed is used.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=3.5, description='Controls how much the model adheres to the prompt. Higher values mean stricter adherence.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.HunyuanImageV3InstructTextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class HunyuanImageV3TextToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Hunyuan Image
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.HunyuanImageV3TextToImage.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt for image-to-image.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='square_hd', description='The desired size of the generated image.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.')
    output_format: nodetool.nodes.fal.text_to_image.HunyuanImageV3TextToImage.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.HunyuanImageV3TextToImage.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=7.5, description='Controls how much the model adheres to the prompt. Higher values mean stricter adherence.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducible results. If None, a random seed is used.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='The negative prompt to guide the image generation away from certain concepts.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='Number of denoising steps.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.HunyuanImageV3TextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class IdeogramV2(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Ideogram V2 is a state-of-the-art image generation model optimized for commercial and creative use, featuring exceptional typography handling and realistic outputs.
        image, generation, ai, typography, realistic, text-to-image, txt2img

        Use cases:
        - Create commercial artwork and designs
        - Generate realistic product visualizations
        - Design marketing materials with text
        - Produce high-quality illustrations
        - Create brand assets and logos
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.IdeogramV2.AspectRatio
    Style: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.IdeogramV2.Style

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from')
    aspect_ratio: nodetool.nodes.fal.text_to_image.IdeogramV2.AspectRatio = Field(default=nodetool.nodes.fal.text_to_image.IdeogramV2.AspectRatio.RATIO_1_1, description='The aspect ratio of the generated image')
    style: nodetool.nodes.fal.text_to_image.IdeogramV2.Style = Field(default=nodetool.nodes.fal.text_to_image.IdeogramV2.Style.AUTO, description='The style of the generated image')
    expand_prompt: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to expand the prompt with MagicPrompt functionality')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    seed: str | OutputHandle[str] = connect_field(default='', description='Seed for reproducible results. Use -1 for random')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='A negative prompt to avoid in the generated image')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.IdeogramV2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class IdeogramV2Turbo(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Ideogram V2 Turbo offers faster image generation with the same exceptional quality and typography handling as V2.
        image, generation, ai, typography, realistic, fast, text-to-image, txt2img

        Use cases:
        - Rapidly generate commercial designs
        - Quick iteration on marketing materials
        - Fast prototyping of visual concepts
        - Real-time design exploration
        - Efficient batch generation of branded content
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.IdeogramV2Turbo.AspectRatio
    Style: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.IdeogramV2Turbo.Style

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from')
    aspect_ratio: nodetool.nodes.fal.text_to_image.IdeogramV2Turbo.AspectRatio = Field(default=nodetool.nodes.fal.text_to_image.IdeogramV2Turbo.AspectRatio.RATIO_1_1, description='The aspect ratio of the generated image')
    style: nodetool.nodes.fal.text_to_image.IdeogramV2Turbo.Style = Field(default=nodetool.nodes.fal.text_to_image.IdeogramV2Turbo.Style.AUTO, description='The style of the generated image')
    expand_prompt: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to expand the prompt with MagicPrompt functionality')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    seed: str | OutputHandle[str] = connect_field(default='', description='Seed for reproducible results. Use -1 for random')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='A negative prompt to avoid in the generated image')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.IdeogramV2Turbo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class IdeogramV3(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Ideogram V3 is the latest generation with enhanced text rendering, superior image quality, and expanded creative controls.
        image, generation, ideogram, typography, text-rendering, text-to-image, txt2img

        Use cases:
        - Create professional graphics with embedded text
        - Design social media posts with perfect typography
        - Generate logos and brand identities
        - Produce marketing materials with text overlays
        - Create educational content with clear text
    """

    RenderingSpeed: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.IdeogramV3.RenderingSpeed

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='square_hd', description='The resolution of the generated image')
    style: str | OutputHandle[str] = connect_field(default='', description='The style preset for the generated image')
    style_preset: str | OutputHandle[str] = connect_field(default='', description='Style preset for generation. The chosen style preset will guide the generation.')
    expand_prompt: bool | OutputHandle[bool] = connect_field(default=True, description='Automatically enhance the prompt for better results')
    rendering_speed: nodetool.nodes.fal.text_to_image.IdeogramV3.RenderingSpeed = Field(default=nodetool.nodes.fal.text_to_image.IdeogramV3.RenderingSpeed.BALANCED, description='The rendering speed to use.')
    style_codes: str | OutputHandle[str] = connect_field(default='', description='A list of 8 character hexadecimal codes representing the style of the image. Cannot be used in conjunction with style_reference_images or style')
    color_palette: str | OutputHandle[str] = connect_field(default='', description='A color palette for generation, must EITHER be specified via one of the presets (name) or explicitly via hexadecimal representations of the color with optional weights (members)')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    seed: str | OutputHandle[str] = connect_field(default='', description='Seed for the random number generator')
    image_urls: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='A set of images to use as style references (maximum total size 10MB across all style references). The images should be in JPEG, PNG or WebP format')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Description of what to exclude from an image. Descriptions in the prompt take precedence to descriptions in the negative prompt.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.IdeogramV3

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class ImagineartImagineart1_5PreviewTextToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Imagineart 1.5 Preview
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ImagineartImagineart1_5PreviewTextToImage.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(default='', description='Text prompt describing the desired image')
    aspect_ratio: nodetool.nodes.fal.text_to_image.ImagineartImagineart1_5PreviewTextToImage.AspectRatio = Field(default=nodetool.nodes.fal.text_to_image.ImagineartImagineart1_5PreviewTextToImage.AspectRatio.RATIO_1_1, description='Image aspect ratio: 1:1, 3:1, 1:3, 16:9, 9:16, 4:3, 3:4, 3:2, 2:3')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Seed for the image generation')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.ImagineartImagineart1_5PreviewTextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class ImagineartImagineart1_5ProPreviewTextToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        ImagineArt 1.5 Pro Preview
        generation, text-to-image, txt2img, ai-art, professional

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ImagineartImagineart1_5ProPreviewTextToImage.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(default='', description='Text prompt describing the desired image')
    aspect_ratio: nodetool.nodes.fal.text_to_image.ImagineartImagineart1_5ProPreviewTextToImage.AspectRatio = Field(default=nodetool.nodes.fal.text_to_image.ImagineartImagineart1_5ProPreviewTextToImage.AspectRatio.RATIO_1_1, description='Image aspect ratio: 1:1, 3:1, 1:3, 16:9, 9:16, 4:3, 3:4, 3:2, 2:3')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Seed for the image generation')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.ImagineartImagineart1_5ProPreviewTextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class LongcatImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Longcat Image generates creative and unique images from text with distinctive AI characteristics.
        image, generation, longcat, creative, text-to-image

        Use cases:
        - Generate creative images
        - Create unique visual content
        - Produce distinctive artwork
        - Generate images with character
        - Create artistic visuals
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.LongcatImage.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.LongcatImage.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    acceleration: nodetool.nodes.fal.text_to_image.LongcatImage.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.LongcatImage.Acceleration.REGULAR, description='The acceleration level to use.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    output_format: nodetool.nodes.fal.text_to_image.LongcatImage.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.LongcatImage.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=4.5, description='The guidance scale to use for the image generation.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same image every time.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.LongcatImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class NanoBanana(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Nano Banana
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.NanoBanana.AspectRatio
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.NanoBanana.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt to generate an image from.')
    aspect_ratio: nodetool.nodes.fal.text_to_image.NanoBanana.AspectRatio = Field(default=nodetool.nodes.fal.text_to_image.NanoBanana.AspectRatio.RATIO_1_1, description='The aspect ratio of the generated image.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    output_format: nodetool.nodes.fal.text_to_image.NanoBanana.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.NanoBanana.OutputFormat.PNG, description='The format of the generated image.')
    limit_generations: bool | OutputHandle[bool] = connect_field(default=False, description='Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.NanoBanana

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class NanoBananaPro(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Nano Banana Pro
        generation, text-to-image, txt2img, ai-art, professional

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.NanoBananaPro.Resolution
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.NanoBananaPro.OutputFormat
    SafetyTolerance: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.NanoBananaPro.SafetyTolerance

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    enable_web_search: bool | OutputHandle[bool] = connect_field(default=False, description='Enable web search for the image generation task. This will allow the model to use the latest information from the web to generate the image.')
    aspect_ratio: str | OutputHandle[str] = connect_field(default='1:1', description='The aspect ratio of the generated image. Use "auto" to let the model decide based on the prompt.')
    resolution: nodetool.nodes.fal.text_to_image.NanoBananaPro.Resolution = Field(default=nodetool.nodes.fal.text_to_image.NanoBananaPro.Resolution.VALUE_1K, description='The resolution of the image to generate.')
    output_format: nodetool.nodes.fal.text_to_image.NanoBananaPro.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.NanoBananaPro.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    safety_tolerance: nodetool.nodes.fal.text_to_image.NanoBananaPro.SafetyTolerance = Field(default=nodetool.nodes.fal.text_to_image.NanoBananaPro.SafetyTolerance.VALUE_4, description='The safety tolerance level for content moderation. 1 is the most strict (blocks most content), 6 is the least strict.')
    seed: str | OutputHandle[str] = connect_field(default='', description='The seed for the random number generator.')
    limit_generations: bool | OutputHandle[bool] = connect_field(default=False, description='Experimental parameter to limit the number of generations from each round of prompting to 1. Set to `True` to to disregard any instructions in the prompt regarding the number of images to generate.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.NanoBananaPro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class OmniGenV1(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        OmniGen V1 is a versatile unified model for multi-modal image generation and editing with text, supporting complex compositional tasks.
        image, generation, multi-modal, editing, unified, text-to-image, txt2img

        Use cases:
        - Generate images with multiple input modalities
        - Edit existing images with text instructions
        - Create complex compositional scenes
        - Combine text and image inputs for generation
        - Perform advanced image manipulations
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.OmniGenV1.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate or edit an image')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='square_hd', description='The size of the generated image.')
    img_guidance_scale: float | OutputHandle[float] = connect_field(default=1.6, description='The Image Guidance scale is a measure of how close you want the model to stick to your input image when looking for a related image to show you.')
    input_image_urls: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='URL of images to use while generating the image, Use <img><|image_1|></img> for the first image and so on.')
    output_format: nodetool.nodes.fal.text_to_image.OmniGenV1.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.OmniGenV1.OutputFormat.JPEG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description='If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=3, description='How strictly to follow the prompt and inputs')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=50, description='Number of denoising steps for generation quality')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Seed for reproducible results. Use -1 for random')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.OmniGenV1

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class OmnigenV2(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Omnigen V2
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Scheduler: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.OmnigenV2.Scheduler
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.OmnigenV2.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description="The prompt to generate or edit an image. Use specific language like 'Add the bird from image 1 to the desk in image 2' for better results.")
    image_size: str | OutputHandle[str] = connect_field(default='square_hd', description='The size of the generated image.')
    scheduler: nodetool.nodes.fal.text_to_image.OmnigenV2.Scheduler = Field(default=nodetool.nodes.fal.text_to_image.OmnigenV2.Scheduler.EULER, description='The scheduler to use for the diffusion process.')
    cfg_range_end: float | OutputHandle[float] = connect_field(default=1, description='CFG range end value.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='(((deformed))), blurry, over saturation, bad anatomy, disfigured, poorly drawn face, mutation, mutated, (extra_limb), (ugly), (poorly drawn hands), fused fingers, messy drawing, broken legs censor, censored, censor_bar', description='Negative prompt to guide what should not be in the image.')
    text_guidance_scale: float | OutputHandle[float] = connect_field(default=5, description='The Text Guidance scale controls how closely the model follows the text prompt. Higher values make the model stick more closely to the prompt.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_guidance_scale: float | OutputHandle[float] = connect_field(default=2, description='The Image Guidance scale controls how closely the model follows the input images. For image editing: 1.3-2.0, for in-context generation: 2.0-3.0')
    input_image_urls: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='URLs of input images to use for image editing or multi-image generation. Support up to 3 images.')
    output_format: nodetool.nodes.fal.text_to_image.OmnigenV2.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.OmnigenV2.OutputFormat.JPEG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    cfg_range_start: float | OutputHandle[float] = connect_field(default=0, description='CFG range start value.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=50, description='The number of inference steps to perform.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same image every time.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.OmnigenV2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class OvisImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Ovis Image
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.OvisImage.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.OvisImage.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    acceleration: nodetool.nodes.fal.text_to_image.OvisImage.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.OvisImage.Acceleration.REGULAR, description='The acceleration level to use.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    output_format: nodetool.nodes.fal.text_to_image.OvisImage.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.OvisImage.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same image every time.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=5, description='The guidance scale to use for the image generation.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='The negative prompt to generate an image from.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.OvisImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Piflow(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Piflow
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Piflow.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='square_hd', description='The size of the generated image. You can choose between some presets or custom height and width that **must be multiples of 8**.')
    output_format: nodetool.nodes.fal.text_to_image.Piflow.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Piflow.OutputFormat.JPEG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=8, description='The number of inference steps to perform.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducible generation. If set to None, a random seed will be used.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Piflow

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class QwenImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Qwen Image
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.QwenImage.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.QwenImage.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate the image with')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    acceleration: nodetool.nodes.fal.text_to_image.QwenImage.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.QwenImage.Acceleration.NONE, description="Acceleration level for image generation. Options: 'none', 'regular', 'high'. Higher acceleration increases speed. 'regular' balances speed and quality. 'high' is recommended for images without text.")
    num_inference_steps: int | OutputHandle[int] = connect_field(default=30, description='The number of inference steps to perform.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    output_format: nodetool.nodes.fal.text_to_image.QwenImage.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.QwenImage.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    loras: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same image every time.')
    use_turbo: bool | OutputHandle[bool] = connect_field(default=False, description='Enable turbo mode for faster generation with high quality. When enabled, uses optimized settings (10 steps, CFG=1.2).')
    negative_prompt: str | OutputHandle[str] = connect_field(default=' ', description='The negative prompt for the generation')
    guidance_scale: float | OutputHandle[float] = connect_field(default=2.5, description='The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.QwenImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class QwenImage2512(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Qwen Image 2512 generates high-resolution images from text with excellent quality and detail.
        image, generation, qwen, 2512, high-resolution, text-to-image

        Use cases:
        - Generate high-resolution images
        - Create detailed visual content
        - Produce quality artwork from text
        - Generate images with fine details
        - Create high-quality visuals
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.QwenImage2512.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.QwenImage2512.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    acceleration: nodetool.nodes.fal.text_to_image.QwenImage2512.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.QwenImage2512.Acceleration.REGULAR, description='The acceleration level to use.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    output_format: nodetool.nodes.fal.text_to_image.QwenImage2512.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.QwenImage2512.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same image every time.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=4, description='The guidance scale to use for the image generation.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='The negative prompt to generate an image from.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.QwenImage2512

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class QwenImage2512Lora(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Qwen Image 2512 with LoRA support enables custom-trained models for specialized image generation.
        image, generation, qwen, 2512, lora, custom

        Use cases:
        - Generate images with custom models
        - Create specialized visual content
        - Produce domain-specific artwork
        - Generate images with fine-tuned models
        - Create customized visuals
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.QwenImage2512Lora.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.QwenImage2512Lora.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    acceleration: nodetool.nodes.fal.text_to_image.QwenImage2512Lora.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.QwenImage2512Lora.Acceleration.REGULAR, description='The acceleration level to use.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    output_format: nodetool.nodes.fal.text_to_image.QwenImage2512Lora.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.QwenImage2512Lora.OutputFormat.PNG, description='The format of the generated image.')
    loras: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The LoRAs to use for the image generation. You can use up to 3 LoRAs and they will be merged together to generate the final image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same image every time.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=4, description='The guidance scale to use for the image generation.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='The negative prompt to generate an image from.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.QwenImage2512Lora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class QwenImageMaxTextToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Qwen Image Max generates premium quality images from text with superior detail and accuracy.
        image, generation, qwen, max, premium, text-to-image

        Use cases:
        - Generate premium quality images
        - Create detailed artwork from text
        - Produce high-fidelity visual content
        - Generate professional-grade images
        - Create superior quality visuals
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.QwenImageMaxTextToImage.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='Text prompt describing the desired image. Supports Chinese and English. Max 800 characters.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='square_hd', description='The size of the generated image.')
    output_format: nodetool.nodes.fal.text_to_image.QwenImageMaxTextToImage.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.QwenImageMaxTextToImage.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=True, description='Enable LLM prompt optimization for better results.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducibility (0-2147483647).')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Enable content moderation for input and output.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Content to avoid in the generated image. Max 500 characters.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.QwenImageMaxTextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class RecraftV3(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Recraft V3 is a powerful image generation model with exceptional control over style and colors, ideal for brand consistency and design work.
        image, generation, design, branding, style, text-to-image, txt2img

        Use cases:
        - Create brand-consistent visual assets
        - Generate designs with specific color palettes
        - Produce stylized illustrations and artwork
        - Design marketing materials with brand colors
        - Create cohesive visual content series
    """

    RecraftV3Style: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.RecraftV3.RecraftV3Style

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from')
    image_size: str | OutputHandle[str] = connect_field(default='square_hd', description='Size preset for the generated image')
    style: nodetool.nodes.fal.text_to_image.RecraftV3.RecraftV3Style = Field(default=nodetool.nodes.fal.text_to_image.RecraftV3.RecraftV3Style.REALISTIC_IMAGE, description='Visual style preset for the generated image')
    colors: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='Specific color palette to use in the generation')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=False, description='If set to true, the safety checker will be enabled.')
    style_id: str | OutputHandle[str] = connect_field(default='', description='Custom style ID for brand-specific styles')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.RecraftV3

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class ReveTextToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Reve
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ReveTextToImage.AspectRatio
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ReveTextToImage.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text description of the desired image.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate')
    aspect_ratio: nodetool.nodes.fal.text_to_image.ReveTextToImage.AspectRatio = Field(default=nodetool.nodes.fal.text_to_image.ReveTextToImage.AspectRatio.RATIO_3_2, description='The desired aspect ratio of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    output_format: nodetool.nodes.fal.text_to_image.ReveTextToImage.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.ReveTextToImage.OutputFormat.PNG, description='Output format for the generated image.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.ReveTextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Sana(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Sana is an efficient high-resolution image generation model that balances quality and speed for practical applications.
        image, generation, efficient, high-resolution, text-to-image, txt2img

        Use cases:
        - Generate high-resolution images efficiently
        - Create detailed artwork with good performance
        - Produce quality visuals with limited compute
        - Generate images for web and mobile applications
        - Balanced quality-speed image production
    """

    ImageSizePreset: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ImageSizePreset
    StyleName: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Sana.StyleName
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.Sana.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: nodetool.nodes.fal.text_to_image.ImageSizePreset = Field(default=nodetool.nodes.fal.text_to_image.ImageSizePreset.LANDSCAPE_4_3, description='Size preset for the generated image')
    style_name: nodetool.nodes.fal.text_to_image.Sana.StyleName = Field(default=nodetool.nodes.fal.text_to_image.Sana.StyleName.NO_STYLE, description='The style to generate the image in.')
    output_format: nodetool.nodes.fal.text_to_image.Sana.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.Sana.OutputFormat.JPEG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=5, description='How strictly to follow the prompt')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=18, description='Number of denoising steps')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same image every time.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Elements to avoid in the generated image')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Sana

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class SkyRaccoon(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Sky Raccoon
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt to guide video generation.')
    image_size: str | OutputHandle[str] = connect_field(default='', description='The size of the generated image.')
    turbo_mode: bool | OutputHandle[bool] = connect_field(default=False, description='If true, the video will be generated faster with no noticeable degradation in the visual quality.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enable prompt expansion.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducibility. If None, a random seed is chosen.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=False, description='If set to true, the safety checker will be enabled.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards', description='Negative prompt for video generation.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=30, description='Number of inference steps for sampling. Higher values give better quality but take longer.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.SkyRaccoon

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class StableDiffusionV35Large(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Stable Diffusion 3.5 Large is a powerful open-weight model with excellent prompt adherence and diverse output capabilities.
        image, generation, stable-diffusion, open-source, text-to-image, txt2img

        Use cases:
        - Generate diverse artistic styles
        - Create high-quality illustrations
        - Produce photorealistic images
        - Generate concept art and designs
        - Create custom visual content
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.StableDiffusionV35Large.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='', description='The size of the generated image. Defaults to landscape_4_3 if no controlnet has been passed, otherwise defaults to the size of the controlnet conditioning image.')
    controlnet: str | OutputHandle[str] = connect_field(default='', description='ControlNet for inference.')
    output_format: nodetool.nodes.fal.text_to_image.StableDiffusionV35Large.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.StableDiffusionV35Large.OutputFormat.JPEG, description='The format of the generated image.')
    ip_adapter: str | OutputHandle[str] = connect_field(default='', description='IP-Adapter to use during inference.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    loras: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The LoRAs to use for the image generation. You can use any number of LoRAs and they will be merged together to generate the final image.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=3.5, description='The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Elements to avoid in the generated image')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same image every time.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.StableDiffusionV35Large

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class ViduQ2TextToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Vidu Q2 generates quality images from text with optimized performance and consistent results.
        image, generation, vidu, q2, optimized, text-to-image

        Use cases:
        - Generate optimized quality images
        - Create consistent visual content
        - Produce balanced artwork
        - Generate images efficiently
        - Create reliable visuals
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ViduQ2TextToImage.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(default='', description='Text prompt for video generation, max 1500 characters')
    aspect_ratio: nodetool.nodes.fal.text_to_image.ViduQ2TextToImage.AspectRatio = Field(default=nodetool.nodes.fal.text_to_image.ViduQ2TextToImage.AspectRatio.RATIO_16_9, description='The aspect ratio of the output video')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for generation')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.ViduQ2TextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class Wan25PreviewTextToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Wan 2.5 Text to Image
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt for image generation. Supports Chinese and English, max 2000 characters.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='Number of images to generate. Values from 1 to 4.')
    image_size: str | OutputHandle[str] = connect_field(default='square', description="The size of the generated image. Can use preset names like 'square', 'landscape_16_9', etc., or specific dimensions. Total pixels must be between 768768 and 14401440, with aspect ratio between [1:4, 4:1].")
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable prompt rewriting using LLM. Improves results for short prompts but increases processing time.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducibility. If None, a random seed is chosen.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Negative prompt to describe content to avoid. Max 500 characters.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.Wan25PreviewTextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class WanV26TextToImage(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Wan v2.6 generates high-quality images from text with advanced capabilities and consistent results.
        image, generation, wan, v2.6, quality, text-to-image

        Use cases:
        - Generate quality images with Wan v2.6
        - Create consistent visual content
        - Produce reliable artwork from text
        - Generate images with advanced model
        - Create high-quality visuals
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='Text prompt describing the desired image. Supports Chinese and English. Max 2000 characters.')
    image_size: str | OutputHandle[str] = connect_field(default='', description="Output image size. If not set: matches input image size (up to 1280*1280). Use presets like 'square_hd', 'landscape_16_9', or specify exact dimensions.")
    max_images: int | OutputHandle[int] = connect_field(default=1, description='Maximum number of images to generate (1-5). Actual count may be less depending on model inference.')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Optional reference image (0 or 1). When provided, can be used for style guidance. Resolution: 384-5000px each dimension. Max size: 10MB. Formats: JPEG, JPG, PNG (no alpha), BMP, WEBP.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Enable content moderation for input and output.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducibility (0-2147483647).')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Content to avoid in the generated image. Max 500 characters.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.WanV26TextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class WanV2_25BTextToImage(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Wan
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    ImageFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.WanV2_25BTextToImage.ImageFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt to guide image generation.')
    image_format: nodetool.nodes.fal.text_to_image.WanV2_25BTextToImage.ImageFormat = Field(default=nodetool.nodes.fal.text_to_image.WanV2_25BTextToImage.ImageFormat.JPEG, description='The format of the output image.')
    image_size: str | OutputHandle[str] = connect_field(default='square_hd', description='The size of the generated image.')
    shift: float | OutputHandle[float] = connect_field(default=2, description='Shift value for the image. Must be between 1.0 and 10.0.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducibility. If None, a random seed is chosen.')
    enable_output_safety_checker: bool | OutputHandle[bool] = connect_field(default=False, description='If set to true, output video will be checked for safety after generation.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=3.5, description='Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=40, description='Number of inference steps for sampling. Higher values give better quality but take longer.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=False, description='If set to true, input data will be checked for safety before processing.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Negative prompt for video generation.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.WanV2_25BTextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class WanV2_2A14BTextToImage(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Wan
        generation, text-to-image, txt2img, ai-art

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.WanV2_2A14BTextToImage.Acceleration

    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt to guide image generation.')
    shift: float | OutputHandle[float] = connect_field(default=2, description='Shift value for the image. Must be between 1.0 and 10.0.')
    image_size: str | OutputHandle[str] = connect_field(default='square_hd', description='The size of the generated image.')
    acceleration: nodetool.nodes.fal.text_to_image.WanV2_2A14BTextToImage.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.WanV2_2A14BTextToImage.Acceleration.REGULAR, description="Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.")
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducibility. If None, a random seed is chosen.')
    enable_output_safety_checker: bool | OutputHandle[bool] = connect_field(default=False, description='If set to true, output video will be checked for safety after generation.')
    guidance_scale_2: float | OutputHandle[float] = connect_field(default=4, description='Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=3.5, description='Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=27, description='Number of inference steps for sampling. Higher values give better quality but take longer.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=False, description='If set to true, input data will be checked for safety before processing.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Negative prompt for video generation.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.WanV2_2A14BTextToImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class WanV2_2A14BTextToImageLora(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Wan v2.2 A14B Text-to-Image A14B with LoRAs
        generation, text-to-image, txt2img, ai-art, lora

        Use cases:
        - AI-powered art generation
        - Marketing and advertising visuals
        - Concept art and ideation
        - Social media content creation
        - Rapid prototyping and mockups
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.WanV2_2A14BTextToImageLora.Acceleration
    ImageFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.WanV2_2A14BTextToImageLora.ImageFormat

    shift: float | OutputHandle[float] = connect_field(default=2, description='Shift value for the image. Must be between 1.0 and 10.0.')
    prompt: str | OutputHandle[str] = connect_field(default='', description='The text prompt to guide image generation.')
    image_size: str | OutputHandle[str] = connect_field(default='square_hd', description='The size of the generated image.')
    acceleration: nodetool.nodes.fal.text_to_image.WanV2_2A14BTextToImageLora.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.WanV2_2A14BTextToImageLora.Acceleration.REGULAR, description="Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.")
    reverse_video: bool | OutputHandle[bool] = connect_field(default=False, description='If true, the video will be reversed.')
    loras: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='LoRA weights to be used in the inference.')
    guidance_scale: float | OutputHandle[float] = connect_field(default=3.5, description='Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=False, description='If set to true, input data will be checked for safety before processing.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='Negative prompt for video generation.')
    image_format: nodetool.nodes.fal.text_to_image.WanV2_2A14BTextToImageLora.ImageFormat = Field(default=nodetool.nodes.fal.text_to_image.WanV2_2A14BTextToImageLora.ImageFormat.JPEG, description='The format of the output image.')
    enable_output_safety_checker: bool | OutputHandle[bool] = connect_field(default=False, description='If set to true, output video will be checked for safety after generation.')
    guidance_scale_2: float | OutputHandle[float] = connect_field(default=4, description='Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducibility. If None, a random seed is chosen.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=27, description='Number of inference steps for sampling. Higher values give better quality but take longer.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.WanV2_2A14BTextToImageLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class ZImageBase(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Z-Image Base generates quality images from text with efficient processing and good results.
        image, generation, z-image, base, efficient, text-to-image

        Use cases:
        - Generate images efficiently
        - Create quality artwork from text
        - Produce visual content quickly
        - Generate images with good performance
        - Create efficient visuals
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ZImageBase.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ZImageBase.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    acceleration: nodetool.nodes.fal.text_to_image.ZImageBase.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.ZImageBase.Acceleration.REGULAR, description='The acceleration level to use.')
    output_format: nodetool.nodes.fal.text_to_image.ZImageBase.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.ZImageBase.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=4, description='The guidance scale to use for the image generation.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same image every time.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='The negative prompt to use for the image generation.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.ZImageBase

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class ZImageBaseLora(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Z-Image Base with LoRA enables efficient custom-trained models for specialized generation tasks.
        image, generation, z-image, base, lora, custom

        Use cases:
        - Generate images with custom efficient models
        - Create specialized content quickly
        - Produce domain-specific visuals
        - Generate with fine-tuned base model
        - Create efficient custom visuals
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ZImageBaseLora.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ZImageBaseLora.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    acceleration: nodetool.nodes.fal.text_to_image.ZImageBaseLora.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.ZImageBaseLora.Acceleration.REGULAR, description='The acceleration level to use.')
    output_format: nodetool.nodes.fal.text_to_image.ZImageBaseLora.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.ZImageBaseLora.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    loras: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='List of LoRA weights to apply (maximum 3).')
    guidance_scale: float | OutputHandle[float] = connect_field(default=4, description='The guidance scale to use for the image generation.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same image every time.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='', description='The negative prompt to use for the image generation.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=28, description='The number of inference steps to perform.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.ZImageBaseLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class ZImageTurbo(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Z-Image Turbo generates images from text with maximum speed for rapid iteration and prototyping.
        image, generation, z-image, turbo, fast, text-to-image

        Use cases:
        - Generate images at maximum speed
        - Create rapid prototypes from text
        - Produce quick visual iterations
        - Generate images for fast workflows
        - Create instant visual content
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ZImageTurbo.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ZImageTurbo.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    acceleration: nodetool.nodes.fal.text_to_image.ZImageTurbo.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.ZImageTurbo.Acceleration.REGULAR, description='The acceleration level to use.')
    output_format: nodetool.nodes.fal.text_to_image.ZImageTurbo.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.ZImageTurbo.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same image every time.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=8, description='The number of inference steps to perform.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.ZImageTurbo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.text_to_image
from nodetool.workflows.base_node import BaseNode

class ZImageTurboLora(SingleOutputGraphNode[types.ImageRef], GraphNode[types.ImageRef]):
    """

        Z-Image Turbo with LoRA combines maximum speed with custom models for fast specialized generation.
        image, generation, z-image, turbo, lora, fast

        Use cases:
        - Generate custom images at turbo speed
        - Create specialized content rapidly
        - Produce quick domain-specific visuals
        - Generate with fast fine-tuned models
        - Create instant custom visuals
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ZImageTurboLora.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.text_to_image.ZImageTurboLora.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate an image from.')
    num_images: int | OutputHandle[int] = connect_field(default=1, description='The number of images to generate.')
    image_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description='The size of the generated image.')
    acceleration: nodetool.nodes.fal.text_to_image.ZImageTurboLora.Acceleration = Field(default=nodetool.nodes.fal.text_to_image.ZImageTurboLora.Acceleration.REGULAR, description='The acceleration level to use.')
    output_format: nodetool.nodes.fal.text_to_image.ZImageTurboLora.OutputFormat = Field(default=nodetool.nodes.fal.text_to_image.ZImageTurboLora.OutputFormat.PNG, description='The format of the generated image.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    loras: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='List of LoRA weights to apply (maximum 3).')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to enable prompt expansion. Note: this will increase the price by 0.0025 credits per request.')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The same seed and the same prompt given to the same version of the model will output the same image every time.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='If set to true, the safety checker will be enabled.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=8, description='The number of inference steps to perform.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.text_to_image.ZImageTurboLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


