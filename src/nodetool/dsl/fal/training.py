# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class Flux2Klein4BBaseTrainer(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Flux 2 Klein 4B Base Trainer
        flux, training, fine-tuning, lora, model-training

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    OutputLoraFormat: typing.ClassVar[type] = nodetool.nodes.fal.training.Flux2Klein4BBaseTrainer.OutputLoraFormat

    steps: int | OutputHandle[int] = connect_field(default=1000, description='Total number of training steps.')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better. The zip can also contain a text file for each image. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail.')
    learning_rate: float | OutputHandle[float] = connect_field(default=5e-05, description='Learning rate applied to trainable parameters.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')
    output_lora_format: nodetool.nodes.fal.training.Flux2Klein4BBaseTrainer.OutputLoraFormat = Field(default=nodetool.nodes.fal.training.Flux2Klein4BBaseTrainer.OutputLoraFormat.FAL, description='Dictates the naming scheme for the output weights')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.Flux2Klein4BBaseTrainer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class Flux2Klein4BBaseTrainerEdit(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Flux 2 Klein 4B Base Trainer
        flux, training, fine-tuning, lora, model-training

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    OutputLoraFormat: typing.ClassVar[type] = nodetool.nodes.fal.training.Flux2Klein4BBaseTrainerEdit.OutputLoraFormat

    steps: int | OutputHandle[int] = connect_field(default=1000, description='Total number of training steps.')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain up to four reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail.')
    learning_rate: float | OutputHandle[float] = connect_field(default=5e-05, description='Learning rate applied to trainable parameters.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')
    output_lora_format: nodetool.nodes.fal.training.Flux2Klein4BBaseTrainerEdit.OutputLoraFormat = Field(default=nodetool.nodes.fal.training.Flux2Klein4BBaseTrainerEdit.OutputLoraFormat.FAL, description='Dictates the naming scheme for the output weights')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.Flux2Klein4BBaseTrainerEdit

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class Flux2Klein9BBaseTrainer(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Flux 2 Klein 9B Base Trainer
        flux, training, fine-tuning, lora, model-training

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    OutputLoraFormat: typing.ClassVar[type] = nodetool.nodes.fal.training.Flux2Klein9BBaseTrainer.OutputLoraFormat

    steps: int | OutputHandle[int] = connect_field(default=1000, description='Total number of training steps.')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better. The zip can also contain a text file for each image. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail.')
    learning_rate: float | OutputHandle[float] = connect_field(default=5e-05, description='Learning rate applied to trainable parameters.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')
    output_lora_format: nodetool.nodes.fal.training.Flux2Klein9BBaseTrainer.OutputLoraFormat = Field(default=nodetool.nodes.fal.training.Flux2Klein9BBaseTrainer.OutputLoraFormat.FAL, description='Dictates the naming scheme for the output weights')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.Flux2Klein9BBaseTrainer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class Flux2Klein9BBaseTrainerEdit(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Flux 2 Klein 9B Base Trainer
        flux, training, fine-tuning, lora, model-training

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    OutputLoraFormat: typing.ClassVar[type] = nodetool.nodes.fal.training.Flux2Klein9BBaseTrainerEdit.OutputLoraFormat

    steps: int | OutputHandle[int] = connect_field(default=1000, description='Total number of training steps.')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain up to four reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail.')
    learning_rate: float | OutputHandle[float] = connect_field(default=5e-05, description='Learning rate applied to trainable parameters.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')
    output_lora_format: nodetool.nodes.fal.training.Flux2Klein9BBaseTrainerEdit.OutputLoraFormat = Field(default=nodetool.nodes.fal.training.Flux2Klein9BBaseTrainerEdit.OutputLoraFormat.FAL, description='Dictates the naming scheme for the output weights')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.Flux2Klein9BBaseTrainerEdit

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class Flux2Trainer(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Fine-tune FLUX.2 [dev] from Black Forest Labs with custom datasets. Create specialized LoRA adaptations for specific styles and domains.
        flux, training, fine-tuning, lora, model-training

        Use cases:
        - Custom model fine-tuning
        - LoRA training for personalization
        - Style-specific model training
        - Brand-specific image generation
        - Specialized domain adaptation
    """

    OutputLoraFormat: typing.ClassVar[type] = nodetool.nodes.fal.training.Flux2Trainer.OutputLoraFormat

    steps: int | OutputHandle[int] = connect_field(default=1000, description='Total number of training steps.')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better. The zip can also contain a text file for each image. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail.')
    learning_rate: float | OutputHandle[float] = connect_field(default=5e-05, description='Learning rate applied to trainable parameters.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')
    output_lora_format: nodetool.nodes.fal.training.Flux2Trainer.OutputLoraFormat = Field(default=nodetool.nodes.fal.training.Flux2Trainer.OutputLoraFormat.FAL, description='Dictates the naming scheme for the output weights')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.Flux2Trainer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class Flux2TrainerEdit(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Fine-tune FLUX.2 [dev] from Black Forest Labs with custom datasets. Create specialized LoRA adaptations for specific editing tasks.
        flux, training, fine-tuning, lora, model-training

        Use cases:
        - Custom model fine-tuning
        - LoRA training for personalization
        - Style-specific model training
        - Brand-specific image generation
        - Specialized domain adaptation
    """

    OutputLoraFormat: typing.ClassVar[type] = nodetool.nodes.fal.training.Flux2TrainerEdit.OutputLoraFormat

    steps: int | OutputHandle[int] = connect_field(default=1000, description='Total number of training steps.')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain up to four reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail.')
    learning_rate: float | OutputHandle[float] = connect_field(default=5e-05, description='Learning rate applied to trainable parameters.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')
    output_lora_format: nodetool.nodes.fal.training.Flux2TrainerEdit.OutputLoraFormat = Field(default=nodetool.nodes.fal.training.Flux2TrainerEdit.OutputLoraFormat.FAL, description='Dictates the naming scheme for the output weights')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.Flux2TrainerEdit

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class Flux2TrainerV2(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Flux 2 Trainer V2
        flux, training, fine-tuning, lora, model-training

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    OutputLoraFormat: typing.ClassVar[type] = nodetool.nodes.fal.training.Flux2TrainerV2.OutputLoraFormat

    steps: int | OutputHandle[int] = connect_field(default=1000, description='Total number of training steps.')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better. The zip can also contain a text file for each image. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail.')
    learning_rate: float | OutputHandle[float] = connect_field(default=5e-05, description='Learning rate applied to trainable parameters.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')
    output_lora_format: nodetool.nodes.fal.training.Flux2TrainerV2.OutputLoraFormat = Field(default=nodetool.nodes.fal.training.Flux2TrainerV2.OutputLoraFormat.FAL, description='Dictates the naming scheme for the output weights')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.Flux2TrainerV2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class Flux2TrainerV2Edit(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Flux 2 Trainer V2
        flux, training, fine-tuning, lora, model-training

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    OutputLoraFormat: typing.ClassVar[type] = nodetool.nodes.fal.training.Flux2TrainerV2Edit.OutputLoraFormat

    steps: int | OutputHandle[int] = connect_field(default=1000, description='Total number of training steps.')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain up to four reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ROOT_start4.EXT, ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail.')
    learning_rate: float | OutputHandle[float] = connect_field(default=5e-05, description='Learning rate applied to trainable parameters.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')
    output_lora_format: nodetool.nodes.fal.training.Flux2TrainerV2Edit.OutputLoraFormat = Field(default=nodetool.nodes.fal.training.Flux2TrainerV2Edit.OutputLoraFormat.FAL, description='Dictates the naming scheme for the output weights')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.Flux2TrainerV2Edit

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class HunyuanVideoLoraTraining(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Train Hunyuan Video lora on people, objects, characters and more!
        training, fine-tuning, lora, model-training

        Use cases:
        - Custom model fine-tuning
        - LoRA training for personalization
        - Style-specific model training
        - Brand-specific image generation
        - Specialized domain adaptation
    """

    trigger_word: str | OutputHandle[str] = connect_field(default='', description='The trigger word to use.')
    images_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to zip archive with images. Try to use at least 4 images in general the more the better. In addition to images the archive can contain text files with captions. Each text file should have the same name as the image file it corresponds to.')
    steps: int | OutputHandle[int] = connect_field(default=0, description='Number of steps to train the LoRA on.')
    data_archive_format: str | OutputHandle[str] = connect_field(default='', description='The format of the archive. If not specified, the format will be inferred from the URL.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0001, description='Learning rate to use for training.')
    do_caption: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to generate captions for the images.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.HunyuanVideoLoraTraining

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class QwenImage2512Trainer(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Qwen Image 2512 Trainer
        training, fine-tuning, lora, model-training

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    steps: int | OutputHandle[int] = connect_field(default=1000, description='Number of steps to train for')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to the input data zip archive for text-to-image training. The zip should contain images with their corresponding text captions: image.EXT and image.txt For example: photo.jpg and photo.txt The text file contains the caption/prompt describing the target image. If no text file is provided for an image, the default_caption will be used. If no default_caption is provided and a text file is missing, the training will fail.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0005, description='Learning rate for LoRA parameters.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.QwenImage2512Trainer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class QwenImage2512TrainerV2(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Qwen Image 2512 Trainer V2
        training, fine-tuning, lora, model-training

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    steps: int | OutputHandle[int] = connect_field(default=2000, description='Number of steps to train for')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to the input data zip archive. The zip should contain pairs of images and corresponding captions. The images should be named: ROOT.EXT. For example: 001.jpg The corresponding captions should be named: ROOT.txt. For example: 001.txt If no text file is provided for an image, the default_caption will be used.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0005, description='Learning rate.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.QwenImage2512TrainerV2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class QwenImageEdit2509Trainer(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Qwen Image Edit 2509 Trainer
        training, fine-tuning, lora, model-training

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    steps: int | OutputHandle[int] = connect_field(default=1000, description='Number of steps to train for')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain more than one reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The Reference Image Count field should be set to the number of reference images. The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0001, description='Learning rate for LoRA parameters.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.QwenImageEdit2509Trainer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class QwenImageEdit2511Trainer(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Qwen Image Edit 2511 Trainer
        training, fine-tuning, lora, model-training

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    steps: int | OutputHandle[int] = connect_field(default=1000, description='Number of steps to train for')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain more than one reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The Reference Image Count field should be set to the number of reference images. The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0001, description='Learning rate for LoRA parameters.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.QwenImageEdit2511Trainer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class QwenImageEditPlusTrainer(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        LoRA trainer for Qwen Image Edit Plus
        training, fine-tuning, lora, model-training

        Use cases:
        - Custom model fine-tuning
        - LoRA training for personalization
        - Style-specific model training
        - Brand-specific image generation
        - Specialized domain adaptation
    """

    steps: int | OutputHandle[int] = connect_field(default=1000, description='Number of steps to train for')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain more than one reference image for each image pair. The reference images should be named: ROOT_start.EXT, ROOT_start2.EXT, ROOT_start3.EXT, ..., ROOT_end.EXT For example: photo_start.jpg, photo_start2.jpg, photo_end.jpg The Reference Image Count field should be set to the number of reference images. The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0001, description='Learning rate for LoRA parameters.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.QwenImageEditPlusTrainer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class QwenImageEditTrainer(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        LoRA trainer for Qwen Image Edit
        training, fine-tuning, lora, model-training

        Use cases:
        - Custom model fine-tuning
        - LoRA training for personalization
        - Style-specific model training
        - Brand-specific image generation
        - Specialized domain adaptation
    """

    steps: int | OutputHandle[int] = connect_field(default=1000, description='Number of steps to train for')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to the input data zip archive. The zip should contain pairs of images. The images should be named: ROOT_start.EXT and ROOT_end.EXT For example: photo_start.jpg and photo_end.jpg The zip can also contain a text file for each image pair. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0001, description='Learning rate for LoRA parameters.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.QwenImageEditTrainer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class QwenImageLayeredTrainer(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Qwen Image Layered Trainer
        training, fine-tuning, lora, model-training

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    steps: int | OutputHandle[int] = connect_field(default=1000, description='Number of steps to train for')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to the input data zip archive. The zip should contain groups of images. The images should be named: ROOT_start.EXT, ROOT_end.EXT, ROOT_end2.EXT, ..., ROOT_endN.EXT For example: photo_start.png, photo_end.png, photo_end2.png, ..., photo_endN.png The start image is the base image that will be decomposed into layers. The end images are the layers that will be added to the base image. ROOT_end.EXT is the first layer, ROOT_end2.EXT is the second layer, and so on. You can have up to 8 layers. All image groups must have the same number of output layers. The end images can contain transparent regions. Only PNG and WebP images are supported since these are the only formats that support transparency. The zip can also contain a text file for each image group. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify a description of the base image. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0001, description='Learning rate for LoRA parameters.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.QwenImageLayeredTrainer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class QwenImageTrainer(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Qwen Image LoRA training
        training, fine-tuning, lora, model-training

        Use cases:
        - Custom model fine-tuning
        - LoRA training for personalization
        - Style-specific model training
        - Brand-specific image generation
        - Specialized domain adaptation
    """

    steps: int | OutputHandle[int] = connect_field(default=1000, description='Total number of training steps to perform. Default is 4000.')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to zip archive with images for training. The archive should contain images and corresponding text files with captions. Each text file should have the same name as the image file it corresponds to (e.g., image1.jpg and image1.txt). If text files are missing for some images, you can provide a trigger_phrase to automatically create them. Supported image formats: PNG, JPG, JPEG, WEBP. Try to use at least 10 images, although more is better.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0005, description='Learning rate for training. Default is 5e-4')
    trigger_phrase: str | OutputHandle[str] = connect_field(default='', description="Default caption to use for images that don't have corresponding text files. If provided, missing .txt files will be created automatically.")

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.QwenImageTrainer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class RecraftV3CreateStyle(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        Recraft V3 Create Style is capable of creating unique styles for Recraft V3 based on your images.
        training, fine-tuning, lora, model-training

        Use cases:
        - Custom model fine-tuning
        - LoRA training for personalization
        - Style-specific model training
        - Brand-specific image generation
        - Specialized domain adaptation
    """

    BaseStyle: typing.ClassVar[type] = nodetool.nodes.fal.training.RecraftV3CreateStyle.BaseStyle

    images_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to zip archive with images, use PNG format. Maximum 5 images are allowed.')
    base_style: nodetool.nodes.fal.training.RecraftV3CreateStyle.BaseStyle = Field(default=nodetool.nodes.fal.training.RecraftV3CreateStyle.BaseStyle.DIGITAL_ILLUSTRATION, description='The base style of the generated images, this topic is covered above.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.RecraftV3CreateStyle

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class TurboFluxTrainer(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        A blazing fast FLUX dev LoRA trainer for subjects and styles.
        flux, training, fine-tuning, lora, model-training, fast

        Use cases:
        - Custom model fine-tuning
        - LoRA training for personalization
        - Style-specific model training
        - Brand-specific image generation
        - Specialized domain adaptation
    """

    TrainingStyle: typing.ClassVar[type] = nodetool.nodes.fal.training.TurboFluxTrainer.TrainingStyle

    images_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better.')
    trigger_phrase: str | OutputHandle[str] = connect_field(default='ohwx', description='Trigger phrase to be used in the captions. If None, a trigger word will not be used. If no captions are provide the trigger_work will be used instead of captions. If captions are provided, the trigger word will replace the `[trigger]` string in the captions.')
    steps: int | OutputHandle[int] = connect_field(default=1000, description='Number of steps to train the LoRA on.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.00115, description='Learning rate for the training.')
    training_style: nodetool.nodes.fal.training.TurboFluxTrainer.TrainingStyle = Field(default=nodetool.nodes.fal.training.TurboFluxTrainer.TrainingStyle.SUBJECT, description='Training style to use.')
    face_crop: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to try to detect the face and crop the images to the face.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.TurboFluxTrainer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class Wan22ImageTrainer(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Wan 2.2 text to image LoRA trainer. Fine-tune Wan 2.2 for subjects and styles with unprecedented detail.
        training, fine-tuning, lora, model-training

        Use cases:
        - Custom model fine-tuning
        - LoRA training for personalization
        - Style-specific model training
        - Brand-specific image generation
        - Specialized domain adaptation
    """

    trigger_phrase: str | OutputHandle[str] = connect_field(default='', description='Trigger phrase for the model.')
    use_masks: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to use masks for the training data.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0007, description='Learning rate for training.')
    use_face_cropping: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to use face cropping for the training data. When enabled, images will be cropped to the face before resizing.')
    training_data_url: str | OutputHandle[str] = connect_field(default='', description='URL to the training data.')
    steps: int | OutputHandle[int] = connect_field(default=1000, description='Number of training steps.')
    include_synthetic_captions: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to include synthetic captions.')
    is_style: bool | OutputHandle[bool] = connect_field(default=False, description='Whether the training data is style data. If true, face specific options like masking and face detection will be disabled.')
    use_face_detection: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to use face detection for the training data. When enabled, images will use the center of the face as the center of the image when resizing.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.Wan22ImageTrainer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class WanTrainer(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Train custom LoRAs for Wan-2.1 I2V 480P
        training, fine-tuning, lora, model-training

        Use cases:
        - Custom model fine-tuning
        - LoRA training for personalization
        - Style-specific model training
        - Brand-specific image generation
        - Specialized domain adaptation
    """

    number_of_steps: int | OutputHandle[int] = connect_field(default=400, description='The number of steps to train for.')
    training_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better. In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.')
    trigger_phrase: str | OutputHandle[str] = connect_field(default='', description='The phrase that will trigger the model to generate an image.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0002, description='The rate at which the model learns. Higher values can lead to faster training, but over-fitting.')
    auto_scale_input: bool | OutputHandle[bool] = connect_field(default=False, description='If true, the input will be automatically scale the video to 81 frames at 16fps.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.WanTrainer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class WanTrainerFlf2v720p(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Train custom LoRAs for Wan-2.1 FLF2V 720P
        training, fine-tuning, lora, model-training

        Use cases:
        - Custom model fine-tuning
        - LoRA training for personalization
        - Style-specific model training
        - Brand-specific image generation
        - Specialized domain adaptation
    """

    number_of_steps: int | OutputHandle[int] = connect_field(default=400, description='The number of steps to train for.')
    training_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better. In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.')
    trigger_phrase: str | OutputHandle[str] = connect_field(default='', description='The phrase that will trigger the model to generate an image.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0002, description='The rate at which the model learns. Higher values can lead to faster training, but over-fitting.')
    auto_scale_input: bool | OutputHandle[bool] = connect_field(default=False, description='If true, the input will be automatically scale the video to 81 frames at 16fps.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.WanTrainerFlf2v720p

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class WanTrainerI2v720p(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Train custom LoRAs for Wan-2.1 I2V 720P
        training, fine-tuning, lora, model-training

        Use cases:
        - Custom model fine-tuning
        - LoRA training for personalization
        - Style-specific model training
        - Brand-specific image generation
        - Specialized domain adaptation
    """

    number_of_steps: int | OutputHandle[int] = connect_field(default=400, description='The number of steps to train for.')
    training_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better. In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.')
    trigger_phrase: str | OutputHandle[str] = connect_field(default='', description='The phrase that will trigger the model to generate an image.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0002, description='The rate at which the model learns. Higher values can lead to faster training, but over-fitting.')
    auto_scale_input: bool | OutputHandle[bool] = connect_field(default=False, description='If true, the input will be automatically scale the video to 81 frames at 16fps.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.WanTrainerI2v720p

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class WanTrainerT2v(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Train custom LoRAs for Wan-2.1 T2V 1.3B
        training, fine-tuning, lora, model-training

        Use cases:
        - Custom model fine-tuning
        - LoRA training for personalization
        - Style-specific model training
        - Brand-specific image generation
        - Specialized domain adaptation
    """

    number_of_steps: int | OutputHandle[int] = connect_field(default=400, description='The number of steps to train for.')
    training_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better. In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.')
    trigger_phrase: str | OutputHandle[str] = connect_field(default='', description='The phrase that will trigger the model to generate an image.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0002, description='The rate at which the model learns. Higher values can lead to faster training, but over-fitting.')
    auto_scale_input: bool | OutputHandle[bool] = connect_field(default=False, description='If true, the input will be automatically scale the video to 81 frames at 16fps.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.WanTrainerT2v

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class WanTrainerT2v14b(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Train custom LoRAs for Wan-2.1 T2V 14B
        training, fine-tuning, lora, model-training

        Use cases:
        - Custom model fine-tuning
        - LoRA training for personalization
        - Style-specific model training
        - Brand-specific image generation
        - Specialized domain adaptation
    """

    number_of_steps: int | OutputHandle[int] = connect_field(default=400, description='The number of steps to train for.')
    training_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to zip archive with images of a consistent style. Try to use at least 10 images and/or videos, although more is better. In addition to images the archive can contain text files with captions. Each text file should have the same name as the image/video file it corresponds to.')
    trigger_phrase: str | OutputHandle[str] = connect_field(default='', description='The phrase that will trigger the model to generate an image.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0002, description='The rate at which the model learns. Higher values can lead to faster training, but over-fitting.')
    auto_scale_input: bool | OutputHandle[bool] = connect_field(default=False, description='If true, the input will be automatically scale the video to 81 frames at 16fps.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.WanTrainerT2v14b

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class ZImageBaseTrainer(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Z-Image Trainer
        training, fine-tuning, lora, model-training

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    steps: int | OutputHandle[int] = connect_field(default=2000, description='Number of steps to train for')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to the input data zip archive. The zip should contain pairs of images and corresponding captions. The images should be named: ROOT.EXT. For example: 001.jpg The corresponding captions should be named: ROOT.txt. For example: 001.txt If no text file is provided for an image, the default_caption will be used.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0005, description='Learning rate.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.ZImageBaseTrainer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class ZImageTrainer(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Train LoRAs on Z-Image Turbo, a super fast text-to-image model of 6B parameters developed by Tongyi-MAI.
        training, fine-tuning, lora, model-training

        Use cases:
        - Custom model fine-tuning
        - LoRA training for personalization
        - Style-specific model training
        - Brand-specific image generation
        - Specialized domain adaptation
    """

    TrainingType: typing.ClassVar[type] = nodetool.nodes.fal.training.ZImageTrainer.TrainingType

    steps: int | OutputHandle[int] = connect_field(default=1000, description='Total number of training steps.')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to zip archive with images of a consistent style. Try to use at least 10 images, although more is better. The zip can also contain a text file for each image. The text file should be named: ROOT.txt For example: photo.txt This text file can be used to specify the edit instructions for the image pair. If no text file is provided, the default_caption will be used. If no default_caption is provided, the training will fail.')
    training_type: nodetool.nodes.fal.training.ZImageTrainer.TrainingType = Field(default=nodetool.nodes.fal.training.ZImageTrainer.TrainingType.BALANCED, description="Type of training to perform. Use 'content' to focus on the content of the images, 'style' to focus on the style of the images, and 'balanced' to focus on a combination of both.")
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0001, description='Learning rate applied to trainable parameters.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.ZImageTrainer

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.training
from nodetool.workflows.base_node import BaseNode

class ZImageTurboTrainerV2(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Z Image Turbo Trainer V2
        training, fine-tuning, lora, model-training, fast

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    steps: int | OutputHandle[int] = connect_field(default=2000, description='Number of steps to train for')
    image_data: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL to the input data zip archive. The zip should contain pairs of images and corresponding captions. The images should be named: ROOT.EXT. For example: 001.jpg The corresponding captions should be named: ROOT.txt. For example: 001.txt If no text file is provided for an image, the default_caption will be used.')
    learning_rate: float | OutputHandle[float] = connect_field(default=0.0005, description='Learning rate.')
    default_caption: str | OutputHandle[str] = connect_field(default='', description='Default caption to use when caption files are missing. If None, missing captions will cause an error.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.training.ZImageTurboTrainerV2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


