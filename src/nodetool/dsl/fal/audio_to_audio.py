# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode


class AceStepAudioInpaint(
    SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]
):
    """

    Modify a portion of provided audio with lyrics and/or style using ACE-Step
    audio, processing, audio-to-audio, transformation

    Use cases:
    - Audio enhancement and processing
    - Voice transformation
    - Audio style transfer
    - Sound quality improvement
    - Audio effect application
    """

    EndTimeRelativeTo: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.AceStepAudioInpaint.EndTimeRelativeTo
    )
    Scheduler: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.AceStepAudioInpaint.Scheduler
    )
    GuidanceType: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.AceStepAudioInpaint.GuidanceType
    )
    StartTimeRelativeTo: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.AceStepAudioInpaint.StartTimeRelativeTo
    )

    number_of_steps: int | OutputHandle[int] = connect_field(
        default=27, description="Number of steps to generate the audio."
    )
    start_time: float | OutputHandle[float] = connect_field(
        default=0, description="start time in seconds for the inpainting process."
    )
    tags: str | OutputHandle[str] = connect_field(
        default="",
        description="Comma-separated list of genre tags to control the style of the generated audio.",
    )
    minimum_guidance_scale: float | OutputHandle[float] = connect_field(
        default=3,
        description="Minimum guidance scale for the generation after the decay.",
    )
    lyrics: str | OutputHandle[str] = connect_field(
        default="",
        description="Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song.",
    )
    end_time_relative_to: (
        nodetool.nodes.fal.audio_to_audio.AceStepAudioInpaint.EndTimeRelativeTo
    ) = Field(
        default=nodetool.nodes.fal.audio_to_audio.AceStepAudioInpaint.EndTimeRelativeTo.START,
        description="Whether the end time is relative to the start or end of the audio.",
    )
    tag_guidance_scale: float | OutputHandle[float] = connect_field(
        default=5, description="Tag guidance scale for the generation."
    )
    scheduler: nodetool.nodes.fal.audio_to_audio.AceStepAudioInpaint.Scheduler = Field(
        default=nodetool.nodes.fal.audio_to_audio.AceStepAudioInpaint.Scheduler.EULER,
        description="Scheduler to use for the generation process.",
    )
    end_time: float | OutputHandle[float] = connect_field(
        default=30, description="end time in seconds for the inpainting process."
    )
    guidance_type: (
        nodetool.nodes.fal.audio_to_audio.AceStepAudioInpaint.GuidanceType
    ) = Field(
        default=nodetool.nodes.fal.audio_to_audio.AceStepAudioInpaint.GuidanceType.APG,
        description="Type of CFG to use for the generation process.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=15, description="Guidance scale for the generation."
    )
    lyric_guidance_scale: float | OutputHandle[float] = connect_field(
        default=1.5, description="Lyric guidance scale for the generation."
    )
    guidance_interval: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)",
    )
    variance: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="Variance for the inpainting process. Higher values can lead to more diverse results.",
    )
    guidance_interval_decay: float | OutputHandle[float] = connect_field(
        default=0,
        description="Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.",
    )
    start_time_relative_to: (
        nodetool.nodes.fal.audio_to_audio.AceStepAudioInpaint.StartTimeRelativeTo
    ) = Field(
        default=nodetool.nodes.fal.audio_to_audio.AceStepAudioInpaint.StartTimeRelativeTo.START,
        description="Whether the start time is relative to the start or end of the audio.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the audio file to be inpainted.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If not provided, a random seed will be used.",
    )
    granularity_scale: int | OutputHandle[int] = connect_field(
        default=10,
        description="Granularity scale for the generation process. Higher values can reduce artifacts.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.AceStepAudioInpaint

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode


class AceStepAudioOutpaint(
    SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]
):
    """

    Extend the beginning or end of provided audio with lyrics and/or style using ACE-Step
    audio, processing, audio-to-audio, transformation

    Use cases:
    - Audio enhancement and processing
    - Voice transformation
    - Audio style transfer
    - Sound quality improvement
    - Audio effect application
    """

    Scheduler: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.AceStepAudioOutpaint.Scheduler
    )
    GuidanceType: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.AceStepAudioOutpaint.GuidanceType
    )

    number_of_steps: int | OutputHandle[int] = connect_field(
        default=27, description="Number of steps to generate the audio."
    )
    tags: str | OutputHandle[str] = connect_field(
        default="",
        description="Comma-separated list of genre tags to control the style of the generated audio.",
    )
    minimum_guidance_scale: float | OutputHandle[float] = connect_field(
        default=3,
        description="Minimum guidance scale for the generation after the decay.",
    )
    extend_after_duration: float | OutputHandle[float] = connect_field(
        default=30, description="Duration in seconds to extend the audio from the end."
    )
    lyrics: str | OutputHandle[str] = connect_field(
        default="",
        description="Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song.",
    )
    tag_guidance_scale: float | OutputHandle[float] = connect_field(
        default=5, description="Tag guidance scale for the generation."
    )
    scheduler: nodetool.nodes.fal.audio_to_audio.AceStepAudioOutpaint.Scheduler = Field(
        default=nodetool.nodes.fal.audio_to_audio.AceStepAudioOutpaint.Scheduler.EULER,
        description="Scheduler to use for the generation process.",
    )
    extend_before_duration: float | OutputHandle[float] = connect_field(
        default=0, description="Duration in seconds to extend the audio from the start."
    )
    guidance_type: (
        nodetool.nodes.fal.audio_to_audio.AceStepAudioOutpaint.GuidanceType
    ) = Field(
        default=nodetool.nodes.fal.audio_to_audio.AceStepAudioOutpaint.GuidanceType.APG,
        description="Type of CFG to use for the generation process.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=15, description="Guidance scale for the generation."
    )
    lyric_guidance_scale: float | OutputHandle[float] = connect_field(
        default=1.5, description="Lyric guidance scale for the generation."
    )
    guidance_interval: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)",
    )
    guidance_interval_decay: float | OutputHandle[float] = connect_field(
        default=0,
        description="Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the audio file to be outpainted.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If not provided, a random seed will be used.",
    )
    granularity_scale: int | OutputHandle[int] = connect_field(
        default=10,
        description="Granularity scale for the generation process. Higher values can reduce artifacts.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.AceStepAudioOutpaint

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode


class AceStepAudioToAudio(
    SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]
):
    """

    Generate music from a lyrics and example audio using ACE-Step
    audio, processing, audio-to-audio, transformation

    Use cases:
    - Audio enhancement and processing
    - Voice transformation
    - Audio style transfer
    - Sound quality improvement
    - Audio effect application
    """

    Scheduler: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.AceStepAudioToAudio.Scheduler
    )
    GuidanceType: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.AceStepAudioToAudio.GuidanceType
    )
    EditMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.AceStepAudioToAudio.EditMode
    )

    number_of_steps: int | OutputHandle[int] = connect_field(
        default=27, description="Number of steps to generate the audio."
    )
    tags: str | OutputHandle[str] = connect_field(
        default="",
        description="Comma-separated list of genre tags to control the style of the generated audio.",
    )
    minimum_guidance_scale: float | OutputHandle[float] = connect_field(
        default=3,
        description="Minimum guidance scale for the generation after the decay.",
    )
    lyrics: str | OutputHandle[str] = connect_field(
        default="",
        description="Lyrics to be sung in the audio. If not provided or if [inst] or [instrumental] is the content of this field, no lyrics will be sung. Use control structures like [verse], [chorus] and [bridge] to control the structure of the song.",
    )
    tag_guidance_scale: float | OutputHandle[float] = connect_field(
        default=5, description="Tag guidance scale for the generation."
    )
    original_lyrics: str | OutputHandle[str] = connect_field(
        default="", description="Original lyrics of the audio file."
    )
    scheduler: nodetool.nodes.fal.audio_to_audio.AceStepAudioToAudio.Scheduler = Field(
        default=nodetool.nodes.fal.audio_to_audio.AceStepAudioToAudio.Scheduler.EULER,
        description="Scheduler to use for the generation process.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=15, description="Guidance scale for the generation."
    )
    guidance_type: (
        nodetool.nodes.fal.audio_to_audio.AceStepAudioToAudio.GuidanceType
    ) = Field(
        default=nodetool.nodes.fal.audio_to_audio.AceStepAudioToAudio.GuidanceType.APG,
        description="Type of CFG to use for the generation process.",
    )
    lyric_guidance_scale: float | OutputHandle[float] = connect_field(
        default=1.5, description="Lyric guidance scale for the generation."
    )
    guidance_interval: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="Guidance interval for the generation. 0.5 means only apply guidance in the middle steps (0.25 * infer_steps to 0.75 * infer_steps)",
    )
    edit_mode: nodetool.nodes.fal.audio_to_audio.AceStepAudioToAudio.EditMode = Field(
        default=nodetool.nodes.fal.audio_to_audio.AceStepAudioToAudio.EditMode.REMIX,
        description="Whether to edit the lyrics only or remix the audio.",
    )
    guidance_interval_decay: float | OutputHandle[float] = connect_field(
        default=0,
        description="Guidance interval decay for the generation. Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the audio file to be outpainted.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If not provided, a random seed will be used.",
    )
    granularity_scale: int | OutputHandle[int] = connect_field(
        default=10,
        description="Granularity scale for the generation process. Higher values can reduce artifacts.",
    )
    original_tags: str | OutputHandle[str] = connect_field(
        default="", description="Original tags of the audio file."
    )
    original_seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Original seed of the audio file."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.AceStepAudioToAudio

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode


class AudioUnderstanding(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

    A audio understanding model to analyze audio content and answer questions about what's happening in the audio based on user prompts.
    audio, processing, audio-to-audio, transformation

    Use cases:
    - Audio enhancement and processing
    - Voice transformation
    - Audio style transfer
    - Sound quality improvement
    - Audio effect application
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The question or prompt about the audio content."
    )
    detailed_analysis: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to request a more detailed analysis of the audio",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the audio file to analyze",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.AudioUnderstanding

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode


class Deepfilternet3(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

    DeepFilterNet3 removes noise and improves audio quality with advanced deep learning filtering.
    audio, noise-reduction, filtering, cleaning, audio-to-audio

    Use cases:
    - Remove noise from audio
    - Clean audio recordings
    - Filter unwanted sounds
    - Improve audio clarity
    - Generate clean audio
    """

    AudioFormat: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.Deepfilternet3.AudioFormat
    )

    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    audio_format: nodetool.nodes.fal.audio_to_audio.Deepfilternet3.AudioFormat = Field(
        default=nodetool.nodes.fal.audio_to_audio.Deepfilternet3.AudioFormat.MP3,
        description="The format for the output audio.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the audio to enhance.",
    )
    bitrate: str | OutputHandle[str] = connect_field(
        default="192k", description="The bitrate of the output audio."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.Deepfilternet3

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode


class Demucs(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

    Demucs separates music into vocals, drums, bass, and other instruments with high quality.
    audio, music-separation, stems, demucs, audio-to-audio

    Use cases:
    - Separate music into stems
    - Extract vocals from songs
    - Isolate instruments in music
    - Create karaoke tracks
    - Generate individual audio stems
    """

    OutputFormat: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.Demucs.OutputFormat
    )
    Model: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_audio.Demucs.Model

    segment_length: str | OutputHandle[str] = connect_field(
        default="",
        description="Length in seconds of each segment for processing. Smaller values use less memory but may reduce quality. Default is model-specific.",
    )
    output_format: nodetool.nodes.fal.audio_to_audio.Demucs.OutputFormat = Field(
        default=nodetool.nodes.fal.audio_to_audio.Demucs.OutputFormat.MP3,
        description="Output audio format for the separated stems",
    )
    stems: str | OutputHandle[str] = connect_field(
        default="",
        description="Specific stems to extract. If None, extracts all available stems. Available stems depend on model: vocals, drums, bass, other, guitar, piano (for 6s model)",
    )
    overlap: float | OutputHandle[float] = connect_field(
        default=0.25,
        description="Overlap between segments (0.0 to 1.0). Higher values may improve quality but increase processing time.",
    )
    model: nodetool.nodes.fal.audio_to_audio.Demucs.Model = Field(
        default=nodetool.nodes.fal.audio_to_audio.Demucs.Model.HTDEMUCS_6S,
        description="Demucs model to use for separation",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the audio file to separate into stems",
    )
    shifts: int | OutputHandle[int] = connect_field(
        default=1,
        description="Number of random shifts for equivariant stabilization. Higher values improve quality but increase processing time.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.Demucs

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode


class DiaTtsVoiceClone(
    SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]
):
    """

    Clone dialog voices from a sample audio and generate dialogs from text prompts using the Dia TTS which leverages advanced AI techniques to create high-quality text-to-speech.
    audio, processing, audio-to-audio, transformation

    Use cases:
    - Audio enhancement and processing
    - Voice transformation
    - Audio style transfer
    - Sound quality improvement
    - Audio effect application
    """

    text: str | OutputHandle[str] = connect_field(
        default="", description="The text to be converted to speech."
    )
    ref_text: str | OutputHandle[str] = connect_field(
        default="", description="The reference text to be used for TTS."
    )
    ref_audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the reference audio file.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.DiaTtsVoiceClone

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode


class ElevenlabsAudioIsolation(
    SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]
):
    """

    Isolate audio tracks using ElevenLabs advanced audio isolation technology.
    audio, processing, audio-to-audio, transformation

    Use cases:
    - Audio enhancement and processing
    - Voice transformation
    - Audio style transfer
    - Sound quality improvement
    - Audio effect application
    """

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="Video file to use for audio isolation. Either `audio_url` or `video_url` must be provided.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the audio file to isolate voice from",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.ElevenlabsAudioIsolation

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode


class ElevenlabsVoiceChanger(
    SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]
):
    """

    ElevenLabs Voice Changer transforms voice characteristics in audio with AI-powered voice conversion.
    audio, voice-change, elevenlabs, transformation, audio-to-audio

    Use cases:
    - Change voice characteristics in audio
    - Transform vocal qualities
    - Create voice variations
    - Modify speaker identity
    - Generate voice-changed audio
    """

    OutputFormat: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.ElevenlabsVoiceChanger.OutputFormat
    )

    voice: str | OutputHandle[str] = connect_field(
        default="Rachel", description="The voice to use for speech generation"
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The input audio file",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for reproducibility."
    )
    output_format: (
        nodetool.nodes.fal.audio_to_audio.ElevenlabsVoiceChanger.OutputFormat
    ) = Field(
        default=nodetool.nodes.fal.audio_to_audio.ElevenlabsVoiceChanger.OutputFormat.MP3_44100_128,
        description="Output format of the generated audio. Formatted as codec_sample_rate_bitrate.",
    )
    remove_background_noise: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set, will remove the background noise from your audio input using our audio isolation model.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.ElevenlabsVoiceChanger

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode


class FfmpegApiMergeAudios(
    SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]
):
    """

    FFmpeg API Merge Audios combines multiple audio files into a single output.
    audio, processing, audio-to-audio, merging, ffmpeg

    Use cases:
    - Combine multiple audio tracks
    - Merge audio segments
    - Create audio compilations
    - Join split audio files
    - Generate combined audio output
    """

    audios: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="List of audio URLs to merge in order. The 0th stream of the audio will be considered as the merge candidate.",
    )
    output_format: str | OutputHandle[str] = connect_field(
        default="",
        description="Output format of the combined audio. If not used, will be determined automatically using FFMPEG. Formatted as codec_sample_rate_bitrate.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.FfmpegApiMergeAudios

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode


class KlingVideoCreateVoice(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

    Create Voices to be used with Kling 2.6 Voice Control
    audio, processing, audio-to-audio, transformation

    Use cases:
    - Audio enhancement and processing
    - Voice transformation
    - Audio style transfer
    - Sound quality improvement
    - Audio effect application
    """

    voice: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the voice audio file. Supports .mp3/.wav audio or .mp4/.mov video. Duration must be 5-30 seconds with clean, single-voice audio.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.KlingVideoCreateVoice

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode


class NovaSr(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """

    Nova SR enhances audio quality through super-resolution processing for clearer and richer sound.
    audio, enhancement, super-resolution, quality, audio-to-audio

    Use cases:
    - Enhance audio quality
    - Improve sound clarity
    - Upscale audio resolution
    - Restore degraded audio
    - Generate high-quality audio
    """

    AudioFormat: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.NovaSr.AudioFormat
    )

    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    audio_format: nodetool.nodes.fal.audio_to_audio.NovaSr.AudioFormat = Field(
        default=nodetool.nodes.fal.audio_to_audio.NovaSr.AudioFormat.MP3,
        description="The format for the output audio.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the audio file to enhance.",
    )
    bitrate: str | OutputHandle[str] = connect_field(
        default="192k", description="The bitrate of the output audio."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.NovaSr

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode


class SamAudioSeparate(
    SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]
):
    """

    SAM Audio Separate isolates and extracts different audio sources from mixed recordings.
    audio, separation, source-extraction, isolation, audio-to-audio

    Use cases:
    - Separate audio sources
    - Extract vocals from music
    - Isolate instruments
    - Remove background sounds
    - Generate separated audio tracks
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.SamAudioSeparate.Acceleration
    )
    OutputFormat: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.SamAudioSeparate.OutputFormat
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt describing the sound to isolate."
    )
    acceleration: nodetool.nodes.fal.audio_to_audio.SamAudioSeparate.Acceleration = (
        Field(
            default=nodetool.nodes.fal.audio_to_audio.SamAudioSeparate.Acceleration.BALANCED,
            description="The acceleration level to use.",
        )
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the audio file to process (WAV, MP3, FLAC supported)",
    )
    predict_spans: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Automatically predict temporal spans where the target sound occurs.",
    )
    output_format: nodetool.nodes.fal.audio_to_audio.SamAudioSeparate.OutputFormat = (
        Field(
            default=nodetool.nodes.fal.audio_to_audio.SamAudioSeparate.OutputFormat.WAV,
            description="Output audio format.",
        )
    )
    reranking_candidates: int | OutputHandle[int] = connect_field(
        default=1,
        description="Number of candidates to generate and rank. Higher improves quality but increases latency and cost.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.SamAudioSeparate

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode


class SamAudioSpanSeparate(
    SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]
):
    """

    SAM Audio Span Separate isolates audio sources across time spans with precise temporal control.
    audio, separation, temporal, span, audio-to-audio

    Use cases:
    - Separate audio by time spans
    - Extract sources in specific periods
    - Isolate temporal audio segments
    - Remove sounds in time ranges
    - Generate time-based separations
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.SamAudioSpanSeparate.Acceleration
    )
    OutputFormat: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.SamAudioSpanSeparate.OutputFormat
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt describing the sound to isolate. Optional but recommended - helps the model identify what type of sound to extract from the span.",
    )
    acceleration: (
        nodetool.nodes.fal.audio_to_audio.SamAudioSpanSeparate.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.audio_to_audio.SamAudioSpanSeparate.Acceleration.BALANCED,
        description="The acceleration level to use.",
    )
    spans: list[types.AudioTimeSpan] | OutputHandle[list[types.AudioTimeSpan]] = (
        connect_field(
            default=[],
            description="Time spans where the target sound occurs which should be isolated.",
        )
    )
    output_format: (
        nodetool.nodes.fal.audio_to_audio.SamAudioSpanSeparate.OutputFormat
    ) = Field(
        default=nodetool.nodes.fal.audio_to_audio.SamAudioSpanSeparate.OutputFormat.WAV,
        description="Output audio format.",
    )
    trim_to_span: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Trim output audio to only include the specified span time range. If False, returns the full audio length with the target sound isolated throughout.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the audio file to process.",
    )
    reranking_candidates: int | OutputHandle[int] = connect_field(
        default=1,
        description="Number of candidates to generate and rank. Higher improves quality but increases latency and cost. Requires text prompt; ignored for span-only separation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.SamAudioSpanSeparate

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode


class SonautoV2Extend(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """

    Extend an existing song
    audio, processing, audio-to-audio, transformation

    Use cases:
    - Audio enhancement and processing
    - Voice transformation
    - Audio style transfer
    - Sound quality improvement
    - Audio effect application
    """

    OutputFormat: typing.ClassVar[type] = (
        nodetool.nodes.fal.audio_to_audio.SonautoV2Extend.OutputFormat
    )
    Side: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_audio.SonautoV2Extend.Side

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="A description of the track you want to generate. This prompt will be used to automatically generate the tags and lyrics unless you manually set them. For example, if you set prompt and tags, then the prompt will be used to generate only the lyrics.",
    )
    lyrics_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The lyrics sung in the generated song. An empty string will generate an instrumental track.",
    )
    tags: str | OutputHandle[str] = connect_field(
        default="",
        description="Tags/styles of the music to generate. You can view a list of all available tags at https://sonauto.ai/tag-explorer.",
    )
    prompt_strength: float | OutputHandle[float] = connect_field(
        default=1.8,
        description="Controls how strongly your prompt influences the output. Greater values adhere more to the prompt but sound less natural. (This is CFG.)",
    )
    output_bit_rate: str | OutputHandle[str] = connect_field(
        default="",
        description="The bit rate to use for mp3 and m4a formats. Not available for other formats.",
    )
    num_songs: int | OutputHandle[int] = connect_field(
        default=1,
        description="Generating 2 songs costs 1.5x the price of generating 1 song. Also, note that using the same seed may not result in identical songs if the number of songs generated is changed.",
    )
    output_format: nodetool.nodes.fal.audio_to_audio.SonautoV2Extend.OutputFormat = (
        Field(
            default=nodetool.nodes.fal.audio_to_audio.SonautoV2Extend.OutputFormat.WAV,
            description=None,
        )
    )
    side: nodetool.nodes.fal.audio_to_audio.SonautoV2Extend.Side = Field(
        default=nodetool.nodes.fal.audio_to_audio.SonautoV2Extend.Side(""),
        description="Add more to the beginning (left) or end (right) of the song",
    )
    balance_strength: float | OutputHandle[float] = connect_field(
        default=0.7,
        description="Greater means more natural vocals. Lower means sharper instrumentals. We recommend 0.7.",
    )
    crop_duration: float | OutputHandle[float] = connect_field(
        default=0,
        description="Duration in seconds to crop from the selected side before extending from that side.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the audio file to alter. Must be a valid publicly accessible URL.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="The seed to use for generation. Will pick a random seed if not provided. Repeating a request with identical parameters (must use lyrics and tags, not prompt) and the same seed will generate the same song.",
    )
    extend_duration: str | OutputHandle[str] = connect_field(
        default="",
        description="Duration in seconds to extend the song. If not provided, will attempt to automatically determine.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.SonautoV2Extend

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode


class StableAudio25AudioToAudio(
    SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]
):
    """

    Stable Audio 2.5 transforms and modifies audio with AI-powered processing and effects.
    audio, transformation, stable-audio, 2.5, audio-to-audio

    Use cases:
    - Transform audio characteristics
    - Apply AI-powered audio effects
    - Modify audio properties
    - Generate audio variations
    - Create processed audio
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to guide the audio generation"
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Sometimes referred to as denoising, this parameter controls how much influence the `audio_url` parameter has on the generated audio. A value of 0 would yield audio that is identical to the input. A value of 1 would be as if you passed in no audio at all.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The audio clip to transform",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="The number of steps to denoise the audio for"
    )
    guidance_scale: int | OutputHandle[int] = connect_field(
        default=1,
        description="How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt).",
    )
    seed: int | OutputHandle[int] = connect_field(default=0, description=None)
    total_seconds: int | OutputHandle[int] = connect_field(
        default=0,
        description="The duration of the audio clip to generate. If not provided, it will be set to the duration of the input audio.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.StableAudio25AudioToAudio

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode


class StableAudio25Inpaint(
    SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]
):
    """

    Generate high quality music and sound effects using Stable Audio 2.5 from StabilityAI
    audio, processing, audio-to-audio, transformation

    Use cases:
    - Audio enhancement and processing
    - Voice transformation
    - Audio style transfer
    - Sound quality improvement
    - Audio effect application
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to guide the audio generation"
    )
    guidance_scale: int | OutputHandle[int] = connect_field(
        default=1,
        description="How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt).",
    )
    mask_end: int | OutputHandle[int] = connect_field(
        default=190, description="The end point of the audio mask"
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The audio clip to inpaint",
    )
    seed: int | OutputHandle[int] = connect_field(default=0, description=None)
    seconds_total: int | OutputHandle[int] = connect_field(
        default=190,
        description="The duration of the audio clip to generate. If not provided, it will be set to the duration of the input audio.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="The number of steps to denoise the audio for"
    )
    mask_start: int | OutputHandle[int] = connect_field(
        default=30, description="The start point of the audio mask"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.StableAudio25Inpaint

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
