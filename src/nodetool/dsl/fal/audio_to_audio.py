# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode

class Deepfilternet3(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        DeepFilterNet3 removes noise and improves audio quality with advanced deep learning filtering.
        audio, noise-reduction, filtering, cleaning, audio-to-audio

        Use cases:
        - Remove noise from audio
        - Clean audio recordings
        - Filter unwanted sounds
        - Improve audio clarity
        - Generate clean audio
    """

    AudioFormat: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_audio.AudioFormat

    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    audio_format: nodetool.nodes.fal.audio_to_audio.AudioFormat = Field(default=nodetool.nodes.fal.audio_to_audio.AudioFormat.MP3, description='The format for the output audio.')
    audio_url: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(default=types.AudioRef(type='audio', uri='', asset_id=None, data=None, metadata=None), description='The URL of the audio to enhance.')
    bitrate: str | OutputHandle[str] = connect_field(default='192k', description='The bitrate of the output audio.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.Deepfilternet3

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode

class Demucs(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Demucs separates music into vocals, drums, bass, and other instruments with high quality.
        audio, music-separation, stems, demucs, audio-to-audio

        Use cases:
        - Separate music into stems
        - Extract vocals from songs
        - Isolate instruments in music
        - Create karaoke tracks
        - Generate individual audio stems
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_audio.OutputFormat
    Model: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_audio.Model

    segment_length: str | OutputHandle[str] = connect_field(default='', description='Length in seconds of each segment for processing. Smaller values use less memory but may reduce quality. Default is model-specific.')
    output_format: nodetool.nodes.fal.audio_to_audio.OutputFormat = Field(default=nodetool.nodes.fal.audio_to_audio.OutputFormat.MP3_44100_128, description='Output audio format for the separated stems')
    stems: str | OutputHandle[str] = connect_field(default='', description='Specific stems to extract. If None, extracts all available stems. Available stems depend on model: vocals, drums, bass, other, guitar, piano (for 6s model)')
    overlap: float | OutputHandle[float] = connect_field(default=0.25, description='Overlap between segments (0.0 to 1.0). Higher values may improve quality but increase processing time.')
    model: nodetool.nodes.fal.audio_to_audio.Model = Field(default=nodetool.nodes.fal.audio_to_audio.Model.HTDEMUCS_6S, description='Demucs model to use for separation')
    audio_url: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(default=types.AudioRef(type='audio', uri='', asset_id=None, data=None, metadata=None), description='URL of the audio file to separate into stems')
    shifts: int | OutputHandle[int] = connect_field(default=1, description='Number of random shifts for equivariant stabilization. Higher values improve quality but increase processing time.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.Demucs

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode

class ElevenlabsVoiceChanger(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """

        ElevenLabs Voice Changer transforms voice characteristics in audio with AI-powered voice conversion.
        audio, voice-change, elevenlabs, transformation, audio-to-audio

        Use cases:
        - Change voice characteristics in audio
        - Transform vocal qualities
        - Create voice variations
        - Modify speaker identity
        - Generate voice-changed audio
    """

    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_audio.OutputFormat

    voice: str | OutputHandle[str] = connect_field(default='Rachel', description='The voice to use for speech generation')
    audio_url: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(default=types.AudioRef(type='audio', uri='', asset_id=None, data=None, metadata=None), description='The input audio file')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='Random seed for reproducibility.')
    output_format: nodetool.nodes.fal.audio_to_audio.OutputFormat = Field(default=nodetool.nodes.fal.audio_to_audio.OutputFormat.MP3_44100_128, description='Output format of the generated audio. Formatted as codec_sample_rate_bitrate.')
    remove_background_noise: bool | OutputHandle[bool] = connect_field(default=False, description='If set, will remove the background noise from your audio input using our audio isolation model.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.ElevenlabsVoiceChanger

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode

class NovaSr(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """

        Nova SR enhances audio quality through super-resolution processing for clearer and richer sound.
        audio, enhancement, super-resolution, quality, audio-to-audio

        Use cases:
        - Enhance audio quality
        - Improve sound clarity
        - Upscale audio resolution
        - Restore degraded audio
        - Generate high-quality audio
    """

    AudioFormat: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_audio.AudioFormat

    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    bitrate: str | OutputHandle[str] = connect_field(default='192k', description='The bitrate of the output audio.')
    audio_url: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(default=types.AudioRef(type='audio', uri='', asset_id=None, data=None, metadata=None), description='The URL of the audio file to enhance.')
    audio_format: nodetool.nodes.fal.audio_to_audio.AudioFormat = Field(default=nodetool.nodes.fal.audio_to_audio.AudioFormat.MP3, description='The format for the output audio.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.NovaSr

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode

class SamAudioSeparate(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        SAM Audio Separate isolates and extracts different audio sources from mixed recordings.
        audio, separation, source-extraction, isolation, audio-to-audio

        Use cases:
        - Separate audio sources
        - Extract vocals from music
        - Isolate instruments
        - Remove background sounds
        - Generate separated audio tracks
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_audio.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_audio.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='Text prompt describing the sound to isolate.')
    acceleration: nodetool.nodes.fal.audio_to_audio.Acceleration = Field(default=nodetool.nodes.fal.audio_to_audio.Acceleration.BALANCED, description='The acceleration level to use.')
    audio_url: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(default=types.AudioRef(type='audio', uri='', asset_id=None, data=None, metadata=None), description='URL of the audio file to process (WAV, MP3, FLAC supported)')
    predict_spans: bool | OutputHandle[bool] = connect_field(default=False, description='Automatically predict temporal spans where the target sound occurs.')
    output_format: nodetool.nodes.fal.audio_to_audio.OutputFormat = Field(default=nodetool.nodes.fal.audio_to_audio.OutputFormat.WAV, description='Output audio format.')
    reranking_candidates: int | OutputHandle[int] = connect_field(default=1, description='Number of candidates to generate and rank. Higher improves quality but increases latency and cost.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.SamAudioSeparate

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode

class SamAudioSpanSeparate(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        SAM Audio Span Separate isolates audio sources across time spans with precise temporal control.
        audio, separation, temporal, span, audio-to-audio

        Use cases:
        - Separate audio by time spans
        - Extract sources in specific periods
        - Isolate temporal audio segments
        - Remove sounds in time ranges
        - Generate time-based separations
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_audio.Acceleration
    OutputFormat: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_audio.OutputFormat

    prompt: str | OutputHandle[str] = connect_field(default='', description='Text prompt describing the sound to isolate. Optional but recommended - helps the model identify what type of sound to extract from the span.')
    acceleration: nodetool.nodes.fal.audio_to_audio.Acceleration = Field(default=nodetool.nodes.fal.audio_to_audio.Acceleration.BALANCED, description='The acceleration level to use.')
    spans: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='Time spans where the target sound occurs which should be isolated.')
    output_format: nodetool.nodes.fal.audio_to_audio.OutputFormat = Field(default=nodetool.nodes.fal.audio_to_audio.OutputFormat.WAV, description='Output audio format.')
    trim_to_span: bool | OutputHandle[bool] = connect_field(default=False, description='Trim output audio to only include the specified span time range. If False, returns the full audio length with the target sound isolated throughout.')
    audio_url: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(default=types.AudioRef(type='audio', uri='', asset_id=None, data=None, metadata=None), description='URL of the audio file to process.')
    reranking_candidates: int | OutputHandle[int] = connect_field(default=1, description='Number of candidates to generate and rank. Higher improves quality but increases latency and cost. Requires text prompt; ignored for span-only separation.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.SamAudioSpanSeparate

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_audio
from nodetool.workflows.base_node import BaseNode

class StableAudio25AudioToAudio(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """

        Stable Audio 2.5 transforms and modifies audio with AI-powered processing and effects.
        audio, transformation, stable-audio, 2.5, audio-to-audio

        Use cases:
        - Transform audio characteristics
        - Apply AI-powered audio effects
        - Modify audio properties
        - Generate audio variations
        - Create processed audio
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to guide the audio generation')
    strength: float | OutputHandle[float] = connect_field(default=0.8, description='Sometimes referred to as denoising, this parameter controls how much influence the `audio_url` parameter has on the generated audio. A value of 0 would yield audio that is identical to the input. A value of 1 would be as if you passed in no audio at all.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    audio_url: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(default=types.AudioRef(type='audio', uri='', asset_id=None, data=None, metadata=None), description='The audio clip to transform')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=8, description='The number of steps to denoise the audio for')
    guidance_scale: int | OutputHandle[int] = connect_field(default=1, description='How strictly the diffusion process adheres to the prompt text (higher values make your audio closer to your prompt).')
    seed: int | OutputHandle[int] = connect_field(default=0, description=None)
    total_seconds: int | OutputHandle[int] = connect_field(default=0, description='The duration of the audio clip to generate. If not provided, it will be set to the duration of the input audio.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_audio.StableAudio25AudioToAudio

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


