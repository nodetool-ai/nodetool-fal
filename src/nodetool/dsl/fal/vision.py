# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class AIDetectorImage(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        AI Detector analyzes images to determine if they were generated by AI or are real photos.
        vision, ai-detection, analysis, classification

        Use cases:
        - Detect AI-generated images
        - Verify image authenticity
        - Filter synthetic content
        - Content moderation for AI images
        - Analyze image provenance
    """

    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL pointing to an image to analyze for AI generation.(Max: 3000 characters)')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.AIDetectorImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class ArbiterImage(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        Arbiter provides comprehensive image analysis and quality metrics.
        vision, analysis, quality, metrics, image-evaluation

        Use cases:
        - Analyze image quality
        - Extract image metrics
        - Evaluate visual properties
        - Assess image characteristics
        - Generate quality reports
    """

    measurements: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The measurements to use for the measurement.')
    inputs: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The inputs to use for the measurement.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.ArbiterImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class ArbiterImageImage(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        Arbiter measures similarity and alignment between reference images.
        vision, similarity, comparison, image-matching, analysis

        Use cases:
        - Compare image similarity
        - Measure visual alignment
        - Find duplicate images
        - Rank image variations
        - Evaluate image consistency
    """

    measurements: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The measurements to use for the measurement.')
    inputs: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The inputs to use for the measurement.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.ArbiterImageImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class ArbiterImageText(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        Arbiter measures semantic alignment between images and text descriptions.
        vision, alignment, similarity, text-image, analysis

        Use cases:
        - Measure image-text alignment
        - Verify prompt accuracy
        - Quality control for generated images
        - Rank images by text relevance
        - Evaluate caption accuracy
    """

    measurements: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The measurements to use for the measurement.')
    inputs: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The inputs to use for the measurement.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.ArbiterImageText

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Florence2Caption(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        Florence-2 Large generates concise, accurate captions for images.
        vision, captioning, description, florence, analysis

        Use cases:
        - Generate image captions
        - Create alt text for images
        - Describe images concisely
        - Automate image descriptions
        - Produce accessibility captions
    """

    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The URL of the image to be processed.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Florence2Caption

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Florence2DetailedCaption(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        Florence-2 Large generates detailed captions with rich contextual information.
        vision, captioning, detailed-description, florence, analysis

        Use cases:
        - Generate detailed captions
        - Create rich image descriptions
        - Produce comprehensive captions
        - Analyze image context
        - Generate informative descriptions
    """

    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The URL of the image to be processed.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Florence2DetailedCaption

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Florence2MoreDetailedCaption(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        Florence-2 Large generates highly detailed, comprehensive image captions.
        vision, captioning, detailed-description, florence, analysis

        Use cases:
        - Generate detailed image descriptions
        - Create comprehensive captions
        - Produce rich image narratives
        - Analyze image content thoroughly
        - Generate long-form descriptions
    """

    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The URL of the image to be processed.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Florence2MoreDetailedCaption

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Florence2OCR(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        Florence-2 Large performs optical character recognition to extract text from images.
        vision, ocr, text-extraction, florence, reading

        Use cases:
        - Extract text from images
        - Read document images
        - Digitize printed text
        - Parse image text content
        - Convert images to text
    """

    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The URL of the image to be processed.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Florence2OCR

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Florence2RegionToCategory(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        Florence-2 Large classifies image regions into semantic categories.
        vision, classification, region-analysis, florence, categorization

        Use cases:
        - Classify image regions
        - Categorize image areas
        - Label image segments
        - Identify region types
        - Semantic region analysis
    """

    region: str | OutputHandle[str] = connect_field(default='', description='The user input coordinates')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The URL of the image to be processed.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Florence2RegionToCategory

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Florence2RegionToDescription(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        Florence-2 Large generates detailed descriptions of specific image regions.
        vision, captioning, region-description, florence, ocr

        Use cases:
        - Describe specific image regions
        - Generate region captions
        - Extract region information
        - Annotate image areas
        - Create detailed region descriptions
    """

    region: str | OutputHandle[str] = connect_field(default='', description='The user input coordinates')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The URL of the image to be processed.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Florence2RegionToDescription

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class GotOcrV2(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        GOT-OCR2 works on a wide range of tasks, including plain document OCR, scene text OCR, formatted document OCR, and even OCR for tables, charts, mathematical formulas, geometric shapes, molecular formulas and sheet music.
        vision, analysis, image-understanding, detection

        Use cases:
        - Image analysis and understanding
        - Object detection and recognition
        - Visual content moderation
        - Automated image captioning
        - Scene understanding
    """

    do_format: bool | OutputHandle[bool] = connect_field(default=False, description='Generate the output in formatted mode.')
    multi_page: bool | OutputHandle[bool] = connect_field(default=False, description='Use provided images to generate a single output.')
    input_image_urls: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='URL of images.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.GotOcrV2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class ImageutilsNsfw(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        Predict the probability of an image being NSFW.
        vision, analysis, image-understanding, detection

        Use cases:
        - Image analysis and understanding
        - Object detection and recognition
        - Visual content moderation
        - Automated image captioning
        - Scene understanding
    """

    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Input image url.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.ImageutilsNsfw

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class LlavaNext(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Vision
        vision, analysis, image-understanding, detection

        Use cases:
        - Image analysis and understanding
        - Object detection and recognition
        - Visual content moderation
        - Automated image captioning
        - Scene understanding
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='Prompt to be used for the image')
    top_p: float | OutputHandle[float] = connect_field(default=1, description='Top P for sampling')
    max_tokens: int | OutputHandle[int] = connect_field(default=64, description='Maximum number of tokens to generate')
    temperature: float | OutputHandle[float] = connect_field(default=0.2, description='Temperature for sampling')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL of the image to be processed')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.LlavaNext

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Moondream2(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        Moondream2 is a highly efficient open-source vision language model that combines powerful image understanding capabilities with a remarkably small footprint. 
        vision, analysis, image-understanding, detection

        Use cases:
        - Image analysis and understanding
        - Object detection and recognition
        - Visual content moderation
        - Automated image captioning
        - Scene understanding
    """

    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL of the image to be processed')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Moondream2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Moondream2ObjectDetection(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Moondream2 is a highly efficient open-source vision language model that combines powerful image understanding capabilities with a remarkably small footprint.
        vision, analysis, image-understanding, detection

        Use cases:
        - Image analysis and understanding
        - Object detection and recognition
        - Visual content moderation
        - Automated image captioning
        - Scene understanding
    """

    object: str | OutputHandle[str] = connect_field(default='', description='Object to be detected in the image')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL of the image to be processed')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Moondream2ObjectDetection

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Moondream2PointObjectDetection(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Moondream2 is a highly efficient open-source vision language model that combines powerful image understanding capabilities with a remarkably small footprint.
        vision, analysis, image-understanding, detection

        Use cases:
        - Image analysis and understanding
        - Object detection and recognition
        - Visual content moderation
        - Automated image captioning
        - Scene understanding
    """

    object: str | OutputHandle[str] = connect_field(default='', description='Object to be detected in the image')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL of the image to be processed')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Moondream2PointObjectDetection

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Moondream2VisualQuery(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        Moondream2 is a highly efficient open-source vision language model that combines powerful image understanding capabilities with a remarkably small footprint.
        vision, analysis, image-understanding, detection

        Use cases:
        - Image analysis and understanding
        - Object detection and recognition
        - Visual content moderation
        - Automated image captioning
        - Scene understanding
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='Query to be asked in the image')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL of the image to be processed')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Moondream2VisualQuery

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Moondream3PreviewCaption(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Moondream3 Preview [Caption]
        vision, analysis, image-understanding, detection

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    Length: typing.ClassVar[type] = nodetool.nodes.fal.vision.Moondream3PreviewCaption.Length

    top_p: float | OutputHandle[float] = connect_field(default=0.0, description='Nucleus sampling probability mass to use, between 0 and 1.')
    length: nodetool.nodes.fal.vision.Moondream3PreviewCaption.Length = Field(default=nodetool.nodes.fal.vision.Moondream3PreviewCaption.Length.NORMAL, description='Length of the caption to generate')
    temperature: float | OutputHandle[float] = connect_field(default=0.0, description='Sampling temperature to use, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If not set, defaults to 0.')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL of the image to be processed Max width: 7000px, Max height: 7000px, Timeout: 20.0s')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Moondream3PreviewCaption

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Moondream3PreviewDetect(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Moondream3 Preview [Detect]
        vision, analysis, image-understanding, detection

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='Object to be detected in the image')
    preview: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to preview the output')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL of the image to be processed Max width: 7000px, Max height: 7000px, Timeout: 20.0s')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Moondream3PreviewDetect

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Moondream3PreviewPoint(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Moondream3 Preview [Point]
        vision, analysis, image-understanding, detection

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='Object to be located in the image')
    preview: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to preview the output')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL of the image to be processed Max width: 7000px, Max height: 7000px, Timeout: 20.0s')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Moondream3PreviewPoint

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Moondream3PreviewQuery(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Moondream 3 Preview [Query]
        vision, analysis, image-understanding, detection

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='Query to be asked in the image')
    top_p: float | OutputHandle[float] = connect_field(default=0.0, description='Nucleus sampling probability mass to use, between 0 and 1.')
    temperature: float | OutputHandle[float] = connect_field(default=0.0, description='Sampling temperature to use, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If not set, defaults to 0.')
    reasoning: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to include detailed reasoning behind the answer')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL of the image to be processed Max width: 7000px, Max height: 7000px, Timeout: 20.0s')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Moondream3PreviewQuery

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class MoondreamBatched(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Answer questions from the images.
        vision, analysis, image-understanding, detection

        Use cases:
        - Image analysis and understanding
        - Object detection and recognition
        - Visual content moderation
        - Automated image captioning
        - Scene understanding
    """

    ModelId: typing.ClassVar[type] = nodetool.nodes.fal.vision.MoondreamBatched.ModelId

    model_id: nodetool.nodes.fal.vision.MoondreamBatched.ModelId = Field(default=nodetool.nodes.fal.vision.MoondreamBatched.ModelId.VIKHYATK_MOONDREAM2, description='Model ID to use for inference')
    repetition_penalty: float | OutputHandle[float] = connect_field(default=1, description='Repetition penalty for sampling')
    inputs: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='List of input prompts and image URLs')
    max_tokens: int | OutputHandle[int] = connect_field(default=64, description='Maximum number of new tokens to generate')
    temperature: float | OutputHandle[float] = connect_field(default=0.2, description='Temperature for sampling')
    top_p: float | OutputHandle[float] = connect_field(default=1, description='Top P for sampling')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.MoondreamBatched

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class MoondreamNext(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        MoonDreamNext is a multimodal vision-language model for captioning, gaze detection, bbox detection, point detection, and more.
        vision, analysis, image-understanding, detection

        Use cases:
        - Image analysis and understanding
        - Object detection and recognition
        - Visual content moderation
        - Automated image captioning
        - Scene understanding
    """

    TaskType: typing.ClassVar[type] = nodetool.nodes.fal.vision.MoondreamNext.TaskType

    prompt: str | OutputHandle[str] = connect_field(default='', description='Prompt for query task')
    task_type: nodetool.nodes.fal.vision.MoondreamNext.TaskType = Field(default=nodetool.nodes.fal.vision.MoondreamNext.TaskType.CAPTION, description='Type of task to perform')
    max_tokens: int | OutputHandle[int] = connect_field(default=64, description='Maximum number of tokens to generate')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Image URL to be processed')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.MoondreamNext

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class MoondreamNextBatch(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        MoonDreamNext Batch is a multimodal vision-language model for batch captioning.
        vision, analysis, image-understanding, detection

        Use cases:
        - Image analysis and understanding
        - Object detection and recognition
        - Visual content moderation
        - Automated image captioning
        - Scene understanding
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='Single prompt to apply to all images')
    images_data_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='List of image URLs to be processed (maximum 32 images)')
    max_tokens: int | OutputHandle[int] = connect_field(default=64, description='Maximum number of tokens to generate')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.MoondreamNextBatch

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class OpenrouterRouterVision(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        OpenRouter [Vision]
        vision, analysis, image-understanding, detection

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='Prompt to be used for the image')
    reasoning: bool | OutputHandle[bool] = connect_field(default=False, description='Should reasoning be the part of the final answer.')
    system_prompt: str | OutputHandle[str] = connect_field(default='', description='System prompt to provide context or instructions to the model')
    model: str | OutputHandle[str] = connect_field(default='', description='Name of the model to use. Charged based on actual token usage.')
    max_tokens: str | OutputHandle[str] = connect_field(default='', description="This sets the upper limit for the number of tokens the model can generate in response. It won't produce more than this limit. The maximum value is the context length minus the prompt length.")
    temperature: float | OutputHandle[float] = connect_field(default=1, description="This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.")
    image_urls: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='List of image URLs to be processed')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.OpenrouterRouterVision

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class PerceptronIsaac01(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Isaac-01 is a multimodal vision-language model from Perceptron for various vision language tasks.
        vision, analysis, image-understanding, detection

        Use cases:
        - Image analysis and understanding
        - Object detection and recognition
        - Visual content moderation
        - Automated image captioning
        - Scene understanding
    """

    ResponseStyle: typing.ClassVar[type] = nodetool.nodes.fal.vision.PerceptronIsaac01.ResponseStyle

    prompt: str | OutputHandle[str] = connect_field(default='', description='Prompt to be used for the image')
    response_style: nodetool.nodes.fal.vision.PerceptronIsaac01.ResponseStyle = Field(default=nodetool.nodes.fal.vision.PerceptronIsaac01.ResponseStyle.TEXT, description='Response style to be used for the image. - text: Model will output text. Good for descriptions and captioning. - box: Model will output a combination of text and bounding boxes. Good for localization. - point: Model will output a combination of text and points. Good for counting many objects. - polygon: Model will output a combination of text and polygons. Good for granular segmentation.')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Image URL to be processed')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.PerceptronIsaac01

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class PerceptronIsaac01OpenaiV1ChatCompletions(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        Isaac 0.1 [OpenAI Compatible Endpoint]
        vision, analysis, image-understanding, detection

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.PerceptronIsaac01OpenaiV1ChatCompletions

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Sa2va4bImage(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Sa2VA is an MLLM capable of question answering, visual prompt understanding, and dense object segmentation at both image and video levels
        vision, analysis, image-understanding, detection

        Use cases:
        - Image analysis and understanding
        - Object detection and recognition
        - Visual content moderation
        - Automated image captioning
        - Scene understanding
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='Prompt to be used for the chat completion')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Url for the Input image.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Sa2va4bImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Sa2va4bVideo(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Sa2VA is an MLLM capable of question answering, visual prompt understanding, and dense object segmentation at both image and video levels
        vision, analysis, image-understanding, detection

        Use cases:
        - Image analysis and understanding
        - Object detection and recognition
        - Visual content moderation
        - Automated image captioning
        - Scene understanding
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='Prompt to be used for the chat completion')
    video_url: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(default=types.VideoRef(type='video', uri='', asset_id=None, data=None, metadata=None, duration=None, format=None), description='The URL of the input video.')
    num_frames_to_sample: int | OutputHandle[int] = connect_field(default=0, description='Number of frames to sample from the video. If not provided, all frames are sampled.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Sa2va4bVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Sa2va8bImage(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Sa2VA is an MLLM capable of question answering, visual prompt understanding, and dense object segmentation at both image and video levels
        vision, analysis, image-understanding, detection

        Use cases:
        - Image analysis and understanding
        - Object detection and recognition
        - Visual content moderation
        - Automated image captioning
        - Scene understanding
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='Prompt to be used for the chat completion')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Url for the Input image.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Sa2va8bImage

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Sa2va8bVideo(SingleOutputGraphNode[dict[str, Any]], GraphNode[dict[str, Any]]):
    """

        Sa2VA is an MLLM capable of question answering, visual prompt understanding, and dense object segmentation at both image and video levels
        vision, analysis, image-understanding, detection

        Use cases:
        - Image analysis and understanding
        - Object detection and recognition
        - Visual content moderation
        - Automated image captioning
        - Scene understanding
    """

    prompt: str | OutputHandle[str] = connect_field(default='', description='Prompt to be used for the chat completion')
    video_url: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(default=types.VideoRef(type='video', uri='', asset_id=None, data=None, metadata=None, duration=None, format=None), description='The URL of the input video.')
    num_frames_to_sample: int | OutputHandle[int] = connect_field(default=0, description='Number of frames to sample from the video. If not provided, all frames are sampled.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Sa2va8bVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class Sam3ImageEmbed(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        Sam 3
        vision, analysis, image-understanding, detection

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='URL of the image to embed.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.Sam3ImageEmbed

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class VideoUnderstanding(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        A video understanding model to analyze video content and answer questions about what's happening in the video based on user prompts.
        vision, analysis, image-understanding, detection

        Use cases:
        - Image analysis and understanding
        - Object detection and recognition
        - Visual content moderation
        - Automated image captioning
        - Scene understanding
    """

    detailed_analysis: bool | OutputHandle[bool] = connect_field(default=False, description='Whether to request a more detailed analysis of the video')
    video_url: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(default=types.VideoRef(type='video', uri='', asset_id=None, data=None, metadata=None, duration=None, format=None), description='URL of the video to analyze')
    prompt: str | OutputHandle[str] = connect_field(default='', description='The question or prompt about the video content.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.VideoUnderstanding

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.vision
from nodetool.workflows.base_node import BaseNode

class XAilabNsfw(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

        Predict whether an image is NSFW or SFW.
        vision, analysis, image-understanding, detection

        Use cases:
        - Image analysis and understanding
        - Object detection and recognition
        - Visual content moderation
        - Automated image captioning
        - Scene understanding
    """

    image_urls: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='List of image URLs to check. If more than 10 images are provided, only the first 10 will be checked.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.vision.XAilabNsfw

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


