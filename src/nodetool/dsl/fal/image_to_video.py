# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class AIAvatar(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    MultiTalk generates talking avatar videos from images and audio files.
    video, avatar, talking-head, multitalk, image-to-video

    Use cases:
    - Create talking avatar videos
    - Animate portrait photos with audio
    - Generate spokesperson videos
    - Produce avatar presentations
    - Create personalized video messages
    """

    AIAvatarResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.AIAvatar.AIAvatarResolution
    )
    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.AIAvatar.Acceleration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    resolution: nodetool.nodes.fal.image_to_video.AIAvatar.AIAvatarResolution = Field(
        default=nodetool.nodes.fal.image_to_video.AIAvatar.AIAvatarResolution.VALUE_480P,
        description="Resolution of the video to generate. Must be either 480p or 720p.",
    )
    acceleration: nodetool.nodes.fal.image_to_video.AIAvatar.Acceleration = Field(
        default=nodetool.nodes.fal.image_to_video.AIAvatar.Acceleration.REGULAR,
        description="The acceleration level to use for generation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the audio file.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=145,
        description="Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=42,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.AIAvatar

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class AIAvatarMulti(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    MultiTalk generates multi-speaker avatar videos with audio synchronization.
    video, avatar, multi-speaker, talking-head, image-to-video

    Use cases:
    - Create multi-speaker videos with audio
    - Generate synchronized dialogue
    - Produce conversation videos
    - Create interactive characters
    - Generate multi-avatar content
    """

    AIAvatarMultiResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.AIAvatarMulti.AIAvatarMultiResolution
    )
    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.AIAvatarMulti.Acceleration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.AIAvatarMulti.AIAvatarMultiResolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.AIAvatarMulti.AIAvatarMultiResolution.VALUE_480P,
        description="Resolution of the video to generate. Must be either 480p or 720p.",
    )
    acceleration: nodetool.nodes.fal.image_to_video.AIAvatarMulti.Acceleration = Field(
        default=nodetool.nodes.fal.image_to_video.AIAvatarMulti.Acceleration.REGULAR,
        description="The acceleration level to use for generation.",
    )
    first_audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the Person 1 audio file.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.",
    )
    second_audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the Person 2 audio file.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=81,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    use_only_first_audio: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to use only the first audio file."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=181,
        description="Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.AIAvatarMulti

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class AIAvatarMultiText(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    MultiTalk generates multi-speaker avatar videos from images and text.
    video, avatar, multi-speaker, talking-head, image-to-video

    Use cases:
    - Create multi-speaker conversations
    - Generate dialogue between avatars
    - Produce interactive presentations
    - Create conversational content
    - Generate multi-character scenes
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.AIAvatarMultiText.Acceleration
    )
    AIAvatarMultiTextResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.AIAvatarMultiText.AIAvatarMultiTextResolution
    )
    Voice2: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.AIAvatarMultiText.Voice2
    )
    Voice1: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.AIAvatarMultiText.Voice1
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    second_text_input: str | OutputHandle[str] = connect_field(
        default="", description="The text input to guide video generation."
    )
    acceleration: nodetool.nodes.fal.image_to_video.AIAvatarMultiText.Acceleration = (
        Field(
            default=nodetool.nodes.fal.image_to_video.AIAvatarMultiText.Acceleration.REGULAR,
            description="The acceleration level to use for generation.",
        )
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.AIAvatarMultiText.AIAvatarMultiTextResolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.AIAvatarMultiText.AIAvatarMultiTextResolution.VALUE_480P,
        description="Resolution of the video to generate. Must be either 480p or 720p.",
    )
    first_text_input: str | OutputHandle[str] = connect_field(
        default="", description="The text input to guide video generation."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.",
    )
    voice2: nodetool.nodes.fal.image_to_video.AIAvatarMultiText.Voice2 = Field(
        default=nodetool.nodes.fal.image_to_video.AIAvatarMultiText.Voice2.ROGER,
        description="The second person's voice to use for speech generation",
    )
    voice1: nodetool.nodes.fal.image_to_video.AIAvatarMultiText.Voice1 = Field(
        default=nodetool.nodes.fal.image_to_video.AIAvatarMultiText.Voice1.SARAH,
        description="The first person's voice to use for speech generation",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=81,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=191,
        description="Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.AIAvatarMultiText

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class AIAvatarSingleText(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    MultiTalk generates talking avatar videos from an image and text input.
    video, avatar, talking-head, text-to-speech, image-to-video

    Use cases:
    - Create avatar videos from text
    - Generate talking heads with TTS
    - Produce text-driven avatars
    - Create virtual presenters
    - Generate automated spokesperson videos
    """

    AIAvatarSingleTextResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.AIAvatarSingleText.AIAvatarSingleTextResolution
    )
    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.AIAvatarSingleText.Acceleration
    )
    Voice: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.AIAvatarSingleText.Voice
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.AIAvatarSingleText.AIAvatarSingleTextResolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.AIAvatarSingleText.AIAvatarSingleTextResolution.VALUE_480P,
        description="Resolution of the video to generate. Must be either 480p or 720p.",
    )
    acceleration: nodetool.nodes.fal.image_to_video.AIAvatarSingleText.Acceleration = (
        Field(
            default=nodetool.nodes.fal.image_to_video.AIAvatarSingleText.Acceleration.REGULAR,
            description="The acceleration level to use for generation.",
        )
    )
    text_input: str | OutputHandle[str] = connect_field(
        default="", description="The text input to guide video generation."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.",
    )
    voice: nodetool.nodes.fal.image_to_video.AIAvatarSingleText.Voice = Field(
        default=nodetool.nodes.fal.image_to_video.AIAvatarSingleText.Voice(""),
        description="The voice to use for speech generation",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=42,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=136,
        description="Number of frames to generate. Must be between 81 to 129 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.AIAvatarSingleText

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class AMTFrameInterpolation(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    AMT Frame Interpolation creates smooth transitions between image frames.
    video, interpolation, frame-generation, amt, image-to-video

    Use cases:
    - Create smooth transitions between images
    - Generate intermediate frames
    - Animate image sequences
    - Create video from image pairs
    - Produce smooth motion effects
    """

    frames: list[types.Frame] | OutputHandle[list[types.Frame]] = connect_field(
        default=[], description="Frames to interpolate"
    )
    recursive_interpolation_passes: int | OutputHandle[int] = connect_field(
        default=4, description="Number of recursive interpolation passes"
    )
    output_fps: int | OutputHandle[int] = connect_field(
        default=24, description="Output frames per second"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.AMTFrameInterpolation

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class ByteDanceVideoStylize(SingleOutputGraphNode[Any], GraphNode[Any]):
    """

    ByteDance Video Stylize applies artistic styles to image-based video generation.
    video, style-transfer, artistic, bytedance, image-to-video

    Use cases:
    - Apply artistic styles to videos
    - Create stylized video content
    - Generate artistic animations
    - Produce style-transferred videos
    - Create visually unique content
    """

    style: str | OutputHandle[str] = connect_field(
        default="",
        description="The style for your character in the video. Please use a short description.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to make the stylized video from.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.ByteDanceVideoStylize

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class BytedanceLynx(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Lynx
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.BytedanceLynx.Resolution
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.BytedanceLynx.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide video generation"
    )
    resolution: nodetool.nodes.fal.image_to_video.BytedanceLynx.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.BytedanceLynx.Resolution.VALUE_720P,
        description="Resolution of the generated video (480p, 580p, or 720p)",
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.BytedanceLynx.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.BytedanceLynx.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video (16:9, 9:16, or 1:1)",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    guidance_scale_2: float | OutputHandle[float] = connect_field(
        default=2,
        description="Image guidance scale. Controls how closely the generated video follows the reference image. Higher values increase adherence to the reference image but may decrease quality.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="Reference image scale. Controls the influence of the reference image on the generated video.",
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 30.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the subject image to be used for video generation",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames in the generated video. Must be between 9 to 100.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="Bright tones, overexposed, blurred background, static, subtitles, style, works, paintings, images, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
        description="Negative prompt to guide what should not appear in the generated video",
    )
    ip_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="Identity preservation scale. Controls how closely the generated video preserves the subject's identity from the reference image.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.BytedanceLynx

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class BytedanceOmnihuman(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    OmniHuman generates video using an image of a human figure paired with an audio file. It produces vivid, high-quality videos where the characterâ€™s emotions and movements maintain a strong correlation with the audio.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    audio: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the audio file to generate the video. Audio must be under 30s long.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image used to generate the video",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.BytedanceOmnihuman

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class BytedanceSeedanceV1LiteImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Seedance 1.0 Lite
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.BytedanceSeedanceV1LiteImageToVideo.AspectRatio
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.BytedanceSeedanceV1LiteImageToVideo.Duration
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.BytedanceSeedanceV1LiteImageToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt used to generate the video"
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.BytedanceSeedanceV1LiteImageToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.BytedanceSeedanceV1LiteImageToVideo.AspectRatio.AUTO,
        description="The aspect ratio of the generated video",
    )
    duration: (
        nodetool.nodes.fal.image_to_video.BytedanceSeedanceV1LiteImageToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.BytedanceSeedanceV1LiteImageToVideo.Duration.VALUE_5,
        description="Duration of the video in seconds",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.BytedanceSeedanceV1LiteImageToVideo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.BytedanceSeedanceV1LiteImageToVideo.Resolution.VALUE_720P,
        description="Video resolution - 480p for faster generation, 720p for higher quality",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image used to generate video",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="If set to true, the safety checker will be enabled."
    )
    camera_fixed: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to fix the camera position"
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image the video ends with. Defaults to None.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed to control video generation. Use -1 for random.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.BytedanceSeedanceV1LiteImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class CogVideoX5BImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    CogVideoX-5B generates high-quality videos from images with advanced motion.
    video, generation, cogvideo, image-to-video, img2vid

    Use cases:
    - Generate videos from images
    - Create dynamic image animations
    - Produce high-quality video content
    - Animate static images
    - Generate motion from photos
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    use_rife: bool | OutputHandle[bool] = connect_field(
        default=True, description="Use RIFE for video interpolation"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL to the image to generate the video from.",
    )
    loras: list[types.LoraWeight] | OutputHandle[list[types.LoraWeight]] = (
        connect_field(
            default=[],
            description="The LoRAs to use for the image generation. We currently support one lora.",
        )
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="", description="The size of the generated video."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related video to show you.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50, description="The number of inference steps to perform."
    )
    export_fps: int | OutputHandle[int] = connect_field(
        default=16, description="The target FPS of the video"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="The negative prompt to generate video from"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.CogVideoX5BImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class CreatifyAurora(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Creatify Aurora generates creative and visually stunning videos from images with unique effects.
    video, generation, creatify, aurora, creative, effects

    Use cases:
    - Generate creative visual effects videos
    - Create stunning video animations
    - Produce artistic video content
    - Generate unique video effects
    - Create visually impressive videos
    """

    CreatifyAuroraResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.CreatifyAurora.CreatifyAuroraResolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="A text prompt to guide the video generation process."
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.CreatifyAurora.CreatifyAuroraResolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.CreatifyAurora.CreatifyAuroraResolution.VALUE_720P,
        description="The resolution of the generated video.",
    )
    audio: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the audio file to be used for video generation.",
    )
    audio_guidance_scale: float | OutputHandle[float] = connect_field(
        default=2, description="Guidance scale to be used for audio adherence."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=1, description="Guidance scale to be used for text prompt adherence."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image file to be used for video generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.CreatifyAurora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class DecartLucy14BImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Decart Lucy 14b
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.DecartLucy14BImageToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.DecartLucy14BImageToVideo.Resolution
    )

    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If set to true, the function will wait for the image to be generated and uploaded before returning the response. This will increase the latency of the function but it allows you to get the image directly in the response without going through the CDN.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.DecartLucy14BImageToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.DecartLucy14BImageToVideo.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text description of the desired video content"
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.DecartLucy14BImageToVideo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.DecartLucy14BImageToVideo.Resolution.VALUE_720P,
        description="Resolution of the generated video",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.DecartLucy14BImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class DecartLucy5bImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Lucy-5B is a model that can create 5-second I2V videos in under 5 seconds, achieving >1x RTF end-to-end
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.DecartLucy5bImageToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.DecartLucy5bImageToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text description of the desired video content"
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.DecartLucy5bImageToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.DecartLucy5bImageToVideo.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.DecartLucy5bImageToVideo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.DecartLucy5bImageToVideo.Resolution.VALUE_720P,
        description="Resolution of the generated video",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.DecartLucy5bImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class FastSvdLcm(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate short video clips from your images using SVD v1.1 at Lightning Speed
    video, animation, image-to-video, img2vid, fast

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    motion_bucket_id: int | OutputHandle[int] = connect_field(
        default=127,
        description="The motion bucket id determines the motion of the generated video. The higher the number, the more motion there will be.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=10,
        description="The FPS of the generated video. The higher the number, the faster the video will play. Total video length is 25 frames.",
    )
    steps: int | OutputHandle[int] = connect_field(
        default=4,
        description="The number of steps to run the model for. The higher the number the better the quality and longer it will take to generate.",
    )
    cond_aug: float | OutputHandle[float] = connect_field(
        default=0.02,
        description="The conditoning augmentation determines the amount of noise that will be added to the conditioning frame. The higher the number, the more noise there will be, and the less the video will look like the initial image. Increase it for more motion.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as a starting point for the generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.FastSvdLcm

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Framepack(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Framepack is an efficient Image-to-video model that autoregressively generates videos.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Framepack.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Framepack.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for video generation (max 500 characters)."
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.Framepack.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.Framepack.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the video to generate.",
    )
    resolution: nodetool.nodes.fal.image_to_video.Framepack.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.Framepack.Resolution.VALUE_480P,
        description="The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=180, description="The number of frames to generate."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image input.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=10, description="Guidance scale for the generation."
    )
    seed: str | OutputHandle[str] = connect_field(
        default="", description="The seed to use for generating the video."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt for video generation."
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=1, description="Classifier-Free Guidance scale for the generation."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Framepack

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class FramepackF1(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Framepack is an efficient Image-to-video model that autoregressively generates videos.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.FramepackF1.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.FramepackF1.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for video generation (max 500 characters)."
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.FramepackF1.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.FramepackF1.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the video to generate.",
    )
    resolution: nodetool.nodes.fal.image_to_video.FramepackF1.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.FramepackF1.Resolution.VALUE_480P,
        description="The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=180, description="The number of frames to generate."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image input.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=10, description="Guidance scale for the generation."
    )
    seed: str | OutputHandle[str] = connect_field(
        default="", description="The seed to use for generating the video."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt for video generation."
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=1, description="Classifier-Free Guidance scale for the generation."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.FramepackF1

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class FramepackFlf2v(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Framepack is an efficient Image-to-video model that autoregressively generates videos.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.FramepackFlf2v.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.FramepackFlf2v.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for video generation (max 500 characters)."
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.FramepackFlf2v.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.FramepackFlf2v.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the video to generate.",
    )
    resolution: nodetool.nodes.fal.image_to_video.FramepackFlf2v.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.FramepackFlf2v.Resolution.VALUE_480P,
        description="The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=240, description="The number of frames to generate."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image input.",
    )
    strength: float | OutputHandle[float] = connect_field(
        default=0.8,
        description="Determines the influence of the final frame on the generated video. Higher values result in the output being more heavily influenced by the last frame.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=10, description="Guidance scale for the generation."
    )
    seed: str | OutputHandle[str] = connect_field(
        default="", description="The seed to use for generating the video."
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the end image input.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt for video generation."
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=1, description="Classifier-Free Guidance scale for the generation."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.FramepackFlf2v

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class HunyuanAvatar(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    HunyuanAvatar is a High-Fidelity Audio-Driven Human Animation model for Multiple Characters .
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    text: str | OutputHandle[str] = connect_field(
        default="A cat is singing.", description="Text prompt describing the scene."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the reference image.",
    )
    turbo_mode: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, the video will be generated faster with no noticeable degradation in the visual quality.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the audio file.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=129,
        description="Number of video frames to generate at 25 FPS. If greater than the input audio length, it will capped to the length of the input audio.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.HunyuanAvatar

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class HunyuanCustom(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    HunyuanCustom revolutionizes video generation with unmatched identity consistency across multiple input types. Its innovative fusion modules and alignment networks outperform competitors, maintaining subject integrity while responding flexibly to text, image, audio, and video conditions.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.HunyuanCustom.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.HunyuanCustom.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for video generation (max 500 characters)."
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.HunyuanCustom.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.HunyuanCustom.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the video to generate.",
    )
    resolution: nodetool.nodes.fal.image_to_video.HunyuanCustom.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.HunyuanCustom.Resolution.VALUE_512P,
        description="The resolution of the video to generate. 720p generations cost 1.5x more than 480p generations.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="If set to true, the safety checker will be enabled."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=129, description="The number of frames to generate."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image input.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed to use for generating the video."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="The number of inference steps to run. Lower gets faster results, higher gets better results.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="Aerial view, aerial view, overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion, blurring, text, subtitles, static, picture, black border.",
        description="Negative prompt for video generation.",
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=7.5, description="Classifier-Free Guidance scale for the generation."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.HunyuanCustom

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class HunyuanPortrait(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    HunyuanPortrait is a diffusion-based framework for generating lifelike, temporally consistent portrait animations.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the driving video.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for generation. If None, a random seed will be used.",
    )
    use_arcface: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use ArcFace for face recognition."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the source image.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.HunyuanPortrait

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class HunyuanVideoImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Image to Video for the high-quality Hunyuan Video I2V model.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.HunyuanVideoImageToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.HunyuanVideoImageToVideo.Resolution
    )
    NumFrames: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.HunyuanVideoImageToVideo.NumFrames
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.HunyuanVideoImageToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.HunyuanVideoImageToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the video to generate.",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.HunyuanVideoImageToVideo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.HunyuanVideoImageToVideo.Resolution.VALUE_720P,
        description="The resolution of the video to generate.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image input.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed to use for generating the video."
    )
    num_frames: nodetool.nodes.fal.image_to_video.HunyuanVideoImageToVideo.NumFrames = (
        Field(
            default=nodetool.nodes.fal.image_to_video.HunyuanVideoImageToVideo.NumFrames(
                129
            ),
            description="The number of frames to generate.",
        )
    )
    i2v_stability: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Turning on I2V Stability reduces hallucination but also reduces motion.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.HunyuanVideoImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class HunyuanVideoImg2vidLora(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Image to Video for the Hunyuan Video model using a custom trained LoRA.
    video, animation, image-to-video, img2vid, lora

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed to use for generating the video."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL to the image to generate the video from. The image must be 960x544 or it will get cropped and resized to that size.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.HunyuanVideoImg2vidLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class HunyuanVideoV15ImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Hunyuan Video v1.5 generates high-quality videos from images with advanced AI capabilities.
    video, generation, hunyuan, v1.5, advanced

    Use cases:
    - Generate advanced quality videos
    - Create sophisticated animations
    - Produce high-fidelity video content
    - Generate videos with AI excellence
    - Create cutting-edge video animations
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.HunyuanVideoV15ImageToVideo.AspectRatio
    )
    HunyuanVideoV15Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.HunyuanVideoV15ImageToVideo.HunyuanVideoV15Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.HunyuanVideoV15ImageToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.HunyuanVideoV15ImageToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the video.",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.HunyuanVideoV15ImageToVideo.HunyuanVideoV15Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.HunyuanVideoV15ImageToVideo.HunyuanVideoV15Resolution.VALUE_480P,
        description="The resolution of the video.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the reference image for image-to-video generation.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Enable prompt expansion to enhance the input prompt."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for reproducibility."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28, description="The number of inference steps."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="The negative prompt to guide what not to generate."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.HunyuanVideoV15ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Kandinsky5ProImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kandinsky5 Pro generates professional quality videos from images with artistic style and control.
    video, generation, kandinsky, pro, artistic

    Use cases:
    - Generate artistic videos from images
    - Create stylized video animations
    - Produce creative video content
    - Generate videos with artistic flair
    - Create professional artistic videos
    """

    Kandinsky5ProResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Kandinsky5ProImageToVideo.Kandinsky5ProResolution
    )
    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Kandinsky5ProImageToVideo.Acceleration
    )
    Kandinsky5ProDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Kandinsky5ProImageToVideo.Kandinsky5ProDuration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.Kandinsky5ProImageToVideo.Kandinsky5ProResolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Kandinsky5ProImageToVideo.Kandinsky5ProResolution.VALUE_512P,
        description="Video resolution: 512p or 1024p.",
    )
    acceleration: (
        nodetool.nodes.fal.image_to_video.Kandinsky5ProImageToVideo.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Kandinsky5ProImageToVideo.Acceleration.REGULAR,
        description="Acceleration level for faster generation.",
    )
    duration: (
        nodetool.nodes.fal.image_to_video.Kandinsky5ProImageToVideo.Kandinsky5ProDuration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Kandinsky5ProImageToVideo.Kandinsky5ProDuration.VALUE_5S,
        description="Video duration.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=28, description=None
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as a reference for the video generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Kandinsky5ProImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoAiAvatarV2Pro(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video AI Avatar v2 Pro creates professional quality animated talking avatars with enhanced realism.
    video, avatar, kling, v2, pro, talking-head

    Use cases:
    - Create professional talking avatars
    - Animate portraits with high quality
    - Generate realistic avatar videos
    - Produce premium speaking characters
    - Create pro-grade AI avatars
    """

    prompt: str | OutputHandle[str] = connect_field(
        default=".", description="The prompt to use for the video generation."
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the audio file.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as your avatar",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoAiAvatarV2Pro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoAiAvatarV2Standard(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video AI Avatar v2 Standard creates animated talking avatars with standard quality.
    video, avatar, kling, v2, standard, talking-head

    Use cases:
    - Create standard quality talking avatars
    - Animate portraits with speech
    - Generate avatar presentations
    - Produce speaking character videos
    - Create AI-driven avatars
    """

    prompt: str | OutputHandle[str] = connect_field(
        default=".", description="The prompt to use for the video generation."
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the audio file.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as your avatar",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoAiAvatarV2Standard

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO1ImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling O1 First Frame Last Frame to Video [Pro]
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoO1ImageToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Use @Image1 to reference the start frame, @Image2 to reference the end frame.",
    )
    duration: nodetool.nodes.fal.image_to_video.KlingVideoO1ImageToVideo.Duration = (
        Field(
            default=nodetool.nodes.fal.image_to_video.KlingVideoO1ImageToVideo.Duration.VALUE_5,
            description="Video duration in seconds.",
        )
    )
    start_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image to use as the first frame of the video. Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image to use as the last frame of the video. Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoO1ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO1ReferenceToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling O1 Reference Image to Video [Pro]
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoO1ReferenceToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoO1ReferenceToVideo.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Take @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.",
    )
    duration: (
        nodetool.nodes.fal.image_to_video.KlingVideoO1ReferenceToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoO1ReferenceToVideo.Duration.VALUE_5,
        description="Video duration in seconds.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.KlingVideoO1ReferenceToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoO1ReferenceToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame.",
    )
    elements: (
        list[types.OmniVideoElementInput]
        | OutputHandle[list[types.OmniVideoElementInput]]
    ) = connect_field(
        default=[],
        description="Elements (characters/objects) to include in the video. Reference in prompt as @Element1, @Element2, etc. Maximum 7 total (elements + reference images + start image).",
    )
    images: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = connect_field(
        default=[],
        description="Additional reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 7 total (elements + reference images + start image).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoO1ReferenceToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO1StandardImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video O1 Standard generates videos with optimized standard quality from images.
    video, generation, kling, o1, standard

    Use cases:
    - Generate standard O1 quality videos
    - Create optimized video animations
    - Produce efficient video content
    - Generate balanced quality videos
    - Create standard tier animations
    """

    KlingVideoO1StandardDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoO1StandardImageToVideo.KlingVideoO1StandardDuration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Use @Image1 to reference the start frame, @Image2 to reference the end frame.",
    )
    duration: (
        nodetool.nodes.fal.image_to_video.KlingVideoO1StandardImageToVideo.KlingVideoO1StandardDuration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoO1StandardImageToVideo.KlingVideoO1StandardDuration.VALUE_5,
        description="Video duration in seconds.",
    )
    start_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image to use as the first frame of the video. Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image to use as the last frame of the video. Max file size: 10.0MB, Min width: 300px, Min height: 300px, Min aspect ratio: 0.40, Max aspect ratio: 2.50, Timeout: 20.0s",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoO1StandardImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO1StandardReferenceToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video O1 Standard generates videos using reference images for style consistency.
    video, generation, kling, o1, standard, reference

    Use cases:
    - Generate videos from reference images
    - Create style-consistent animations
    - Produce reference-guided content
    - Generate videos matching examples
    - Create standardized reference videos
    """

    KlingVideoO1StandardReferenceToVideoDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoO1StandardReferenceToVideo.KlingVideoO1StandardReferenceToVideoDuration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoO1StandardReferenceToVideo.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Take @Element1, @Element2 to reference elements and @Image1, @Image2 to reference images in order.",
    )
    duration: (
        nodetool.nodes.fal.image_to_video.KlingVideoO1StandardReferenceToVideo.KlingVideoO1StandardReferenceToVideoDuration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoO1StandardReferenceToVideo.KlingVideoO1StandardReferenceToVideoDuration.VALUE_5,
        description="Video duration in seconds.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.KlingVideoO1StandardReferenceToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoO1StandardReferenceToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame.",
    )
    elements: (
        list[types.OmniVideoElementInput]
        | OutputHandle[list[types.OmniVideoElementInput]]
    ) = connect_field(
        default=[],
        description="Elements (characters/objects) to include in the video. Reference in prompt as @Element1, @Element2, etc. Maximum 7 total (elements + reference images + start image).",
    )
    images: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = connect_field(
        default=[],
        description="Additional reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 7 total (elements + reference images + start image).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoO1StandardReferenceToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO3ProImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video O3 Pro generates professional quality videos from images with enhanced fidelity.
    video, generation, kling, o3, pro, image-to-video, img2vid

    Use cases:
    - Create professional image-driven video content
    - Generate premium animations from photos
    - Produce cinematic video clips from images
    - Create high-fidelity marketing videos from photos
    - Generate polished video sequences from images
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoO3ProImageToVideo.Duration
    )
    ShotType: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoO3ProImageToVideo.ShotType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.",
    )
    duration: nodetool.nodes.fal.image_to_video.KlingVideoO3ProImageToVideo.Duration = (
        Field(
            default=nodetool.nodes.fal.image_to_video.KlingVideoO3ProImageToVideo.Duration.VALUE_5,
            description="Video duration in seconds (3-15s).",
        )
    )
    multi_prompt: (
        list[types.KlingV3MultiPromptElement]
        | OutputHandle[list[types.KlingV3MultiPromptElement]]
    ) = connect_field(
        default=[], description="List of prompts for multi-shot video generation."
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to generate native audio for the video."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the start frame image.",
    )
    shot_type: (
        nodetool.nodes.fal.image_to_video.KlingVideoO3ProImageToVideo.ShotType
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoO3ProImageToVideo.ShotType.CUSTOMIZE,
        description="The type of multi-shot video generation.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the end frame image (optional).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoO3ProImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO3ProReferenceToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video O3 Pro generates high-fidelity videos using reference images for style and structure.
    video, generation, kling, o3, pro, reference

    Use cases:
    - Generate high-fidelity videos from reference images
    - Create premium style-consistent animations
    - Produce reference-guided professional content
    - Generate videos matching premium examples
    - Create polished reference-based video clips
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoO3ProReferenceToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoO3ProReferenceToVideo.AspectRatio
    )
    ShotType: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoO3ProReferenceToVideo.ShotType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.",
    )
    duration: (
        nodetool.nodes.fal.image_to_video.KlingVideoO3ProReferenceToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoO3ProReferenceToVideo.Duration.VALUE_5,
        description="Video duration in seconds (3-15s).",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.KlingVideoO3ProReferenceToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoO3ProReferenceToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame.",
    )
    multi_prompt: (
        list[types.KlingV3MultiPromptElement]
        | OutputHandle[list[types.KlingV3MultiPromptElement]]
    ) = connect_field(
        default=[], description="List of prompts for multi-shot video generation."
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to generate native audio for the video."
    )
    shot_type: (
        nodetool.nodes.fal.image_to_video.KlingVideoO3ProReferenceToVideo.ShotType
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoO3ProReferenceToVideo.ShotType.CUSTOMIZE,
        description="The type of multi-shot video generation.",
    )
    start_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image to use as the first frame of the video.",
    )
    images: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = connect_field(
        default=[],
        description="Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image to use as the last frame of the video.",
    )
    elements: (
        list[types.KlingV3ComboElementInput]
        | OutputHandle[list[types.KlingV3ComboElementInput]]
    ) = connect_field(
        default=[],
        description="Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoO3ProReferenceToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO3StandardImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video O3 Standard generates videos from images with balanced quality and speed.
    video, generation, kling, o3, standard, image-to-video, img2vid

    Use cases:
    - Animate static images into videos
    - Create balanced quality image animations
    - Produce efficient video content from photos
    - Generate consistent video clips from images
    - Create standard-tier visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoO3StandardImageToVideo.Duration
    )
    ShotType: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoO3StandardImageToVideo.ShotType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.",
    )
    duration: (
        nodetool.nodes.fal.image_to_video.KlingVideoO3StandardImageToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoO3StandardImageToVideo.Duration.VALUE_5,
        description="Video duration in seconds (3-15s).",
    )
    multi_prompt: (
        list[types.KlingV3MultiPromptElement]
        | OutputHandle[list[types.KlingV3MultiPromptElement]]
    ) = connect_field(
        default=[], description="List of prompts for multi-shot video generation."
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to generate native audio for the video."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the start frame image.",
    )
    shot_type: (
        nodetool.nodes.fal.image_to_video.KlingVideoO3StandardImageToVideo.ShotType
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoO3StandardImageToVideo.ShotType.CUSTOMIZE,
        description="The type of multi-shot video generation.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the end frame image (optional).",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoO3StandardImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoO3StandardReferenceToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video O3 Standard generates videos using reference images for style consistency.
    video, generation, kling, o3, standard, reference

    Use cases:
    - Generate videos from reference images
    - Create style-consistent animations
    - Produce reference-guided content
    - Generate videos matching examples
    - Create standardized reference videos
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoO3StandardReferenceToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoO3StandardReferenceToVideo.AspectRatio
    )
    ShotType: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoO3StandardReferenceToVideo.ShotType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.",
    )
    duration: (
        nodetool.nodes.fal.image_to_video.KlingVideoO3StandardReferenceToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoO3StandardReferenceToVideo.Duration.VALUE_5,
        description="Video duration in seconds (3-15s).",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.KlingVideoO3StandardReferenceToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoO3StandardReferenceToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame.",
    )
    multi_prompt: (
        list[types.KlingV3MultiPromptElement]
        | OutputHandle[list[types.KlingV3MultiPromptElement]]
    ) = connect_field(
        default=[], description="List of prompts for multi-shot video generation."
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to generate native audio for the video."
    )
    shot_type: (
        nodetool.nodes.fal.image_to_video.KlingVideoO3StandardReferenceToVideo.ShotType
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoO3StandardReferenceToVideo.ShotType.CUSTOMIZE,
        description="The type of multi-shot video generation.",
    )
    start_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image to use as the first frame of the video.",
    )
    images: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = connect_field(
        default=[],
        description="Reference images for style/appearance. Reference in prompt as @Image1, @Image2, etc. Maximum 4 total (elements + reference images) when using video.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image to use as the last frame of the video.",
    )
    elements: (
        list[types.KlingV3ComboElementInput]
        | OutputHandle[list[types.KlingV3ComboElementInput]]
    ) = connect_field(
        default=[],
        description="Elements (characters/objects) to include. Reference in prompt as @Element1, @Element2.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoO3StandardReferenceToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV15ProImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips from your images using Kling 1.5 (pro)
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoV15ProImageToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoV15ProImageToVideo.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    duration: (
        nodetool.nodes.fal.image_to_video.KlingVideoV15ProImageToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoV15ProImageToVideo.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.KlingVideoV15ProImageToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoV15ProImageToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description=None,
    )
    static_mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image for Static Brush Application Area (Mask image created by users using the motion brush)",
    )
    dynamic_masks: list[types.DynamicMask] | OutputHandle[list[types.DynamicMask]] = (
        connect_field(default=[], description="List of dynamic masks")
    )
    tail_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to be used for the end of the video",
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoV15ProImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV16ProElements(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips from your multiple image references using Kling 1.6 (pro)
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoV16ProElements.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoV16ProElements.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    duration: nodetool.nodes.fal.image_to_video.KlingVideoV16ProElements.Duration = (
        Field(
            default=nodetool.nodes.fal.image_to_video.KlingVideoV16ProElements.Duration.VALUE_5,
            description="The duration of the generated video in seconds",
        )
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.KlingVideoV16ProElements.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoV16ProElements.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )
    input_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="List of image URLs to use for video generation. Supports up to 4 images.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoV16ProElements

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV16StandardElements(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips from your multiple image references using Kling 1.6 (standard)
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoV16StandardElements.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoV16StandardElements.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    duration: (
        nodetool.nodes.fal.image_to_video.KlingVideoV16StandardElements.Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoV16StandardElements.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.KlingVideoV16StandardElements.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoV16StandardElements.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )
    input_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="List of image URLs to use for video generation. Supports up to 4 images.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoV16StandardElements

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV16StandardImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips from your images using Kling 1.6 (std)
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoV16StandardImageToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    duration: (
        nodetool.nodes.fal.image_to_video.KlingVideoV16StandardImageToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoV16StandardImageToVideo.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description=None,
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoV16StandardImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV1ProAiAvatar(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling AI Avatar Pro
    video, animation, image-to-video, img2vid, professional

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    prompt: str | OutputHandle[str] = connect_field(
        default=".", description="The prompt to use for the video generation."
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the audio file.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as your avatar",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoV1ProAiAvatar

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV1StandardAiAvatar(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling AI Avatar
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    prompt: str | OutputHandle[str] = connect_field(
        default=".", description="The prompt to use for the video generation."
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the audio file.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as your avatar",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoV1StandardAiAvatar

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV1StandardImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video v1 Standard generates videos from images with balanced quality.
    video, generation, kling, standard, image-to-video

    Use cases:
    - Generate standard quality videos
    - Create balanced video animations
    - Produce efficient video content
    - Generate videos for web use
    - Create moderate quality outputs
    """

    KlingVideoV1StandardDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoV1StandardImageToVideo.KlingVideoV1StandardDuration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for the video"
    )
    duration: (
        nodetool.nodes.fal.image_to_video.KlingVideoV1StandardImageToVideo.KlingVideoV1StandardDuration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoV1StandardImageToVideo.KlingVideoV1StandardDuration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to be used for the video",
    )
    static_mask: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image for Static Brush Application Area (Mask image created by users using the motion brush)",
    )
    dynamic_masks: list[types.DynamicMask] | OutputHandle[list[types.DynamicMask]] = (
        connect_field(default=[], description="List of dynamic masks")
    )
    tail_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to be used for the end of the video",
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoV1StandardImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV21ProImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling 2.1 Pro is an advanced endpoint for the Kling 2.1 model, offering professional-grade videos with enhanced visual fidelity, precise camera movements, and dynamic motion control, perfect for cinematic storytelling.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoV21ProImageToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    duration: (
        nodetool.nodes.fal.image_to_video.KlingVideoV21ProImageToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoV21ProImageToVideo.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    tail_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to be used for the end of the video",
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to be used for the video",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoV21ProImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV25TurboStandardImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video
    video, animation, image-to-video, img2vid, fast

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoV25TurboStandardImageToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    duration: (
        nodetool.nodes.fal.image_to_video.KlingVideoV25TurboStandardImageToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoV25TurboStandardImageToVideo.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to be used for the video",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoV25TurboStandardImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV26ProImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video v2.6 Pro generates professional quality videos with latest model improvements.
    video, generation, kling, v2.6, pro

    Use cases:
    - Generate professional v2.6 videos
    - Create latest quality animations
    - Produce premium video content
    - Generate advanced videos
    - Create pro-tier animations
    """

    KlingVideoV26ProDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoV26ProImageToVideo.KlingVideoV26ProDuration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    duration: (
        nodetool.nodes.fal.image_to_video.KlingVideoV26ProImageToVideo.KlingVideoV26ProDuration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoV26ProImageToVideo.KlingVideoV26ProDuration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    voice_ids: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase.",
    )
    start_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to be used for the video",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to be used for the end of the video",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoV26ProImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV3ProImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video V3 Pro generates professional quality videos from images with enhanced visual fidelity using the latest V3 model.
    video, generation, kling, v3, pro, image-to-video

    Use cases:
    - Create professional-grade video animations from images
    - Generate cinematic video content with precise motion
    - Produce high-fidelity product showcase videos
    - Animate images with enhanced visual quality
    - Create premium video content for advertising
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoV3ProImageToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoV3ProImageToVideo.AspectRatio
    )
    ShotType: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoV3ProImageToVideo.ShotType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.",
    )
    voice_ids: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice",
    )
    duration: nodetool.nodes.fal.image_to_video.KlingVideoV3ProImageToVideo.Duration = (
        Field(
            default=nodetool.nodes.fal.image_to_video.KlingVideoV3ProImageToVideo.Duration.VALUE_5,
            description="The duration of the generated video in seconds",
        )
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase.",
    )
    multi_prompt: (
        list[types.KlingV3MultiPromptElement]
        | OutputHandle[list[types.KlingV3MultiPromptElement]]
    ) = connect_field(
        default=[],
        description="List of prompts for multi-shot video generation. If provided, divides the video into multiple shots.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.KlingVideoV3ProImageToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoV3ProImageToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )
    start_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to be used for the video",
    )
    shot_type: (
        nodetool.nodes.fal.image_to_video.KlingVideoV3ProImageToVideo.ShotType
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoV3ProImageToVideo.ShotType.CUSTOMIZE,
        description="The type of multi-shot video generation. Required when multi_prompt is provided.",
    )
    elements: (
        list[types.KlingV3ComboElementInput]
        | OutputHandle[list[types.KlingV3ComboElementInput]]
    ) = connect_field(
        default=[],
        description="Elements (characters/objects) to include in the video. Each example can either be an image set (frontal + reference images) or a video. Reference in prompt as @Element1, @Element2, etc.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to be used for the end of the video",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoV3ProImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoV3StandardImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Kling Video V3 Standard generates videos from images with balanced quality and speed using the latest V3 model.
    video, generation, kling, v3, standard, image-to-video

    Use cases:
    - Animate static images into short video clips
    - Create engaging social media content from photos
    - Generate product demonstration videos
    - Produce marketing and promotional videos
    - Transform images into cinematic animations
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoV3StandardImageToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoV3StandardImageToVideo.AspectRatio
    )
    ShotType: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingVideoV3StandardImageToVideo.ShotType
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for video generation. Either prompt or multi_prompt must be provided, but not both.",
    )
    voice_ids: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="Optional Voice IDs for video generation. Reference voices in your prompt with <<<voice_1>>> and <<<voice_2>>> (maximum 2 voices per task). Get voice IDs from the kling video create-voice endpoint: https://fal.ai/models/fal-ai/kling-video/create-voice",
    )
    duration: (
        nodetool.nodes.fal.image_to_video.KlingVideoV3StandardImageToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoV3StandardImageToVideo.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to generate native audio for the video. Supports Chinese and English voice output. Other languages are automatically translated to English. For English speech, use lowercase letters; for acronyms or proper nouns, use uppercase.",
    )
    multi_prompt: (
        list[types.KlingV3MultiPromptElement]
        | OutputHandle[list[types.KlingV3MultiPromptElement]]
    ) = connect_field(
        default=[],
        description="List of prompts for multi-shot video generation. If provided, divides the video into multiple shots.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.KlingVideoV3StandardImageToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoV3StandardImageToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )
    start_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to be used for the video",
    )
    shot_type: (
        nodetool.nodes.fal.image_to_video.KlingVideoV3StandardImageToVideo.ShotType
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.KlingVideoV3StandardImageToVideo.ShotType.CUSTOMIZE,
        description="The type of multi-shot video generation. Required when multi_prompt is provided.",
    )
    elements: (
        list[types.KlingV3ComboElementInput]
        | OutputHandle[list[types.KlingV3ComboElementInput]]
    ) = connect_field(
        default=[],
        description="Elements (characters/objects) to include in the video. Each example can either be an image set (frontal + reference images) or a video. Reference in prompt as @Element1, @Element2, etc.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to be used for the end of the video",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blur, distort, and low quality", description=None
    )
    cfg_scale: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoV3StandardImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class LTXImageToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    LTX Video generates temporally consistent videos from images.
    video, generation, ltx, temporal, image-to-video

    Use cases:
    - Generate temporally consistent videos
    - Create smooth image animations
    - Produce coherent video sequences
    - Animate with temporal awareness
    - Generate fluid motion videos
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3, description="The guidance scale to use."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed to use for random number generation."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="The number of inference steps to take."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly",
        description="The negative prompt to generate the video from.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to generate the video from.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.LTXImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class LiveAvatar(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Live Avatar creates animated talking avatars from portrait images with realistic lip-sync and expressions.
    video, avatar, talking-head, animation, portrait

    Use cases:
    - Create talking avatar videos
    - Animate portrait images
    - Generate lip-synced avatars
    - Produce speaking character videos
    - Create animated presenters
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LiveAvatar.Acceleration
    )

    frames_per_clip: int | OutputHandle[int] = connect_field(
        default=48,
        description="Number of frames per clip. Must be a multiple of 4. Higher values = smoother but slower generation.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="A text prompt describing the scene and character. Helps guide the video generation style and context.",
    )
    acceleration: nodetool.nodes.fal.image_to_video.LiveAvatar.Acceleration = Field(
        default=nodetool.nodes.fal.image_to_video.LiveAvatar.Acceleration.NONE,
        description="Acceleration level for faster video decoding",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the reference image for avatar generation. The character in this image will be animated.",
    )
    num_clips: int | OutputHandle[int] = connect_field(
        default=10,
        description="Number of video clips to generate. Each clip is approximately 3 seconds. Set higher for longer videos.",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the driving audio file (WAV or MP3). The avatar will be animated to match this audio.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for reproducible generation."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=0,
        description="Classifier-free guidance scale. Higher values follow the prompt more closely.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Enable safety checker for content moderation."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.LiveAvatar

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class LivePortrait(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Transfer expression from a video to a portrait.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    smile: float | OutputHandle[float] = connect_field(
        default=0, description="Amount to smile"
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the video to drive the lip syncing.",
    )
    flag_stitching: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to enable stitching. Recommended to set to True.",
    )
    eyebrow: float | OutputHandle[float] = connect_field(
        default=0, description="Amount to raise or lower eyebrows"
    )
    wink: float | OutputHandle[float] = connect_field(
        default=0, description="Amount to wink"
    )
    rotate_pitch: float | OutputHandle[float] = connect_field(
        default=0, description="Amount to rotate the face in pitch"
    )
    blink: float | OutputHandle[float] = connect_field(
        default=0, description="Amount to blink the eyes"
    )
    scale: float | OutputHandle[float] = connect_field(
        default=2.3, description="Scaling factor for the face crop."
    )
    eee: float | OutputHandle[float] = connect_field(
        default=0, description="Amount to shape mouth in 'eee' position"
    )
    flag_pasteback: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to paste-back/stitch the animated face cropping from the face-cropping space to the original image space.",
    )
    pupil_y: float | OutputHandle[float] = connect_field(
        default=0, description="Amount to move pupils vertically"
    )
    rotate_yaw: float | OutputHandle[float] = connect_field(
        default=0, description="Amount to rotate the face in yaw"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to be animated",
    )
    woo: float | OutputHandle[float] = connect_field(
        default=0, description="Amount to shape mouth in 'woo' position"
    )
    aaa: float | OutputHandle[float] = connect_field(
        default=0, description="Amount to open mouth in 'aaa' shape"
    )
    flag_do_rot: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to conduct the rotation when flag_do_crop is True.",
    )
    flag_relative: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use relative motion."
    )
    flag_eye_retargeting: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable eye retargeting."
    )
    flag_lip_zero: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to set the lip to closed state before animation. Only takes effect when flag_eye_retargeting and flag_lip_retargeting are False.",
    )
    batch_size: int | OutputHandle[int] = connect_field(
        default=32,
        description="Batch size for the model. The larger the batch size, the faster the model will run, but the more memory it will consume.",
    )
    rotate_roll: float | OutputHandle[float] = connect_field(
        default=0, description="Amount to rotate the face in roll"
    )
    dsize: int | OutputHandle[int] = connect_field(
        default=512, description="Size of the output image."
    )
    vy_ratio: float | OutputHandle[float] = connect_field(
        default=-0.125,
        description="Vertical offset ratio for face crop. Positive values move up, negative values move down.",
    )
    pupil_x: float | OutputHandle[float] = connect_field(
        default=0, description="Amount to move pupils horizontally"
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to enable the safety checker. If enabled, the model will check if the input image contains a face before processing it. The safety checker will process the input image",
    )
    vx_ratio: float | OutputHandle[float] = connect_field(
        default=0, description="Horizontal offset ratio for face crop."
    )
    flag_lip_retargeting: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable lip retargeting."
    )
    flag_do_crop: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to crop the source portrait to the face-cropping space.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.LivePortrait

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class LongcatVideoDistilledImageToVideo480P(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LongCat Video Distilled
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo480P.VideoWriteMode
    )
    VideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo480P.VideoOutputType
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo480P.VideoQuality
    )

    video_write_mode: (
        nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo480P.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo480P.VideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    video_output_type: (
        nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo480P.VideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo480P.VideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k",
        description="The prompt to guide the video generation.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=15, description="The frame rate of the generated video."
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to generate a video from.",
    )
    video_quality: (
        nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo480P.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo480P.VideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable safety checker."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=12, description="The number of inference steps to use."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=162, description="The number of frames to generate."
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo480P

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class LongcatVideoDistilledImageToVideo720P(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LongCat Video Distilled
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo720P.VideoWriteMode
    )
    VideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo720P.VideoOutputType
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo720P.VideoQuality
    )

    video_write_mode: (
        nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo720P.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo720P.VideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    video_output_type: (
        nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo720P.VideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo720P.VideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k",
        description="The prompt to guide the video generation.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=30, description="The frame rate of the generated video."
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    num_refine_inference_steps: int | OutputHandle[int] = connect_field(
        default=12, description="The number of inference steps to use for refinement."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to generate a video from.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable safety checker."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=12, description="The number of inference steps to use."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=162, description="The number of frames to generate."
    )
    video_quality: (
        nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo720P.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo720P.VideoQuality.HIGH,
        description="The quality of the generated video.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.LongcatVideoDistilledImageToVideo720P

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class LongcatVideoImageToVideo480P(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LongCat Video
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.Acceleration
    )
    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.VideoWriteMode
    )
    VideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.VideoOutputType
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.VideoQuality
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k",
        description="The prompt to guide the video generation.",
    )
    acceleration: (
        nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.Acceleration.REGULAR,
        description="The acceleration level to use for the video generation.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=15, description="The frame rate of the generated video."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=4, description="The guidance scale to use for the video generation."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=162, description="The number of frames to generate."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable safety checker."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
        description="The negative prompt to use for the video generation.",
    )
    video_write_mode: (
        nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.VideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    video_output_type: (
        nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.VideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.VideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to generate a video from.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: (
        nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P.VideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40,
        description="The number of inference steps to use for the video generation.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo480P

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class LongcatVideoImageToVideo720P(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LongCat Video
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.Acceleration
    )
    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.VideoWriteMode
    )
    VideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.VideoOutputType
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.VideoQuality
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="First-person view from the cockpit of a Formula 1 car. The driver's gloved hands firmly grip the intricate, carbon-fiber steering wheel adorned with numerous colorful buttons and a vibrant digital display showing race data. Beyond the windshield, a sun-drenched racetrack stretches ahead, lined with cheering spectators in the grandstands. Several rival cars are visible in the distance, creating a dynamic sense of competition. The sky above is a clear, brilliant blue, reflecting the exhilarating atmosphere of a high-speed race. high resolution 4k",
        description="The prompt to guide the video generation.",
    )
    acceleration: (
        nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.Acceleration.REGULAR,
        description="The acceleration level to use for the video generation.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=30, description="The frame rate of the generated video."
    )
    num_refine_inference_steps: int | OutputHandle[int] = connect_field(
        default=40, description="The number of inference steps to use for refinement."
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=4, description="The guidance scale to use for the video generation."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=162, description="The number of frames to generate."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable safety checker."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards",
        description="The negative prompt to use for the video generation.",
    )
    video_write_mode: (
        nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.VideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    video_output_type: (
        nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.VideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.VideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to generate a video from.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: (
        nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P.VideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40,
        description="The number of inference steps to use for the video generation.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.LongcatVideoImageToVideo720P

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Ltx219BDistilledImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LTX-2 19B Distilled generates videos efficiently using knowledge distillation from the 19B model.
    video, generation, ltx-2, 19b, distilled, efficient

    Use cases:
    - Generate videos efficiently with distilled model
    - Create fast quality video animations
    - Produce optimized video content
    - Generate videos with good performance
    - Create balanced quality-speed videos
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.Acceleration
    )
    CameraLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.CameraLora
    )
    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.VideoWriteMode
    )
    VideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.VideoOutputType
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.VideoQuality
    )
    InterpolationDirection: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.InterpolationDirection
    )

    use_multiscale: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt used for the generation."
    )
    acceleration: (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.Acceleration.NONE,
        description="The acceleration level to use.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    fps: float | OutputHandle[float] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    camera_lora: (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.CameraLora
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.CameraLora.NONE,
        description="The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="auto", description="The size of the generated video."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    camera_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as the end of the video.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
        description="The negative prompt to generate the video from.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )
    video_write_mode: (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.VideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    video_output_type: (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.VideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.VideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the image to use for the video generation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to generate the video from.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.VideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    end_image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the end image to use for the video generation.",
    )
    interpolation_direction: (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.InterpolationDirection
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo.InterpolationDirection.FORWARD,
        description="The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Ltx219BDistilledImageToVideoLora(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LTX-2 19B Distilled with LoRA combines efficient generation with custom-trained models.
    video, generation, ltx-2, 19b, distilled, lora

    Use cases:
    - Generate videos with custom distilled model
    - Create efficient specialized content
    - Produce fast domain-specific videos
    - Generate with optimized custom model
    - Create quick customized animations
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.Acceleration
    )
    CameraLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.CameraLora
    )
    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.VideoWriteMode
    )
    VideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.VideoOutputType
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.VideoQuality
    )
    InterpolationDirection: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.InterpolationDirection
    )

    use_multiscale: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt used for the generation."
    )
    acceleration: (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.Acceleration.NONE,
        description="The acceleration level to use.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    fps: float | OutputHandle[float] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    loras: list[types.LoRAInput] | OutputHandle[list[types.LoRAInput]] = connect_field(
        default=[], description="The LoRAs to use for the generation."
    )
    camera_lora: (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.CameraLora
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.CameraLora.NONE,
        description="The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="auto", description="The size of the generated video."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    camera_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as the end of the video.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
        description="The negative prompt to generate the video from.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )
    video_write_mode: (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.VideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    video_output_type: (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.VideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.VideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the image to use for the video generation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to generate the video from.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    video_quality: (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.VideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    end_image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the end image to use for the video generation.",
    )
    interpolation_direction: (
        nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.InterpolationDirection
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora.InterpolationDirection.FORWARD,
        description="The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Ltx219BDistilledImageToVideoLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Ltx219BImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LTX-2 19B generates high-quality videos from images using the powerful 19-billion parameter model.
    video, generation, ltx-2, 19b, large-model

    Use cases:
    - Generate high-quality videos with large model
    - Create detailed video animations
    - Produce superior video content
    - Generate videos with powerful AI
    - Create premium video animations
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.Acceleration
    )
    CameraLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.CameraLora
    )
    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.VideoWriteMode
    )
    VideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.VideoOutputType
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.VideoQuality
    )
    InterpolationDirection: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.InterpolationDirection
    )

    use_multiscale: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt used for the generation."
    )
    acceleration: nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.Acceleration = (
        Field(
            default=nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.Acceleration.REGULAR,
            description="The acceleration level to use.",
        )
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    fps: float | OutputHandle[float] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    camera_lora: nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.CameraLora = (
        Field(
            default=nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.CameraLora.NONE,
            description="The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
        )
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="auto", description="The size of the generated video."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    camera_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as the end of the video.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
        description="The negative prompt to generate the video from.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3, description="The guidance scale to use."
    )
    video_write_mode: (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.VideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    video_output_type: (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.VideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.VideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )
    image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the image to use for the video generation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to generate the video from.",
    )
    video_quality: (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.VideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    end_image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the end image to use for the video generation.",
    )
    interpolation_direction: (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.InterpolationDirection
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo.InterpolationDirection.FORWARD,
        description="The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40, description="The number of inference steps to use."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Ltx219BImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Ltx219BImageToVideoLora(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    LTX-2 19B with LoRA enables custom-trained 19B models for specialized video generation.
    video, generation, ltx-2, 19b, lora, custom

    Use cases:
    - Generate videos with custom 19B model
    - Create specialized video content
    - Produce domain-specific animations
    - Generate with fine-tuned large model
    - Create customized video animations
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.Acceleration
    )
    CameraLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.CameraLora
    )
    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.VideoWriteMode
    )
    VideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.VideoOutputType
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.VideoQuality
    )
    InterpolationDirection: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.InterpolationDirection
    )

    use_multiscale: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt used for the generation."
    )
    acceleration: (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.Acceleration.REGULAR,
        description="The acceleration level to use.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    fps: float | OutputHandle[float] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    loras: list[types.LoRAInput] | OutputHandle[list[types.LoRAInput]] = connect_field(
        default=[], description="The LoRAs to use for the generation."
    )
    camera_lora: (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.CameraLora
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.CameraLora.NONE,
        description="The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    video_size: str | OutputHandle[str] = connect_field(
        default="auto", description="The size of the generated video."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    camera_lora_scale: float | OutputHandle[float] = connect_field(
        default=1,
        description="The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as the end of the video.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.",
        description="The negative prompt to generate the video from.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3, description="The guidance scale to use."
    )
    video_write_mode: (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.VideoWriteMode.BALANCED,
        description="The write mode of the generated video.",
    )
    video_output_type: (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.VideoOutputType
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.VideoOutputType.X264_MP4,
        description="The output type of the generated video.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames to generate."
    )
    image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the image to use for the video generation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to generate the video from.",
    )
    video_quality: (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.VideoQuality.HIGH,
        description="The quality of the generated video.",
    )
    sync_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    end_image_strength: float | OutputHandle[float] = connect_field(
        default=1,
        description="The strength of the end image to use for the video generation.",
    )
    interpolation_direction: (
        nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.InterpolationDirection
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora.InterpolationDirection.FORWARD,
        description="The direction to interpolate the image sequence in. 'Forward' goes from the start image to the end image, 'Backward' goes from the end image to the start image.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40, description="The number of inference steps to use."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Ltx219BImageToVideoLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class LtxVideo13bDevImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate videos from prompts and images using LTX Video-0.9.7 13B and custom LoRA
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LtxVideo13bDevImageToVideo.Resolution
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LtxVideo13bDevImageToVideo.AspectRatio
    )

    second_pass_skip_initial_steps: int | OutputHandle[int] = connect_field(
        default=17,
        description="The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.",
    )
    first_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="Number of inference steps during the first pass."
    )
    frame_rate: int | OutputHandle[int] = connect_field(
        default=30, description="The frame rate of the video."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide generation"
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to reverse the video."
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to expand the prompt using a language model.",
    )
    loras: list[types.LoRAWeight] | OutputHandle[list[types.LoRAWeight]] = (
        connect_field(default=[], description="LoRA weights to use for generation")
    )
    second_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="Number of inference steps during the second pass."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames in the video."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, inconsistent motion, blurry, jittery, distorted",
        description="Negative prompt for generation",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.LtxVideo13bDevImageToVideo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LtxVideo13bDevImageToVideo.Resolution.VALUE_720P,
        description="Resolution of the generated video (480p or 720p).",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.LtxVideo13bDevImageToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LtxVideo13bDevImageToVideo.AspectRatio.AUTO,
        description="The aspect ratio of the video.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image URL for Image-to-Video task",
    )
    constant_rate_factor: int | OutputHandle[int] = connect_field(
        default=35,
        description="The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.",
    )
    first_pass_skip_final_steps: int | OutputHandle[int] = connect_field(
        default=3,
        description="Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.LtxVideo13bDevImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class LtxVideo13bDistilledImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate videos from prompts and images using LTX Video-0.9.7 13B Distilled and custom LoRA
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LtxVideo13bDistilledImageToVideo.Resolution
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LtxVideo13bDistilledImageToVideo.AspectRatio
    )

    second_pass_skip_initial_steps: int | OutputHandle[int] = connect_field(
        default=5,
        description="The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.",
    )
    first_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the first pass."
    )
    frame_rate: int | OutputHandle[int] = connect_field(
        default=30, description="The frame rate of the video."
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to reverse the video."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide generation"
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to expand the prompt using a language model.",
    )
    loras: list[types.LoRAWeight] | OutputHandle[list[types.LoRAWeight]] = (
        connect_field(default=[], description="LoRA weights to use for generation")
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames in the video."
    )
    second_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the second pass."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, inconsistent motion, blurry, jittery, distorted",
        description="Negative prompt for generation",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.LtxVideo13bDistilledImageToVideo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LtxVideo13bDistilledImageToVideo.Resolution.VALUE_720P,
        description="Resolution of the generated video (480p or 720p).",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.LtxVideo13bDistilledImageToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LtxVideo13bDistilledImageToVideo.AspectRatio.AUTO,
        description="The aspect ratio of the video.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image URL for Image-to-Video task",
    )
    constant_rate_factor: int | OutputHandle[int] = connect_field(
        default=35,
        description="The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.",
    )
    first_pass_skip_final_steps: int | OutputHandle[int] = connect_field(
        default=1,
        description="Number of inference steps to skip in the final steps of the first pass. By skipping some steps at the end, the first pass can focus on larger changes instead of smaller details.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.LtxVideo13bDistilledImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class LtxVideoLoraImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate videos from prompts and images using LTX Video-0.9.7 and custom LoRA
    video, animation, image-to-video, img2vid, lora

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LtxVideoLoraImageToVideo.Resolution
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LtxVideoLoraImageToVideo.AspectRatio
    )

    number_of_steps: int | OutputHandle[int] = connect_field(
        default=30, description="The number of inference steps to use."
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.LtxVideoLoraImageToVideo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LtxVideoLoraImageToVideo.Resolution.VALUE_720P,
        description="The resolution of the video.",
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to reverse the video."
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.LtxVideoLoraImageToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LtxVideoLoraImageToVideo.AspectRatio.AUTO,
        description="The aspect ratio of the video.",
    )
    frame_rate: int | OutputHandle[int] = connect_field(
        default=25, description="The frame rate of the video."
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to expand the prompt using the LLM."
    )
    number_of_frames: int | OutputHandle[int] = connect_field(
        default=89, description="The number of frames in the video."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as input.",
    )
    loras: list[types.LoRAWeight] | OutputHandle[list[types.LoRAWeight]] = (
        connect_field(default=[], description="The LoRA weights to use for generation.")
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed to use for generation."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, low quality, low resolution, inconsistent motion, jittery, distorted",
        description="The negative prompt to use.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.LtxVideoLoraImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Ltxv13b098DistilledImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate long videos from prompts and images using LTX Video-0.9.8 13B Distilled and custom LoRA
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltxv13b098DistilledImageToVideo.Resolution
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Ltxv13b098DistilledImageToVideo.AspectRatio
    )

    second_pass_skip_initial_steps: int | OutputHandle[int] = connect_field(
        default=5,
        description="The number of inference steps to skip in the initial steps of the second pass. By skipping some steps at the beginning, the second pass can focus on smaller details instead of larger changes.",
    )
    first_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the first pass."
    )
    frame_rate: int | OutputHandle[int] = connect_field(
        default=24, description="The frame rate of the video."
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to reverse the video."
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide generation"
    )
    expand_prompt: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to expand the prompt using a language model.",
    )
    temporal_adain_factor: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="The factor for adaptive instance normalization (AdaIN) applied to generated video chunks after the first. This can help deal with a gradual increase in saturation/contrast in the generated video by normalizing the color distribution across the video. A high value will ensure the color distribution is more consistent across the video, while a low value will allow for more variation in color distribution.",
    )
    loras: list[types.LoRAWeight] | OutputHandle[list[types.LoRAWeight]] = (
        connect_field(default=[], description="LoRA weights to use for generation")
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable the safety checker."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="The number of frames in the video."
    )
    second_pass_num_inference_steps: int | OutputHandle[int] = connect_field(
        default=8, description="Number of inference steps during the second pass."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="worst quality, inconsistent motion, blurry, jittery, distorted",
        description="Negative prompt for generation",
    )
    enable_detail_pass: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to use a detail pass. If True, the model will perform a second pass to refine the video and enhance details. This incurs a 2.0x cost multiplier on the base price.",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.Ltxv13b098DistilledImageToVideo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltxv13b098DistilledImageToVideo.Resolution.VALUE_720P,
        description="Resolution of the generated video.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.Ltxv13b098DistilledImageToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Ltxv13b098DistilledImageToVideo.AspectRatio.AUTO,
        description="The aspect ratio of the video.",
    )
    tone_map_compression_ratio: float | OutputHandle[float] = connect_field(
        default=0,
        description="The compression ratio for tone mapping. This is used to compress the dynamic range of the video to improve visual quality. A value of 0.0 means no compression, while a value of 1.0 means maximum compression.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Image URL for Image-to-Video task",
    )
    constant_rate_factor: int | OutputHandle[int] = connect_field(
        default=29,
        description="The constant rate factor (CRF) to compress input media with. Compressed input media more closely matches the model's training data, which can improve motion quality.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Ltxv13b098DistilledImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class LumaDreamMachine(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips from your images using Luma Dream Machine v1.5. Supports various aspect ratios and optional end-frame blending.
    video, generation, animation, blending, aspect-ratio, img2vid, image-to-video

    Use cases:
    - Create seamless video loops
    - Generate video transitions
    - Transform images into animations
    - Create motion graphics
    - Produce video content
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LumaDreamMachine.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: nodetool.nodes.fal.image_to_video.LumaDreamMachine.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.image_to_video.LumaDreamMachine.AspectRatio.RATIO_16_9,
            description="The aspect ratio of the generated video",
        )
    )
    loop: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether the video should loop (end of video is blended with the beginning)",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="An image to blend the end of the video with",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description=None,
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.LumaDreamMachine

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class LumaDreamMachineRay2FlashImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Ray2 Flash is a fast video generative model capable of creating realistic visuals with natural, coherent motion.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2FlashImageToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2FlashImageToVideo.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2FlashImageToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2FlashImageToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2FlashImageToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2FlashImageToVideo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2FlashImageToVideo.Resolution.VALUE_540P,
        description="The resolution of the generated video (720p costs 2x more, 1080p costs 4x more)",
    )
    loop: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether the video should loop (end of video is blended with the beginning)",
    )
    duration: (
        nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2FlashImageToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2FlashImageToVideo.Duration.VALUE_5S,
        description="The duration of the generated video",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Initial image to start the video from. Can be used together with end_image_url.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Final image to end the video with. Can be used together with image_url.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2FlashImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class LumaDreamMachineRay2ImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Ray2 is a large-scale video generative model capable of creating realistic visuals with natural, coherent motion.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2ImageToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2ImageToVideo.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2ImageToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2ImageToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2ImageToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2ImageToVideo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2ImageToVideo.Resolution.VALUE_540P,
        description="The resolution of the generated video (720p costs 2x more, 1080p costs 4x more)",
    )
    loop: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether the video should loop (end of video is blended with the beginning)",
    )
    duration: (
        nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2ImageToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2ImageToVideo.Duration.VALUE_5S,
        description="The duration of the generated video",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Initial image to start the video from. Can be used together with end_image_url.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Final image to end the video with. Can be used together with image_url.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.LumaDreamMachineRay2ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class MinimaxHailuo02FastImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Create blazing fast and economical videos with MiniMax Hailuo-02 Image To Video API at 512p resolution
    video, animation, image-to-video, img2vid, fast

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.MinimaxHailuo02FastImageToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    duration: (
        nodetool.nodes.fal.image_to_video.MinimaxHailuo02FastImageToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.MinimaxHailuo02FastImageToVideo.Duration.VALUE_6,
        description="The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution.",
    )
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the model's prompt optimizer"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description=None,
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.MinimaxHailuo02FastImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class MinimaxHailuo02ProImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    MiniMax Hailuo-02 Image To Video API (Pro, 1080p): Advanced image-to-video generation model with 1080p resolution
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the model's prompt optimizer"
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Optional URL of the image to use as the last frame of the video",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description=None,
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.MinimaxHailuo02ProImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class MinimaxHailuo23FastProImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    MiniMax Hailuo 2.3 Fast [Pro] (Image to Video)
    video, animation, image-to-video, img2vid, fast, professional

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for video generation"
    )
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the model's prompt optimizer"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.MinimaxHailuo23FastProImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class MinimaxHailuo23FastStandardImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    MiniMax Hailuo 2.3 Fast [Standard] (Image to Video)
    video, animation, image-to-video, img2vid, fast, professional

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.MinimaxHailuo23FastStandardImageToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for video generation"
    )
    duration: (
        nodetool.nodes.fal.image_to_video.MinimaxHailuo23FastStandardImageToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.MinimaxHailuo23FastStandardImageToVideo.Duration.VALUE_6,
        description="The duration of the video in seconds.",
    )
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the model's prompt optimizer"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.MinimaxHailuo23FastStandardImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class MinimaxHailuo23StandardImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    MiniMax Hailuo 2.3 [Standard] (Image to Video)
    video, animation, image-to-video, img2vid, professional

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.MinimaxHailuo23StandardImageToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for video generation"
    )
    duration: (
        nodetool.nodes.fal.image_to_video.MinimaxHailuo23StandardImageToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.MinimaxHailuo23StandardImageToVideo.Duration.VALUE_6,
        description="The duration of the video in seconds.",
    )
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the model's prompt optimizer"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.MinimaxHailuo23StandardImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class MinimaxVideo01DirectorImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips more accurately with respect to initial image, natural language descriptions, and using camera movement instructions for shot control.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Text prompt for video generation. Camera movement instructions can be added using square brackets (e.g. [Pan left] or [Zoom in]). You can use up to 3 combined movements per prompt. Supported movements: Truck left/right, Pan left/right, Push in/Pull out, Pedestal up/down, Tilt up/down, Zoom in/out, Shake, Tracking shot, Static shot. For example: [Truck left, Pan right, Zoom in]. For a more detailed guide, refer https://sixth-switch-2ac.notion.site/T2V-01-Director-Model-Tutorial-with-camera-movement-1886c20a98eb80f395b8e05291ad8645",
    )
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the model's prompt optimizer"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.MinimaxVideo01DirectorImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class MinimaxVideo01LiveImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips from your images using MiniMax Video model
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the model's prompt optimizer"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.MinimaxVideo01LiveImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class MinimaxVideo01SubjectReference(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips maintaining consistent, realistic facial features and identity across dynamic video content
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the model's prompt optimizer"
    )
    subject_reference_image: types.ImageRef | OutputHandle[types.ImageRef] = (
        connect_field(
            default=types.ImageRef(
                type="image", uri="", asset_id=None, data=None, metadata=None
            ),
            description="URL of the subject reference image to use for consistent subject appearance",
        )
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.MinimaxVideo01SubjectReference

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class MoonvalleyMareyI2v(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate a video starting from an image as the first frame with Marey, a generative video model trained exclusively on fully licensed data.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.MoonvalleyMareyI2v.Duration
    )
    Dimensions: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.MoonvalleyMareyI2v.Dimensions
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate a video from"
    )
    duration: nodetool.nodes.fal.image_to_video.MoonvalleyMareyI2v.Duration = Field(
        default=nodetool.nodes.fal.image_to_video.MoonvalleyMareyI2v.Duration.VALUE_5S,
        description="The duration of the generated video.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as the first frame of the video.",
    )
    dimensions: nodetool.nodes.fal.image_to_video.MoonvalleyMareyI2v.Dimensions = Field(
        default=nodetool.nodes.fal.image_to_video.MoonvalleyMareyI2v.Dimensions.VALUE_1920X1080,
        description="The dimensions of the generated video in width x height format.",
    )
    guidance_scale: str | OutputHandle[str] = connect_field(
        default="",
        description="Controls how strongly the generation is guided by the prompt (0-20). Higher values follow the prompt more closely.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default=-1,
        description="Seed for random number generation. Use -1 for random seed each run.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="<synthetic> <scene cut> low-poly, flat shader, bad rigging, stiff animation, uncanny eyes, low-quality textures, looping glitch, cheap effect, overbloom, bloom spam, default lighting, game asset, stiff face, ugly specular, AI artifacts",
        description="Negative prompt used to guide the model away from undesirable features.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.MoonvalleyMareyI2v

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Musetalk(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    MuseTalk is a real-time high quality audio-driven lip-syncing model. Use MuseTalk to animate a face with your own audio.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    source_video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the source video",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the audio",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Musetalk

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class OmniHumanV15(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    OmniHuman v1.5 generates realistic human videos from images.
    video, human, realistic, bytedance, image-to-video

    Use cases:
    - Generate realistic human videos
    - Create human motion animations
    - Produce lifelike character videos
    - Generate human performances
    - Create realistic human content
    """

    OmniHumanV15Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.OmniHumanV15.OmniHumanV15Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt used to guide the video generation."
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.OmniHumanV15.OmniHumanV15Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.OmniHumanV15.OmniHumanV15Resolution.VALUE_1080P,
        description="The resolution of the generated video. Defaults to 1080p. 720p generation is faster and higher in quality. 1080p generation is limited to 30s audio and 720p generation is limited to 60s audio.",
    )
    turbo_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Generate a video at a faster rate with a slight quality trade-off.",
    )
    audio: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="The URL of the audio file to generate the video. Audio must be under 30s long for 1080p generation and under 60s long for 720p generation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image used to generate the video",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.OmniHumanV15

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class OviImageToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Ovi
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    seed: str | OutputHandle[str] = connect_field(
        default="",
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30, description="The number of inference steps."
    )
    audio_negative_prompt: str | OutputHandle[str] = connect_field(
        default="robotic, muffled, echo, distorted",
        description="Negative prompt for audio generation.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="jitter, bad hands, blur, distortion",
        description="Negative prompt for video generation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image URL to guide video generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.OviImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PikaV15Pikaffects(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Pika Effects are AI-powered video effects designed to modify objects, characters, and environments in a fun, engaging, and visually compelling manner.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Pikaffect: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PikaV15Pikaffects.Pikaffect
    )

    pikaffect: nodetool.nodes.fal.image_to_video.PikaV15Pikaffects.Pikaffect = Field(
        default=nodetool.nodes.fal.image_to_video.PikaV15Pikaffects.Pikaffect(""),
        description="The Pikaffect to apply",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide the effect"
    )
    seed: str | OutputHandle[str] = connect_field(
        default="", description="The seed for the random number generator"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to guide the model"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PikaV15Pikaffects

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PikaV21ImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Turn photos into mind-blowing, dynamic videos. Your images can can come to life with sharp details, impressive character control and cinematic camera moves.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PikaV21ImageToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    resolution: nodetool.nodes.fal.image_to_video.PikaV21ImageToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.image_to_video.PikaV21ImageToVideo.Resolution.VALUE_720P,
            description="The resolution of the generated video",
        )
    )
    duration: int | OutputHandle[int] = connect_field(
        default=5, description="The duration of the generated video in seconds"
    )
    seed: str | OutputHandle[str] = connect_field(
        default="", description="The seed for the random number generator"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A negative prompt to guide the model"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description=None,
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PikaV21ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PikaV22Pikaframes(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Pika
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PikaV22Pikaframes.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Default prompt for all transitions. Individual transition prompts override this.",
    )
    resolution: nodetool.nodes.fal.image_to_video.PikaV22Pikaframes.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.PikaV22Pikaframes.Resolution.VALUE_720P,
        description="The resolution of the generated video",
    )
    transitions: (
        list[types.KeyframeTransition] | OutputHandle[list[types.KeyframeTransition]]
    ) = connect_field(
        default=[],
        description="Configuration for each transition. Length must be len(image_urls) - 1. Total duration of all transitions must not exceed 25 seconds. If not provided, uses default 5-second transitions with the global prompt.",
    )
    seed: str | OutputHandle[str] = connect_field(
        default="", description="The seed for the random number generator"
    )
    images: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = connect_field(
        default=[],
        description="URLs of keyframe images (2-5 images) to create transitions between",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A negative prompt to guide the model"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PikaV22Pikaframes

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PikaV2TurboImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Turbo is the model to use when you feel the need for speed. Turn your image to stunning video up to 3x faster â€“ all with high quality outputs.
    video, animation, image-to-video, img2vid, fast

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PikaV2TurboImageToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    resolution: nodetool.nodes.fal.image_to_video.PikaV2TurboImageToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.image_to_video.PikaV2TurboImageToVideo.Resolution.VALUE_720P,
            description="The resolution of the generated video",
        )
    )
    duration: int | OutputHandle[int] = connect_field(
        default=5, description="The duration of the generated video in seconds"
    )
    seed: str | OutputHandle[str] = connect_field(
        default="", description="The seed for the random number generator"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A negative prompt to guide the model"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description=None,
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PikaV2TurboImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseSwap(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Pixverse
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Mode: typing.ClassVar[type] = nodetool.nodes.fal.image_to_video.PixverseSwap.Mode
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseSwap.Resolution
    )

    original_sound_switch: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to keep the original audio"
    )
    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the external video to swap",
    )
    keyframe_id: int | OutputHandle[int] = connect_field(
        default=1, description="The keyframe ID (from 1 to the last frame position)"
    )
    mode: nodetool.nodes.fal.image_to_video.PixverseSwap.Mode = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseSwap.Mode.PERSON,
        description="The swap mode to use",
    )
    resolution: nodetool.nodes.fal.image_to_video.PixverseSwap.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseSwap.Resolution.VALUE_720P,
        description="The output resolution (1080p not supported)",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the target image for swapping",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseSwap

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV35Effects(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate high quality video clips with different effects using PixVerse v3.5
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV35Effects.Duration
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV35Effects.Resolution
    )
    Effect: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV35Effects.Effect
    )

    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )
    duration: nodetool.nodes.fal.image_to_video.PixverseV35Effects.Duration = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV35Effects.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    resolution: nodetool.nodes.fal.image_to_video.PixverseV35Effects.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV35Effects.Resolution.VALUE_720P,
        description="The resolution of the generated video.",
    )
    effect: nodetool.nodes.fal.image_to_video.PixverseV35Effects.Effect = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV35Effects.Effect(""),
        description="The effect to apply to the video",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Optional URL of the image to use as the first frame. If not provided, generates from text",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV35Effects

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV35ImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate high quality video clips from text and image prompts using PixVerse v3.5
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideo.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    resolution: nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideo.Resolution.VALUE_720P,
            description="The resolution of the generated video",
        )
    )
    duration: nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideo.Duration = (
        Field(
            default=nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideo.Duration.VALUE_5,
            description="The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds",
        )
    )
    style: (
        nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideo.Style
        | OutputHandle[nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideo.Style]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV35ImageToVideoFast(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate high quality video clips from text and image prompts quickly using PixVerse v3.5 Fast
    video, animation, image-to-video, img2vid, fast

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideoFast.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    resolution: (
        nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideoFast.Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideoFast.Resolution.VALUE_720P,
        description="The resolution of the generated video",
    )
    style: (
        nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideoFast.Style
        | OutputHandle[
            nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideoFast.Style
        ]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV35ImageToVideoFast

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV35Transition(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Create seamless transition between images using PixVerse v3.5
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV35Transition.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV35Transition.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV35Transition.Duration
    )

    first_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.PixverseV35Transition.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV35Transition.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: nodetool.nodes.fal.image_to_video.PixverseV35Transition.Resolution = (
        Field(
            default=nodetool.nodes.fal.image_to_video.PixverseV35Transition.Resolution.VALUE_720P,
            description="The resolution of the generated video",
        )
    )
    style: (
        nodetool.nodes.fal.image_to_video.PixverseV35Transition.Style
        | OutputHandle[nodetool.nodes.fal.image_to_video.PixverseV35Transition.Style]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for the transition"
    )
    duration: nodetool.nodes.fal.image_to_video.PixverseV35Transition.Duration = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV35Transition.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the last frame",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV35Transition

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV45Effects(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate high quality video clips with different effects using PixVerse v4.5
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV45Effects.Duration
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV45Effects.Resolution
    )
    Effect: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV45Effects.Effect
    )

    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )
    duration: nodetool.nodes.fal.image_to_video.PixverseV45Effects.Duration = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV45Effects.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    resolution: nodetool.nodes.fal.image_to_video.PixverseV45Effects.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV45Effects.Resolution.VALUE_720P,
        description="The resolution of the generated video.",
    )
    effect: nodetool.nodes.fal.image_to_video.PixverseV45Effects.Effect = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV45Effects.Effect(""),
        description="The effect to apply to the video",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Optional URL of the image to use as the first frame. If not provided, generates from text",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV45Effects

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV45ImageToVideoFast(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate fast high quality video clips from text and image prompts using PixVerse v4.5
    video, animation, image-to-video, img2vid, fast

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV45ImageToVideoFast.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    resolution: (
        nodetool.nodes.fal.image_to_video.PixverseV45ImageToVideoFast.Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV45ImageToVideoFast.Resolution.VALUE_720P,
        description="The resolution of the generated video",
    )
    style: (
        nodetool.nodes.fal.image_to_video.PixverseV45ImageToVideoFast.Style
        | OutputHandle[
            nodetool.nodes.fal.image_to_video.PixverseV45ImageToVideoFast.Style
        ]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    camera_movement: (
        nodetool.nodes.fal.image_to_video.PixverseV45ImageToVideoFast.CameraMovement
        | OutputHandle[
            nodetool.nodes.fal.image_to_video.PixverseV45ImageToVideoFast.CameraMovement
        ]
        | None
    ) = connect_field(
        default=None, description="The type of camera movement to apply to the video"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV45ImageToVideoFast

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV45Transition(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Create seamless transition between images using PixVerse v4.5
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV45Transition.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV45Transition.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV45Transition.Duration
    )

    first_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.PixverseV45Transition.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV45Transition.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: nodetool.nodes.fal.image_to_video.PixverseV45Transition.Resolution = (
        Field(
            default=nodetool.nodes.fal.image_to_video.PixverseV45Transition.Resolution.VALUE_720P,
            description="The resolution of the generated video",
        )
    )
    style: (
        nodetool.nodes.fal.image_to_video.PixverseV45Transition.Style
        | OutputHandle[nodetool.nodes.fal.image_to_video.PixverseV45Transition.Style]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for the transition"
    )
    duration: nodetool.nodes.fal.image_to_video.PixverseV45Transition.Duration = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV45Transition.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the last frame",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV45Transition

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV4Effects(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate high quality video clips with different effects using PixVerse v4
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV4Effects.Duration
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV4Effects.Resolution
    )
    Effect: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV4Effects.Effect
    )

    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )
    duration: nodetool.nodes.fal.image_to_video.PixverseV4Effects.Duration = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV4Effects.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    resolution: nodetool.nodes.fal.image_to_video.PixverseV4Effects.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV4Effects.Resolution.VALUE_720P,
        description="The resolution of the generated video.",
    )
    effect: nodetool.nodes.fal.image_to_video.PixverseV4Effects.Effect = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV4Effects.Effect(""),
        description="The effect to apply to the video",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Optional URL of the image to use as the first frame. If not provided, generates from text",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV4Effects

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV4ImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate high quality video clips from text and image prompts using PixVerse v4
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideo.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    resolution: nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideo.Resolution.VALUE_720P,
            description="The resolution of the generated video",
        )
    )
    duration: nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideo.Duration = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideo.Duration.VALUE_5,
        description="The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds",
    )
    style: (
        nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideo.Style
        | OutputHandle[nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideo.Style]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    camera_movement: (
        nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideo.CameraMovement
        | OutputHandle[
            nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideo.CameraMovement
        ]
        | None
    ) = connect_field(
        default=None, description="The type of camera movement to apply to the video"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV4ImageToVideoFast(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate fast high quality video clips from text and image prompts using PixVerse v4
    video, animation, image-to-video, img2vid, fast

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideoFast.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    resolution: (
        nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideoFast.Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideoFast.Resolution.VALUE_720P,
        description="The resolution of the generated video",
    )
    style: (
        nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideoFast.Style
        | OutputHandle[
            nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideoFast.Style
        ]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    camera_movement: (
        nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideoFast.CameraMovement
        | OutputHandle[
            nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideoFast.CameraMovement
        ]
        | None
    ) = connect_field(
        default=None, description="The type of camera movement to apply to the video"
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV4ImageToVideoFast

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV55Effects(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Pixverse
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV55Effects.Duration
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV55Effects.Resolution
    )
    Effect: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV55Effects.Effect
    )

    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )
    duration: nodetool.nodes.fal.image_to_video.PixverseV55Effects.Duration = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV55Effects.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    resolution: nodetool.nodes.fal.image_to_video.PixverseV55Effects.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV55Effects.Resolution.VALUE_720P,
        description="The resolution of the generated video.",
    )
    thinking_type: (
        nodetool.nodes.fal.image_to_video.PixverseV55Effects.ThinkingType
        | OutputHandle[
            nodetool.nodes.fal.image_to_video.PixverseV55Effects.ThinkingType
        ]
        | None
    ) = connect_field(
        default=None,
        description="Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision",
    )
    effect: nodetool.nodes.fal.image_to_video.PixverseV55Effects.Effect = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV55Effects.Effect(""),
        description="The effect to apply to the video",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Optional URL of the image to use as the first frame. If not provided, generates from text",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV55Effects

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV55ImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Pixverse
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV55ImageToVideo.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV55ImageToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    resolution: nodetool.nodes.fal.image_to_video.PixverseV55ImageToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.image_to_video.PixverseV55ImageToVideo.Resolution.VALUE_720P,
            description="The resolution of the generated video",
        )
    )
    duration: nodetool.nodes.fal.image_to_video.PixverseV55ImageToVideo.Duration = (
        Field(
            default=nodetool.nodes.fal.image_to_video.PixverseV55ImageToVideo.Duration.VALUE_5,
            description="The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds",
        )
    )
    style: (
        nodetool.nodes.fal.image_to_video.PixverseV55ImageToVideo.Style
        | OutputHandle[nodetool.nodes.fal.image_to_video.PixverseV55ImageToVideo.Style]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    thinking_type: (
        nodetool.nodes.fal.image_to_video.PixverseV55ImageToVideo.ThinkingType
        | OutputHandle[
            nodetool.nodes.fal.image_to_video.PixverseV55ImageToVideo.ThinkingType
        ]
        | None
    ) = connect_field(
        default=None,
        description="Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision",
    )
    generate_multi_clip_switch: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Enable multi-clip generation with dynamic camera changes",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )
    generate_audio_switch: bool | OutputHandle[bool] = connect_field(
        default=False, description="Enable audio generation (BGM, SFX, dialogue)"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV55ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV55Transition(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Pixverse
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV55Transition.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV55Transition.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV55Transition.Duration
    )

    first_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.PixverseV55Transition.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV55Transition.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: nodetool.nodes.fal.image_to_video.PixverseV55Transition.Resolution = (
        Field(
            default=nodetool.nodes.fal.image_to_video.PixverseV55Transition.Resolution.VALUE_720P,
            description="The resolution of the generated video",
        )
    )
    style: (
        nodetool.nodes.fal.image_to_video.PixverseV55Transition.Style
        | OutputHandle[nodetool.nodes.fal.image_to_video.PixverseV55Transition.Style]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    thinking_type: (
        nodetool.nodes.fal.image_to_video.PixverseV55Transition.ThinkingType
        | OutputHandle[
            nodetool.nodes.fal.image_to_video.PixverseV55Transition.ThinkingType
        ]
        | None
    ) = connect_field(
        default=None,
        description="Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for the transition"
    )
    duration: nodetool.nodes.fal.image_to_video.PixverseV55Transition.Duration = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV55Transition.Duration.VALUE_5,
        description="The duration of the generated video in seconds. Longer durations cost more. 1080p videos are limited to 5 or 8 seconds",
    )
    generate_audio_switch: bool | OutputHandle[bool] = connect_field(
        default=False, description="Enable audio generation (BGM, SFX, dialogue)"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the last frame",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV55Transition

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV56ImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate high-quality videos from images with Pixverse v5.6.
    video, generation, pixverse, v5.6, image-to-video, img2vid

    Use cases:
    - Animate photos into professional video clips
    - Create dynamic product showcase videos
    - Generate stylized video content from artwork
    - Produce high-resolution social media animations
    - Transform static images with various visual styles
    """

    PixverseV56Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56Resolution
    )
    PixverseV56Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56Duration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt describing the desired video motion"
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56Resolution.VALUE_720P,
        description="The resolution quality of the output video",
    )
    duration: (
        nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56Duration.FIVE_SECONDS,
        description="The duration of the generated video in seconds",
    )
    style: (
        nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56Style
        | OutputHandle[
            nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56Style
        ]
        | None
    ) = connect_field(default=None, description="Optional visual style for the video")
    thinking_type: (
        nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56ThinkingType
        | OutputHandle[
            nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo.PixverseV56ThinkingType
        ]
        | None
    ) = connect_field(default=None, description="Thinking mode for video generation")
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to transform into a video",
    )
    generate_audio_switch: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to generate audio for the video"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Optional seed for reproducible generation"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="What to avoid in the generated video"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV56ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV56Transition(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Pixverse v5.6 Transition creates smooth video transitions between two images with professional effects.
    video, transition, pixverse, v5.6, effects

    Use cases:
    - Create smooth transitions between images
    - Generate professional video effects
    - Produce seamless image morphing
    - Create transition animations
    - Generate video connecting two scenes
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV56Transition.AspectRatio
    )
    PixverseV56TransitionResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV56Transition.PixverseV56TransitionResolution
    )
    PixverseV56TransitionDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV56Transition.PixverseV56TransitionDuration
    )

    first_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.PixverseV56Transition.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV56Transition.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.PixverseV56Transition.PixverseV56TransitionResolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV56Transition.PixverseV56TransitionResolution.VALUE_720P,
        description="The resolution of the generated video",
    )
    style: (
        nodetool.nodes.fal.image_to_video.PixverseV56Transition.Style
        | OutputHandle[nodetool.nodes.fal.image_to_video.PixverseV56Transition.Style]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    thinking_type: (
        nodetool.nodes.fal.image_to_video.PixverseV56Transition.ThinkingType
        | OutputHandle[
            nodetool.nodes.fal.image_to_video.PixverseV56Transition.ThinkingType
        ]
        | None
    ) = connect_field(
        default=None,
        description="Prompt optimization mode: 'enabled' to optimize, 'disabled' to turn off, 'auto' for model decision",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for the transition"
    )
    duration: (
        nodetool.nodes.fal.image_to_video.PixverseV56Transition.PixverseV56TransitionDuration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV56Transition.PixverseV56TransitionDuration.VALUE_5,
        description="The duration of the generated video in seconds. 1080p videos are limited to 5 or 8 seconds",
    )
    generate_audio_switch: bool | OutputHandle[bool] = connect_field(
        default=False, description="Enable audio generation (BGM, SFX, dialogue)"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the last frame",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV56Transition

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV5Effects(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate high quality video clips with different effects using PixVerse v5
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV5Effects.Duration
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV5Effects.Resolution
    )
    Effect: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV5Effects.Effect
    )

    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )
    duration: nodetool.nodes.fal.image_to_video.PixverseV5Effects.Duration = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV5Effects.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    resolution: nodetool.nodes.fal.image_to_video.PixverseV5Effects.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV5Effects.Resolution.VALUE_720P,
        description="The resolution of the generated video.",
    )
    effect: nodetool.nodes.fal.image_to_video.PixverseV5Effects.Effect = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV5Effects.Effect(""),
        description="The effect to apply to the video",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="Optional URL of the image to use as the first frame. If not provided, generates from text",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV5Effects

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV5ImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate high quality video clips from text and image prompts using PixVerse v5
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV5ImageToVideo.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV5ImageToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(default="", description=None)
    resolution: nodetool.nodes.fal.image_to_video.PixverseV5ImageToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.image_to_video.PixverseV5ImageToVideo.Resolution.VALUE_720P,
            description="The resolution of the generated video",
        )
    )
    duration: nodetool.nodes.fal.image_to_video.PixverseV5ImageToVideo.Duration = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV5ImageToVideo.Duration.VALUE_5,
        description="The duration of the generated video in seconds. 8s videos cost double. 1080p videos are limited to 5 seconds",
    )
    style: (
        nodetool.nodes.fal.image_to_video.PixverseV5ImageToVideo.Style
        | OutputHandle[nodetool.nodes.fal.image_to_video.PixverseV5ImageToVideo.Style]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV5ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixverseV5Transition(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Create seamless transition between images using PixVerse v5
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV5Transition.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV5Transition.Resolution
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PixverseV5Transition.Duration
    )

    first_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.PixverseV5Transition.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.image_to_video.PixverseV5Transition.AspectRatio.RATIO_16_9,
            description="The aspect ratio of the generated video",
        )
    )
    resolution: nodetool.nodes.fal.image_to_video.PixverseV5Transition.Resolution = (
        Field(
            default=nodetool.nodes.fal.image_to_video.PixverseV5Transition.Resolution.VALUE_720P,
            description="The resolution of the generated video",
        )
    )
    style: (
        nodetool.nodes.fal.image_to_video.PixverseV5Transition.Style
        | OutputHandle[nodetool.nodes.fal.image_to_video.PixverseV5Transition.Style]
        | None
    ) = connect_field(default=None, description="The style of the generated video")
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt for the transition"
    )
    duration: nodetool.nodes.fal.image_to_video.PixverseV5Transition.Duration = Field(
        default=nodetool.nodes.fal.image_to_video.PixverseV5Transition.Duration.VALUE_5,
        description="The duration of the generated video in seconds",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of the model will output the same video every time.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the last frame",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt to be used for the generation"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixverseV5Transition

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Sadtalker(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    FaceModelResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Sadtalker.FaceModelResolution
    )
    Preprocess: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Sadtalker.Preprocess
    )

    pose_style: int | OutputHandle[int] = connect_field(
        default=0, description="The style of the pose"
    )
    source_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the source image",
    )
    driven_audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the driven audio",
    )
    face_enhancer: (
        nodetool.nodes.fal.image_to_video.Sadtalker.FaceEnhancer
        | OutputHandle[nodetool.nodes.fal.image_to_video.Sadtalker.FaceEnhancer]
        | None
    ) = connect_field(default=None, description="The type of face enhancer to use")
    expression_scale: float | OutputHandle[float] = connect_field(
        default=1, description="The scale of the expression"
    )
    face_model_resolution: (
        nodetool.nodes.fal.image_to_video.Sadtalker.FaceModelResolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Sadtalker.FaceModelResolution.VALUE_256,
        description="The resolution of the face model",
    )
    still_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to use still mode. Fewer head motion, works with preprocess `full`.",
    )
    preprocess: nodetool.nodes.fal.image_to_video.Sadtalker.Preprocess = Field(
        default=nodetool.nodes.fal.image_to_video.Sadtalker.Preprocess.CROP,
        description="The type of preprocessing to use",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Sadtalker

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class SadtalkerReference(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    FaceModelResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.SadtalkerReference.FaceModelResolution
    )
    Preprocess: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.SadtalkerReference.Preprocess
    )

    pose_style: int | OutputHandle[int] = connect_field(
        default=0, description="The style of the pose"
    )
    source_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the source image",
    )
    reference_pose_video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the reference video",
    )
    driven_audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the driven audio",
    )
    face_enhancer: (
        nodetool.nodes.fal.image_to_video.SadtalkerReference.FaceEnhancer
        | OutputHandle[
            nodetool.nodes.fal.image_to_video.SadtalkerReference.FaceEnhancer
        ]
        | None
    ) = connect_field(default=None, description="The type of face enhancer to use")
    expression_scale: float | OutputHandle[float] = connect_field(
        default=1, description="The scale of the expression"
    )
    face_model_resolution: (
        nodetool.nodes.fal.image_to_video.SadtalkerReference.FaceModelResolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.SadtalkerReference.FaceModelResolution.VALUE_256,
        description="The resolution of the face model",
    )
    still_mode: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to use still mode. Fewer head motion, works with preprocess `full`.",
    )
    preprocess: nodetool.nodes.fal.image_to_video.SadtalkerReference.Preprocess = Field(
        default=nodetool.nodes.fal.image_to_video.SadtalkerReference.Preprocess.CROP,
        description="The type of preprocessing to use",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.SadtalkerReference

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class SeeDanceV15ProImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    SeeDance v1.5 Pro generates high-quality dance videos from images.
    video, dance, animation, seedance, bytedance, image-to-video

    Use cases:
    - Animate photos into dance videos
    - Create dance choreography from images
    - Generate dance performances
    - Produce music video content
    - Create dance training materials
    """

    SeeDanceV15ProAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.SeeDanceV15ProImageToVideo.SeeDanceV15ProAspectRatio
    )
    SeeDanceV15ProDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.SeeDanceV15ProImageToVideo.SeeDanceV15ProDuration
    )
    SeeDanceV15ProResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.SeeDanceV15ProImageToVideo.SeeDanceV15ProResolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt used to generate the video"
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.SeeDanceV15ProImageToVideo.SeeDanceV15ProAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.SeeDanceV15ProImageToVideo.SeeDanceV15ProAspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    duration: (
        nodetool.nodes.fal.image_to_video.SeeDanceV15ProImageToVideo.SeeDanceV15ProDuration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.SeeDanceV15ProImageToVideo.SeeDanceV15ProDuration.VALUE_5,
        description="Duration of the video in seconds",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video"
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.SeeDanceV15ProImageToVideo.SeeDanceV15ProResolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.SeeDanceV15ProImageToVideo.SeeDanceV15ProResolution.VALUE_720P,
        description="Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image used to generate video",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="If set to true, the safety checker will be enabled."
    )
    camera_fixed: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to fix the camera position"
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image the video ends with. Defaults to None.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed to control video generation. Use -1 for random.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.SeeDanceV15ProImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class SeeDanceV1LiteReferenceToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    SeeDance v1 Lite generates lightweight dance videos using reference images.
    video, dance, lite, reference, seedance, image-to-video

    Use cases:
    - Generate efficient dance videos
    - Create reference-based animations
    - Produce lightweight dance content
    - Generate quick dance outputs
    - Create optimized dance videos
    """

    SeeDanceV1LiteAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.SeeDanceV1LiteReferenceToVideo.SeeDanceV1LiteAspectRatio
    )
    SeeDanceV1LiteDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.SeeDanceV1LiteReferenceToVideo.SeeDanceV1LiteDuration
    )
    SeeDanceV1LiteResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.SeeDanceV1LiteReferenceToVideo.SeeDanceV1LiteResolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt used to generate the video"
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.SeeDanceV1LiteReferenceToVideo.SeeDanceV1LiteAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.SeeDanceV1LiteReferenceToVideo.SeeDanceV1LiteAspectRatio.AUTO,
        description="The aspect ratio of the generated video",
    )
    duration: (
        nodetool.nodes.fal.image_to_video.SeeDanceV1LiteReferenceToVideo.SeeDanceV1LiteDuration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.SeeDanceV1LiteReferenceToVideo.SeeDanceV1LiteDuration.VALUE_5,
        description="Duration of the video in seconds",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.SeeDanceV1LiteReferenceToVideo.SeeDanceV1LiteResolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.SeeDanceV1LiteReferenceToVideo.SeeDanceV1LiteResolution.VALUE_720P,
        description="Video resolution - 480p for faster generation, 720p for higher quality",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="If set to true, the safety checker will be enabled."
    )
    camera_fixed: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to fix the camera position"
    )
    reference_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[], description="Reference images to generate the video with."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed to control video generation. Use -1 for random.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.SeeDanceV1LiteReferenceToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class SeeDanceV1ProFastImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    SeeDance v1 Pro Fast generates dance videos quickly from images.
    video, dance, fast, seedance, bytedance, image-to-video

    Use cases:
    - Rapidly generate dance videos
    - Quick dance animation
    - Fast dance prototypes
    - Create dance previews
    - Efficient dance video generation
    """

    SeeDanceV1ProFastAspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.SeeDanceV1ProFastImageToVideo.SeeDanceV1ProFastAspectRatio
    )
    SeeDanceV1ProFastDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.SeeDanceV1ProFastImageToVideo.SeeDanceV1ProFastDuration
    )
    SeeDanceV1ProFastResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.SeeDanceV1ProFastImageToVideo.SeeDanceV1ProFastResolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt used to generate the video"
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.SeeDanceV1ProFastImageToVideo.SeeDanceV1ProFastAspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.SeeDanceV1ProFastImageToVideo.SeeDanceV1ProFastAspectRatio.AUTO,
        description="The aspect ratio of the generated video",
    )
    duration: (
        nodetool.nodes.fal.image_to_video.SeeDanceV1ProFastImageToVideo.SeeDanceV1ProFastDuration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.SeeDanceV1ProFastImageToVideo.SeeDanceV1ProFastDuration.VALUE_5,
        description="Duration of the video in seconds",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.SeeDanceV1ProFastImageToVideo.SeeDanceV1ProFastResolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.SeeDanceV1ProFastImageToVideo.SeeDanceV1ProFastResolution.VALUE_1080P,
        description="Video resolution - 480p for faster generation, 720p for balance, 1080p for higher quality",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image used to generate video",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="If set to true, the safety checker will be enabled."
    )
    camera_fixed: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to fix the camera position"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed to control video generation. Use -1 for random.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.SeeDanceV1ProFastImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class SkyreelsI2v(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    SkyReels V1 is the first and most advanced open-source human-centric video foundation model. By fine-tuning HunyuanVideo on O(10M) high-quality film and television clips
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.SkyreelsI2v.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate the video from."
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.SkyreelsI2v.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.SkyreelsI2v.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the output video",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image input.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=6, description="Guidance scale for generation (between 1.0 and 20.0)"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for generation. If not provided, a random seed will be used.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of denoising steps (between 1 and 50). Higher values give better quality but take longer.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Negative prompt to guide generation away from certain attributes.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.SkyreelsI2v

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class StableVideoImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Stable Video generates consistent video animations from images.
    video, generation, stable, consistent, image-to-video

    Use cases:
    - Generate stable video animations
    - Create consistent motion
    - Produce reliable video outputs
    - Animate images consistently
    - Generate predictable videos
    """

    motion_bucket_id: int | OutputHandle[int] = connect_field(
        default=127,
        description="The motion bucket id determines the motion of the generated video. The higher the number, the more motion there will be.",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=25, description="The frames per second of the generated video."
    )
    cond_aug: float | OutputHandle[float] = connect_field(
        default=0.02,
        description="The conditoning augmentation determines the amount of noise that will be added to the conditioning frame. The higher the number, the more noise there will be, and the less the video will look like the initial image. Increase it for more motion.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="The same seed and the same prompt given to the same version of Stable Diffusion will output the same image every time.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The URL of the image to use as a starting point for the generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.StableVideoImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class VeedFabric10(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Fabric 1.0
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.VeedFabric10.Resolution
    )

    resolution: nodetool.nodes.fal.image_to_video.VeedFabric10.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.VeedFabric10.Resolution(""),
        description="Resolution",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description=None,
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description=None,
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.VeedFabric10

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class VeedFabric10Fast(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Fabric 1.0 Fast
    video, animation, image-to-video, img2vid, fast

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.VeedFabric10Fast.Resolution
    )

    resolution: nodetool.nodes.fal.image_to_video.VeedFabric10Fast.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.VeedFabric10Fast.Resolution(""),
        description="Resolution",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description=None,
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description=None,
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.VeedFabric10Fast

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Veo31FastFirstLastFrameToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Veo 3.1 Fast
    video, animation, image-to-video, img2vid, fast

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo31FastFirstLastFrameToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo31FastFirstLastFrameToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo31FastFirstLastFrameToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt describing the video you want to generate",
    )
    duration: (
        nodetool.nodes.fal.image_to_video.Veo31FastFirstLastFrameToVideo.Duration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Veo31FastFirstLastFrameToVideo.Duration.VALUE_8S,
        description="The duration of the generated video.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.Veo31FastFirstLastFrameToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Veo31FastFirstLastFrameToVideo.AspectRatio.AUTO,
        description="The aspect ratio of the generated video.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    auto_fix: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.Veo31FastFirstLastFrameToVideo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Veo31FastFirstLastFrameToVideo.Resolution.VALUE_720P,
        description="The resolution of the generated video.",
    )
    first_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the first frame of the video",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    last_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the last frame of the video",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A negative prompt to guide the video generation."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Veo31FastFirstLastFrameToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Veo31FastImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Veo 3.1 Fast
    video, animation, image-to-video, img2vid, fast

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo31FastImageToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo31FastImageToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo31FastImageToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt describing the video you want to generate",
    )
    duration: nodetool.nodes.fal.image_to_video.Veo31FastImageToVideo.Duration = Field(
        default=nodetool.nodes.fal.image_to_video.Veo31FastImageToVideo.Duration.VALUE_8S,
        description="The duration of the generated video.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.Veo31FastImageToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Veo31FastImageToVideo.AspectRatio.AUTO,
        description="The aspect ratio of the generated video. Only 16:9 and 9:16 are supported.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    auto_fix: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.",
    )
    resolution: nodetool.nodes.fal.image_to_video.Veo31FastImageToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.image_to_video.Veo31FastImageToVideo.Resolution.VALUE_720P,
            description="The resolution of the generated video.",
        )
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A negative prompt to guide the video generation."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Veo31FastImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Veo31FirstLastFrameToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Veo 3.1
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo31FirstLastFrameToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo31FirstLastFrameToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo31FirstLastFrameToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt describing the video you want to generate",
    )
    duration: nodetool.nodes.fal.image_to_video.Veo31FirstLastFrameToVideo.Duration = (
        Field(
            default=nodetool.nodes.fal.image_to_video.Veo31FirstLastFrameToVideo.Duration.VALUE_8S,
            description="The duration of the generated video.",
        )
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.Veo31FirstLastFrameToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Veo31FirstLastFrameToVideo.AspectRatio.AUTO,
        description="The aspect ratio of the generated video.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    auto_fix: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.Veo31FirstLastFrameToVideo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Veo31FirstLastFrameToVideo.Resolution.VALUE_720P,
        description="The resolution of the generated video.",
    )
    first_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the first frame of the video",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    last_frame: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the last frame of the video",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A negative prompt to guide the video generation."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Veo31FirstLastFrameToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Veo31ImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Veo 3.1
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo31ImageToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo31ImageToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo31ImageToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt describing the video you want to generate",
    )
    duration: nodetool.nodes.fal.image_to_video.Veo31ImageToVideo.Duration = Field(
        default=nodetool.nodes.fal.image_to_video.Veo31ImageToVideo.Duration.VALUE_8S,
        description="The duration of the generated video.",
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.Veo31ImageToVideo.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.image_to_video.Veo31ImageToVideo.AspectRatio.AUTO,
            description="The aspect ratio of the generated video. Only 16:9 and 9:16 are supported.",
        )
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    auto_fix: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.",
    )
    resolution: nodetool.nodes.fal.image_to_video.Veo31ImageToVideo.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.Veo31ImageToVideo.Resolution.VALUE_720P,
        description="The resolution of the generated video.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The seed for the random number generator."
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="A negative prompt to guide the video generation."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Veo31ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Veo31ReferenceToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Veo 3.1
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo31ReferenceToVideo.Duration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo31ReferenceToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo31ReferenceToVideo.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt describing the video you want to generate",
    )
    duration: nodetool.nodes.fal.image_to_video.Veo31ReferenceToVideo.Duration = Field(
        default=nodetool.nodes.fal.image_to_video.Veo31ReferenceToVideo.Duration.VALUE_8S,
        description="The duration of the generated video.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.Veo31ReferenceToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.Veo31ReferenceToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video.",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    resolution: nodetool.nodes.fal.image_to_video.Veo31ReferenceToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.image_to_video.Veo31ReferenceToVideo.Resolution.VALUE_720P,
            description="The resolution of the generated video.",
        )
    )
    auto_fix: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.",
    )
    images: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = connect_field(
        default=[],
        description="URLs of the reference images to use for consistent subject appearance",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Veo31ReferenceToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Veo3FastImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Now with a 50% price drop. Generate videos from your image prompts using Veo 3 fast.
    video, animation, image-to-video, img2vid, fast

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo3FastImageToVideo.Resolution
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo3FastImageToVideo.AspectRatio
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo3FastImageToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt describing how the image should be animated",
    )
    resolution: nodetool.nodes.fal.image_to_video.Veo3FastImageToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.image_to_video.Veo3FastImageToVideo.Resolution.VALUE_720P,
            description="The resolution of the generated video.",
        )
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.Veo3FastImageToVideo.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.image_to_video.Veo3FastImageToVideo.AspectRatio.AUTO,
            description="The aspect ratio of the generated video.",
        )
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    auto_fix: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.",
    )
    duration: nodetool.nodes.fal.image_to_video.Veo3FastImageToVideo.Duration = Field(
        default=nodetool.nodes.fal.image_to_video.Veo3FastImageToVideo.Duration.VALUE_8S,
        description="The duration of the generated video.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Veo3FastImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Veo3ImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Veo 3 is the latest state-of-the art video generation model from Google DeepMind
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo3ImageToVideo.Resolution
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo3ImageToVideo.AspectRatio
    )
    Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.Veo3ImageToVideo.Duration
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt describing how the image should be animated",
    )
    resolution: nodetool.nodes.fal.image_to_video.Veo3ImageToVideo.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.Veo3ImageToVideo.Resolution.VALUE_720P,
        description="The resolution of the generated video.",
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.Veo3ImageToVideo.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.image_to_video.Veo3ImageToVideo.AspectRatio.AUTO,
            description="The aspect ratio of the generated video.",
        )
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video."
    )
    auto_fix: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to automatically attempt to fix prompts that fail content policy or other validation checks by rewriting them.",
    )
    duration: nodetool.nodes.fal.image_to_video.Veo3ImageToVideo.Duration = Field(
        default=nodetool.nodes.fal.image_to_video.Veo3ImageToVideo.Duration.VALUE_8S,
        description="The duration of the generated video.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image to animate. Should be 720p or higher resolution in 16:9 or 9:16 aspect ratio. If the image is not in 16:9 or 9:16 aspect ratio, it will be cropped to fit.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Veo3ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class ViduImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Vidu Image to Video generates high-quality videos with exceptional visual quality and motion diversity from a single image
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    MovementAmplitude: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.ViduImageToVideo.MovementAmplitude
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for video generation, max 1500 characters"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )
    movement_amplitude: (
        nodetool.nodes.fal.image_to_video.ViduImageToVideo.MovementAmplitude
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.ViduImageToVideo.MovementAmplitude.AUTO,
        description="The movement amplitude of objects in the frame",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.ViduImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class ViduQ1ImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Vidu Q1 Image to Video generates high-quality 1080p videos with exceptional visual quality and motion diversity from a single image
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    MovementAmplitude: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.ViduQ1ImageToVideo.MovementAmplitude
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for video generation, max 1500 characters"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for the random number generator"
    )
    movement_amplitude: (
        nodetool.nodes.fal.image_to_video.ViduQ1ImageToVideo.MovementAmplitude
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.ViduQ1ImageToVideo.MovementAmplitude.AUTO,
        description="The movement amplitude of objects in the frame",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.ViduQ1ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class ViduQ1ReferenceToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips from your multiple image references using Vidu Q1
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.ViduQ1ReferenceToVideo.AspectRatio
    )
    MovementAmplitude: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.ViduQ1ReferenceToVideo.MovementAmplitude
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for video generation, max 1500 characters"
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.ViduQ1ReferenceToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.ViduQ1ReferenceToVideo.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the output video",
    )
    bgm: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to add background music to the generated video",
    )
    reference_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="URLs of the reference images to use for consistent subject appearance. Q1 model supports up to 7 reference images.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )
    movement_amplitude: (
        nodetool.nodes.fal.image_to_video.ViduQ1ReferenceToVideo.MovementAmplitude
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.ViduQ1ReferenceToVideo.MovementAmplitude.AUTO,
        description="The movement amplitude of objects in the frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.ViduQ1ReferenceToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class ViduQ1StartEndToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Vidu Q1 Start-End to Video generates smooth transition 1080p videos between specified start and end images.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    MovementAmplitude: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.ViduQ1StartEndToVideo.MovementAmplitude
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for video generation, max 1500 characters"
    )
    start_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the last frame",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Seed for the random number generator"
    )
    movement_amplitude: (
        nodetool.nodes.fal.image_to_video.ViduQ1StartEndToVideo.MovementAmplitude
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.ViduQ1StartEndToVideo.MovementAmplitude.AUTO,
        description="The movement amplitude of objects in the frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.ViduQ1StartEndToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class ViduQ2ReferenceToVideoPro(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Vidu Q2 Reference-to-Video Pro generates professional quality videos using reference images for style and content.
    video, generation, vidu, q2, pro, reference

    Use cases:
    - Generate pro videos from references
    - Create style-consistent animations
    - Produce reference-guided videos
    - Generate videos matching examples
    - Create professional reference-based content
    """

    ViduQ2ReferenceToVideoProResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.ViduQ2ReferenceToVideoPro.ViduQ2ReferenceToVideoProResolution
    )
    MovementAmplitude: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.ViduQ2ReferenceToVideoPro.MovementAmplitude
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for video generation, max 2000 characters"
    )
    duration: int | OutputHandle[int] = connect_field(
        default=4,
        description="Duration of the video in seconds (0 for automatic duration)",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.ViduQ2ReferenceToVideoPro.ViduQ2ReferenceToVideoProResolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.ViduQ2ReferenceToVideoPro.ViduQ2ReferenceToVideoProResolution.VALUE_720P,
        description="Output video resolution",
    )
    aspect_ratio: str | OutputHandle[str] = connect_field(
        default="16:9",
        description="Aspect ratio of the output video (e.g., auto, 16:9, 9:16, 1:1, or any W:H)",
    )
    reference_videos: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="URLs of the reference videos for video editing or motion reference. Supports up to 2 videos.",
    )
    bgm: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to add background music to the generated video",
    )
    reference_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="URLs of the reference images for subject appearance. If videos are provided, up to 4 images are allowed; otherwise up to 7 images.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    movement_amplitude: (
        nodetool.nodes.fal.image_to_video.ViduQ2ReferenceToVideoPro.MovementAmplitude
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.ViduQ2ReferenceToVideoPro.MovementAmplitude.AUTO,
        description="The movement amplitude of objects in the frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.ViduQ2ReferenceToVideoPro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class ViduReferenceToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Vidu Reference to Video creates videos by using a reference images and combining them with a prompt.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.ViduReferenceToVideo.AspectRatio
    )
    MovementAmplitude: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.ViduReferenceToVideo.MovementAmplitude
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for video generation, max 1500 characters"
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.ViduReferenceToVideo.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.image_to_video.ViduReferenceToVideo.AspectRatio.RATIO_16_9,
            description="The aspect ratio of the output video",
        )
    )
    reference_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="URLs of the reference images to use for consistent subject appearance",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )
    movement_amplitude: (
        nodetool.nodes.fal.image_to_video.ViduReferenceToVideo.MovementAmplitude
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.ViduReferenceToVideo.MovementAmplitude.AUTO,
        description="The movement amplitude of objects in the frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.ViduReferenceToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class ViduStartEndToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Vidu Start-End to Video generates smooth transition videos between specified start and end images.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    MovementAmplitude: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.ViduStartEndToVideo.MovementAmplitude
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt for video generation, max 1500 characters"
    )
    start_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the last frame",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )
    movement_amplitude: (
        nodetool.nodes.fal.image_to_video.ViduStartEndToVideo.MovementAmplitude
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.ViduStartEndToVideo.MovementAmplitude.AUTO,
        description="The movement amplitude of objects in the frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.ViduStartEndToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class ViduTemplateToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Vidu Template to Video lets you create different effects by applying motion templates to your images.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.ViduTemplateToVideo.AspectRatio
    )
    Template: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.ViduTemplateToVideo.Template
    )

    aspect_ratio: nodetool.nodes.fal.image_to_video.ViduTemplateToVideo.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.image_to_video.ViduTemplateToVideo.AspectRatio.RATIO_16_9,
            description="The aspect ratio of the output video",
        )
    )
    template: nodetool.nodes.fal.image_to_video.ViduTemplateToVideo.Template = Field(
        default=nodetool.nodes.fal.image_to_video.ViduTemplateToVideo.Template.HUG,
        description="AI video template to use. Pricing varies by template: Standard templates (hug, kiss, love_pose, etc.) cost 4 credits ($0.20), Premium templates (lunar_newyear, dynasty_dress, dreamy_wedding, etc.) cost 6 credits ($0.30), and Advanced templates (live_photo) cost 10 credits ($0.50).",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for generation"
    )
    input_images: list[str] | OutputHandle[list[str]] = connect_field(
        default=[],
        description="URLs of the images to use with the template. Number of images required varies by template: 'dynasty_dress' and 'shop_frame' accept 1-2 images, 'wish_sender' requires exactly 3 images, all other templates accept only 1 image.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.ViduTemplateToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class WanAti(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Wan Ati
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanAti.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    resolution: nodetool.nodes.fal.image_to_video.WanAti.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.WanAti.Resolution.VALUE_480P,
        description="Resolution of the generated video (480p, 580p, 720p).",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image.",
    )
    track: list[list[str]] | OutputHandle[list[list[str]]] = connect_field(
        default=[],
        description="Motion tracks to guide video generation. Each track is a sequence of points defining a motion trajectory. Multiple tracks can control different elements or objects in the video. Expected format: array of tracks, where each track is an array of points with 'x' and 'y' coordinates (up to 121 points per track). Points will be automatically padded to 121 if fewer are provided. Coordinates should be within the image dimensions.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.WanAti

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class WanFlf2v(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Wan-2.1 flf2v generates dynamic videos by intelligently bridging a given first frame to a desired end frame through smooth, coherent motion sequences.
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanFlf2v.Acceleration
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanFlf2v.Resolution
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanFlf2v.AspectRatio
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    acceleration: nodetool.nodes.fal.image_to_video.WanFlf2v.Acceleration = Field(
        default=nodetool.nodes.fal.image_to_video.WanFlf2v.Acceleration.REGULAR,
        description="Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.",
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 24.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    start_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the starting image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the ending image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 100 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.",
    )
    resolution: nodetool.nodes.fal.image_to_video.WanFlf2v.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.WanFlf2v.Resolution.VALUE_720P,
        description="Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.",
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.WanFlf2v.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.WanFlf2v.AspectRatio.AUTO,
        description="Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    guide_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.WanFlf2v

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class WanI2vLora(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Add custom LoRAs to Wan-2.1 is a image-to-video model that generates high-quality videos with high visual quality and motion diversity from images
    video, animation, image-to-video, img2vid, lora

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanI2vLora.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanI2vLora.Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    shift: float | OutputHandle[float] = connect_field(
        default=5, description="Shift parameter for video generation."
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False, description="If true, the video will be reversed."
    )
    loras: list[types.LoraWeight] | OutputHandle[list[types.LoraWeight]] = (
        connect_field(
            default=[], description="LoRA weights to be used in the inference."
        )
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 5 to 24.",
    )
    turbo_mode: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, the video will be generated faster with no noticeable degradation in the visual quality.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False, description="If set to true, the safety checker will be enabled."
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 81 to 100 (inclusive). If the number of frames is greater than 81, the video will be generated with 1.25x more billing units.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="bright colors, overexposed, static, blurred details, subtitles, style, artwork, painting, picture, still, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, malformed limbs, fused fingers, still picture, cluttered background, three legs, many people in the background, walking backwards",
        description="Negative prompt for video generation.",
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.WanI2vLora.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.WanI2vLora.AspectRatio.RATIO_16_9,
        description="Aspect ratio of the output video.",
    )
    resolution: nodetool.nodes.fal.image_to_video.WanI2vLora.Resolution = Field(
        default=nodetool.nodes.fal.image_to_video.WanI2vLora.Resolution.VALUE_720P,
        description="Resolution of the generated video (480p or 720p). 480p is 0.5 billing units, and 720p is 1 billing unit.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Whether to enable prompt expansion."
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    guide_scale: float | OutputHandle[float] = connect_field(
        default=5,
        description="Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.WanI2vLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class WanMove(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Wan Move generates videos with natural motion and movement from static images.
    video, generation, wan, motion, animation

    Use cases:
    - Add natural motion to images
    - Create animated movements
    - Produce dynamic video content
    - Generate moving scenes from stills
    - Create motion animations
    """

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Text prompt to guide the video generation."
    )
    trajectories: list[list[str]] | OutputHandle[list[list[str]]] = connect_field(
        default=[],
        description="A list of trajectories. Each trajectory list means the movement of one object.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5,
        description="Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£Žæ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½Žè´¨é‡ï¼ŒJPEGåŽ‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èžåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°",
        description="Negative prompt to guide the video generation.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.WanMove

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class WanV225bImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan 2.2's 5B model produces up to 5 seconds of video 720p at 24FPS with fluid motion and powerful prompt understanding
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.VideoWriteMode
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.Resolution
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.VideoQuality
    )
    InterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.InterpolatorModel
    )

    shift: float | OutputHandle[float] = connect_field(
        default=5,
        description="Shift value for the video. Must be between 1.0 and 10.0.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=0,
        description="Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.",
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=24,
        description="Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5,
        description="Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 17 to 161 (inclusive).",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, input data will be checked for safety before processing.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt for video generation."
    )
    video_write_mode: (
        nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.VideoWriteMode.BALANCED,
        description="The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.",
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.AspectRatio = (
        Field(
            default=nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.AspectRatio.AUTO,
            description="Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.",
        )
    )
    resolution: nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.Resolution.VALUE_720P,
            description="Resolution of the generated video (580p or 720p).",
        )
    )
    enable_output_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, output video will be checked for safety after generation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.",
    )
    video_quality: (
        nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.VideoQuality.HIGH,
        description="The quality of the output video. Higher quality means better visual quality but larger file size.",
    )
    adjust_fps_for_interpolation: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.InterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV225bImageToVideo.InterpolatorModel.FILM,
        description="The model to use for frame interpolation. If None, no interpolation is applied.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.WanV225bImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class WanV22A14bImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    fal-ai/wan/v2.2-A14B/image-to-video
    video, animation, image-to-video, img2vid

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.Acceleration
    )
    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.VideoWriteMode
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.Resolution
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.VideoQuality
    )
    InterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.InterpolatorModel
    )

    shift: float | OutputHandle[float] = connect_field(
        default=5,
        description="Shift value for the video. Must be between 1.0 and 10.0.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=1,
        description="Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.",
    )
    acceleration: (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.Acceleration.REGULAR,
        description="Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.",
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5,
        description="Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 17 to 161 (inclusive).",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the end image.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt for video generation."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, input data will be checked for safety before processing.",
    )
    video_write_mode: (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.VideoWriteMode.BALANCED,
        description="The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.AspectRatio.AUTO,
        description="Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.",
    )
    resolution: nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.Resolution = (
        Field(
            default=nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.Resolution.VALUE_720P,
            description="Resolution of the generated video (480p, 580p, or 720p).",
        )
    )
    enable_output_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, output video will be checked for safety after generation.",
    )
    guidance_scale_2: float | OutputHandle[float] = connect_field(
        default=3.5,
        description="Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.",
    )
    video_quality: (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.VideoQuality.HIGH,
        description="The quality of the output video. Higher quality means better visual quality but larger file size.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.",
    )
    adjust_fps_for_interpolation: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.InterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo.InterpolatorModel.FILM,
        description="The model to use for frame interpolation. If None, no interpolation is applied.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=27,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class WanV22A14bImageToVideoLora(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan-2.2 image-to-video is a video model that generates high-quality videos with high visual quality and motion diversity from text prompts and images. This endpoint supports LoRAs made for Wan 2.2
    video, animation, image-to-video, img2vid, lora

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.Acceleration
    )
    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.VideoWriteMode
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.AspectRatio
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.Resolution
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.VideoQuality
    )
    InterpolatorModel: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.InterpolatorModel
    )

    shift: float | OutputHandle[float] = connect_field(
        default=5,
        description="Shift value for the video. Must be between 1.0 and 10.0.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    num_interpolated_frames: int | OutputHandle[int] = connect_field(
        default=1,
        description="Number of frames to interpolate between each pair of generated frames. Must be between 0 and 4.",
    )
    acceleration: (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.Acceleration.REGULAR,
        description="Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.",
    )
    reverse_video: bool | OutputHandle[bool] = connect_field(
        default=False, description="If true, the video will be reversed."
    )
    loras: list[types.LoRAWeight] | OutputHandle[list[types.LoRAWeight]] = (
        connect_field(
            default=[], description="LoRA weights to be used in the inference."
        )
    )
    frames_per_second: int | OutputHandle[int] = connect_field(
        default=16,
        description="Frames per second of the generated video. Must be between 4 to 60. When using interpolation and `adjust_fps_for_interpolation` is set to true (default true,) the final FPS will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If `adjust_fps_for_interpolation` is set to false, this value will be used as-is.",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.5,
        description="Classifier-free guidance scale. Higher values give better adherence to the prompt but may decrease quality.",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=81,
        description="Number of frames to generate. Must be between 17 to 161 (inclusive).",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the end image.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="", description="Negative prompt for video generation."
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, input data will be checked for safety before processing.",
    )
    video_write_mode: (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.VideoWriteMode.BALANCED,
        description="The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.AspectRatio.AUTO,
        description="Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.Resolution.VALUE_720P,
        description="Resolution of the generated video (480p, 580p, or 720p).",
    )
    enable_output_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, output video will be checked for safety after generation.",
    )
    guidance_scale_2: float | OutputHandle[float] = connect_field(
        default=4,
        description="Guidance scale for the second stage of the model. This is used to control the adherence to the prompt in the second stage of the model.",
    )
    video_quality: (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.VideoQuality.HIGH,
        description="The quality of the output video. Higher quality means better visual quality but larger file size.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.",
    )
    adjust_fps_for_interpolation: bool | OutputHandle[bool] = connect_field(
        default=True,
        description="If true, the number of frames per second will be multiplied by the number of interpolated frames plus one. For example, if the generated frames per second is 16 and the number of interpolated frames is 1, the final frames per second will be 32. If false, the passed frames per second will be used as-is.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    interpolator_model: (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.InterpolatorModel
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora.InterpolatorModel.FILM,
        description="The model to use for frame interpolation. If None, no interpolation is applied.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=27,
        description="Number of inference steps for sampling. Higher values give better quality but take longer.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class WanV22A14bImageToVideoTurbo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan-2.2 Turbo image-to-video is a video model that generates high-quality videos with high visual quality and motion diversity from text prompts.
    video, animation, image-to-video, img2vid, fast

    Use cases:
    - Animate static images
    - Create engaging social media content
    - Product demonstrations
    - Marketing and promotional videos
    - Visual storytelling
    """

    VideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.VideoWriteMode
    )
    Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.Resolution
    )
    Acceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.Acceleration
    )
    AspectRatio: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.AspectRatio
    )
    VideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.VideoQuality
    )

    video_write_mode: (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.VideoWriteMode
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.VideoWriteMode.BALANCED,
        description="The write mode of the output video. Faster write mode means faster results but larger file size, balanced write mode is a good compromise between speed and quality, and small write mode is the slowest but produces the smallest file size.",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.Resolution.VALUE_720P,
        description="Resolution of the generated video (480p, 580p, or 720p).",
    )
    acceleration: (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.Acceleration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.Acceleration.REGULAR,
        description="Acceleration level to use. The more acceleration, the faster the generation, but with lower quality. The recommended value is 'regular'.",
    )
    aspect_ratio: (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.AspectRatio
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.AspectRatio.AUTO,
        description="Aspect ratio of the generated video. If 'auto', the aspect ratio will be determined automatically based on the input image.",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The text prompt to guide video generation."
    )
    enable_output_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, output video will be checked for safety after generation.",
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the input image. If the input image does not match the chosen aspect ratio, it is resized and center cropped.",
    )
    video_quality: (
        nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.VideoQuality
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo.VideoQuality.HIGH,
        description="The quality of the output video. Higher quality means better visual quality but larger file size.",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="If set to true, input data will be checked for safety before processing.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    end_image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the end image.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to enable prompt expansion. This will use a large language model to expand the prompt with additional details while maintaining the original meaning.",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.WanV22A14bImageToVideoTurbo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class WanV26ImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan v2.6 generates high-quality videos from images with balanced quality and performance.
    video, generation, wan, v2.6, image-to-video

    Use cases:
    - Generate quality videos from images
    - Create balanced video animations
    - Produce reliable video content
    - Generate consistent videos
    - Create professional animations
    """

    WanV26Duration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV26ImageToVideo.WanV26Duration
    )
    WanV26Resolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV26ImageToVideo.WanV26Resolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt describing the desired video motion. Max 800 characters.",
    )
    duration: nodetool.nodes.fal.image_to_video.WanV26ImageToVideo.WanV26Duration = (
        Field(
            default=nodetool.nodes.fal.image_to_video.WanV26ImageToVideo.WanV26Duration.VALUE_5,
            description="Duration of the generated video in seconds. Choose between 5, 10 or 15 seconds.",
        )
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.WanV26ImageToVideo.WanV26Resolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV26ImageToVideo.WanV26Resolution.VALUE_1080P,
        description="Video resolution. Valid values: 720p, 1080p",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="If set to true, the safety checker will be enabled."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame. Must be publicly accessible or base64 data URI. Image dimensions must be between 240 and 7680.",
    )
    audio: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the audio to use as the background music. Must be publicly accessible. Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds), the audio is truncated to the first N seconds, and the rest is discarded. If the audio is shorter than the video, the remaining part of the video will be silent. For example, if the audio is 3 seconds long and the video duration is 5 seconds, the first 3 seconds of the output video will have sound, and the last 2 seconds will be silent. - Format: WAV, MP3. - Duration: 3 to 30 s. - File size: Up to 15 MB.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    multi_shots: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="When true, enables intelligent multi-shot segmentation. Only active when enable_prompt_expansion is True. Set to false for single-shot generation.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Negative prompt to describe content to avoid. Max 500 characters.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt rewriting using LLM."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.WanV26ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class WanV26ImageToVideoFlash(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Wan v2.6 Flash generates videos from images with ultra-fast processing for rapid iteration.
    video, generation, wan, v2.6, flash, fast

    Use cases:
    - Generate videos at maximum speed
    - Create rapid video prototypes
    - Produce instant video previews
    - Generate quick video iterations
    - Create fast video animations
    """

    WanV26FlashDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV26ImageToVideoFlash.WanV26FlashDuration
    )
    WanV26FlashResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.WanV26ImageToVideoFlash.WanV26FlashResolution
    )

    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The text prompt describing the desired video motion. Max 800 characters.",
    )
    duration: (
        nodetool.nodes.fal.image_to_video.WanV26ImageToVideoFlash.WanV26FlashDuration
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV26ImageToVideoFlash.WanV26FlashDuration.VALUE_5,
        description="Duration of the generated video in seconds. Choose between 5, 10 or 15 seconds.",
    )
    resolution: (
        nodetool.nodes.fal.image_to_video.WanV26ImageToVideoFlash.WanV26FlashResolution
    ) = Field(
        default=nodetool.nodes.fal.image_to_video.WanV26ImageToVideoFlash.WanV26FlashResolution.VALUE_1080P,
        description="Video resolution. Valid values: 720p, 1080p",
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="If set to true, the safety checker will be enabled."
    )
    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the image to use as the first frame. Must be publicly accessible or base64 data URI. Image dimensions must be between 240 and 7680.",
    )
    audio: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the audio to use as the background music. Must be publicly accessible. Limit handling: If the audio duration exceeds the duration value (5, 10, or 15 seconds), the audio is truncated to the first N seconds, and the rest is discarded. If the audio is shorter than the video, the remaining part of the video will be silent. For example, if the audio is 3 seconds long and the video duration is 5 seconds, the first 3 seconds of the output video will have sound, and the last 2 seconds will be silent. - Format: WAV, MP3. - Duration: 3 to 30 s. - File size: Up to 15 MB.",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Random seed for reproducibility. If None, a random seed is chosen.",
    )
    multi_shots: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="When true, enables intelligent multi-shot segmentation. Only active when enable_prompt_expansion is True. Set to false for single-shot generation.",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="Negative prompt to describe content to avoid. Max 500 characters.",
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to enable prompt rewriting using LLM."
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.WanV26ImageToVideoFlash

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
