# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class AMTInterpolation(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Interpolate between image frames to create smooth video transitions. Supports configurable FPS and recursive interpolation passes for higher quality results.
    video, interpolation, transitions, frames, smoothing, img2vid, image-to-video

    Use cases:
    - Create smooth frame transitions
    - Generate fluid animations
    - Enhance video frame rates
    - Produce slow-motion effects
    - Create seamless video blends
    """

    frames: list[types.ImageRef] | OutputHandle[list[types.ImageRef]] = connect_field(
        default=[
            types.ImageRef(
                type="image", uri="", asset_id=None, data=None, metadata=None
            ),
            types.ImageRef(
                type="image", uri="", asset_id=None, data=None, metadata=None
            ),
        ],
        description="List of frames to interpolate between (minimum 2 frames required)",
    )
    output_fps: int | OutputHandle[int] = connect_field(
        default=24, description="Output frames per second"
    )
    recursive_interpolation_passes: int | OutputHandle[int] = connect_field(
        default=4,
        description="Number of recursive interpolation passes (higher = smoother)",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.AMTInterpolation

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class CogVideoX(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate videos from images using CogVideoX-5B. Features high-quality motion synthesis with configurable parameters for fine-tuned control over the output.
    video, generation, motion, synthesis, control, img2vid, image-to-video

    Use cases:
    - Create controlled video animations
    - Generate precise motion effects
    - Produce customized video content
    - Create fine-tuned animations
    - Generate motion sequences
    """

    VideoSize: typing.ClassVar[type] = nodetool.nodes.fal.image_to_video.VideoSize

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to transform into a video",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="A description of the desired video motion and style"
    )
    video_size: nodetool.nodes.fal.image_to_video.VideoSize = Field(
        default=nodetool.nodes.fal.image_to_video.VideoSize.LANDSCAPE_16_9,
        description="The size/aspect ratio of the generated video",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms",
        description="What to avoid in the generated video",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50,
        description="Number of denoising steps (higher = better quality but slower)",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.0,
        description="How closely to follow the prompt (higher = more faithful but less creative)",
    )
    use_rife: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use RIFE for video interpolation"
    )
    export_fps: int | OutputHandle[int] = connect_field(
        default=16, description="Target frames per second for the output video"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same video every time"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.CogVideoX

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class FastSVD(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate short video clips from your images using SVD v1.1 at Lightning Speed. Features high-quality motion synthesis with configurable parameters for rapid video generation.
    video, generation, fast, motion, synthesis, img2vid, image-to-video

    Use cases:
    - Create quick video animations
    - Generate rapid motion content
    - Produce fast video transitions
    - Create instant visual effects
    - Generate quick previews
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to transform into a video",
    )
    motion_bucket_id: int | OutputHandle[int] = connect_field(
        default=127, description="Controls motion intensity (higher = more motion)"
    )
    cond_aug: float | OutputHandle[float] = connect_field(
        default=0.02,
        description="Amount of noise added to conditioning (higher = more motion)",
    )
    steps: int | OutputHandle[int] = connect_field(
        default=4,
        description="Number of inference steps (higher = better quality but slower)",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=10,
        description="Frames per second of the output video (total length is 25 frames)",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same video every time"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.FastSVD

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class HaiperImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Transform images into hyper-realistic videos with Haiper 2.0. Experience industry-leading resolution, fluid motion, and rapid generation for stunning AI videos.
    video, generation, hyper-realistic, motion, animation, image-to-video, img2vid

    Use cases:
    - Create cinematic animations
    - Generate dynamic video content
    - Transform static images into motion
    - Produce high-resolution videos
    - Create visual effects
    """

    VideoDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.VideoDuration
    )

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to transform into a video",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="A description of the desired video motion and style"
    )
    duration: nodetool.nodes.fal.image_to_video.VideoDuration = Field(
        default=nodetool.nodes.fal.image_to_video.VideoDuration.FOUR_SECONDS,
        description="The duration of the generated video in seconds",
    )
    prompt_enhancer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the model's prompt enhancer"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same video every time"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.HaiperImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate video clips from your images using Kling 1.6. Supports multiple durations and aspect ratios.
    video, generation, animation, duration, aspect-ratio, img2vid, image-to-video

    Use cases:
    - Create custom video content
    - Generate video animations
    - Transform static images
    - Produce motion graphics
    - Create visual presentations
    """

    KlingDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingDuration
    )
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.image_to_video.AspectRatio

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to transform into a video",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="A description of the desired video motion and style"
    )
    duration: nodetool.nodes.fal.image_to_video.KlingDuration = Field(
        default=nodetool.nodes.fal.image_to_video.KlingDuration.FIVE_SECONDS,
        description="The duration of the generated video",
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class KlingVideoPro(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate video clips from your images using Kling 1.6 Pro. The professional version offers enhanced quality and performance compared to the standard version.
    video, generation, professional, quality, performance, img2vid, image-to-video

    Use cases:
    - Create professional video content
    - Generate high-quality animations
    - Produce commercial video assets
    - Create advanced motion graphics
    - Generate premium visual content
    """

    KlingDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.KlingDuration
    )
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.image_to_video.AspectRatio

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to transform into a video",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="A description of the desired video motion and style"
    )
    duration: nodetool.nodes.fal.image_to_video.KlingDuration = Field(
        default=nodetool.nodes.fal.image_to_video.KlingDuration.FIVE_SECONDS,
        description="The duration of the generated video",
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video frame",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.KlingVideoPro

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class LTX219BImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video with audio from images using LTX-2 19B model. A state-of-the-art video generation model with camera motion control and multi-scale generation.
    video, generation, ltx, ltx-2, image-to-video, motion-control, camera, audio

    Use cases:
    - Generate high-quality videos from images
    - Create videos with synchronized audio
    - Control camera movements with LoRA
    - Produce professional video content
    - Animate static images with fluid motion
    """

    LtxVideoSize: typing.ClassVar[type] = nodetool.nodes.fal.image_to_video.LtxVideoSize
    LtxAcceleration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LtxAcceleration
    )
    LtxCameraLora: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LtxCameraLora
    )
    LtxVideoOutputType: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LtxVideoOutputType
    )
    LtxVideoQuality: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LtxVideoQuality
    )
    LtxVideoWriteMode: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.LtxVideoWriteMode
    )

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to generate the video from",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="The prompt describing the desired video motion and style",
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=121, description="Number of frames to generate"
    )
    video_size: nodetool.nodes.fal.image_to_video.LtxVideoSize = Field(
        default=nodetool.nodes.fal.image_to_video.LtxVideoSize.AUTO,
        description="Size of the generated video",
    )
    generate_audio: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to generate audio for the video"
    )
    use_multiscale: bool | OutputHandle[bool] = connect_field(
        default=True, description="Use multi-scale generation for better coherence"
    )
    fps: float | OutputHandle[float] = connect_field(
        default=25, description="Frames per second"
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3, description="Guidance scale for generation"
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=40, description="Number of inference steps"
    )
    acceleration: nodetool.nodes.fal.image_to_video.LtxAcceleration = Field(
        default=nodetool.nodes.fal.image_to_video.LtxAcceleration.REGULAR,
        description="Acceleration level",
    )
    camera_lora: nodetool.nodes.fal.image_to_video.LtxCameraLora = Field(
        default=nodetool.nodes.fal.image_to_video.LtxCameraLora.NONE,
        description="Camera movement LoRA",
    )
    camera_lora_scale: float | OutputHandle[float] = connect_field(
        default=1, description="Camera LoRA scale"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur",
        description="Negative prompt to avoid",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="Random seed for reproducibility"
    )
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(
        default=False, description="Enable prompt expansion"
    )
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(
        default=True, description="Enable safety checker"
    )
    video_output_type: nodetool.nodes.fal.image_to_video.LtxVideoOutputType = Field(
        default=nodetool.nodes.fal.image_to_video.LtxVideoOutputType.X264_MP4,
        description="Output video format",
    )
    video_quality: nodetool.nodes.fal.image_to_video.LtxVideoQuality = Field(
        default=nodetool.nodes.fal.image_to_video.LtxVideoQuality.HIGH,
        description="Video quality",
    )
    video_write_mode: nodetool.nodes.fal.image_to_video.LtxVideoWriteMode = Field(
        default=nodetool.nodes.fal.image_to_video.LtxVideoWriteMode.BALANCED,
        description="Video write mode",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.LTX219BImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class LTXVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate videos from images using LTX Video. Best results with 768x512 images and detailed, chronological descriptions of actions and scenes.
    video, generation, chronological, scenes, actions, img2vid, image-to-video

    Use cases:
    - Create scene-based animations
    - Generate sequential video content
    - Produce narrative videos
    - Create storyboard animations
    - Generate action sequences
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to transform into a video (768x512 recommended)",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="",
        description="A detailed description of the desired video motion and style",
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="low quality, worst quality, deformed, distorted, disfigured, motion smear, motion artifacts, fused fingers, bad anatomy, weird hand, ugly",
        description="What to avoid in the generated video",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=30,
        description="Number of inference steps (higher = better quality but slower)",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=3.0,
        description="How closely to follow the prompt (higher = more faithful)",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same video every time"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.LTXVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class LumaDreamMachine(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Generate video clips from your images using Luma Dream Machine v1.5. Supports various aspect ratios and optional end-frame blending.
    video, generation, animation, blending, aspect-ratio, img2vid, image-to-video

    Use cases:
    - Create seamless video loops
    - Generate video transitions
    - Transform images into animations
    - Create motion graphics
    - Produce video content
    """

    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.image_to_video.AspectRatio

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to transform into a video",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="A description of the desired video motion and style"
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    loop: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether the video should loop (end blends with beginning)",
    )
    end_image: (
        nodetool.metadata.types.ImageRef
        | OutputHandle[nodetool.metadata.types.ImageRef]
        | None
    ) = connect_field(
        default=None, description="Optional image to blend the end of the video with"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.LumaDreamMachine

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class MiniMaxHailuo02(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Create videos from your images with MiniMax Hailuo-02 Standard. Choose the
    clip length and optionally enhance prompts for sharper results.
    video, generation, minimax, prompt-optimizer, img2vid, image-to-video

    Use cases:
    - Produce social media clips
    - Generate cinematic sequences
    - Visualize storyboards
    - Create promotional videos
    - Animate still graphics
    """

    HailuoDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.HailuoDuration
    )

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to transform into a video",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt describing the video"
    )
    duration: nodetool.nodes.fal.image_to_video.HailuoDuration = Field(
        default=nodetool.nodes.fal.image_to_video.HailuoDuration.SIX_SECONDS,
        description="The duration of the video in seconds. 10 seconds videos are not supported for 1080p resolution.",
    )
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the model's prompt optimizer"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.MiniMaxHailuo02

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class MiniMaxVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate video clips from your images using MiniMax Video model. Transform static art into dynamic masterpieces with enhanced smoothness and vivid motion.
    video, generation, art, motion, smoothness, img2vid, image-to-video

    Use cases:
    - Transform artwork into videos
    - Create smooth animations
    - Generate artistic motion content
    - Produce dynamic visualizations
    - Create video art pieces
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to transform into a video",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="A description of the desired video motion and style"
    )
    prompt_optimizer: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use the model's prompt optimizer"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.MiniMaxVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class MuseTalk(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Real-time high quality audio-driven lip-syncing model. Animate a face video with custom audio for natural-looking speech animation.
    video, lip-sync, animation, speech, real-time, wav2vid, audio-to-video

    Use cases:
    - Create lip-synced videos
    - Generate speech animations
    - Produce dubbed content
    - Create animated presentations
    - Generate voice-over videos
    """

    video: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(
        default=types.VideoRef(
            type="video",
            uri="",
            asset_id=None,
            data=None,
            metadata=None,
            duration=None,
            format=None,
        ),
        description="URL of the source video to animate",
    )
    audio: types.AudioRef | OutputHandle[types.AudioRef] = connect_field(
        default=types.AudioRef(
            type="audio", uri="", asset_id=None, data=None, metadata=None
        ),
        description="URL of the audio file to drive the lip sync",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.MuseTalk

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class PixVerse(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate dynamic videos from images with PixVerse v4.5. Create high-quality motion
    with detailed prompt control and advanced diffusion parameters.
    video, generation, pixverse, motion, diffusion, img2vid, image-to-video

    Use cases:
    - Animate illustrations and photos
    - Produce engaging social media clips
    - Generate short cinematic shots
    - Create motion for product showcases
    - Experiment with creative video effects
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to transform into a video",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="A description of the desired video motion and style"
    )
    negative_prompt: str | OutputHandle[str] = connect_field(
        default="low quality, worst quality, distorted, blurred",
        description="What to avoid in the generated video",
    )
    num_inference_steps: int | OutputHandle[int] = connect_field(
        default=50,
        description="Number of inference steps (higher = better quality but slower)",
    )
    guidance_scale: float | OutputHandle[float] = connect_field(
        default=7.5,
        description="How closely to follow the prompt (higher = more faithful)",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same video every time"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.PixVerse

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class SadTalker(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate talking face animations from a single image and audio file. Features configurable face model resolution and expression controls.
    video, animation, face, talking, expression, img2vid, image-to-video, audio-to-video, wav2vid

    Use cases:
    - Create talking head videos
    - Generate lip-sync animations
    - Produce character animations
    - Create video presentations
    - Generate facial expressions
    """

    FaceModelResolution: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.FaceModelResolution
    )
    PreprocessType: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.PreprocessType
    )

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The source image to animate",
    )
    audio: str | OutputHandle[str] = connect_field(
        default="", description="URL of the audio file to drive the animation"
    )
    face_model_resolution: nodetool.nodes.fal.image_to_video.FaceModelResolution = (
        Field(
            default=nodetool.nodes.fal.image_to_video.FaceModelResolution.RESOLUTION_256,
            description="Resolution of the face model",
        )
    )
    expression_scale: float | OutputHandle[float] = connect_field(
        default=1.0, description="Scale of the expression (1.0 = normal)"
    )
    still_mode: bool | OutputHandle[bool] = connect_field(
        default=False, description="Reduce head motion (works with preprocess 'full')"
    )
    preprocess: nodetool.nodes.fal.image_to_video.PreprocessType = Field(
        default=nodetool.nodes.fal.image_to_video.PreprocessType.CROP,
        description="Type of image preprocessing to apply",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.SadTalker

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class StableVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate short video clips from your images using Stable Video Diffusion v1.1. Features high-quality motion synthesis with configurable parameters.
    video, generation, diffusion, motion, synthesis, img2vid, image-to-video

    Use cases:
    - Create stable video animations
    - Generate motion content
    - Transform images into videos
    - Produce smooth transitions
    - Create visual effects
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to transform into a video",
    )
    motion_bucket_id: int | OutputHandle[int] = connect_field(
        default=127, description="Controls motion intensity (higher = more motion)"
    )
    cond_aug: float | OutputHandle[float] = connect_field(
        default=0.02,
        description="Amount of noise added to conditioning (higher = more motion)",
    )
    fps: int | OutputHandle[int] = connect_field(
        default=25, description="Frames per second of the output video"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same video every time"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.StableVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Veo2(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate videos from text prompts using Veo 2. Creates short clips with
    optional control over duration and aspect ratio.
    video, text-to-video, generation, prompt, veo2

    Use cases:
    - Produce cinematic video clips from descriptions
    - Generate marketing or social media footage
    - Create animated scenes from storyboards
    - Experiment with visual concepts rapidly
    """

    VideoDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.VideoDuration
    )
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.image_to_video.AspectRatio

    prompt: str | OutputHandle[str] = connect_field(
        default="", description="The prompt to generate a video from"
    )
    duration: nodetool.nodes.fal.image_to_video.VideoDuration = Field(
        default=nodetool.nodes.fal.image_to_video.VideoDuration.FOUR_SECONDS,
        description="The duration of the generated video in seconds",
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same video every time"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Veo2

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class Veo2ImageToVideo(
    SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]
):
    """

    Animate a single image into a Veo 2 video clip. Provides control over
    duration and aspect ratio while following an optional prompt.
    video, image-to-video, veo2, animation

    Use cases:
    - Bring still artwork to life
    - Create dynamic social media posts
    - Generate quick product showcase videos
    - Produce animated storyboards
    """

    VideoDuration: typing.ClassVar[type] = (
        nodetool.nodes.fal.image_to_video.VideoDuration
    )
    AspectRatio: typing.ClassVar[type] = nodetool.nodes.fal.image_to_video.AspectRatio

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The image to transform into a video",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Optional description of the desired motion"
    )
    duration: nodetool.nodes.fal.image_to_video.VideoDuration = Field(
        default=nodetool.nodes.fal.image_to_video.VideoDuration.FOUR_SECONDS,
        description="The duration of the generated video in seconds",
    )
    aspect_ratio: nodetool.nodes.fal.image_to_video.AspectRatio = Field(
        default=nodetool.nodes.fal.image_to_video.AspectRatio.RATIO_16_9,
        description="The aspect ratio of the generated video",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same video every time"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.Veo2ImageToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.image_to_video
from nodetool.workflows.base_node import BaseNode


class WanFlf2v(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

    Generate short video clips from a single image using the WAN FLF2V model. This model converts a still image into an animated clip guided by a text prompt.
    video, generation, animation, image-to-video, wan

    Use cases:
    - Animate still images into short clips
    - Create dynamic content from artwork
    - Produce promotional video snippets
    - Generate visual effects for social posts
    - Explore creative motion ideas
    """

    image: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(
        default=types.ImageRef(
            type="image", uri="", asset_id=None, data=None, metadata=None
        ),
        description="The source image for video generation",
    )
    prompt: str | OutputHandle[str] = connect_field(
        default="", description="Description of the desired motion and style"
    )
    num_frames: int | OutputHandle[int] = connect_field(
        default=16, description="Number of frames to generate"
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1, description="The same seed will output the same video every time"
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.image_to_video.WanFlf2v

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
