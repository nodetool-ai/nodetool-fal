# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_video
from nodetool.workflows.base_node import BaseNode

class ElevenlabsDubbing(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        ElevenLabs Dubbing
        video, generation, audio-to-video, visualization

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    video_url: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(default=types.VideoRef(type='video', uri='', asset_id=None, data=None, metadata=None, duration=None, format=None), description='URL of the video file to dub. Either audio_url or video_url must be provided. If both are provided, video_url takes priority.')
    highest_resolution: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to use the highest resolution for dubbing.')
    audio_url: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(default=types.VideoRef(type='video', uri='', asset_id=None, data=None, metadata=None, duration=None, format=None), description='URL of the audio file to dub. Either audio_url or video_url must be provided.')
    target_lang: str | OutputHandle[str] = connect_field(default='', description='Target language code for dubbing (ISO 639-1)')
    num_speakers: str | OutputHandle[str] = connect_field(default='', description='Number of speakers in the audio. If not provided, will be auto-detected.')
    source_lang: str | OutputHandle[str] = connect_field(default='', description='Source language code. If not provided, will be auto-detected.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_video.ElevenlabsDubbing

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_video
from nodetool.workflows.base_node import BaseNode

class LongcatMultiAvatarImageAudioToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        Longcat Multi Avatar
        video, generation, audio-to-video, visualization

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    Resolution: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.LongcatMultiAvatarImageAudioToVideo.Resolution
    AudioType: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.LongcatMultiAvatarImageAudioToVideo.AudioType

    prompt: str | OutputHandle[str] = connect_field(default='Two people are having a conversation with natural expressions and movements.', description='The prompt to guide the video generation.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=30, description='The number of inference steps to use.')
    audio_url_person2: str | OutputHandle[str] = connect_field(default='https://raw.githubusercontent.com/meituan-longcat/LongCat-Video/refs/heads/main/assets/avatar/multi/sing_woman.WAV', description='The URL of the audio file for person 2 (right side).')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable safety checker.')
    bbox_person1: str | OutputHandle[str] = connect_field(default='', description='Bounding box for person 1. If not provided, defaults to left half of image.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='Close-up, Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards', description='The negative prompt to avoid in the video generation.')
    text_guidance_scale: float | OutputHandle[float] = connect_field(default=4, description='The text guidance scale for classifier-free guidance.')
    resolution: nodetool.nodes.fal.audio_to_video.LongcatMultiAvatarImageAudioToVideo.Resolution = Field(default=nodetool.nodes.fal.audio_to_video.LongcatMultiAvatarImageAudioToVideo.Resolution.VALUE_480P, description='Resolution of the generated video (480p or 720p). Billing is per video-second (16 frames): 480p is 1 unit per second and 720p is 4 units per second.')
    audio_type: nodetool.nodes.fal.audio_to_video.LongcatMultiAvatarImageAudioToVideo.AudioType = Field(default=nodetool.nodes.fal.audio_to_video.LongcatMultiAvatarImageAudioToVideo.AudioType.PARA, description="How to combine the two audio tracks. 'para' (parallel) plays both simultaneously, 'add' (sequential) plays person 1 first then person 2.")
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The URL of the image containing two speakers.')
    audio_url_person1: str | OutputHandle[str] = connect_field(default='https://raw.githubusercontent.com/meituan-longcat/LongCat-Video/refs/heads/main/assets/avatar/multi/sing_man.WAV', description='The URL of the audio file for person 1 (left side).')
    seed: int | OutputHandle[int] = connect_field(default=-1, description='The seed for the random number generator.')
    audio_guidance_scale: float | OutputHandle[float] = connect_field(default=4, description='The audio guidance scale. Higher values may lead to exaggerated mouth movements.')
    bbox_person2: str | OutputHandle[str] = connect_field(default='', description='Bounding box for person 2. If not provided, defaults to right half of image.')
    num_segments: int | OutputHandle[int] = connect_field(default=1, description='Number of video segments to generate. Each segment adds ~5 seconds of video. First segment is ~5.8s, additional segments are 5s each.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_video.LongcatMultiAvatarImageAudioToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_video
from nodetool.workflows.base_node import BaseNode

class Ltx219BAudioToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        LTX-2 19B
        video, generation, audio-to-video, visualization

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.Acceleration
    CameraLora: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.CameraLora
    VideoWriteMode: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.VideoWriteMode
    VideoOutputType: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.VideoOutputType
    VideoQuality: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.VideoQuality

    match_audio_length: bool | OutputHandle[bool] = connect_field(default=True, description='When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames.')
    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate the video from.')
    acceleration: nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.Acceleration = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.Acceleration.REGULAR, description='The acceleration level to use.')
    use_multiscale: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=40, description='The number of inference steps to use.')
    fps: float | OutputHandle[float] = connect_field(default=25, description='The frames per second of the generated video.')
    camera_lora: nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.CameraLora = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.CameraLora.NONE, description='The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.')
    video_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description="The size of the generated video. Use 'auto' to match the input image dimensions if provided.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=3, description='The guidance scale to use.')
    camera_lora_scale: float | OutputHandle[float] = connect_field(default=1, description='The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.')
    image_strength: float | OutputHandle[float] = connect_field(default=1, description='The strength of the image to use for the video generation.')
    preprocess_audio: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to preprocess the audio before using it as conditioning.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.', description='The negative prompt to generate the video from.')
    video_write_mode: nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.VideoWriteMode = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.VideoWriteMode.BALANCED, description='The write mode of the generated video.')
    video_output_type: nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.VideoOutputType = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.VideoOutputType.X264__MP4, description='The output type of the generated video.')
    end_image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The URL of the image to use as the end of the video.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the safety checker.')
    num_frames: int | OutputHandle[int] = connect_field(default=121, description='The number of frames to generate.')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Optional URL of an image to use as the first frame of the video.')
    video_quality: nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.VideoQuality = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo.VideoQuality.HIGH, description='The quality of the generated video.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable prompt expansion.')
    audio_strength: float | OutputHandle[float] = connect_field(default=1, description='Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification.')
    end_image_strength: float | OutputHandle[float] = connect_field(default=1, description='The strength of the end image to use for the video generation.')
    audio_url: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(default=types.VideoRef(type='video', uri='', asset_id=None, data=None, metadata=None, duration=None, format=None), description='The URL of the audio to generate the video from.')
    seed: str | OutputHandle[str] = connect_field(default='', description='The seed for the random number generator.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_video
from nodetool.workflows.base_node import BaseNode

class Ltx219BAudioToVideoLora(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        LTX-2 19B
        video, generation, audio-to-video, visualization, lora

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.Acceleration
    CameraLora: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.CameraLora
    VideoWriteMode: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.VideoWriteMode
    VideoOutputType: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.VideoOutputType
    VideoQuality: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.VideoQuality

    match_audio_length: bool | OutputHandle[bool] = connect_field(default=True, description='When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames.')
    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate the video from.')
    acceleration: nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.Acceleration = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.Acceleration.REGULAR, description='The acceleration level to use.')
    use_multiscale: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.')
    num_inference_steps: int | OutputHandle[int] = connect_field(default=40, description='The number of inference steps to use.')
    fps: float | OutputHandle[float] = connect_field(default=25, description='The frames per second of the generated video.')
    loras: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The LoRAs to use for the generation.')
    camera_lora: nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.CameraLora = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.CameraLora.NONE, description='The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.')
    video_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description="The size of the generated video. Use 'auto' to match the input image dimensions if provided.")
    guidance_scale: float | OutputHandle[float] = connect_field(default=3, description='The guidance scale to use.')
    camera_lora_scale: float | OutputHandle[float] = connect_field(default=1, description='The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.')
    image_strength: float | OutputHandle[float] = connect_field(default=1, description='The strength of the image to use for the video generation.')
    preprocess_audio: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to preprocess the audio before using it as conditioning.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.', description='The negative prompt to generate the video from.')
    video_write_mode: nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.VideoWriteMode = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.VideoWriteMode.BALANCED, description='The write mode of the generated video.')
    video_output_type: nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.VideoOutputType = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.VideoOutputType.X264__MP4, description='The output type of the generated video.')
    end_image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The URL of the image to use as the end of the video.')
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the safety checker.')
    num_frames: int | OutputHandle[int] = connect_field(default=121, description='The number of frames to generate.')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Optional URL of an image to use as the first frame of the video.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    video_quality: nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.VideoQuality = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora.VideoQuality.HIGH, description='The quality of the generated video.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable prompt expansion.')
    audio_strength: float | OutputHandle[float] = connect_field(default=1, description='Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification.')
    end_image_strength: float | OutputHandle[float] = connect_field(default=1, description='The strength of the end image to use for the video generation.')
    audio_url: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(default=types.VideoRef(type='video', uri='', asset_id=None, data=None, metadata=None, duration=None, format=None), description='The URL of the audio to generate the video from.')
    seed: str | OutputHandle[str] = connect_field(default='', description='The seed for the random number generator.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_video.Ltx219BAudioToVideoLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_video
from nodetool.workflows.base_node import BaseNode

class Ltx219BDistilledAudioToVideo(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        LTX-2 19B Distilled
        video, generation, audio-to-video, visualization

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.Acceleration
    CameraLora: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.CameraLora
    VideoWriteMode: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.VideoWriteMode
    VideoOutputType: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.VideoOutputType
    VideoQuality: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.VideoQuality

    match_audio_length: bool | OutputHandle[bool] = connect_field(default=True, description='When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames.')
    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate the video from.')
    acceleration: nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.Acceleration = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.Acceleration.NONE, description='The acceleration level to use.')
    use_multiscale: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.')
    fps: float | OutputHandle[float] = connect_field(default=25, description='The frames per second of the generated video.')
    camera_lora: nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.CameraLora = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.CameraLora.NONE, description='The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.')
    video_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description="The size of the generated video. Use 'auto' to match the input image dimensions if provided.")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the safety checker.')
    camera_lora_scale: float | OutputHandle[float] = connect_field(default=1, description='The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.')
    image_strength: float | OutputHandle[float] = connect_field(default=1, description='The strength of the image to use for the video generation.')
    preprocess_audio: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to preprocess the audio before using it as conditioning.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.', description='The negative prompt to generate the video from.')
    video_write_mode: nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.VideoWriteMode = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.VideoWriteMode.BALANCED, description='The write mode of the generated video.')
    video_output_type: nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.VideoOutputType = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.VideoOutputType.X264__MP4, description='The output type of the generated video.')
    end_image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The URL of the image to use as the end of the video.')
    num_frames: int | OutputHandle[int] = connect_field(default=121, description='The number of frames to generate.')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Optional URL of an image to use as the first frame of the video.')
    video_quality: nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.VideoQuality = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo.VideoQuality.HIGH, description='The quality of the generated video.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable prompt expansion.')
    audio_strength: float | OutputHandle[float] = connect_field(default=1, description='Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification.')
    end_image_strength: float | OutputHandle[float] = connect_field(default=1, description='The strength of the end image to use for the video generation.')
    audio_url: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(default=types.VideoRef(type='video', uri='', asset_id=None, data=None, metadata=None, duration=None, format=None), description='The URL of the audio to generate the video from.')
    seed: str | OutputHandle[str] = connect_field(default='', description='The seed for the random number generator.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideo

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.fal.audio_to_video
from nodetool.workflows.base_node import BaseNode

class Ltx219BDistilledAudioToVideoLora(SingleOutputGraphNode[types.VideoRef], GraphNode[types.VideoRef]):
    """

        LTX-2 19B Distilled
        video, generation, audio-to-video, visualization, lora

        Use cases:
        - Automated content generation
        - Creative workflows
        - Batch processing
        - Professional applications
        - Rapid prototyping
    """

    Acceleration: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.Acceleration
    CameraLora: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.CameraLora
    VideoWriteMode: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.VideoWriteMode
    VideoOutputType: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.VideoOutputType
    VideoQuality: typing.ClassVar[type] = nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.VideoQuality

    match_audio_length: bool | OutputHandle[bool] = connect_field(default=True, description='When enabled, the number of frames will be calculated based on the audio duration and FPS. When disabled, use the specified num_frames.')
    prompt: str | OutputHandle[str] = connect_field(default='', description='The prompt to generate the video from.')
    acceleration: nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.Acceleration = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.Acceleration.NONE, description='The acceleration level to use.')
    use_multiscale: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to use multi-scale generation. If True, the model will generate the video at a smaller scale first, then use the smaller video to guide the generation of a video at or above your requested size. This results in better coherence and details.')
    fps: float | OutputHandle[float] = connect_field(default=25, description='The frames per second of the generated video.')
    loras: list[str] | OutputHandle[list[str]] = connect_field(default=[], description='The LoRAs to use for the generation.')
    camera_lora: nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.CameraLora = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.CameraLora.NONE, description='The camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.')
    video_size: str | OutputHandle[str] = connect_field(default='landscape_4_3', description="The size of the generated video. Use 'auto' to match the input image dimensions if provided.")
    enable_safety_checker: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable the safety checker.')
    camera_lora_scale: float | OutputHandle[float] = connect_field(default=1, description='The scale of the camera LoRA to use. This allows you to control the camera movement of the generated video more accurately than just prompting the model to move the camera.')
    image_strength: float | OutputHandle[float] = connect_field(default=1, description='The strength of the image to use for the video generation.')
    preprocess_audio: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to preprocess the audio before using it as conditioning.')
    negative_prompt: str | OutputHandle[str] = connect_field(default='blurry, out of focus, overexposed, underexposed, low contrast, washed out colors, excessive noise, grainy texture, poor lighting, flickering, motion blur, distorted proportions, unnatural skin tones, deformed facial features, asymmetrical face, missing facial features, extra limbs, disfigured hands, wrong hand count, artifacts around text, inconsistent perspective, camera shake, incorrect depth of field, background too sharp, background clutter, distracting reflections, harsh shadows, inconsistent lighting direction, color banding, cartoonish rendering, 3D CGI look, unrealistic materials, uncanny valley effect, incorrect ethnicity, wrong gender, exaggerated expressions, wrong gaze direction, mismatched lip sync, silent or muted audio, distorted voice, robotic voice, echo, background noise, off-sync audio,incorrect dialogue, added dialogue, repetitive speech, jittery movement, awkward pauses, incorrect timing, unnatural transitions, inconsistent framing, tilted camera, flat lighting, inconsistent tone, cinematic oversaturation, stylized filters, or AI artifacts.', description='The negative prompt to generate the video from.')
    video_write_mode: nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.VideoWriteMode = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.VideoWriteMode.BALANCED, description='The write mode of the generated video.')
    video_output_type: nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.VideoOutputType = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.VideoOutputType.X264__MP4, description='The output type of the generated video.')
    end_image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='The URL of the image to use as the end of the video.')
    num_frames: int | OutputHandle[int] = connect_field(default=121, description='The number of frames to generate.')
    image_url: types.ImageRef | OutputHandle[types.ImageRef] = connect_field(default=types.ImageRef(type='image', uri='', asset_id=None, data=None, metadata=None), description='Optional URL of an image to use as the first frame of the video.')
    sync_mode: bool | OutputHandle[bool] = connect_field(default=False, description="If `True`, the media will be returned as a data URI and the output data won't be available in the request history.")
    video_quality: nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.VideoQuality = Field(default=nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora.VideoQuality.HIGH, description='The quality of the generated video.')
    enable_prompt_expansion: bool | OutputHandle[bool] = connect_field(default=True, description='Whether to enable prompt expansion.')
    audio_strength: float | OutputHandle[float] = connect_field(default=1, description='Audio conditioning strength. Values below 1.0 will allow the model to change the audio, while a value of exactly 1.0 will use the input audio without modification.')
    end_image_strength: float | OutputHandle[float] = connect_field(default=1, description='The strength of the end image to use for the video generation.')
    audio_url: types.VideoRef | OutputHandle[types.VideoRef] = connect_field(default=types.VideoRef(type='video', uri='', asset_id=None, data=None, metadata=None, duration=None, format=None), description='The URL of the audio to generate the video from.')
    seed: str | OutputHandle[str] = connect_field(default='', description='The seed for the random number generator.')

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.fal.audio_to_video.Ltx219BDistilledAudioToVideoLora

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


